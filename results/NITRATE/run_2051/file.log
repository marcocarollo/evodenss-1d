2025-03-09 13:21:23,039 :: INFO :: __main__ :: [2051] -- Starting fresh run
2025-03-09 13:21:23,432 :: INFO :: __main__ :: [2051] -- Dataset partition sizes:
2025-03-09 13:21:23,432 :: INFO :: __main__ :: [2051] -- DatasetType.EVO_TEST size -- 125
2025-03-09 13:21:23,432 :: INFO :: __main__ :: [2051] -- DatasetType.VALIDATION size -- 125
2025-03-09 13:21:23,432 :: INFO :: __main__ :: [2051] -- DatasetType.DOWNSTREAM_TRAIN size -- 2248
2025-03-09 13:21:23,432 :: INFO :: __main__ :: [2051] -- DatasetType.TEST size -- 626
2025-03-09 13:21:23,432 :: INFO :: __main__ :: [2051] -- Starting evolution for run 2051
2025-03-09 13:21:23,433 :: INFO :: __main__ :: [2051] -- PERFORMING PREDICTION FOR THE VARIABLE: NITRATE
2025-03-09 13:21:23,433 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 0
2025-03-09 13:21:23,433 :: INFO :: evodenss.evolution.engine :: [2051] -- Creating the initial population
2025-03-09 13:21:23,445 :: INFO :: evodenss.networks.module :: [2051] -- Using ARGO grammar for features module
2025-03-09 13:21:23,448 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 13:21:23,618 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 13:21:24,616 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 13:21:24,617 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 13:21:26,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 287639.031
2025-03-09 13:21:26,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:26,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 13:21:28,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 168536.562
2025-03-09 13:21:28,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:28,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 13:21:29,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 150977.375
2025-03-09 13:21:29,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:30,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 13:21:31,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 141737.125
2025-03-09 13:21:31,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:32,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 13:21:33,632 :: INFO :: evodenss.train.trainers :: [2051] -- [1.27s] TRAIN epoch 4 -- loss: tensor([134015.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:21:33,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 134015.812
2025-03-09 13:21:33,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:34,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 13:21:35,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 126342.297
2025-03-09 13:21:35,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:36,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 13:21:37,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 121106.922
2025-03-09 13:21:37,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:37,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 13:21:39,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 118048.5
2025-03-09 13:21:39,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:39,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 13:21:40,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114725.867
2025-03-09 13:21:40,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:41,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 13:21:42,762 :: INFO :: evodenss.train.trainers :: [2051] -- [1.23s] TRAIN epoch 9 -- loss: tensor([110943.1953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:21:42,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110943.195
2025-03-09 13:21:42,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:43,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 13:21:44,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108856.727
2025-03-09 13:21:44,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:45,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 13:21:46,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106882.883
2025-03-09 13:21:46,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:47,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 13:21:48,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105298.625
2025-03-09 13:21:48,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:48,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 13:21:50,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104055.938
2025-03-09 13:21:50,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:50,766 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 13:21:52,072 :: INFO :: evodenss.train.trainers :: [2051] -- [1.3s] TRAIN epoch 14 -- loss: tensor([102412.7734], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:21:52,073 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102412.773
2025-03-09 13:21:52,073 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:52,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 13:21:53,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101181.477
2025-03-09 13:21:53,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:54,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 13:21:55,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99407.023
2025-03-09 13:21:55,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:56,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 13:21:57,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99833.453
2025-03-09 13:21:57,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:21:58,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 13:21:59,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97472.453
2025-03-09 13:21:59,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:00,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 13:22:01,397 :: INFO :: evodenss.train.trainers :: [2051] -- [1.31s] TRAIN epoch 19 -- loss: tensor([97349.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:22:01,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97349.297
2025-03-09 13:22:01,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:01,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 13:22:03,289 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96425.414
2025-03-09 13:22:03,290 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:03,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 13:22:05,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95937.758
2025-03-09 13:22:05,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:05,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 13:22:06,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94747.781
2025-03-09 13:22:06,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:07,535 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 13:22:08,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93934.078
2025-03-09 13:22:08,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:09,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 13:22:10,690 :: INFO :: evodenss.train.trainers :: [2051] -- [1.33s] TRAIN epoch 24 -- loss: tensor([93151.9219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:22:10,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93151.922
2025-03-09 13:22:10,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:11,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 13:22:12,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92093.648
2025-03-09 13:22:12,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:13,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 13:22:14,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92046.438
2025-03-09 13:22:14,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:14,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 13:22:16,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91611.57
2025-03-09 13:22:16,266 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:16,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 13:22:18,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91430.812
2025-03-09 13:22:18,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:18,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 13:22:19,987 :: INFO :: evodenss.train.trainers :: [2051] -- [1.25s] TRAIN epoch 29 -- loss: tensor([90818.0469], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:22:19,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90818.047
2025-03-09 13:22:19,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:20,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 13:22:21,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90325.008
2025-03-09 13:22:21,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:22,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 13:22:23,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89433.555
2025-03-09 13:22:23,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:24,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 13:22:25,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89620.312
2025-03-09 13:22:25,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:26,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 13:22:27,533 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88147.867
2025-03-09 13:22:27,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:28,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 13:22:29,439 :: INFO :: evodenss.train.trainers :: [2051] -- [1.33s] TRAIN epoch 34 -- loss: tensor([88360.4297], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:22:29,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88360.43
2025-03-09 13:22:29,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:30,008 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 13:22:31,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87855.758
2025-03-09 13:22:31,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:31,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 13:22:33,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86919.031
2025-03-09 13:22:33,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:33,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 13:22:35,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86650.008
2025-03-09 13:22:35,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:35,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 13:22:37,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86855.008
2025-03-09 13:22:37,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:37,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 13:22:38,907 :: INFO :: evodenss.train.trainers :: [2051] -- [1.3s] TRAIN epoch 39 -- loss: tensor([86422.7969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:22:38,907 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86422.797
2025-03-09 13:22:38,907 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:39,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 13:22:40,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86533.016
2025-03-09 13:22:40,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:41,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 13:22:42,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85842.516
2025-03-09 13:22:42,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:43,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 13:22:44,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85682.43
2025-03-09 13:22:44,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:45,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 13:22:46,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86163.93
2025-03-09 13:22:46,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:46,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 13:22:48,154 :: INFO :: evodenss.train.trainers :: [2051] -- [1.32s] TRAIN epoch 44 -- loss: tensor([84746.3828], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:22:48,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84746.383
2025-03-09 13:22:48,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:48,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 13:22:50,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84379.0
2025-03-09 13:22:50,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:50,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 13:22:51,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84582.156
2025-03-09 13:22:51,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:52,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 13:22:53,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84850.297
2025-03-09 13:22:53,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:54,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 13:22:55,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83800.945
2025-03-09 13:22:55,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:56,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 13:22:57,450 :: INFO :: evodenss.train.trainers :: [2051] -- [1.33s] TRAIN epoch 49 -- loss: tensor([83532.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:22:57,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83532.477
2025-03-09 13:22:57,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:58,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 13:22:59,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82946.961
2025-03-09 13:22:59,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:22:59,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 13:23:01,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83263.703
2025-03-09 13:23:01,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:01,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 13:23:02,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83107.633
2025-03-09 13:23:02,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:03,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 13:23:04,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82799.453
2025-03-09 13:23:04,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:05,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 13:23:06,674 :: INFO :: evodenss.train.trainers :: [2051] -- [1.3s] TRAIN epoch 54 -- loss: tensor([82600.6719], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:23:06,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82600.672
2025-03-09 13:23:06,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:07,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 13:23:08,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81841.641
2025-03-09 13:23:08,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:09,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 13:23:10,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81897.984
2025-03-09 13:23:10,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:10,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 13:23:12,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81573.82
2025-03-09 13:23:12,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:12,767 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 13:23:14,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81417.578
2025-03-09 13:23:14,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:14,628 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 13:23:15,967 :: INFO :: evodenss.train.trainers :: [2051] -- [1.34s] TRAIN epoch 59 -- loss: tensor([81421.8281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:23:15,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81421.828
2025-03-09 13:23:15,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:16,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 13:23:18,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80564.164
2025-03-09 13:23:18,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:18,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 13:23:19,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81485.562
2025-03-09 13:23:19,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:20,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 13:23:21,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80239.359
2025-03-09 13:23:21,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:22,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 13:23:23,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80964.375
2025-03-09 13:23:23,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:24,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 13:23:25,645 :: INFO :: evodenss.train.trainers :: [2051] -- [1.27s] TRAIN epoch 64 -- loss: tensor([80537.2891], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:23:25,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80537.289
2025-03-09 13:23:25,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:26,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 13:23:27,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80021.375
2025-03-09 13:23:27,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:28,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 13:23:29,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79975.086
2025-03-09 13:23:29,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:30,007 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 13:23:31,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80381.641
2025-03-09 13:23:31,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:31,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 13:23:33,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79658.039
2025-03-09 13:23:33,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:33,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 13:23:35,128 :: INFO :: evodenss.train.trainers :: [2051] -- [1.35s] TRAIN epoch 69 -- loss: tensor([79251.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:23:35,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79251.938
2025-03-09 13:23:35,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:35,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 13:23:37,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79217.984
2025-03-09 13:23:37,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:37,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 13:23:38,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79566.938
2025-03-09 13:23:38,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:39,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 13:23:40,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80169.789
2025-03-09 13:23:40,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:41,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 13:23:42,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78053.664
2025-03-09 13:23:42,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:43,187 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 13:23:44,410 :: INFO :: evodenss.train.trainers :: [2051] -- [1.22s] TRAIN epoch 74 -- loss: tensor([78021.4609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:23:44,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78021.461
2025-03-09 13:23:44,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:44,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 13:23:46,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79296.57
2025-03-09 13:23:46,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:46,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 13:23:48,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78130.0
2025-03-09 13:23:48,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:48,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 13:23:50,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77690.352
2025-03-09 13:23:50,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:50,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 13:23:52,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78209.773
2025-03-09 13:23:52,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:52,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 13:23:53,961 :: INFO :: evodenss.train.trainers :: [2051] -- [1.35s] TRAIN epoch 79 -- loss: tensor([77448.9453], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:23:53,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77448.945
2025-03-09 13:23:53,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:54,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 13:23:55,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77652.516
2025-03-09 13:23:55,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:56,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 13:23:57,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76859.305
2025-03-09 13:23:57,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:23:58,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 13:23:59,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77375.461
2025-03-09 13:23:59,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:00,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 13:24:01,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77336.242
2025-03-09 13:24:01,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:01,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 13:24:03,283 :: INFO :: evodenss.train.trainers :: [2051] -- [1.3s] TRAIN epoch 84 -- loss: tensor([77325.0781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:24:03,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77325.078
2025-03-09 13:24:03,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:03,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 13:24:05,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76823.656
2025-03-09 13:24:05,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:05,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 13:24:07,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76602.195
2025-03-09 13:24:07,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:07,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 13:24:08,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76241.109
2025-03-09 13:24:08,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:09,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 13:24:10,763 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76752.5
2025-03-09 13:24:10,763 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:11,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 13:24:12,675 :: INFO :: evodenss.train.trainers :: [2051] -- [1.33s] TRAIN epoch 89 -- loss: tensor([76455.5703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:24:12,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76455.57
2025-03-09 13:24:12,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:13,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 13:24:14,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76613.133
2025-03-09 13:24:14,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:15,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 13:24:16,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75888.734
2025-03-09 13:24:16,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:17,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 13:24:18,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75544.133
2025-03-09 13:24:18,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:18,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 13:24:20,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75843.125
2025-03-09 13:24:20,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:20,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 13:24:21,919 :: INFO :: evodenss.train.trainers :: [2051] -- [1.21s] TRAIN epoch 94 -- loss: tensor([74979.1328], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:24:21,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74979.133
2025-03-09 13:24:21,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:22,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 13:24:23,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74997.656
2025-03-09 13:24:23,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:24,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 13:24:25,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74581.633
2025-03-09 13:24:25,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:26,290 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 13:24:27,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74657.141
2025-03-09 13:24:27,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:28,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 13:24:29,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74791.734
2025-03-09 13:24:29,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:30,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 13:24:31,354 :: INFO :: evodenss.train.trainers :: [2051] -- [1.32s] TRAIN epoch 99 -- loss: tensor([74539.5234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:24:31,355 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74539.523
2025-03-09 13:24:31,355 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:32,558 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 13:24:32,559 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 0, id: 0
2025-03-09 13:24:32,559 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000]
2025-03-09 13:24:32,559 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0]
2025-03-09 13:24:32,562 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [4096.40234]
2025-03-09 13:24:32,996 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([21254.3574], device='cuda:0')
2025-03-09 13:24:32,996 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 0: 4096.40234
2025-03-09 13:24:32,996 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 4096.40234



2025-03-09 13:24:33,029 :: INFO :: __main__ :: [2051] -- Printing the best individual in the current run.

2025-03-09 13:24:33,655 :: DEBUG :: matplotlib.pyplot :: [2051] -- Loaded backend agg version v2.2.
2025-03-09 13:24:33,660 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0.
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/cmb10.ttf', name='cmb10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-BoldOblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/cmr10.ttf', name='cmr10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBolIta.ttf', name='STIXGeneral', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/cmex10.ttf', name='cmex10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFiveSymReg.ttf', name='STIXSizeFiveSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymReg.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymBol.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2025-03-09 13:24:33,661 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf', name='cmtt10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansDisplay.ttf', name='DejaVu Sans Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralItalic.ttf', name='STIXGeneral', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBolIta.ttf', name='STIXNonUnicode', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymBol.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/cmsy10.ttf', name='cmsy10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerifDisplay.ttf', name='DejaVu Serif Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymReg.ttf', name='STIXSizeThreeSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymReg.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBol.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUni.ttf', name='STIXNonUnicode', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymReg.ttf', name='STIXSizeOneSym', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneral.ttf', name='STIXGeneral', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymBol.ttf', name='STIXSizeTwoSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBol.ttf', name='STIXGeneral', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymBol.ttf', name='STIXSizeFourSym', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,662 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/cmmi10.ttf', name='cmmi10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/cmss10.ttf', name='cmss10', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/dejavu-sans-fonts/DejaVuSansCondensed.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 0.25
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/dejavu-sans-fonts/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/dejavu-sans-fonts/DejaVuSans.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 0.05
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/dejavu-sans-fonts/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-Black.otf', name='Source Code Pro', style='normal', variant='normal', weight=900, stretch='normal', size='scalable')) = 10.525
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-BoldIt.otf', name='Source Code Pro', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/cantarell/Cantarell-ExtraBold.otf', name='Cantarell', style='normal', variant='normal', weight=800, stretch='normal', size='scalable')) = 10.43
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-Medium.otf', name='Source Code Pro', style='normal', variant='normal', weight=500, stretch='normal', size='scalable')) = 10.145
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/dejavu-sans-fonts/DejaVuSansCondensed-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='condensed', size='scalable')) = 1.25
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-BlackIt.otf', name='Source Code Pro', style='italic', variant='normal', weight=900, stretch='normal', size='scalable')) = 11.525
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-It.otf', name='Source Code Pro', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-Bold.otf', name='Source Code Pro', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/dejavu-sans-fonts/DejaVuSans-ExtraLight.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=200, stretch='normal', size='scalable')) = 0.24
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-ExtraLightIt.otf', name='Source Code Pro', style='italic', variant='normal', weight=200, stretch='normal', size='scalable')) = 11.24
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-SemiboldIt.otf', name='Source Code Pro', style='italic', variant='normal', weight=600, stretch='normal', size='scalable')) = 11.24
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-ExtraLight.otf', name='Source Code Pro', style='normal', variant='normal', weight=200, stretch='normal', size='scalable')) = 10.24
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/dejavu-sans-fonts/DejaVuSansCondensed-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='condensed', size='scalable')) = 1.535
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-Regular.otf', name='Source Code Pro', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-Semibold.otf', name='Source Code Pro', style='normal', variant='normal', weight=600, stretch='normal', size='scalable')) = 10.24
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/dejavu-sans-fonts/DejaVuSansCondensed-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 0.5349999999999999
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/cantarell/Cantarell-Light.otf', name='Cantarell', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145
2025-03-09 13:24:33,663 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/cantarell/Cantarell-Thin.otf', name='Cantarell', style='normal', variant='normal', weight=100, stretch='normal', size='scalable')) = 10.335
2025-03-09 13:24:33,664 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-Light.otf', name='Source Code Pro', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145
2025-03-09 13:24:33,664 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/cantarell/Cantarell-Regular.otf', name='Cantarell', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05
2025-03-09 13:24:33,664 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-LightIt.otf', name='Source Code Pro', style='italic', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145
2025-03-09 13:24:33,664 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/cantarell/Cantarell-Bold.otf', name='Cantarell', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335
2025-03-09 13:24:33,664 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/dejavu-sans-fonts/DejaVuSans-BoldOblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=700, stretch='normal', size='scalable')) = 1.335
2025-03-09 13:24:33,664 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: score(FontEntry(fname='/usr/share/fonts/adobe-source-code-pro/SourceCodePro-MediumIt.otf', name='Source Code Pro', style='italic', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145
2025-03-09 13:24:33,664 :: DEBUG :: matplotlib.font_manager :: [2051] -- findfont: Matching sans\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/u/mcarollo/.conda/envs/argo/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.
2025-03-09 13:24:44,706 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 1
2025-03-09 13:24:44,706 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 13:24:44,717 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have an extra layer at Module 0: features; Position: 13. Reused?: False
2025-03-09 13:24:44,718 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 13:24:44,719 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 13:24:44,719 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 13:24:44,720 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 13:24:44,722 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a layer removed from Module 0: features; Position: 12
2025-03-09 13:24:44,723 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 13:24:44,723 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 13:24:44,724 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 13:24:44,725 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 13:24:44,728 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have an extra layer at Module 0: features; Position: 5. Reused?: False
2025-03-09 13:24:44,728 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 13:24:44,729 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 13:24:44,730 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 13:24:44,731 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 13:24:44,731 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 13:24:44,732 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 13:24:44,732 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 13:24:44,733 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 13:24:44,735 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 13:24:44,739 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 13:24:44,741 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 13:24:44,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 13:24:44,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 13:24:46,120 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 389794.031
2025-03-09 13:24:46,120 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:46,734 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 13:24:48,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 329102.5
2025-03-09 13:24:48,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:48,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 13:24:50,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 220451.719
2025-03-09 13:24:50,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:50,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 13:24:52,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 199158.984
2025-03-09 13:24:52,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:52,715 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 13:24:54,102 :: INFO :: evodenss.train.trainers :: [2051] -- [1.38s] TRAIN epoch 4 -- loss: tensor([170482.2344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:24:54,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 170482.234
2025-03-09 13:24:54,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:54,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 13:24:56,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 136409.781
2025-03-09 13:24:56,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:56,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 13:24:58,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 126044.57
2025-03-09 13:24:58,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:24:58,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 13:25:00,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 119722.82
2025-03-09 13:25:00,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:00,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 13:25:02,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 115379.195
2025-03-09 13:25:02,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:02,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 13:25:04,062 :: INFO :: evodenss.train.trainers :: [2051] -- [1.37s] TRAIN epoch 9 -- loss: tensor([112404.8438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:25:04,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112404.844
2025-03-09 13:25:04,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:04,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 13:25:06,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108805.68
2025-03-09 13:25:06,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:06,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 13:25:08,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107103.883
2025-03-09 13:25:08,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:08,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 13:25:09,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104804.406
2025-03-09 13:25:09,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:10,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 13:25:11,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103109.414
2025-03-09 13:25:11,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:12,592 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 13:25:13,947 :: INFO :: evodenss.train.trainers :: [2051] -- [1.35s] TRAIN epoch 14 -- loss: tensor([101846.6797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:25:13,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101846.68
2025-03-09 13:25:13,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:14,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 13:25:15,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98459.523
2025-03-09 13:25:15,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:16,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 13:25:18,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98997.766
2025-03-09 13:25:18,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:18,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 13:25:20,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97583.07
2025-03-09 13:25:20,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:20,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 13:25:22,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97036.688
2025-03-09 13:25:22,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:22,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 13:25:24,038 :: INFO :: evodenss.train.trainers :: [2051] -- [1.38s] TRAIN epoch 19 -- loss: tensor([96025.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:25:24,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96025.5
2025-03-09 13:25:24,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:24,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 13:25:26,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94460.461
2025-03-09 13:25:26,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:26,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 13:25:28,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94031.625
2025-03-09 13:25:28,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:28,638 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 13:25:29,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93807.961
2025-03-09 13:25:29,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:30,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 13:25:31,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92337.719
2025-03-09 13:25:31,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:32,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 13:25:33,800 :: INFO :: evodenss.train.trainers :: [2051] -- [1.34s] TRAIN epoch 24 -- loss: tensor([91446.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:25:33,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91446.297
2025-03-09 13:25:33,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:34,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 13:25:35,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91222.945
2025-03-09 13:25:35,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:36,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 13:25:37,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90337.961
2025-03-09 13:25:37,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:38,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 13:25:39,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89021.727
2025-03-09 13:25:39,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:40,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 13:25:41,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90016.172
2025-03-09 13:25:41,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:42,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 13:25:43,296 :: INFO :: evodenss.train.trainers :: [2051] -- [1.28s] TRAIN epoch 29 -- loss: tensor([88960.4609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:25:43,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88960.461
2025-03-09 13:25:43,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:43,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 13:25:45,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88458.836
2025-03-09 13:25:45,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:45,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 13:25:47,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87783.742
2025-03-09 13:25:47,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:48,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 13:25:49,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87332.977
2025-03-09 13:25:49,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:50,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 13:25:51,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87753.695
2025-03-09 13:25:51,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:52,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 13:25:53,355 :: INFO :: evodenss.train.trainers :: [2051] -- [1.29s] TRAIN epoch 34 -- loss: tensor([86134.7578], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:25:53,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86134.758
2025-03-09 13:25:53,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:53,951 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 13:25:55,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87546.297
2025-03-09 13:25:55,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:55,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 13:25:57,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85287.992
2025-03-09 13:25:57,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:57,925 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 13:25:59,290 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86370.641
2025-03-09 13:25:59,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:25:59,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 13:26:01,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85141.547
2025-03-09 13:26:01,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:01,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 13:26:03,284 :: INFO :: evodenss.train.trainers :: [2051] -- [1.39s] TRAIN epoch 39 -- loss: tensor([85368.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:26:03,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85368.0
2025-03-09 13:26:03,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:03,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 13:26:05,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85030.492
2025-03-09 13:26:05,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:05,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 13:26:07,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84478.844
2025-03-09 13:26:07,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:07,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 13:26:09,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84631.07
2025-03-09 13:26:09,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:09,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 13:26:11,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83890.016
2025-03-09 13:26:11,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:11,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 13:26:13,122 :: INFO :: evodenss.train.trainers :: [2051] -- [1.31s] TRAIN epoch 44 -- loss: tensor([84350.7344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:26:13,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84350.734
2025-03-09 13:26:13,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:13,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 13:26:15,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83190.102
2025-03-09 13:26:15,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:15,720 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 13:26:17,054 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82466.57
2025-03-09 13:26:17,054 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:17,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 13:26:19,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82949.516
2025-03-09 13:26:19,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:19,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 13:26:21,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83091.883
2025-03-09 13:26:21,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:21,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 13:26:22,992 :: INFO :: evodenss.train.trainers :: [2051] -- [1.37s] TRAIN epoch 49 -- loss: tensor([82482.6562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:26:22,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82482.656
2025-03-09 13:26:22,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:23,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 13:26:24,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81114.172
2025-03-09 13:26:24,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:25,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 13:26:26,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82201.75
2025-03-09 13:26:26,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:27,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 13:26:28,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81467.977
2025-03-09 13:26:28,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:29,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 13:26:30,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81533.078
2025-03-09 13:26:30,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:31,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 13:26:32,824 :: INFO :: evodenss.train.trainers :: [2051] -- [1.36s] TRAIN epoch 54 -- loss: tensor([81537.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:26:32,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81537.438
2025-03-09 13:26:32,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:33,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 13:26:34,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82617.469
2025-03-09 13:26:34,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:35,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 13:26:36,797 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80544.781
2025-03-09 13:26:36,798 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:37,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 13:26:38,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80406.094
2025-03-09 13:26:38,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:39,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 13:26:40,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81167.094
2025-03-09 13:26:40,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:41,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 13:26:42,722 :: INFO :: evodenss.train.trainers :: [2051] -- [1.35s] TRAIN epoch 59 -- loss: tensor([80777.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:26:42,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80777.938
2025-03-09 13:26:42,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:43,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 13:26:44,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79963.5
2025-03-09 13:26:44,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:45,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 13:26:46,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81306.172
2025-03-09 13:26:46,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:47,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 13:26:48,594 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80293.984
2025-03-09 13:26:48,594 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:49,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 13:26:50,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79632.68
2025-03-09 13:26:50,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:51,212 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 13:26:52,587 :: INFO :: evodenss.train.trainers :: [2051] -- [1.37s] TRAIN epoch 64 -- loss: tensor([79802.7188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:26:52,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79802.719
2025-03-09 13:26:52,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:53,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 13:26:54,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79208.961
2025-03-09 13:26:54,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:55,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 13:26:56,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79827.383
2025-03-09 13:26:56,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:57,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 13:26:58,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79557.922
2025-03-09 13:26:58,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:26:59,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 13:27:00,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79762.0
2025-03-09 13:27:00,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:01,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 13:27:02,952 :: INFO :: evodenss.train.trainers :: [2051] -- [1.38s] TRAIN epoch 69 -- loss: tensor([78495.7344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:27:02,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78495.734
2025-03-09 13:27:02,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:03,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 13:27:04,957 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79272.211
2025-03-09 13:27:04,957 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:05,592 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 13:27:06,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78991.961
2025-03-09 13:27:06,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:07,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 13:27:08,916 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79761.203
2025-03-09 13:27:08,916 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:09,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 13:27:10,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78047.945
2025-03-09 13:27:10,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:11,539 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 13:27:12,815 :: INFO :: evodenss.train.trainers :: [2051] -- [1.27s] TRAIN epoch 74 -- loss: tensor([77408.2031], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:27:12,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77408.203
2025-03-09 13:27:12,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:13,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 13:27:14,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78783.562
2025-03-09 13:27:14,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:15,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 13:27:16,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77903.102
2025-03-09 13:27:16,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:17,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 13:27:18,700 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78172.109
2025-03-09 13:27:18,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:19,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 13:27:20,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78129.344
2025-03-09 13:27:20,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:21,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 13:27:22,558 :: INFO :: evodenss.train.trainers :: [2051] -- [1.36s] TRAIN epoch 79 -- loss: tensor([77882.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:27:22,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77882.297
2025-03-09 13:27:22,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:23,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 13:27:24,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78506.969
2025-03-09 13:27:24,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:25,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 13:27:26,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77873.43
2025-03-09 13:27:26,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:27,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 13:27:28,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76782.453
2025-03-09 13:27:28,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:29,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 13:27:30,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76602.375
2025-03-09 13:27:30,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:31,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 13:27:32,487 :: INFO :: evodenss.train.trainers :: [2051] -- [1.36s] TRAIN epoch 84 -- loss: tensor([77620.2266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:27:32,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77620.227
2025-03-09 13:27:32,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:33,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 13:27:34,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77758.039
2025-03-09 13:27:34,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:35,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 13:27:36,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76694.891
2025-03-09 13:27:36,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:27:37,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 13:27:38,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77516.062
2025-03-09 13:27:38,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:28:25,146 :: INFO :: __main__ :: [2051] -- Loading previous checkpoint
2025-03-09 13:28:25,542 :: INFO :: __main__ :: [2051] -- Dataset partition sizes:
2025-03-09 13:28:25,542 :: INFO :: __main__ :: [2051] -- DatasetType.EVO_TEST size -- 125
2025-03-09 13:28:25,542 :: INFO :: __main__ :: [2051] -- DatasetType.VALIDATION size -- 125
2025-03-09 13:28:25,542 :: INFO :: __main__ :: [2051] -- DatasetType.DOWNSTREAM_TRAIN size -- 2248
2025-03-09 13:28:25,542 :: INFO :: __main__ :: [2051] -- DatasetType.TEST size -- 626
2025-03-09 13:28:25,542 :: INFO :: __main__ :: [2051] -- Starting evolution for run 2051
2025-03-09 13:28:25,543 :: INFO :: __main__ :: [2051] -- PERFORMING PREDICTION FOR THE VARIABLE: NITRATE
2025-03-09 13:28:25,543 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 1
2025-03-09 13:28:25,544 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 13:28:25,554 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have an extra layer at Module 0: features; Position: 13. Reused?: False
2025-03-09 13:28:25,555 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 13:28:25,555 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 13:28:25,556 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 13:28:25,556 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 13:28:25,559 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a layer removed from Module 0: features; Position: 12
2025-03-09 13:28:25,559 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 13:28:25,560 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 13:28:25,561 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 13:28:25,561 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 13:28:25,564 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 13:28:25,565 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 13:28:25,566 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 13:28:25,567 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 13:28:25,567 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 13:28:25,568 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 13:28:25,570 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 13:28:25,574 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 13:28:25,575 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 13:28:26,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 13:28:26,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 13:28:33,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 393296.0
2025-03-09 13:28:33,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:28:33,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 13:28:35,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 302335.062
2025-03-09 13:28:35,734 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:28:36,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 13:28:38,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 215387.547
2025-03-09 13:28:38,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:28:38,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 13:28:40,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 192545.438
2025-03-09 13:28:40,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:28:41,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 13:28:43,399 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 4 -- loss: tensor([172955.9688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:28:43,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 172955.969
2025-03-09 13:28:43,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:28:44,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 13:28:45,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 142480.812
2025-03-09 13:28:45,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:28:46,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 13:28:48,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 127072.391
2025-03-09 13:28:48,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:28:49,387 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 13:28:51,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 118982.234
2025-03-09 13:28:51,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:28:51,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 13:28:53,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 115750.664
2025-03-09 13:28:53,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:28:54,573 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 13:28:56,400 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 9 -- loss: tensor([110730.7969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:28:56,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110730.797
2025-03-09 13:28:56,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:28:57,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 13:28:58,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108592.398
2025-03-09 13:28:58,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:28:59,734 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 13:29:01,573 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105672.586
2025-03-09 13:29:01,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:02,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 13:29:04,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103304.367
2025-03-09 13:29:04,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:04,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 13:29:06,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101510.453
2025-03-09 13:29:06,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:07,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 13:29:09,328 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 14 -- loss: tensor([100513.1484], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:29:09,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100513.148
2025-03-09 13:29:09,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:10,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 13:29:11,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98552.406
2025-03-09 13:29:11,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:12,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 13:29:14,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96996.547
2025-03-09 13:29:14,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:15,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 13:29:17,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96976.062
2025-03-09 13:29:17,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:17,897 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 13:29:19,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95476.828
2025-03-09 13:29:19,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:20,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 13:29:22,239 :: INFO :: evodenss.train.trainers :: [2051] -- [1.75s] TRAIN epoch 19 -- loss: tensor([94791.5781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:29:22,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94791.578
2025-03-09 13:29:22,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:23,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 13:29:24,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93367.492
2025-03-09 13:29:24,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:25,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 13:29:27,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92978.57
2025-03-09 13:29:27,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:28,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 13:29:30,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92437.555
2025-03-09 13:29:30,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:30,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 13:29:32,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91827.297
2025-03-09 13:29:32,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:33,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 13:29:35,214 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 24 -- loss: tensor([90515.9453], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:29:35,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90515.945
2025-03-09 13:29:35,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:35,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 13:29:37,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89318.281
2025-03-09 13:29:37,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:38,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 13:29:40,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89209.094
2025-03-09 13:29:40,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:41,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 13:29:43,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89617.875
2025-03-09 13:29:43,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:43,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 13:29:45,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87341.914
2025-03-09 13:29:45,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:46,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 13:29:48,263 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 29 -- loss: tensor([87778.1719], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:29:48,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87778.172
2025-03-09 13:29:48,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:49,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 13:29:50,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87569.82
2025-03-09 13:29:50,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:51,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 13:29:53,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87660.945
2025-03-09 13:29:53,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:54,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 13:29:56,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86263.133
2025-03-09 13:29:56,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:56,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 13:29:58,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86425.773
2025-03-09 13:29:58,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:29:59,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 13:30:01,248 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 34 -- loss: tensor([86058.1172], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:30:01,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86058.117
2025-03-09 13:30:01,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:02,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 13:30:03,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84665.555
2025-03-09 13:30:03,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:04,637 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 13:30:06,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85912.031
2025-03-09 13:30:06,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:07,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 13:30:09,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84589.344
2025-03-09 13:30:09,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:09,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 13:30:11,576 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84455.469
2025-03-09 13:30:11,576 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:12,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 13:30:14,107 :: INFO :: evodenss.train.trainers :: [2051] -- [1.79s] TRAIN epoch 39 -- loss: tensor([83797.3438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:30:14,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83797.344
2025-03-09 13:30:14,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:14,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 13:30:16,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83398.047
2025-03-09 13:30:16,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:17,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 13:30:19,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84462.508
2025-03-09 13:30:19,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:20,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 13:30:21,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83559.258
2025-03-09 13:30:21,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:22,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 13:30:24,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83303.82
2025-03-09 13:30:24,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:25,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 13:30:27,062 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 44 -- loss: tensor([83236.8828], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:30:27,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83236.883
2025-03-09 13:30:27,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:27,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 13:30:29,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82569.812
2025-03-09 13:30:29,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:30,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 13:30:32,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81232.219
2025-03-09 13:30:32,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:33,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 13:30:34,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82871.688
2025-03-09 13:30:34,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:35,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 13:30:37,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82479.117
2025-03-09 13:30:37,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:38,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 13:30:40,134 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 49 -- loss: tensor([81498.7266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:30:40,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81498.727
2025-03-09 13:30:40,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:40,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 13:30:42,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82121.336
2025-03-09 13:30:42,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:43,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 13:30:45,289 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81719.008
2025-03-09 13:30:45,290 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:46,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 13:30:47,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81092.625
2025-03-09 13:30:47,929 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:48,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 13:30:51,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81654.945
2025-03-09 13:30:51,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:52,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 13:30:54,475 :: INFO :: evodenss.train.trainers :: [2051] -- [1.74s] TRAIN epoch 54 -- loss: tensor([80699.0391], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:30:54,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80699.039
2025-03-09 13:30:54,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:55,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 13:30:56,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80501.789
2025-03-09 13:30:56,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:30:57,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 13:30:59,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81334.008
2025-03-09 13:30:59,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:00,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 13:31:02,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79542.195
2025-03-09 13:31:02,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:03,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 13:31:04,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79760.508
2025-03-09 13:31:04,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:05,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 13:31:07,402 :: INFO :: evodenss.train.trainers :: [2051] -- [1.78s] TRAIN epoch 59 -- loss: tensor([79311.8438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:31:07,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79311.844
2025-03-09 13:31:07,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:08,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 13:31:10,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79396.562
2025-03-09 13:31:10,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:10,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 13:31:12,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79841.266
2025-03-09 13:31:12,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:13,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 13:31:15,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79710.562
2025-03-09 13:31:15,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:15,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 13:31:17,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79641.023
2025-03-09 13:31:17,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:18,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 13:31:20,487 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 64 -- loss: tensor([78542.3594], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:31:20,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78542.359
2025-03-09 13:31:20,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:21,279 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 13:31:23,055 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78893.156
2025-03-09 13:31:23,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:23,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 13:31:25,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79115.469
2025-03-09 13:31:25,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:26,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 13:31:28,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78233.5
2025-03-09 13:31:28,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:29,120 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 13:31:30,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78123.008
2025-03-09 13:31:30,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:31,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 13:31:33,577 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 69 -- loss: tensor([78187.5234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:31:33,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78187.523
2025-03-09 13:31:33,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:34,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 13:31:36,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78159.883
2025-03-09 13:31:36,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:36,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 13:31:38,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77797.555
2025-03-09 13:31:38,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:39,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 13:31:41,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78077.594
2025-03-09 13:31:41,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:42,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 13:31:43,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76706.562
2025-03-09 13:31:43,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:44,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 13:31:46,560 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 74 -- loss: tensor([77551.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:31:46,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77551.625
2025-03-09 13:31:46,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:47,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 13:31:49,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77609.75
2025-03-09 13:31:49,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:49,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 13:31:51,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77518.711
2025-03-09 13:31:51,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:52,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 13:31:54,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77477.344
2025-03-09 13:31:54,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:55,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 13:31:56,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77725.156
2025-03-09 13:31:56,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:31:57,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 13:31:59,553 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 79 -- loss: tensor([76763.2188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:31:59,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76763.219
2025-03-09 13:31:59,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:00,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 13:32:02,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76541.031
2025-03-09 13:32:02,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:02,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 13:32:04,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76574.07
2025-03-09 13:32:04,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:05,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 13:32:07,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76838.664
2025-03-09 13:32:07,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:08,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 13:32:10,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75721.562
2025-03-09 13:32:10,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:10,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 13:32:12,607 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 84 -- loss: tensor([76234.2656], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:32:12,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76234.266
2025-03-09 13:32:12,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:13,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 13:32:15,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76450.266
2025-03-09 13:32:15,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:15,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 13:32:17,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75971.609
2025-03-09 13:32:17,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:18,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 13:32:20,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76090.805
2025-03-09 13:32:20,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:21,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 13:32:23,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75939.328
2025-03-09 13:32:23,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:23,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 13:32:25,560 :: INFO :: evodenss.train.trainers :: [2051] -- [1.77s] TRAIN epoch 89 -- loss: tensor([75283.5547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:32:25,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75283.555
2025-03-09 13:32:25,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:26,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 13:32:28,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74378.883
2025-03-09 13:32:28,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:28,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 13:32:30,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75034.539
2025-03-09 13:32:30,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:31,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 13:32:33,234 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75110.414
2025-03-09 13:32:33,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:33,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 13:32:35,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75685.195
2025-03-09 13:32:35,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:36,589 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 13:32:38,446 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 94 -- loss: tensor([75103.7109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:32:38,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75103.711
2025-03-09 13:32:38,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:39,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 13:32:41,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74563.273
2025-03-09 13:32:41,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:41,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 13:32:43,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74614.867
2025-03-09 13:32:43,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:44,393 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 13:32:46,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74439.914
2025-03-09 13:32:46,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:46,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 13:32:48,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74641.586
2025-03-09 13:32:48,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:49,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 13:32:51,459 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 99 -- loss: tensor([74054.9453], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:32:51,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74054.945
2025-03-09 13:32:51,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:53,054 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 4112.39697
2025-03-09 13:32:53,059 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 13:32:53,060 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 
layer13: :conv1d out_channels:66 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 13:32:53,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 13:32:53,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 13:32:54,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 844698.375
2025-03-09 13:32:54,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:55,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 13:32:57,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 823764.562
2025-03-09 13:32:57,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:32:58,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 13:33:00,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 832997.312
2025-03-09 13:33:00,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:01,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 13:33:03,025 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 823987.5
2025-03-09 13:33:03,025 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:03,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 13:33:05,693 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 4 -- loss: tensor([810763.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:33:05,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 810763.875
2025-03-09 13:33:05,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:06,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 13:33:08,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811168.875
2025-03-09 13:33:08,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:09,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 13:33:11,079 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 860282.312
2025-03-09 13:33:11,079 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:11,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 13:33:13,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 822192.812
2025-03-09 13:33:13,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:14,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 13:33:16,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 828456.812
2025-03-09 13:33:16,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:17,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 13:33:19,168 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 9 -- loss: tensor([657801.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:33:19,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 657801.562
2025-03-09 13:33:19,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:19,976 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 13:33:21,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 315552.219
2025-03-09 13:33:21,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:22,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 13:33:24,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 172199.766
2025-03-09 13:33:24,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:25,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 13:33:27,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 146199.969
2025-03-09 13:33:27,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:28,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 13:33:29,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 136313.359
2025-03-09 13:33:29,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:30,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 13:33:32,649 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 14 -- loss: tensor([129388.6562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:33:32,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 129388.656
2025-03-09 13:33:32,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:33,465 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 13:33:35,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 123964.062
2025-03-09 13:33:35,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:36,191 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 13:33:38,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 121628.398
2025-03-09 13:33:38,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:38,957 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 13:33:40,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 118373.656
2025-03-09 13:33:40,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:41,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 13:33:43,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 115554.617
2025-03-09 13:33:43,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:44,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 13:33:46,252 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 19 -- loss: tensor([113691.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:33:46,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 113691.5
2025-03-09 13:33:46,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:47,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 13:33:48,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112865.523
2025-03-09 13:33:48,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:49,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 13:33:51,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110146.125
2025-03-09 13:33:51,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:52,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 13:33:54,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108085.773
2025-03-09 13:33:54,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:55,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 13:33:57,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108433.984
2025-03-09 13:33:57,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:33:57,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 13:33:59,853 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 24 -- loss: tensor([108790.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:33:59,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108790.625
2025-03-09 13:33:59,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:00,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 13:34:02,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106069.531
2025-03-09 13:34:02,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:03,393 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 13:34:05,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105520.516
2025-03-09 13:34:05,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:06,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 13:34:08,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103312.227
2025-03-09 13:34:08,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:08,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 13:34:10,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104206.922
2025-03-09 13:34:10,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:11,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 13:34:13,588 :: INFO :: evodenss.train.trainers :: [2051] -- [1.98s] TRAIN epoch 29 -- loss: tensor([103383.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:34:13,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103383.5
2025-03-09 13:34:13,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:14,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 13:34:16,339 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102283.766
2025-03-09 13:34:16,339 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:17,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 13:34:19,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103258.195
2025-03-09 13:34:19,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:19,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 13:34:21,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100276.789
2025-03-09 13:34:21,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:22,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 13:34:24,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102314.609
2025-03-09 13:34:24,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:25,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 13:34:27,275 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 34 -- loss: tensor([99707.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:34:27,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99707.875
2025-03-09 13:34:27,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:28,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 13:34:29,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99680.078
2025-03-09 13:34:29,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:30,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 13:34:32,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100534.383
2025-03-09 13:34:32,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:33,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 13:34:35,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100125.391
2025-03-09 13:34:35,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:36,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 13:34:38,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98356.32
2025-03-09 13:34:38,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:38,994 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 13:34:40,909 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 39 -- loss: tensor([97952.2188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:34:40,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97952.219
2025-03-09 13:34:40,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:41,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 13:34:43,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98151.492
2025-03-09 13:34:43,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:44,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 13:34:46,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97936.016
2025-03-09 13:34:46,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:47,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 13:34:49,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96869.016
2025-03-09 13:34:49,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:49,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 13:34:51,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96821.336
2025-03-09 13:34:51,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:52,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 13:34:54,624 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 44 -- loss: tensor([96425.8672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:34:54,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96425.867
2025-03-09 13:34:54,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:55,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 13:34:57,361 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96341.492
2025-03-09 13:34:57,361 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:34:58,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 13:35:00,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97017.453
2025-03-09 13:35:00,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:00,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 13:35:02,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95479.406
2025-03-09 13:35:02,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:03,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 13:35:05,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95543.156
2025-03-09 13:35:05,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:06,456 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 13:35:08,359 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 49 -- loss: tensor([95614.5859], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:35:08,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95614.586
2025-03-09 13:35:08,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:09,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 13:35:11,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94609.266
2025-03-09 13:35:11,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:11,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 13:35:13,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92755.039
2025-03-09 13:35:13,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:14,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 13:35:16,662 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94481.75
2025-03-09 13:35:16,662 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:17,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 13:35:19,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96174.516
2025-03-09 13:35:19,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:20,309 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 13:35:22,227 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 54 -- loss: tensor([94038.1797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:35:22,227 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94038.18
2025-03-09 13:35:22,227 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:23,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 13:35:24,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94110.57
2025-03-09 13:35:24,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:25,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 13:35:27,720 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93294.398
2025-03-09 13:35:27,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:28,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 13:35:30,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93487.945
2025-03-09 13:35:30,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:31,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 13:35:33,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92468.086
2025-03-09 13:35:33,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:34,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 13:35:36,027 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 59 -- loss: tensor([93687.6016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:35:36,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93687.602
2025-03-09 13:35:36,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:36,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 13:35:38,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92619.984
2025-03-09 13:35:38,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:39,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 13:35:41,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92356.125
2025-03-09 13:35:41,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:42,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 13:35:44,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93881.25
2025-03-09 13:35:44,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:45,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 13:35:47,079 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92109.711
2025-03-09 13:35:47,079 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:47,925 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 13:35:49,902 :: INFO :: evodenss.train.trainers :: [2051] -- [1.98s] TRAIN epoch 64 -- loss: tensor([93335.0391], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:35:49,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93335.039
2025-03-09 13:35:49,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:50,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 13:35:52,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92256.258
2025-03-09 13:35:52,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:53,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 13:35:55,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91795.023
2025-03-09 13:35:55,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:56,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 13:35:58,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91663.023
2025-03-09 13:35:58,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:35:59,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 13:36:00,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90759.32
2025-03-09 13:36:00,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:01,797 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 13:36:03,703 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 69 -- loss: tensor([90372.3672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:36:03,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90372.367
2025-03-09 13:36:03,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:04,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 13:36:06,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91208.031
2025-03-09 13:36:06,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:07,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 13:36:09,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90015.781
2025-03-09 13:36:09,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:10,058 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 13:36:11,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90410.242
2025-03-09 13:36:11,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:12,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 13:36:14,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92026.266
2025-03-09 13:36:14,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:15,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 13:36:17,495 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 74 -- loss: tensor([91234.8672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:36:17,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91234.867
2025-03-09 13:36:17,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:18,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 13:36:20,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89847.648
2025-03-09 13:36:20,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:21,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 13:36:23,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91773.82
2025-03-09 13:36:23,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:23,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 13:36:25,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90679.297
2025-03-09 13:36:25,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:26,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 13:36:28,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89555.188
2025-03-09 13:36:28,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:29,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 13:36:31,183 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 79 -- loss: tensor([90244.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:36:31,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90244.938
2025-03-09 13:36:31,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:31,979 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 13:36:33,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89813.125
2025-03-09 13:36:33,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:34,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 13:36:36,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89623.234
2025-03-09 13:36:36,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:37,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 13:36:39,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90723.68
2025-03-09 13:36:39,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:39,957 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 13:36:41,870 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89293.344
2025-03-09 13:36:41,870 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:42,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 13:36:44,530 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 84 -- loss: tensor([89212.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:36:44,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89212.688
2025-03-09 13:36:44,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:45,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 13:36:47,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89411.578
2025-03-09 13:36:47,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:48,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 13:36:49,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88279.641
2025-03-09 13:36:49,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:50,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 13:36:52,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88520.742
2025-03-09 13:36:52,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:53,466 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 13:36:55,355 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89577.0
2025-03-09 13:36:55,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:56,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 13:36:58,049 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 89 -- loss: tensor([88283.5859], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:36:58,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88283.586
2025-03-09 13:36:58,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:36:58,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 13:37:00,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88340.18
2025-03-09 13:37:00,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:01,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 13:37:03,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87585.664
2025-03-09 13:37:03,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:04,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 13:37:06,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90269.789
2025-03-09 13:37:06,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:06,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 13:37:08,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87938.641
2025-03-09 13:37:08,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:09,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 13:37:11,553 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 94 -- loss: tensor([88671.0078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:37:11,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88671.008
2025-03-09 13:37:11,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:12,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 13:37:14,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87327.555
2025-03-09 13:37:14,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:15,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 13:37:16,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88181.062
2025-03-09 13:37:16,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:17,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 13:37:19,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88096.188
2025-03-09 13:37:19,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:20,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 13:37:22,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86508.195
2025-03-09 13:37:22,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:23,303 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 13:37:25,170 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 99 -- loss: tensor([86226.3828], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:37:25,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86226.383
2025-03-09 13:37:25,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:26,902 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 4634.74902
2025-03-09 13:37:26,906 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 13:37:26,907 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :deconv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :fc act:selu out_features:200 bias:True input:11 learning:rmsprop lr:0.13436482909236644 alpha:0.9021447769998236 weight_decay:2.3365448853196182e-05 batch_size:32 epochs:100
2025-03-09 13:37:26,916 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 13:37:26,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 13:37:28,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 953235.938
2025-03-09 13:37:28,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:29,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 13:37:31,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:37:31,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:32,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 13:37:34,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 885611.875
2025-03-09 13:37:34,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:34,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 13:37:36,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 6743451.5
2025-03-09 13:37:36,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:37,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 13:37:39,294 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 4 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:37:39,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:37:39,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:40,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 13:37:41,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 807219.062
2025-03-09 13:37:41,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:42,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 13:37:44,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1336419.625
2025-03-09 13:37:44,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:45,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 13:37:47,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:37:47,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:47,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 13:37:49,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:37:49,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:50,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 13:37:52,501 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 9 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:37:52,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:37:52,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:53,303 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 13:37:55,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 4073124.75
2025-03-09 13:37:55,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:55,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 13:37:57,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:37:57,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:37:58,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 13:38:00,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 965412.375
2025-03-09 13:38:00,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:00,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 13:38:02,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2201861.25
2025-03-09 13:38:02,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:03,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 13:38:05,287 :: INFO :: evodenss.train.trainers :: [2051] -- [1.77s] TRAIN epoch 14 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:38:05,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:38:05,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:06,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 13:38:07,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3693987.5
2025-03-09 13:38:07,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:08,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 13:38:10,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1076695.375
2025-03-09 13:38:10,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:11,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 13:38:13,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 836091.938
2025-03-09 13:38:13,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:13,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 13:38:15,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1769810.5
2025-03-09 13:38:15,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:16,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 13:38:18,227 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 19 -- loss: tensor([1019358.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:38:18,227 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1019358.312
2025-03-09 13:38:18,227 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:19,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 13:38:20,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 809618.312
2025-03-09 13:38:20,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:21,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 13:38:23,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1057908.375
2025-03-09 13:38:23,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:24,289 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 13:38:26,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 858399.125
2025-03-09 13:38:26,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:26,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 13:38:28,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 831588.062
2025-03-09 13:38:28,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:29,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 13:38:31,245 :: INFO :: evodenss.train.trainers :: [2051] -- [1.77s] TRAIN epoch 24 -- loss: tensor([855648.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:38:31,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 855648.188
2025-03-09 13:38:31,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:32,055 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 13:38:33,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1110336.875
2025-03-09 13:38:33,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:34,643 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 13:38:36,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 913153.25
2025-03-09 13:38:36,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:37,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 13:38:39,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 830682.25
2025-03-09 13:38:39,060 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:39,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 13:38:41,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 828979.812
2025-03-09 13:38:41,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:42,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 13:38:44,216 :: INFO :: evodenss.train.trainers :: [2051] -- [1.78s] TRAIN epoch 29 -- loss: tensor([855445.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:38:44,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 855445.375
2025-03-09 13:38:44,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:45,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 13:38:46,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 930664.875
2025-03-09 13:38:46,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:47,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 13:38:49,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 832123.5
2025-03-09 13:38:49,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:50,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 13:38:52,032 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 829247.625
2025-03-09 13:38:52,032 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:52,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 13:38:54,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 890129.562
2025-03-09 13:38:54,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:55,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 13:38:57,157 :: INFO :: evodenss.train.trainers :: [2051] -- [1.77s] TRAIN epoch 34 -- loss: tensor([1000943.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:38:57,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1000943.25
2025-03-09 13:38:57,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:38:57,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 13:38:59,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 822262.25
2025-03-09 13:38:59,763 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:00,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 13:39:02,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 840252.688
2025-03-09 13:39:02,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:03,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 13:39:04,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 926956.938
2025-03-09 13:39:04,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:05,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 13:39:07,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 957314.25
2025-03-09 13:39:07,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:08,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 13:39:10,112 :: INFO :: evodenss.train.trainers :: [2051] -- [1.79s] TRAIN epoch 39 -- loss: tensor([840950.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:39:10,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 840950.688
2025-03-09 13:39:10,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:10,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 13:39:12,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 843315.0
2025-03-09 13:39:12,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:13,511 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 13:39:15,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 873626.062
2025-03-09 13:39:15,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:16,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 13:39:17,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 945768.062
2025-03-09 13:39:17,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:18,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 13:39:20,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 898875.438
2025-03-09 13:39:20,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:21,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 13:39:23,097 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 44 -- loss: tensor([810551.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:39:23,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 810551.688
2025-03-09 13:39:23,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:23,897 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 13:39:25,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 881423.812
2025-03-09 13:39:25,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:26,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 13:39:28,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 928694.875
2025-03-09 13:39:28,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:29,050 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 13:39:30,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 879603.875
2025-03-09 13:39:30,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:31,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 13:39:33,462 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 826769.562
2025-03-09 13:39:33,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:34,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 13:39:36,066 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 49 -- loss: tensor([869918.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:39:36,067 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 869918.688
2025-03-09 13:39:36,067 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:36,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 13:39:38,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 931714.125
2025-03-09 13:39:38,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:39,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 13:39:41,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 883518.062
2025-03-09 13:39:41,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:42,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 13:39:43,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 842798.438
2025-03-09 13:39:43,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:44,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 13:39:46,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 884205.125
2025-03-09 13:39:46,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:47,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 13:39:49,013 :: INFO :: evodenss.train.trainers :: [2051] -- [1.78s] TRAIN epoch 54 -- loss: tensor([909513.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:39:49,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 909513.875
2025-03-09 13:39:49,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:49,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 13:39:51,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 882485.188
2025-03-09 13:39:51,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:52,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 13:39:54,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 895744.25
2025-03-09 13:39:54,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:55,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 13:39:56,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 875349.938
2025-03-09 13:39:56,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:39:57,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 13:39:59,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 920515.5
2025-03-09 13:39:59,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:00,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 13:40:02,036 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 59 -- loss: tensor([881308.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:40:02,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 881308.875
2025-03-09 13:40:02,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:02,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 13:40:04,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 847499.562
2025-03-09 13:40:04,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:05,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 13:40:08,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 855523.125
2025-03-09 13:40:08,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:09,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 13:40:11,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 957173.5
2025-03-09 13:40:11,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:12,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 13:40:13,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 874851.75
2025-03-09 13:40:13,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:14,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 13:40:16,495 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 64 -- loss: tensor([854695.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:40:16,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 854695.0
2025-03-09 13:40:16,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:17,303 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 13:40:19,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 865606.0
2025-03-09 13:40:19,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:19,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 13:40:21,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 875823.812
2025-03-09 13:40:21,628 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:22,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 13:40:24,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 923921.188
2025-03-09 13:40:24,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:24,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 13:40:26,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 902861.188
2025-03-09 13:40:26,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:27,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 13:40:29,356 :: INFO :: evodenss.train.trainers :: [2051] -- [1.79s] TRAIN epoch 69 -- loss: tensor([834574.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:40:29,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 834574.812
2025-03-09 13:40:29,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:30,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 13:40:31,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 879080.812
2025-03-09 13:40:31,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:32,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 13:40:34,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 855439.688
2025-03-09 13:40:34,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:35,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 13:40:37,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 963099.812
2025-03-09 13:40:37,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:37,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 13:40:39,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 881990.25
2025-03-09 13:40:39,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:40,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 13:40:42,357 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 74 -- loss: tensor([907273.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:40:42,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 907273.0
2025-03-09 13:40:42,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:43,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 13:40:44,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 850730.25
2025-03-09 13:40:44,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:45,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 13:40:47,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 941227.375
2025-03-09 13:40:47,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:48,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 13:40:50,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 865313.375
2025-03-09 13:40:50,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:50,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 13:40:52,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 866963.938
2025-03-09 13:40:52,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:53,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 13:40:55,280 :: INFO :: evodenss.train.trainers :: [2051] -- [1.79s] TRAIN epoch 79 -- loss: tensor([922017.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:40:55,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 922017.75
2025-03-09 13:40:55,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:56,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 13:40:57,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 893448.5
2025-03-09 13:40:57,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:40:58,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 13:41:00,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 881192.438
2025-03-09 13:41:00,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:01,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 13:41:03,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 884207.562
2025-03-09 13:41:03,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:03,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 13:41:05,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 851257.062
2025-03-09 13:41:05,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:06,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 13:41:08,205 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 84 -- loss: tensor([903507.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:41:08,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 903507.625
2025-03-09 13:41:08,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:09,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 13:41:10,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 853824.938
2025-03-09 13:41:10,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:11,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 13:41:13,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 880631.5
2025-03-09 13:41:13,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:14,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 13:41:16,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 871474.75
2025-03-09 13:41:16,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:16,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 13:41:18,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 884732.25
2025-03-09 13:41:18,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:19,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 13:41:21,219 :: INFO :: evodenss.train.trainers :: [2051] -- [1.76s] TRAIN epoch 89 -- loss: tensor([882148.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:41:21,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 882148.062
2025-03-09 13:41:21,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:22,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 13:41:23,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 876251.25
2025-03-09 13:41:23,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:24,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 13:41:26,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 875561.188
2025-03-09 13:41:26,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:27,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 13:41:29,101 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 941066.438
2025-03-09 13:41:29,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:29,916 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 13:41:31,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 887452.062
2025-03-09 13:41:31,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:32,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 13:41:34,270 :: INFO :: evodenss.train.trainers :: [2051] -- [1.75s] TRAIN epoch 94 -- loss: tensor([885531.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:41:34,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 885531.875
2025-03-09 13:41:34,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:35,091 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 13:41:36,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 835716.062
2025-03-09 13:41:36,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:37,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 13:41:39,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 899154.125
2025-03-09 13:41:39,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:40,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 13:41:42,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 937679.312
2025-03-09 13:41:42,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:42,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 13:41:44,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 903062.812
2025-03-09 13:41:44,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:45,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 13:41:47,288 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 99 -- loss: tensor([857252.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:41:47,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 857252.438
2025-03-09 13:41:47,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:49,032 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 1180140.25
2025-03-09 13:41:49,037 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 13:41:49,038 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:36 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :deconv1d out_channels:27 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :conv1d out_channels:78 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.18593489360610754 beta1:0.8871517421296558 beta2:0.8879306712944008 weight_decay:5.3379628344897027e-05 batch_size:32 epochs:100
2025-03-09 13:41:49,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 13:41:49,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 13:41:51,489 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 445383.062
2025-03-09 13:41:51,489 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:52,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 13:41:54,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 418361.062
2025-03-09 13:41:54,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:55,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 13:41:57,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 400917.281
2025-03-09 13:41:57,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:41:58,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 13:42:00,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 383636.281
2025-03-09 13:42:00,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:01,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 13:42:03,570 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 4 -- loss: tensor([394546.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:42:03,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 394546.688
2025-03-09 13:42:03,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:04,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 13:42:06,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 419884.375
2025-03-09 13:42:06,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:07,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 13:42:09,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 394067.188
2025-03-09 13:42:09,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:10,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 13:42:12,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 398292.781
2025-03-09 13:42:12,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:13,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 13:42:15,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 403127.906
2025-03-09 13:42:15,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:16,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 13:42:18,691 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 9 -- loss: tensor([415985.9062], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:42:18,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 415985.906
2025-03-09 13:42:18,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:19,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 13:42:21,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 397595.031
2025-03-09 13:42:21,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:22,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 13:42:24,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 388248.125
2025-03-09 13:42:24,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:25,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 13:42:27,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 387628.312
2025-03-09 13:42:27,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:28,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 13:42:30,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 420195.938
2025-03-09 13:42:30,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:31,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 13:42:33,899 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 14 -- loss: tensor([431576.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:42:33,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 431576.062
2025-03-09 13:42:33,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:34,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 13:42:36,894 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 395090.312
2025-03-09 13:42:36,894 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:37,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 13:42:39,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 388054.75
2025-03-09 13:42:39,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:40,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 13:42:42,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 384259.281
2025-03-09 13:42:42,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:43,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 13:42:45,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 386769.031
2025-03-09 13:42:45,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:46,766 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 13:42:48,946 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 19 -- loss: tensor([392032.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:42:48,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 392032.438
2025-03-09 13:42:48,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:49,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 13:42:51,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 393781.812
2025-03-09 13:42:51,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:52,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 13:42:54,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 393471.812
2025-03-09 13:42:54,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:55,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 13:42:58,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 394968.656
2025-03-09 13:42:58,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:42:58,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 13:43:00,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 416236.312
2025-03-09 13:43:00,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:01,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 13:43:04,001 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 24 -- loss: tensor([398979.2812], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:43:04,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 398979.281
2025-03-09 13:43:04,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:04,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 13:43:06,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 399905.344
2025-03-09 13:43:06,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:07,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 13:43:10,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 410292.594
2025-03-09 13:43:10,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:10,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 13:43:13,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 412639.062
2025-03-09 13:43:13,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:13,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 13:43:15,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 405205.969
2025-03-09 13:43:15,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:16,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 13:43:18,987 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 29 -- loss: tensor([403695.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:43:18,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 403695.75
2025-03-09 13:43:18,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:19,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 13:43:22,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 386166.969
2025-03-09 13:43:22,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:22,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 13:43:25,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 386115.375
2025-03-09 13:43:25,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:25,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 13:43:28,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 391610.219
2025-03-09 13:43:28,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:28,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 13:43:31,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 387674.531
2025-03-09 13:43:31,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:31,925 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 13:43:34,132 :: INFO :: evodenss.train.trainers :: [2051] -- [2.2s] TRAIN epoch 34 -- loss: tensor([393149.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:43:34,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 393149.125
2025-03-09 13:43:34,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:34,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 13:43:37,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 412993.938
2025-03-09 13:43:37,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:37,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 13:43:40,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 442074.812
2025-03-09 13:43:40,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:40,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 13:43:43,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 381623.156
2025-03-09 13:43:43,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:44,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 13:43:46,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 400273.688
2025-03-09 13:43:46,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:47,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 13:43:49,249 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 39 -- loss: tensor([397632.8438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:43:49,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 397632.844
2025-03-09 13:43:49,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:50,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 13:43:52,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 437266.469
2025-03-09 13:43:52,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:53,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 13:43:55,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 385805.656
2025-03-09 13:43:55,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:56,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 13:43:58,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 385732.25
2025-03-09 13:43:58,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:43:59,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 13:44:01,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 393911.625
2025-03-09 13:44:01,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:02,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 13:44:04,339 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 44 -- loss: tensor([398381.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:44:04,339 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 398381.312
2025-03-09 13:44:04,339 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:05,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 13:44:07,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 405846.406
2025-03-09 13:44:07,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:08,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 13:44:10,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 403367.938
2025-03-09 13:44:10,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:11,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 13:44:13,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 410685.25
2025-03-09 13:44:13,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:14,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 13:44:16,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 387640.5
2025-03-09 13:44:16,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:17,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 13:44:19,607 :: INFO :: evodenss.train.trainers :: [2051] -- [2.27s] TRAIN epoch 49 -- loss: tensor([392839.0938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:44:19,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 392839.094
2025-03-09 13:44:19,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:20,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 13:44:22,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 393420.562
2025-03-09 13:44:22,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:23,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 13:44:25,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 391508.469
2025-03-09 13:44:25,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:26,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 13:44:28,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 407107.25
2025-03-09 13:44:28,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:29,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 13:44:31,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 407347.562
2025-03-09 13:44:31,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:32,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 13:44:34,735 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 54 -- loss: tensor([395988.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:44:34,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 395988.625
2025-03-09 13:44:34,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:35,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 13:44:37,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 401214.656
2025-03-09 13:44:37,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:38,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 13:44:40,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 417338.125
2025-03-09 13:44:40,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:41,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 13:44:43,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 420084.594
2025-03-09 13:44:43,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:44,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 13:44:46,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 419290.406
2025-03-09 13:44:46,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:47,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 13:44:49,966 :: INFO :: evodenss.train.trainers :: [2051] -- [2.26s] TRAIN epoch 59 -- loss: tensor([412378.1562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:44:49,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 412378.156
2025-03-09 13:44:49,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:50,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 13:44:53,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 391125.312
2025-03-09 13:44:53,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:53,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 13:44:56,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 397715.781
2025-03-09 13:44:56,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:56,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 13:44:59,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 407076.688
2025-03-09 13:44:59,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:44:59,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 13:45:02,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 394433.562
2025-03-09 13:45:02,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:03,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 13:45:05,236 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 64 -- loss: tensor([427063.7188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:45:05,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 427063.719
2025-03-09 13:45:05,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:06,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 13:45:08,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 393369.531
2025-03-09 13:45:08,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:09,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 13:45:11,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 410283.812
2025-03-09 13:45:11,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:12,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 13:45:14,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 409459.969
2025-03-09 13:45:14,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:15,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 13:45:17,392 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 390601.375
2025-03-09 13:45:17,392 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:18,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 13:45:20,448 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 69 -- loss: tensor([407769.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:45:20,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 407769.125
2025-03-09 13:45:20,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:21,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 13:45:23,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 390594.594
2025-03-09 13:45:23,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:24,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 13:45:26,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 403823.219
2025-03-09 13:45:26,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:27,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 13:45:29,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 408586.656
2025-03-09 13:45:29,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:30,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 13:45:32,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 397991.469
2025-03-09 13:45:32,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:33,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 13:45:35,517 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 74 -- loss: tensor([414609.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:45:35,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 414609.312
2025-03-09 13:45:35,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:36,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 13:45:38,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 403936.281
2025-03-09 13:45:38,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:39,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 13:45:41,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 408173.531
2025-03-09 13:45:41,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:42,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 13:45:44,562 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 400005.938
2025-03-09 13:45:44,562 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:45,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 13:45:47,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 408764.5
2025-03-09 13:45:47,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:48,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 13:45:50,648 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 79 -- loss: tensor([397073.5312], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:45:50,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 397073.531
2025-03-09 13:45:50,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:51,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 13:45:53,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 411815.844
2025-03-09 13:45:53,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:54,462 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 13:45:56,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 394094.75
2025-03-09 13:45:56,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:45:57,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 13:45:59,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 395627.594
2025-03-09 13:45:59,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:00,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 13:46:02,715 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 396234.562
2025-03-09 13:46:02,715 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:03,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 13:46:05,738 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 84 -- loss: tensor([403471.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:46:05,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 403471.312
2025-03-09 13:46:05,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:06,576 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 13:46:08,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 391213.969
2025-03-09 13:46:08,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:09,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 13:46:11,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 394498.125
2025-03-09 13:46:11,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:12,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 13:46:14,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 402854.531
2025-03-09 13:46:14,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:15,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 13:46:17,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 401370.062
2025-03-09 13:46:17,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:18,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 13:46:20,966 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 89 -- loss: tensor([412370.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:46:20,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 412370.875
2025-03-09 13:46:20,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:21,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 13:46:24,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 409117.125
2025-03-09 13:46:24,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:24,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 13:46:27,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 416074.906
2025-03-09 13:46:27,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:27,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 13:46:30,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 408615.688
2025-03-09 13:46:30,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:30,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 13:46:33,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 414292.25
2025-03-09 13:46:33,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:34,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 13:46:36,252 :: INFO :: evodenss.train.trainers :: [2051] -- [2.25s] TRAIN epoch 94 -- loss: tensor([399701.4688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:46:36,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 399701.469
2025-03-09 13:46:36,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:37,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 13:46:39,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 380944.688
2025-03-09 13:46:39,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:40,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 13:46:42,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 419662.031
2025-03-09 13:46:42,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:43,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 13:46:45,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 412611.219
2025-03-09 13:46:45,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:46,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 13:46:48,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 408110.875
2025-03-09 13:46:48,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:49,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 13:46:51,675 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 99 -- loss: tensor([392105.8438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:46:51,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 392105.844
2025-03-09 13:46:51,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:53,409 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 22923.03711
2025-03-09 13:46:53,409 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 13:46:53,410 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 0, id: 0
2025-03-09 13:46:53,410 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 1000, 1000]
2025-03-09 13:46:53,410 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 13:46:53,412 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [4112.39697, 4634.74902, 1180140.25, 22923.03711]
2025-03-09 13:46:53,671 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([21594.4668], device='cuda:0')
2025-03-09 13:46:53,671 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 1: 4112.39697
2025-03-09 13:46:53,671 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 4096.40234



2025-03-09 13:46:53,744 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 2
2025-03-09 13:46:53,744 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 13:46:53,753 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have an extra layer at Module 0: features; Position: 6. Reused?: False
2025-03-09 13:46:53,754 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 13:46:53,755 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 13:46:53,755 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 13:46:53,756 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 13:46:53,757 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 13:46:53,758 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 13:46:53,758 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 13:46:53,760 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a layer removed from Module 0: features; Position: 8
2025-03-09 13:46:53,761 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 9. Reused?: True
2025-03-09 13:46:53,761 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 13. Reused?: True
2025-03-09 13:46:53,762 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 13:46:53,762 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 13:46:53,763 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 13:46:53,764 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 13:46:53,764 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 13:46:53,765 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 13:46:53,766 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 13:46:53,766 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 13:46:53,768 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 total training time extended to 2000
2025-03-09 13:46:53,770 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 13:46:53,774 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 13:46:53,775 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 13:46:53,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 13:46:53,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 13:46:55,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 393084.719
2025-03-09 13:46:55,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:56,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 13:46:58,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 270742.281
2025-03-09 13:46:58,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:46:59,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 13:47:01,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 216484.203
2025-03-09 13:47:01,179 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:02,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 13:47:03,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 199114.875
2025-03-09 13:47:03,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:04,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 13:47:06,733 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 4 -- loss: tensor([190343.1406], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:47:06,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 190343.141
2025-03-09 13:47:06,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:07,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 13:47:09,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 172106.531
2025-03-09 13:47:09,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:10,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 13:47:12,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 136736.625
2025-03-09 13:47:12,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:13,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 13:47:15,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 125587.406
2025-03-09 13:47:15,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:15,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 13:47:17,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 120836.555
2025-03-09 13:47:17,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:18,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 13:47:20,634 :: INFO :: evodenss.train.trainers :: [2051] -- [1.96s] TRAIN epoch 9 -- loss: tensor([116763.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:47:20,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 116763.875
2025-03-09 13:47:20,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:21,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 13:47:23,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112966.883
2025-03-09 13:47:23,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:24,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 13:47:26,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110432.484
2025-03-09 13:47:26,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:27,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 13:47:28,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106398.508
2025-03-09 13:47:28,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:29,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 13:47:31,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106558.32
2025-03-09 13:47:31,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:32,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 13:47:34,533 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 14 -- loss: tensor([103658.4219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:47:34,533 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103658.422
2025-03-09 13:47:34,533 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:35,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 13:47:37,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102147.414
2025-03-09 13:47:37,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:38,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 13:47:40,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100275.68
2025-03-09 13:47:40,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:40,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 13:47:42,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99229.273
2025-03-09 13:47:42,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:43,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 13:47:45,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98512.102
2025-03-09 13:47:45,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:46,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 13:47:48,437 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 19 -- loss: tensor([96885.3359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:47:48,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96885.336
2025-03-09 13:47:48,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:49,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 13:47:51,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95551.609
2025-03-09 13:47:51,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:52,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 13:47:54,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94728.211
2025-03-09 13:47:54,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:54,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 13:47:56,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94892.414
2025-03-09 13:47:56,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:47:57,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 13:47:59,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93744.766
2025-03-09 13:47:59,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:00,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 13:48:02,326 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 24 -- loss: tensor([92983.2812], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:48:02,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92983.281
2025-03-09 13:48:02,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:03,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 13:48:05,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91889.188
2025-03-09 13:48:05,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:05,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 13:48:07,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92277.5
2025-03-09 13:48:07,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:08,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 13:48:10,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91673.102
2025-03-09 13:48:10,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:11,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 13:48:13,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90826.922
2025-03-09 13:48:13,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:14,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 13:48:16,193 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 29 -- loss: tensor([90614.3984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:48:16,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90614.398
2025-03-09 13:48:16,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:17,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 13:48:18,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88417.094
2025-03-09 13:48:18,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:19,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 13:48:21,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88712.266
2025-03-09 13:48:21,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:22,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 13:48:24,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88716.922
2025-03-09 13:48:24,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:25,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 13:48:27,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87921.039
2025-03-09 13:48:27,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:28,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 13:48:30,171 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 34 -- loss: tensor([88075.7188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:48:30,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88075.719
2025-03-09 13:48:30,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:31,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 13:48:32,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87135.469
2025-03-09 13:48:32,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:33,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 13:48:35,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86452.18
2025-03-09 13:48:35,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:36,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 13:48:40,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85749.875
2025-03-09 13:48:40,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:40,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 13:48:42,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85261.234
2025-03-09 13:48:42,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:43,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 13:48:45,696 :: INFO :: evodenss.train.trainers :: [2051] -- [1.95s] TRAIN epoch 39 -- loss: tensor([84930.0859], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:48:45,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84930.086
2025-03-09 13:48:45,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:46,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 13:48:48,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84793.516
2025-03-09 13:48:48,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:49,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 13:48:51,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84892.242
2025-03-09 13:48:51,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:52,202 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 13:48:54,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84041.617
2025-03-09 13:48:54,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:55,007 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 13:48:56,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83228.703
2025-03-09 13:48:56,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:48:57,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 13:48:59,754 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 44 -- loss: tensor([82221.9766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:48:59,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82221.977
2025-03-09 13:48:59,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:00,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 13:49:02,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82963.273
2025-03-09 13:49:02,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:03,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 13:49:05,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82881.781
2025-03-09 13:49:05,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:06,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 13:49:08,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81774.93
2025-03-09 13:49:08,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:08,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 13:49:10,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82223.914
2025-03-09 13:49:10,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:11,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 13:49:13,786 :: INFO :: evodenss.train.trainers :: [2051] -- [1.95s] TRAIN epoch 49 -- loss: tensor([81736.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:49:13,786 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81736.625
2025-03-09 13:49:13,786 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:14,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 13:49:16,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81493.797
2025-03-09 13:49:16,616 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:17,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 13:49:19,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81863.562
2025-03-09 13:49:19,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:20,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 13:49:22,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81408.672
2025-03-09 13:49:22,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:23,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 13:49:24,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79959.695
2025-03-09 13:49:24,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:25,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 13:49:27,717 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 54 -- loss: tensor([80660.0391], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:49:27,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80660.039
2025-03-09 13:49:27,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:28,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 13:49:30,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80579.375
2025-03-09 13:49:30,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:31,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 13:49:33,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80355.789
2025-03-09 13:49:33,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:34,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 13:49:36,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79659.281
2025-03-09 13:49:36,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:37,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 13:49:38,976 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79668.352
2025-03-09 13:49:38,976 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:39,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 13:49:41,712 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 59 -- loss: tensor([79563.1484], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:49:41,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79563.148
2025-03-09 13:49:41,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:42,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 13:49:44,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79042.297
2025-03-09 13:49:44,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:45,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 13:49:47,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79283.305
2025-03-09 13:49:47,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:48,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 13:49:50,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78632.711
2025-03-09 13:49:50,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:50,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 13:49:52,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78328.961
2025-03-09 13:49:52,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:53,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 13:49:55,641 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 64 -- loss: tensor([78536.1172], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:49:55,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78536.117
2025-03-09 13:49:55,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:56,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 13:49:58,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77809.391
2025-03-09 13:49:58,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:49:59,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 13:50:01,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77741.203
2025-03-09 13:50:01,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:02,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 13:50:04,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78106.547
2025-03-09 13:50:04,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:04,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 13:50:06,894 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77194.531
2025-03-09 13:50:06,894 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:07,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 13:50:09,707 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 69 -- loss: tensor([78258.8047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:50:09,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78258.805
2025-03-09 13:50:09,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:10,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 13:50:12,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77081.312
2025-03-09 13:50:12,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:13,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 13:50:15,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78099.867
2025-03-09 13:50:15,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:16,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 13:50:18,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76910.586
2025-03-09 13:50:18,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:19,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 13:50:20,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77460.516
2025-03-09 13:50:20,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:21,829 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 13:50:23,704 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 74 -- loss: tensor([77666.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:50:23,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77666.477
2025-03-09 13:50:23,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:24,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 13:50:26,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76454.719
2025-03-09 13:50:26,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:27,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 13:50:29,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76576.102
2025-03-09 13:50:29,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:30,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 13:50:32,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76677.156
2025-03-09 13:50:32,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:33,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 13:50:34,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76162.781
2025-03-09 13:50:34,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:35,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 13:50:37,716 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 79 -- loss: tensor([76404.5859], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:50:37,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76404.586
2025-03-09 13:50:37,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:38,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 13:50:40,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76084.062
2025-03-09 13:50:40,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:41,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 13:50:43,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75835.055
2025-03-09 13:50:43,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:44,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 13:50:46,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76033.836
2025-03-09 13:50:46,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:46,978 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 13:50:48,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75439.602
2025-03-09 13:50:48,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:49,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 13:50:51,731 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 84 -- loss: tensor([76040.7109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:50:51,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76040.711
2025-03-09 13:50:51,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:52,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 13:50:54,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75206.672
2025-03-09 13:50:54,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:55,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 13:50:57,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75301.453
2025-03-09 13:50:57,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:50:57,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 13:50:59,820 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74725.992
2025-03-09 13:50:59,820 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:00,637 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 13:51:02,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75461.305
2025-03-09 13:51:02,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:03,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 13:51:05,184 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 89 -- loss: tensor([75557.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:51:05,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75557.562
2025-03-09 13:51:05,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:06,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 13:51:07,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74400.125
2025-03-09 13:51:07,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:08,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 13:51:10,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74234.055
2025-03-09 13:51:10,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:11,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 13:51:13,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74738.133
2025-03-09 13:51:13,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:14,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 13:51:15,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75083.398
2025-03-09 13:51:15,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:16,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 13:51:18,653 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 94 -- loss: tensor([74858.6484], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:51:18,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74858.648
2025-03-09 13:51:18,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:19,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 13:51:21,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73990.469
2025-03-09 13:51:21,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:22,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 13:51:24,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74288.133
2025-03-09 13:51:24,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:24,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 13:51:26,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73740.336
2025-03-09 13:51:26,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:27,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 13:51:29,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74303.188
2025-03-09 13:51:29,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:30,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 13:51:32,038 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 99 -- loss: tensor([75125.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:51:32,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75125.0
2025-03-09 13:51:32,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:33,694 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 3973.1875
2025-03-09 13:51:33,698 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 13:51:33,699 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:33 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:119 kernel_size:8 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:79 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 13:51:33,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 13:51:33,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 13:51:36,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 267036.5
2025-03-09 13:51:36,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:37,120 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 13:51:39,436 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 165373.594
2025-03-09 13:51:39,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:40,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 13:51:42,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 153554.328
2025-03-09 13:51:42,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:43,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 13:51:45,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 143843.25
2025-03-09 13:51:45,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:46,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 13:51:48,825 :: INFO :: evodenss.train.trainers :: [2051] -- [2.31s] TRAIN epoch 4 -- loss: tensor([139458.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:51:48,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 139458.375
2025-03-09 13:51:48,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:49,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 13:51:51,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 134928.453
2025-03-09 13:51:51,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:52,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 13:51:55,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 128588.242
2025-03-09 13:51:55,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:55,894 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 13:51:58,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 126184.57
2025-03-09 13:51:58,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:51:59,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 13:52:01,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 120512.984
2025-03-09 13:52:01,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:02,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 13:52:04,470 :: INFO :: evodenss.train.trainers :: [2051] -- [2.3s] TRAIN epoch 9 -- loss: tensor([118211.0938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:52:04,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 118211.094
2025-03-09 13:52:04,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:05,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 13:52:07,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114861.375
2025-03-09 13:52:07,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:08,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 13:52:10,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 113527.742
2025-03-09 13:52:10,688 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:11,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 13:52:13,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110684.664
2025-03-09 13:52:13,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:14,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 13:52:16,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108648.492
2025-03-09 13:52:16,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:17,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 13:52:20,103 :: INFO :: evodenss.train.trainers :: [2051] -- [2.35s] TRAIN epoch 14 -- loss: tensor([106797.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:52:20,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106797.297
2025-03-09 13:52:20,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:20,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 13:52:23,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106060.094
2025-03-09 13:52:23,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:24,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 13:52:26,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104238.758
2025-03-09 13:52:26,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:27,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 13:52:29,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102742.695
2025-03-09 13:52:29,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:30,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 13:52:32,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101742.742
2025-03-09 13:52:32,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:33,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 13:52:35,676 :: INFO :: evodenss.train.trainers :: [2051] -- [2.31s] TRAIN epoch 19 -- loss: tensor([100845.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:52:35,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100845.875
2025-03-09 13:52:35,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:36,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 13:52:38,829 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99773.898
2025-03-09 13:52:38,829 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:39,643 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 13:52:41,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99492.477
2025-03-09 13:52:41,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:42,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 13:52:44,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98223.383
2025-03-09 13:52:44,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:45,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 13:52:48,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98256.984
2025-03-09 13:52:48,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:48,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 13:52:51,250 :: INFO :: evodenss.train.trainers :: [2051] -- [2.31s] TRAIN epoch 24 -- loss: tensor([96148.3516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:52:51,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96148.352
2025-03-09 13:52:51,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:52,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 13:52:54,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96225.68
2025-03-09 13:52:54,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:55,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 13:52:57,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95666.805
2025-03-09 13:52:57,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:52:58,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 13:53:00,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93965.695
2025-03-09 13:53:00,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:01,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 13:53:03,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93467.461
2025-03-09 13:53:03,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:04,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 13:53:06,835 :: INFO :: evodenss.train.trainers :: [2051] -- [2.29s] TRAIN epoch 29 -- loss: tensor([92092.0781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:53:06,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92092.078
2025-03-09 13:53:06,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:07,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 13:53:09,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91928.508
2025-03-09 13:53:09,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:10,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 13:53:13,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91370.25
2025-03-09 13:53:13,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:13,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 13:53:16,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90419.109
2025-03-09 13:53:16,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:16,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 13:53:19,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92223.156
2025-03-09 13:53:19,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:20,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 13:53:22,460 :: INFO :: evodenss.train.trainers :: [2051] -- [2.29s] TRAIN epoch 34 -- loss: tensor([89978.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:53:22,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89978.0
2025-03-09 13:53:22,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:23,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 13:53:25,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89943.0
2025-03-09 13:53:25,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:26,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 13:53:28,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89582.047
2025-03-09 13:53:28,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:29,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 13:53:31,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90691.734
2025-03-09 13:53:31,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:32,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 13:53:34,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90238.508
2025-03-09 13:53:34,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:35,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 13:53:38,065 :: INFO :: evodenss.train.trainers :: [2051] -- [2.33s] TRAIN epoch 39 -- loss: tensor([87880.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:53:38,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87880.312
2025-03-09 13:53:38,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:38,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 13:53:41,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87244.82
2025-03-09 13:53:41,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:42,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 13:53:44,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87483.617
2025-03-09 13:53:44,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:45,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 13:53:47,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87791.133
2025-03-09 13:53:47,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:48,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 13:53:50,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87135.117
2025-03-09 13:53:50,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:51,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 13:53:53,835 :: INFO :: evodenss.train.trainers :: [2051] -- [2.29s] TRAIN epoch 44 -- loss: tensor([86171.2031], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:53:53,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86171.203
2025-03-09 13:53:53,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:54,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 13:53:56,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85365.609
2025-03-09 13:53:56,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:53:57,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 13:54:00,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85330.648
2025-03-09 13:54:00,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:00,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 13:54:03,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84531.398
2025-03-09 13:54:03,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:04,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 13:54:06,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84624.391
2025-03-09 13:54:06,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:07,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 13:54:09,658 :: INFO :: evodenss.train.trainers :: [2051] -- [2.33s] TRAIN epoch 49 -- loss: tensor([83618.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:54:09,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83618.312
2025-03-09 13:54:09,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:10,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 13:54:12,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84299.641
2025-03-09 13:54:12,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:13,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 13:54:16,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82843.188
2025-03-09 13:54:16,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:16,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 13:54:19,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83954.609
2025-03-09 13:54:19,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:20,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 13:54:22,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84344.266
2025-03-09 13:54:22,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:23,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 13:54:25,582 :: INFO :: evodenss.train.trainers :: [2051] -- [2.35s] TRAIN epoch 54 -- loss: tensor([83141.9219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:54:25,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83141.922
2025-03-09 13:54:25,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:26,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 13:54:28,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83228.898
2025-03-09 13:54:28,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:29,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 13:54:31,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82616.445
2025-03-09 13:54:31,951 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:32,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 13:54:35,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82347.055
2025-03-09 13:54:35,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:36,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 13:54:38,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82661.367
2025-03-09 13:54:38,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:39,237 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 13:54:41,587 :: INFO :: evodenss.train.trainers :: [2051] -- [2.35s] TRAIN epoch 59 -- loss: tensor([82161.3203], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:54:41,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82161.32
2025-03-09 13:54:41,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:42,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 13:54:44,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81837.406
2025-03-09 13:54:44,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:45,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 13:54:47,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81825.703
2025-03-09 13:54:47,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:48,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 13:54:51,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81751.992
2025-03-09 13:54:51,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:52,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 13:54:54,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81079.094
2025-03-09 13:54:54,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:55,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 13:54:57,592 :: INFO :: evodenss.train.trainers :: [2051] -- [2.34s] TRAIN epoch 64 -- loss: tensor([80344.8828], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:54:57,593 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80344.883
2025-03-09 13:54:57,593 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:54:58,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 13:55:00,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82449.258
2025-03-09 13:55:00,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:01,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 13:55:04,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81541.961
2025-03-09 13:55:04,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:04,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 13:55:07,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79510.883
2025-03-09 13:55:07,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:08,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 13:55:10,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79947.523
2025-03-09 13:55:10,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:11,289 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 13:55:13,621 :: INFO :: evodenss.train.trainers :: [2051] -- [2.33s] TRAIN epoch 69 -- loss: tensor([80421.8203], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:55:13,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80421.82
2025-03-09 13:55:13,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:14,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 13:55:16,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80284.734
2025-03-09 13:55:16,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:17,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 13:55:20,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79116.914
2025-03-09 13:55:20,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:20,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 13:55:23,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80094.047
2025-03-09 13:55:23,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:24,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 13:55:26,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80007.648
2025-03-09 13:55:26,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:27,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 13:55:29,520 :: INFO :: evodenss.train.trainers :: [2051] -- [2.31s] TRAIN epoch 74 -- loss: tensor([79496.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:55:29,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79496.75
2025-03-09 13:55:29,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:30,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 13:55:32,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79884.383
2025-03-09 13:55:32,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:33,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 13:55:35,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79620.102
2025-03-09 13:55:35,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:36,580 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 13:55:38,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79394.711
2025-03-09 13:55:38,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:39,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 13:55:41,976 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78478.68
2025-03-09 13:55:41,976 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:42,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 13:55:45,123 :: INFO :: evodenss.train.trainers :: [2051] -- [2.3s] TRAIN epoch 79 -- loss: tensor([78977.3438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:55:45,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78977.344
2025-03-09 13:55:45,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:45,926 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 13:55:48,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79209.18
2025-03-09 13:55:48,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:49,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 13:55:51,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78696.125
2025-03-09 13:55:51,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:52,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 13:55:54,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78495.766
2025-03-09 13:55:54,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:55,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 13:55:57,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78580.234
2025-03-09 13:55:57,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:55:58,389 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 13:56:00,690 :: INFO :: evodenss.train.trainers :: [2051] -- [2.3s] TRAIN epoch 84 -- loss: tensor([78241.5938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:56:00,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78241.594
2025-03-09 13:56:00,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:01,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 13:56:03,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78390.789
2025-03-09 13:56:03,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:04,662 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 13:56:06,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77612.57
2025-03-09 13:56:06,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:07,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 13:56:10,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78663.188
2025-03-09 13:56:10,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:10,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 13:56:13,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77481.492
2025-03-09 13:56:13,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:14,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 13:56:16,296 :: INFO :: evodenss.train.trainers :: [2051] -- [2.29s] TRAIN epoch 89 -- loss: tensor([77681.3984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:56:16,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77681.398
2025-03-09 13:56:16,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:17,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 13:56:19,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77432.938
2025-03-09 13:56:19,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:20,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 13:56:22,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77186.359
2025-03-09 13:56:22,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:23,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 13:56:25,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77743.859
2025-03-09 13:56:25,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:26,589 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 13:56:28,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77495.25
2025-03-09 13:56:28,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:29,720 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 13:56:32,015 :: INFO :: evodenss.train.trainers :: [2051] -- [2.29s] TRAIN epoch 94 -- loss: tensor([77350.3906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:56:32,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77350.391
2025-03-09 13:56:32,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:32,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 13:56:35,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77254.086
2025-03-09 13:56:35,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:35,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 13:56:38,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76944.266
2025-03-09 13:56:38,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:39,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 13:56:41,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76050.164
2025-03-09 13:56:41,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:42,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 13:56:44,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76880.906
2025-03-09 13:56:44,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:45,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 13:56:47,607 :: INFO :: evodenss.train.trainers :: [2051] -- [2.3s] TRAIN epoch 99 -- loss: tensor([77433.4688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:56:47,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77433.469
2025-03-09 13:56:47,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:49,352 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 4447.9624
2025-03-09 13:56:49,356 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 13:56:49,358 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 
layer8: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 13:56:49,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 13:56:49,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 13:56:51,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 920818.438
2025-03-09 13:56:51,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:52,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 13:56:54,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804794.812
2025-03-09 13:56:54,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:54,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 13:56:56,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 815130.688
2025-03-09 13:56:56,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:56:57,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 13:56:59,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:56:59,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:00,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 13:57:02,126 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 4 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:57:02,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:02,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:02,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 13:57:04,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:04,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:05,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 13:57:07,500 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:07,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:08,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 13:57:10,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:10,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:11,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 13:57:12,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:12,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:13,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 13:57:15,515 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 9 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:57:15,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:15,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:16,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 13:57:18,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:18,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:19,025 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 13:57:20,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:20,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:21,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 13:57:23,596 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:23,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:24,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 13:57:26,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:26,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:27,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 13:57:28,925 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 14 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:57:28,925 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:28,925 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:29,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 13:57:31,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:31,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:32,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 13:57:34,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:34,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:35,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 13:57:36,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:36,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:37,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 13:57:39,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:39,700 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:40,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 13:57:42,369 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 19 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:57:42,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:42,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:43,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 13:57:45,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:45,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:45,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 13:57:47,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:47,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:48,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 13:57:50,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:50,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:51,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 13:57:53,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:53,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:54,058 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 13:57:55,927 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 24 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:57:55,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:55,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:56,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 13:57:58,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:57:58,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:57:59,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 13:58:01,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:01,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:02,202 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 13:58:04,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:04,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:04,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 13:58:06,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:06,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:07,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 13:58:09,448 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 29 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:58:09,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:09,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:10,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 13:58:13,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:13,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:14,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 13:58:16,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:16,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:16,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 13:58:18,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:18,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:19,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 13:58:21,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:21,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:22,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 13:58:24,380 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 34 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:58:24,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:24,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:25,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 13:58:27,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:27,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:28,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 13:58:29,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:29,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:30,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 13:58:32,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:32,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:33,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 13:58:35,433 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:35,433 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:36,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 13:58:38,192 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 39 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:58:38,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:38,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:39,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 13:58:40,953 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:40,953 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:41,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 13:58:43,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:43,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:44,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 13:58:46,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:46,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:47,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 13:58:49,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:49,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:50,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 13:58:52,080 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 44 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:58:52,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:52,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:52,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 13:58:54,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:54,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:55,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 13:58:57,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:58:57,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:58:58,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 13:59:00,393 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:00,393 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:01,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 13:59:03,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:03,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:04,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 13:59:05,885 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 49 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:59:05,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:05,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:06,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 13:59:08,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:08,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:09,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 13:59:11,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:11,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:12,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 13:59:14,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:14,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:15,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 13:59:16,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:16,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:17,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 13:59:19,696 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 54 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:59:19,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:19,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:20,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 13:59:22,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:22,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:23,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 13:59:25,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:25,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:25,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 13:59:27,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:27,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:28,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 13:59:30,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:30,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:31,206 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 13:59:33,030 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 59 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:59:33,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:33,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:33,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 13:59:35,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:35,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:36,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 13:59:38,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:38,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:39,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 13:59:40,994 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:40,994 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:41,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 13:59:43,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:43,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:44,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 13:59:46,298 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 64 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:59:46,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:46,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:47,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 13:59:48,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:48,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:49,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 13:59:51,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:51,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:52,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 13:59:54,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:54,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:55,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 13:59:57,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:57,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 13:59:57,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 13:59:59,693 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 69 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 13:59:59,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 13:59:59,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:00,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:00:02,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:02,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:03,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:00:05,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:05,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:05,820 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:00:07,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:07,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:08,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:00:10,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:10,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:11,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:00:13,002 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 74 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:00:13,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:13,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:13,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:00:15,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:15,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:16,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:00:18,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:18,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:19,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:00:21,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:21,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:21,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:00:23,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:23,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:24,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:00:26,381 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 79 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:00:26,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:26,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:27,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:00:29,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:29,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:29,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:00:31,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:31,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:32,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:00:34,355 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:34,355 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:35,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:00:37,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:37,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:37,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:00:39,672 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 84 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:00:39,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:39,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:40,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:00:42,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:42,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:43,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:00:44,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:44,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:45,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:00:47,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:47,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:48,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:00:50,339 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:50,339 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:51,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:00:53,016 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 89 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:00:53,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:53,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:53,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:00:55,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:55,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:56,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:00:58,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:00:58,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:00:59,260 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:01:01,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:01:01,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:01,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:01:03,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:01:03,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:04,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:01:06,512 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 94 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:01:06,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:01:06,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:07,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:01:09,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:01:09,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:10,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:01:11,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:01:11,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:12,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:01:14,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:01:14,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:15,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:01:17,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:01:17,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:18,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:01:20,006 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 99 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:01:20,006 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:01:20,006 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:21,809 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 44536.77344
2025-03-09 14:01:21,813 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 14:01:21,814 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:01:21,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:01:22,695 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 4112.39697
2025-03-09 14:01:22,695 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 14:01:22,696 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 0, id: 0
2025-03-09 14:01:22,696 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 1000, 2000]
2025-03-09 14:01:22,696 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 14:01:22,699 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [3973.1875, 4447.9624, 44536.77344, 4112.39697]
2025-03-09 14:01:23,060 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([20939.4082], device='cuda:0')
2025-03-09 14:01:23,061 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 2: 3973.1875
2025-03-09 14:01:23,061 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3973.1875



2025-03-09 14:01:23,133 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 3
2025-03-09 14:01:23,133 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 14:01:23,142 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a layer removed from Module 0: features; Position: 6
2025-03-09 14:01:23,143 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have an extra layer at Module 0: features; Position: 5. Reused?: True
2025-03-09 14:01:23,144 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 14:01:23,144 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 14:01:23,146 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a layer removed from Module 0: features; Position: 4
2025-03-09 14:01:23,147 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 10. Reused?: False
2025-03-09 14:01:23,148 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 14:01:23,148 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 14:01:23,149 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 14:01:23,150 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 14:01:23,150 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 14:01:23,153 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 4
2025-03-09 14:01:23,154 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have an extra layer at Module 0: features; Position: 6. Reused?: False
2025-03-09 14:01:23,154 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 10
2025-03-09 14:01:23,155 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have an extra layer at Module 0: features; Position: 11. Reused?: False
2025-03-09 14:01:23,155 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 14:01:23,156 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 14:01:23,156 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 14:01:23,159 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 14:01:23,162 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 14:01:23,163 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:01:23,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:01:23,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:01:25,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 285469.062
2025-03-09 14:01:25,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:25,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:01:27,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 169993.906
2025-03-09 14:01:27,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:28,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:01:30,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 154926.984
2025-03-09 14:01:30,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:31,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:01:33,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 144834.562
2025-03-09 14:01:33,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:34,067 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:01:35,953 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 4 -- loss: tensor([136458.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:01:35,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 136458.953
2025-03-09 14:01:35,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:36,763 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:01:38,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 130787.516
2025-03-09 14:01:38,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:39,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:01:41,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 124687.758
2025-03-09 14:01:41,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:42,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:01:44,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 120632.664
2025-03-09 14:01:44,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:44,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:01:46,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 117284.523
2025-03-09 14:01:46,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:47,580 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:01:49,533 :: INFO :: evodenss.train.trainers :: [2051] -- [1.95s] TRAIN epoch 9 -- loss: tensor([114407.1953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:01:49,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114407.195
2025-03-09 14:01:49,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:50,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:01:52,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112171.227
2025-03-09 14:01:52,260 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:53,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:01:54,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109617.641
2025-03-09 14:01:54,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:55,796 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:01:57,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107715.328
2025-03-09 14:01:57,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:01:58,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:02:00,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106042.508
2025-03-09 14:02:00,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:01,212 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:02:03,092 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 14 -- loss: tensor([104984.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:02:03,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104984.25
2025-03-09 14:02:03,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:03,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:02:05,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103542.664
2025-03-09 14:02:05,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:06,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:02:08,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102676.75
2025-03-09 14:02:08,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:09,387 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:02:11,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100861.695
2025-03-09 14:02:11,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:12,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:02:14,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99683.18
2025-03-09 14:02:14,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:14,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:02:16,735 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 19 -- loss: tensor([98196.6641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:02:16,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98196.664
2025-03-09 14:02:16,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:17,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:02:19,513 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98020.648
2025-03-09 14:02:19,513 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:20,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:02:22,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96884.938
2025-03-09 14:02:22,227 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:23,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:02:24,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95788.617
2025-03-09 14:02:24,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:25,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:02:27,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95393.617
2025-03-09 14:02:27,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:28,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:02:30,415 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 24 -- loss: tensor([94715.7656], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:02:30,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94715.766
2025-03-09 14:02:30,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:31,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 14:02:33,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94546.523
2025-03-09 14:02:33,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:33,951 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 14:02:35,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93202.664
2025-03-09 14:02:35,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:36,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 14:02:38,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92492.219
2025-03-09 14:02:38,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:39,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 14:02:41,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92373.234
2025-03-09 14:02:41,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:42,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 14:02:43,992 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 29 -- loss: tensor([91305.8516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:02:43,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91305.852
2025-03-09 14:02:43,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:44,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 14:02:46,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90111.039
2025-03-09 14:02:46,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:47,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 14:02:49,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90070.062
2025-03-09 14:02:49,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:50,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 14:02:52,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89951.891
2025-03-09 14:02:52,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:53,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 14:02:54,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89945.352
2025-03-09 14:02:54,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:55,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 14:02:57,681 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 34 -- loss: tensor([88767.7812], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:02:57,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88767.781
2025-03-09 14:02:57,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:02:58,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 14:03:00,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88994.508
2025-03-09 14:03:00,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:01,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 14:03:03,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88165.562
2025-03-09 14:03:03,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:04,006 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 14:03:05,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87207.297
2025-03-09 14:03:05,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:06,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 14:03:08,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88563.992
2025-03-09 14:03:08,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:09,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 14:03:11,407 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 39 -- loss: tensor([87426.9141], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:03:11,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87426.914
2025-03-09 14:03:11,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:12,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 14:03:14,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86931.094
2025-03-09 14:03:14,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:15,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 14:03:16,829 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86864.594
2025-03-09 14:03:16,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:17,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 14:03:19,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86550.094
2025-03-09 14:03:19,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:20,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 14:03:22,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85498.773
2025-03-09 14:03:22,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:23,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 14:03:25,040 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 44 -- loss: tensor([85288.0703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:03:25,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85288.07
2025-03-09 14:03:25,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:25,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 14:03:27,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85075.234
2025-03-09 14:03:27,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:28,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 14:03:30,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84918.203
2025-03-09 14:03:30,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:31,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 14:03:33,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85103.195
2025-03-09 14:03:33,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:34,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 14:03:35,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83978.602
2025-03-09 14:03:35,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:36,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 14:03:38,611 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 49 -- loss: tensor([84461.3906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:03:38,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84461.391
2025-03-09 14:03:38,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:39,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 14:03:41,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83706.227
2025-03-09 14:03:41,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:42,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 14:03:44,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83799.391
2025-03-09 14:03:44,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:44,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 14:03:46,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82790.305
2025-03-09 14:03:46,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:47,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 14:03:49,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82168.898
2025-03-09 14:03:49,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:50,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 14:03:52,152 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 54 -- loss: tensor([82878.1719], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:03:52,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82878.172
2025-03-09 14:03:52,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:53,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 14:03:54,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83057.617
2025-03-09 14:03:54,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:55,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 14:03:57,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82962.977
2025-03-09 14:03:57,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:03:58,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 14:04:00,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81604.719
2025-03-09 14:04:00,303 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:01,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 14:04:03,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81900.82
2025-03-09 14:04:03,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:03,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 14:04:05,692 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 59 -- loss: tensor([81831.8672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:04:05,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81831.867
2025-03-09 14:04:05,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:06,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 14:04:08,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81234.039
2025-03-09 14:04:08,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:09,212 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 14:04:11,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80860.258
2025-03-09 14:04:11,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:11,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 14:04:13,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81606.609
2025-03-09 14:04:13,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:14,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 14:04:16,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80212.969
2025-03-09 14:04:16,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:17,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 14:04:19,191 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 64 -- loss: tensor([80826.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:04:19,191 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80826.297
2025-03-09 14:04:19,191 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:20,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 14:04:21,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80052.875
2025-03-09 14:04:21,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:22,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 14:04:24,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80304.438
2025-03-09 14:04:24,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:25,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 14:04:27,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79269.914
2025-03-09 14:04:27,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:28,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 14:04:29,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80385.234
2025-03-09 14:04:29,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:30,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 14:04:32,675 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 69 -- loss: tensor([79879.4609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:04:32,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79879.461
2025-03-09 14:04:32,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:33,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:04:35,338 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79808.664
2025-03-09 14:04:35,338 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:36,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:04:38,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79537.031
2025-03-09 14:04:38,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:38,926 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:04:40,806 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79900.445
2025-03-09 14:04:40,806 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:41,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:04:43,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78319.852
2025-03-09 14:04:43,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:44,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:04:46,145 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 74 -- loss: tensor([79907.0234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:04:46,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79907.023
2025-03-09 14:04:46,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:46,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:04:48,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78181.875
2025-03-09 14:04:48,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:49,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:04:51,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79216.422
2025-03-09 14:04:51,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:52,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:04:54,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78505.539
2025-03-09 14:04:54,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:55,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:04:57,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78873.117
2025-03-09 14:04:57,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:04:57,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:04:59,787 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 79 -- loss: tensor([77562.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:04:59,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77562.297
2025-03-09 14:04:59,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:00,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:05:02,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78902.344
2025-03-09 14:05:02,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:03,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:05:05,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78054.211
2025-03-09 14:05:05,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:06,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:05:07,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77795.211
2025-03-09 14:05:07,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:08,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:05:10,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77780.781
2025-03-09 14:05:10,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:11,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:05:13,421 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 84 -- loss: tensor([77697.9766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:05:13,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77697.977
2025-03-09 14:05:13,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:14,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:05:16,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77117.469
2025-03-09 14:05:16,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:17,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:05:18,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76850.508
2025-03-09 14:05:18,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:19,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:05:21,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77478.812
2025-03-09 14:05:21,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:22,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:05:24,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76899.977
2025-03-09 14:05:24,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:25,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:05:27,103 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 89 -- loss: tensor([76325.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:05:27,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76325.953
2025-03-09 14:05:27,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:27,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:05:29,720 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77083.953
2025-03-09 14:05:29,720 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:30,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:05:32,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76734.672
2025-03-09 14:05:32,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:33,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:05:35,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76470.922
2025-03-09 14:05:35,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:35,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:05:37,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76239.305
2025-03-09 14:05:37,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:38,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:05:40,423 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 94 -- loss: tensor([76647.9609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:05:40,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76647.961
2025-03-09 14:05:40,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:41,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:05:43,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77565.398
2025-03-09 14:05:43,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:43,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:05:45,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76260.367
2025-03-09 14:05:45,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:46,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:05:48,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75995.922
2025-03-09 14:05:48,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:49,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:05:51,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76505.078
2025-03-09 14:05:51,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:52,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:05:53,934 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 99 -- loss: tensor([75754.3984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:05:53,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75754.398
2025-03-09 14:05:53,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:55,667 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 4002.26025
2025-03-09 14:05:55,672 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 14:05:55,672 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:05:55,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:05:55,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:05:57,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 390863.938
2025-03-09 14:05:57,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:05:58,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:06:00,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 224557.25
2025-03-09 14:06:00,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:01,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:06:03,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 157982.938
2025-03-09 14:06:03,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:04,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:06:06,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 142425.406
2025-03-09 14:06:06,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:06,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:06:08,805 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 4 -- loss: tensor([132839.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:06:08,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 132839.812
2025-03-09 14:06:08,806 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:09,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:06:11,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 126447.32
2025-03-09 14:06:11,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:12,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:06:14,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 123616.344
2025-03-09 14:06:14,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:15,106 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:06:16,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 119047.812
2025-03-09 14:06:16,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:17,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:06:19,798 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 116400.422
2025-03-09 14:06:19,798 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:20,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:06:22,507 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 9 -- loss: tensor([113703.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:06:22,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 113703.75
2025-03-09 14:06:22,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:23,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:06:25,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 111409.742
2025-03-09 14:06:25,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:26,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:06:28,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107686.938
2025-03-09 14:06:28,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:28,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:06:30,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105854.703
2025-03-09 14:06:30,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:31,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:06:33,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102874.641
2025-03-09 14:06:33,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:34,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:06:36,200 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 14 -- loss: tensor([101172.6953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:06:36,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101172.695
2025-03-09 14:06:36,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:37,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:06:39,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99822.578
2025-03-09 14:06:39,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:40,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:06:41,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97342.93
2025-03-09 14:06:41,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:42,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:06:44,715 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96461.68
2025-03-09 14:06:44,715 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:45,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:06:47,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94700.648
2025-03-09 14:06:47,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:48,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:06:50,274 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 19 -- loss: tensor([92756.3516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:06:50,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92756.352
2025-03-09 14:06:50,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:51,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:06:53,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91717.484
2025-03-09 14:06:53,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:53,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:06:55,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91125.883
2025-03-09 14:06:55,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:56,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:06:58,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91206.648
2025-03-09 14:06:58,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:06:59,304 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:07:01,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90160.188
2025-03-09 14:07:01,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:02,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:07:03,931 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 24 -- loss: tensor([88079.9844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:07:03,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88079.984
2025-03-09 14:07:03,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:04,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 14:07:06,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89188.906
2025-03-09 14:07:06,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:07,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 14:07:09,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86819.18
2025-03-09 14:07:09,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:10,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 14:07:12,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87515.484
2025-03-09 14:07:12,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:13,054 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 14:07:14,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85958.406
2025-03-09 14:07:14,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:15,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 14:07:17,672 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 29 -- loss: tensor([85984.5547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:07:17,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85984.555
2025-03-09 14:07:17,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:18,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 14:07:20,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84957.422
2025-03-09 14:07:20,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:21,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 14:07:23,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84716.094
2025-03-09 14:07:23,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:24,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 14:07:25,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84346.203
2025-03-09 14:07:25,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:26,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 14:07:28,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84074.797
2025-03-09 14:07:28,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:29,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 14:07:31,396 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 34 -- loss: tensor([83185.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:07:31,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83185.062
2025-03-09 14:07:31,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:32,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 14:07:34,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82966.516
2025-03-09 14:07:34,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:34,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 14:07:36,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83136.594
2025-03-09 14:07:36,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:37,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 14:07:39,644 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82510.258
2025-03-09 14:07:39,644 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:40,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 14:07:42,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82200.5
2025-03-09 14:07:42,389 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:43,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 14:07:45,174 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 39 -- loss: tensor([81171.9688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:07:45,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81171.969
2025-03-09 14:07:45,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:46,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 14:07:47,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82113.266
2025-03-09 14:07:47,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:48,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 14:07:50,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80628.938
2025-03-09 14:07:50,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:51,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 14:07:53,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80961.828
2025-03-09 14:07:53,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:54,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 14:07:56,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79863.211
2025-03-09 14:07:56,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:57,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 14:07:59,081 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 44 -- loss: tensor([80055.6719], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:07:59,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80055.672
2025-03-09 14:07:59,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:07:59,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 14:08:01,869 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78747.922
2025-03-09 14:08:01,869 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:02,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 14:08:04,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79318.523
2025-03-09 14:08:04,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:05,500 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 14:08:07,408 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79718.938
2025-03-09 14:08:07,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:08,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 14:08:10,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78552.172
2025-03-09 14:08:10,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:11,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 14:08:12,991 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 49 -- loss: tensor([78270.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:08:12,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78270.688
2025-03-09 14:08:12,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:13,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 14:08:15,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78484.125
2025-03-09 14:08:15,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:16,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 14:08:18,596 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77946.609
2025-03-09 14:08:18,596 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:19,539 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 14:08:21,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77092.023
2025-03-09 14:08:21,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:22,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 14:08:24,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77461.164
2025-03-09 14:08:24,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:25,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 14:08:27,026 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 54 -- loss: tensor([77785.0391], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:08:27,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77785.039
2025-03-09 14:08:27,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:27,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 14:08:29,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77085.398
2025-03-09 14:08:29,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:30,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 14:08:32,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76915.961
2025-03-09 14:08:32,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:33,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 14:08:35,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76158.07
2025-03-09 14:08:35,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:36,303 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 14:08:38,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76390.586
2025-03-09 14:08:38,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:39,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 14:08:41,000 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 59 -- loss: tensor([76557.6484], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:08:41,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76557.648
2025-03-09 14:08:41,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:41,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 14:08:43,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75790.984
2025-03-09 14:08:43,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:44,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 14:08:46,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75644.594
2025-03-09 14:08:46,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:47,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 14:08:49,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76279.023
2025-03-09 14:08:49,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:50,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 14:08:52,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75058.031
2025-03-09 14:08:52,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:53,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 14:08:55,034 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 64 -- loss: tensor([75466.7188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:08:55,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75466.719
2025-03-09 14:08:55,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:55,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 14:08:57,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75614.125
2025-03-09 14:08:57,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:08:58,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 14:09:00,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76045.578
2025-03-09 14:09:00,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:01,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 14:09:03,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74761.273
2025-03-09 14:09:03,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:03,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 14:09:05,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75072.484
2025-03-09 14:09:05,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:06,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 14:09:08,450 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 69 -- loss: tensor([75285.1172], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:09:08,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75285.117
2025-03-09 14:09:08,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:09,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:09:11,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74458.453
2025-03-09 14:09:11,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:12,006 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:09:13,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74758.969
2025-03-09 14:09:13,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:14,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:09:16,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73735.125
2025-03-09 14:09:16,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:17,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:09:19,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74553.008
2025-03-09 14:09:19,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:20,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:09:21,961 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 74 -- loss: tensor([73888.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:09:21,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73888.188
2025-03-09 14:09:21,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:22,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:09:24,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73682.945
2025-03-09 14:09:24,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:25,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:09:27,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73821.188
2025-03-09 14:09:27,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:28,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:09:30,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72883.195
2025-03-09 14:09:30,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:30,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:09:32,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73472.719
2025-03-09 14:09:32,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:33,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:09:35,449 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 79 -- loss: tensor([72862.7188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:09:35,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72862.719
2025-03-09 14:09:35,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:36,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:09:38,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73939.633
2025-03-09 14:09:38,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:38,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:09:40,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73724.766
2025-03-09 14:09:40,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:41,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:09:43,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72932.688
2025-03-09 14:09:43,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:44,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:09:46,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72544.656
2025-03-09 14:09:46,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:47,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:09:49,024 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 84 -- loss: tensor([72816.5547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:09:49,025 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72816.555
2025-03-09 14:09:49,025 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:49,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:09:51,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72737.531
2025-03-09 14:09:51,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:52,638 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:09:54,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72255.586
2025-03-09 14:09:54,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:55,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:09:57,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72576.906
2025-03-09 14:09:57,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:09:58,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:10:00,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72188.219
2025-03-09 14:10:00,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:00,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:10:02,777 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 89 -- loss: tensor([72561.6172], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:10:02,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72561.617
2025-03-09 14:10:02,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:03,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:10:05,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71913.484
2025-03-09 14:10:05,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:06,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:10:08,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71721.43
2025-03-09 14:10:08,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:09,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:10:11,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71850.008
2025-03-09 14:10:11,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:11,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:10:13,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71857.523
2025-03-09 14:10:13,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:14,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:10:16,383 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 94 -- loss: tensor([71766.2422], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:10:16,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71766.242
2025-03-09 14:10:16,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:17,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:10:19,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72077.203
2025-03-09 14:10:19,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:19,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:10:21,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70680.961
2025-03-09 14:10:21,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:22,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:10:24,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71731.305
2025-03-09 14:10:24,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:25,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:10:27,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71061.062
2025-03-09 14:10:27,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:27,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:10:29,859 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 99 -- loss: tensor([70125.5469], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:10:29,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70125.547
2025-03-09 14:10:29,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:31,551 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 3818.04004
2025-03-09 14:10:31,555 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 14:10:31,556 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 
layer7: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 
layer8: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 
layer10: :conv1d out_channels:48 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:10:31,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:10:31,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:10:33,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 287728.031
2025-03-09 14:10:33,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:34,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:10:36,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 158714.188
2025-03-09 14:10:36,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:37,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:10:39,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 144911.797
2025-03-09 14:10:39,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:39,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:10:41,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 139385.047
2025-03-09 14:10:41,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:42,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:10:44,398 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 4 -- loss: tensor([134676.8906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:10:44,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 134676.891
2025-03-09 14:10:44,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:45,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:10:47,025 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 130170.539
2025-03-09 14:10:47,025 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:47,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:10:49,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 125888.703
2025-03-09 14:10:49,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:50,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:10:52,408 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 123101.273
2025-03-09 14:10:52,408 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:53,237 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:10:55,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 119162.023
2025-03-09 14:10:55,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:55,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:10:57,727 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 9 -- loss: tensor([116771.3828], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:10:57,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 116771.383
2025-03-09 14:10:57,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:10:58,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:11:00,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114595.695
2025-03-09 14:11:00,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:01,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:11:03,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112363.469
2025-03-09 14:11:03,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:03,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:11:05,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 111221.516
2025-03-09 14:11:05,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:06,539 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:11:08,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109259.516
2025-03-09 14:11:08,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:09,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:11:11,038 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 14 -- loss: tensor([107138.0234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:11:11,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107138.023
2025-03-09 14:11:11,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:11,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:11:13,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105895.141
2025-03-09 14:11:13,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:14,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:11:16,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105000.664
2025-03-09 14:11:16,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:17,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:11:19,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104094.789
2025-03-09 14:11:19,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:19,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:11:21,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103149.859
2025-03-09 14:11:21,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:22,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:11:24,363 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 19 -- loss: tensor([101786.8984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:11:24,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101786.898
2025-03-09 14:11:24,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:25,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:11:27,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100282.383
2025-03-09 14:11:27,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:27,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:11:29,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99534.602
2025-03-09 14:11:29,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:30,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:11:32,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96908.008
2025-03-09 14:11:32,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:33,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:11:35,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97170.172
2025-03-09 14:11:35,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:35,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:11:37,713 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 24 -- loss: tensor([96512.3281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:11:37,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96512.328
2025-03-09 14:11:37,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:38,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 14:11:40,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95508.297
2025-03-09 14:11:40,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:41,191 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 14:11:43,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95745.641
2025-03-09 14:11:43,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:43,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 14:11:45,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94034.688
2025-03-09 14:11:45,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:46,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 14:11:48,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93639.438
2025-03-09 14:11:48,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:49,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 14:11:51,000 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 29 -- loss: tensor([92683.1406], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:11:51,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92683.141
2025-03-09 14:11:51,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:51,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 14:11:53,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92763.18
2025-03-09 14:11:53,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:54,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 14:11:56,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92128.312
2025-03-09 14:11:56,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:57,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 14:11:58,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91382.016
2025-03-09 14:11:58,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:11:59,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 14:12:01,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90697.352
2025-03-09 14:12:01,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:02,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 14:12:04,332 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 34 -- loss: tensor([90695.3516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:12:04,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90695.352
2025-03-09 14:12:04,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:05,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 14:12:06,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90148.117
2025-03-09 14:12:06,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:07,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 14:12:09,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89282.969
2025-03-09 14:12:09,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:10,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 14:12:12,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89145.852
2025-03-09 14:12:12,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:13,187 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 14:12:15,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89206.711
2025-03-09 14:12:15,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:15,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 14:12:17,679 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 39 -- loss: tensor([87704.6328], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:12:17,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87704.633
2025-03-09 14:12:17,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:18,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 14:12:20,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87957.734
2025-03-09 14:12:20,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:21,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 14:12:23,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87835.398
2025-03-09 14:12:23,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:23,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 14:12:25,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86101.828
2025-03-09 14:12:25,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:26,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 14:12:28,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86683.133
2025-03-09 14:12:28,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:29,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 14:12:31,100 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 44 -- loss: tensor([85724.5234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:12:31,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85724.523
2025-03-09 14:12:31,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:31,953 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 14:12:33,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85771.18
2025-03-09 14:12:33,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:34,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 14:12:36,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85544.93
2025-03-09 14:12:36,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:37,339 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 14:12:39,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84728.953
2025-03-09 14:12:39,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:40,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 14:12:41,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85239.688
2025-03-09 14:12:41,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:42,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 14:12:44,548 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 49 -- loss: tensor([84587.4688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:12:44,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84587.469
2025-03-09 14:12:44,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:45,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 14:12:47,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83634.727
2025-03-09 14:12:47,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:48,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 14:12:50,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84564.109
2025-03-09 14:12:50,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:50,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 14:12:52,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83704.875
2025-03-09 14:12:52,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:53,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 14:12:55,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83004.617
2025-03-09 14:12:55,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:56,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 14:12:58,193 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 54 -- loss: tensor([83294.0078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:12:58,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83294.008
2025-03-09 14:12:58,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:12:59,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 14:13:00,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82865.0
2025-03-09 14:13:00,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:01,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 14:13:03,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83057.148
2025-03-09 14:13:03,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:04,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 14:13:06,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82657.344
2025-03-09 14:13:06,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:07,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 14:13:08,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82448.508
2025-03-09 14:13:08,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:09,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 14:13:11,635 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 59 -- loss: tensor([81945.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:13:11,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81945.953
2025-03-09 14:13:11,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:12,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 14:13:14,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81915.68
2025-03-09 14:13:14,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:15,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 14:13:17,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81579.273
2025-03-09 14:13:17,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:17,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 14:13:19,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80827.516
2025-03-09 14:13:19,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:20,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 14:13:22,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80953.188
2025-03-09 14:13:22,562 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:23,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 14:13:25,274 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 64 -- loss: tensor([80501.1953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:13:25,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80501.195
2025-03-09 14:13:25,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:26,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 14:13:27,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81117.438
2025-03-09 14:13:27,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:28,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 14:13:30,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80229.961
2025-03-09 14:13:30,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:31,592 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 14:13:33,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79924.469
2025-03-09 14:13:33,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:34,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 14:13:36,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79761.648
2025-03-09 14:13:36,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:37,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 14:13:38,902 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 69 -- loss: tensor([79930.2344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:13:38,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79930.234
2025-03-09 14:13:38,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:39,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:13:41,584 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79846.18
2025-03-09 14:13:41,584 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:42,454 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:13:44,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79603.133
2025-03-09 14:13:44,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:45,179 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:13:47,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79601.156
2025-03-09 14:13:47,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:47,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:13:49,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79339.07
2025-03-09 14:13:49,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:50,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:13:52,545 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 74 -- loss: tensor([79319.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:13:52,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79319.297
2025-03-09 14:13:52,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:53,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:13:55,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78224.727
2025-03-09 14:13:55,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:56,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:13:57,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78216.055
2025-03-09 14:13:57,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:13:58,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:14:00,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78760.297
2025-03-09 14:14:00,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:01,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:14:03,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79124.375
2025-03-09 14:14:03,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:04,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:14:06,049 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 79 -- loss: tensor([78189.4609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:14:06,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78189.461
2025-03-09 14:14:06,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:06,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:14:08,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77813.023
2025-03-09 14:14:08,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:09,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:14:11,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77182.984
2025-03-09 14:14:11,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:12,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:14:14,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77838.898
2025-03-09 14:14:14,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:15,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:14:16,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77840.25
2025-03-09 14:14:16,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:17,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:14:19,610 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 84 -- loss: tensor([77382.4922], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:14:19,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77382.492
2025-03-09 14:14:19,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:20,465 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:14:22,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76161.031
2025-03-09 14:14:22,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:23,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:14:24,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77358.172
2025-03-09 14:14:24,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:25,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:14:27,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77078.016
2025-03-09 14:14:27,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:28,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:14:30,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77084.633
2025-03-09 14:14:30,303 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:31,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:14:32,913 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 89 -- loss: tensor([77193.2422], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:14:32,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77193.242
2025-03-09 14:14:32,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:33,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:14:35,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77005.25
2025-03-09 14:14:35,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:36,423 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:14:38,234 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76732.016
2025-03-09 14:14:38,234 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:39,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:14:40,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77213.984
2025-03-09 14:14:40,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:41,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:14:43,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76996.602
2025-03-09 14:14:43,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:44,348 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:14:46,191 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 94 -- loss: tensor([76741.5156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:14:46,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76741.516
2025-03-09 14:14:46,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:47,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:14:48,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76154.438
2025-03-09 14:14:48,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:49,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:14:51,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76823.164
2025-03-09 14:14:51,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:52,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:14:54,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75876.797
2025-03-09 14:14:54,187 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:55,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:14:56,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76089.297
2025-03-09 14:14:56,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:14:57,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:14:59,524 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 99 -- loss: tensor([75017.9688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:14:59,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75017.969
2025-03-09 14:14:59,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:01,215 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 4134.28027
2025-03-09 14:15:01,220 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 14:15:01,221 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :deconv1d out_channels:45 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :deconv1d out_channels:91 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:15:01,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:15:01,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:15:03,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 391265.75
2025-03-09 14:15:03,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:04,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:15:06,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 347417.0
2025-03-09 14:15:06,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:06,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:15:08,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 221400.828
2025-03-09 14:15:08,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:09,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:15:11,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 185830.125
2025-03-09 14:15:11,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:12,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:15:14,028 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 4 -- loss: tensor([154742.3906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:15:14,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 154742.391
2025-03-09 14:15:14,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:14,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:15:16,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 130563.141
2025-03-09 14:15:16,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:17,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:15:19,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 124012.82
2025-03-09 14:15:19,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:20,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:15:22,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 118814.281
2025-03-09 14:15:22,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:22,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:15:24,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114992.594
2025-03-09 14:15:24,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:25,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:15:27,347 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 9 -- loss: tensor([112123.6719], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:15:27,348 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112123.672
2025-03-09 14:15:27,348 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:28,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:15:30,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109598.852
2025-03-09 14:15:30,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:30,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:15:32,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107418.562
2025-03-09 14:15:32,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:33,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:15:35,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106729.781
2025-03-09 14:15:35,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:36,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:15:38,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105146.945
2025-03-09 14:15:38,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:38,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:15:40,670 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 14 -- loss: tensor([103416.0156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:15:40,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103416.016
2025-03-09 14:15:40,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:41,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:15:43,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102425.211
2025-03-09 14:15:43,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:44,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:15:45,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101237.109
2025-03-09 14:15:45,978 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:46,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:15:48,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101675.086
2025-03-09 14:15:48,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:49,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:15:51,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99900.539
2025-03-09 14:15:51,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:52,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:15:53,979 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 19 -- loss: tensor([99432.4453], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:15:53,979 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99432.445
2025-03-09 14:15:53,979 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:54,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:15:56,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98136.641
2025-03-09 14:15:56,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:15:57,489 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:15:59,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98258.93
2025-03-09 14:15:59,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:00,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:16:01,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97327.344
2025-03-09 14:16:01,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:02,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:16:04,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96610.758
2025-03-09 14:16:04,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:05,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:16:07,490 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 24 -- loss: tensor([96055.3203], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:16:07,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96055.32
2025-03-09 14:16:07,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:08,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 14:16:10,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94670.617
2025-03-09 14:16:10,187 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:11,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 14:16:12,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94653.844
2025-03-09 14:16:12,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:13,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 14:16:15,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94322.852
2025-03-09 14:16:15,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:16,465 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 14:16:18,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93608.18
2025-03-09 14:16:18,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:19,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 14:16:21,001 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 29 -- loss: tensor([93329.3047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:16:21,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93329.305
2025-03-09 14:16:21,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:21,869 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 14:16:23,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92871.672
2025-03-09 14:16:23,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:24,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 14:16:26,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91821.086
2025-03-09 14:16:26,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:27,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 14:16:29,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92798.82
2025-03-09 14:16:29,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:30,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 14:16:31,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91629.172
2025-03-09 14:16:31,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:32,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 14:16:34,547 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 34 -- loss: tensor([91251.8359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:16:34,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91251.836
2025-03-09 14:16:34,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:35,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 14:16:37,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91284.711
2025-03-09 14:16:37,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:38,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 14:16:39,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90718.211
2025-03-09 14:16:39,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:40,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 14:16:42,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89918.539
2025-03-09 14:16:42,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:43,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 14:16:45,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90124.039
2025-03-09 14:16:45,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:46,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 14:16:48,063 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 39 -- loss: tensor([89243.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:16:48,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89243.625
2025-03-09 14:16:48,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:48,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 14:16:50,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89182.984
2025-03-09 14:16:50,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:51,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 14:16:53,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89248.242
2025-03-09 14:16:53,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:54,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 14:16:56,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88940.398
2025-03-09 14:16:56,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:57,079 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 14:16:58,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88008.453
2025-03-09 14:16:58,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:16:59,795 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 14:17:01,669 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 44 -- loss: tensor([87838.3047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:17:01,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87838.305
2025-03-09 14:17:01,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:02,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 14:17:04,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87155.789
2025-03-09 14:17:04,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:05,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 14:17:07,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87625.523
2025-03-09 14:17:07,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:07,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 14:17:09,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86816.734
2025-03-09 14:17:09,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:10,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 14:17:12,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86194.898
2025-03-09 14:17:12,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:13,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 14:17:15,295 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 49 -- loss: tensor([85626.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:17:15,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85626.188
2025-03-09 14:17:15,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:16,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 14:17:18,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86095.812
2025-03-09 14:17:18,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:19,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 14:17:20,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85697.711
2025-03-09 14:17:20,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:21,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 14:17:23,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85288.703
2025-03-09 14:17:23,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:24,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 14:17:26,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85682.281
2025-03-09 14:17:26,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:27,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 14:17:29,109 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 54 -- loss: tensor([84934.7656], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:17:29,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84934.766
2025-03-09 14:17:29,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:30,006 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 14:17:31,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85188.773
2025-03-09 14:17:31,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:32,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 14:17:34,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83996.93
2025-03-09 14:17:34,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:35,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 14:17:37,462 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83749.844
2025-03-09 14:17:37,462 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:38,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 14:17:40,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84189.148
2025-03-09 14:17:40,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:41,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 14:17:42,967 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 59 -- loss: tensor([83659.1641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:17:42,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83659.164
2025-03-09 14:17:42,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:43,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 14:17:45,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83244.562
2025-03-09 14:17:45,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:46,592 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 14:17:48,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83653.164
2025-03-09 14:17:48,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:49,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 14:17:51,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83252.516
2025-03-09 14:17:51,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:52,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 14:17:53,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82967.352
2025-03-09 14:17:53,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:54,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 14:17:56,708 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 64 -- loss: tensor([83606.7969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:17:56,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83606.797
2025-03-09 14:17:56,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:17:57,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 14:17:59,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82397.984
2025-03-09 14:17:59,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:00,338 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 14:18:02,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82107.883
2025-03-09 14:18:02,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:03,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 14:18:04,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83300.555
2025-03-09 14:18:04,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:05,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 14:18:07,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82700.68
2025-03-09 14:18:07,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:08,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 14:18:10,471 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 69 -- loss: tensor([80901.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:18:10,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80901.297
2025-03-09 14:18:10,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:11,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:18:13,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81763.352
2025-03-09 14:18:13,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:14,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:18:15,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82754.461
2025-03-09 14:18:15,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:16,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:18:18,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82304.719
2025-03-09 14:18:18,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:19,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:18:21,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81557.141
2025-03-09 14:18:21,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:22,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:18:24,265 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 74 -- loss: tensor([81248.5938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:18:24,266 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81248.594
2025-03-09 14:18:24,266 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:25,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:18:26,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80223.32
2025-03-09 14:18:26,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:27,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:18:29,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81221.562
2025-03-09 14:18:29,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:30,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:18:32,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81446.578
2025-03-09 14:18:32,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:33,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:18:35,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80932.727
2025-03-09 14:18:35,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:36,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:18:37,947 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 79 -- loss: tensor([80046.3516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:18:37,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80046.352
2025-03-09 14:18:37,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:38,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:18:40,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80504.656
2025-03-09 14:18:40,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:41,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:18:43,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80736.711
2025-03-09 14:18:43,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:44,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:18:46,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79579.945
2025-03-09 14:18:46,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:47,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:18:48,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80982.273
2025-03-09 14:18:48,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:49,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:18:51,711 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 84 -- loss: tensor([80349.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:18:51,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80349.812
2025-03-09 14:18:51,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:52,617 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:18:54,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79176.609
2025-03-09 14:18:54,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:55,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:18:57,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79154.367
2025-03-09 14:18:57,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:18:58,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:18:59,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79647.656
2025-03-09 14:18:59,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:00,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:19:02,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79805.133
2025-03-09 14:19:02,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:03,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:19:05,530 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 89 -- loss: tensor([79441.3672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:19:05,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79441.367
2025-03-09 14:19:05,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:06,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:19:08,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79310.164
2025-03-09 14:19:08,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:09,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:19:10,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78975.984
2025-03-09 14:19:10,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:11,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:19:13,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79229.438
2025-03-09 14:19:13,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:14,436 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:19:16,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79147.734
2025-03-09 14:19:16,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:17,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:19:18,959 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 94 -- loss: tensor([78601.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:19:18,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78601.953
2025-03-09 14:19:18,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:19,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:19:21,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78813.867
2025-03-09 14:19:21,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:22,466 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:19:24,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78037.016
2025-03-09 14:19:24,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:25,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:19:26,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77442.727
2025-03-09 14:19:26,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:27,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:19:29,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77776.078
2025-03-09 14:19:29,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:30,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:19:32,315 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 99 -- loss: tensor([77540.0781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:19:32,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77540.078
2025-03-09 14:19:32,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:34,088 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 4305.87207
2025-03-09 14:19:34,088 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 14:19:34,088 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 1, id: 1
2025-03-09 14:19:34,088 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 1000, 1000]
2025-03-09 14:19:34,089 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 14:19:34,091 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [4002.26025, 3818.04004, 4134.28027, 4305.87207]
2025-03-09 14:19:34,531 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([20650.8887], device='cuda:0')
2025-03-09 14:19:34,531 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 3: 3818.04004
2025-03-09 14:19:34,531 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3818.04004



2025-03-09 14:19:34,608 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 4
2025-03-09 14:19:34,608 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 14:19:34,616 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 total training time extended to 2000
2025-03-09 14:19:34,619 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 10. Reused?: False
2025-03-09 14:19:34,620 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a layer removed from Module 0: features; Position: 4
2025-03-09 14:19:34,620 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 12. Reused?: False
2025-03-09 14:19:34,621 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 14:19:34,622 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 14:19:34,623 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 14:19:34,623 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 14:19:34,624 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 14:19:34,625 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 14:19:34,625 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 14:19:34,628 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 11
2025-03-09 14:19:34,628 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 14:19:34,629 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 14:19:34,630 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 14:19:34,630 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 14:19:34,632 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 14:19:34,639 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 14:19:34,640 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:19:34,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:19:34,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:19:36,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 297206.906
2025-03-09 14:19:36,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:37,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:19:39,303 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 162818.266
2025-03-09 14:19:39,303 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:40,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:19:42,041 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 146582.047
2025-03-09 14:19:42,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:42,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:19:44,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 136636.078
2025-03-09 14:19:44,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:45,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:19:47,526 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 4 -- loss: tensor([130067.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:19:47,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 130067.75
2025-03-09 14:19:47,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:48,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:19:50,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 123504.438
2025-03-09 14:19:50,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:51,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:19:53,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 118451.891
2025-03-09 14:19:53,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:53,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:19:55,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114362.836
2025-03-09 14:19:55,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:56,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:19:58,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 111502.547
2025-03-09 14:19:58,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:19:59,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:20:01,290 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 9 -- loss: tensor([110309.7109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:20:01,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110309.711
2025-03-09 14:20:01,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:02,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:20:04,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107713.438
2025-03-09 14:20:04,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:04,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:20:06,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106135.211
2025-03-09 14:20:06,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:07,662 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:20:09,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104571.039
2025-03-09 14:20:09,573 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:10,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:20:12,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103812.578
2025-03-09 14:20:12,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:13,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:20:15,005 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 14 -- loss: tensor([102436.5234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:20:15,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102436.523
2025-03-09 14:20:15,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:15,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:20:17,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100272.531
2025-03-09 14:20:17,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:18,663 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:20:20,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99341.195
2025-03-09 14:20:20,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:21,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:20:23,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97829.422
2025-03-09 14:20:23,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:24,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:20:26,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96753.836
2025-03-09 14:20:26,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:26,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:20:28,892 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 19 -- loss: tensor([95652.6328], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:20:28,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95652.633
2025-03-09 14:20:28,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:29,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:20:31,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94026.047
2025-03-09 14:20:31,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:32,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:20:34,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93058.758
2025-03-09 14:20:34,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:35,260 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:20:37,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93148.688
2025-03-09 14:20:37,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:38,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:20:39,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92119.141
2025-03-09 14:20:39,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:40,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:20:42,716 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 24 -- loss: tensor([91746.2109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:20:42,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91746.211
2025-03-09 14:20:42,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:43,592 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 14:20:45,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91196.438
2025-03-09 14:20:45,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:46,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 14:20:48,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90028.094
2025-03-09 14:20:48,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:49,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 14:20:50,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90328.516
2025-03-09 14:20:50,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:51,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 14:20:53,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88990.086
2025-03-09 14:20:53,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:54,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 14:20:56,588 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 29 -- loss: tensor([88754.0078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:20:56,589 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88754.008
2025-03-09 14:20:56,589 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:20:57,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 14:20:59,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88787.008
2025-03-09 14:20:59,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:00,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 14:21:02,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88170.102
2025-03-09 14:21:02,181 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:03,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 14:21:04,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87318.328
2025-03-09 14:21:04,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:05,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 14:21:07,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86952.547
2025-03-09 14:21:07,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:08,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 14:21:10,540 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 34 -- loss: tensor([86621.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:21:10,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86621.75
2025-03-09 14:21:10,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:11,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 14:21:13,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85820.992
2025-03-09 14:21:13,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:14,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 14:21:16,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85667.344
2025-03-09 14:21:16,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:17,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 14:21:18,953 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85948.32
2025-03-09 14:21:18,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:19,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 14:21:21,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85102.695
2025-03-09 14:21:21,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:22,688 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 14:21:24,615 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 39 -- loss: tensor([85775.7031], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:21:24,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85775.703
2025-03-09 14:21:24,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:25,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 14:21:27,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83782.078
2025-03-09 14:21:27,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:28,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 14:21:30,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84473.031
2025-03-09 14:21:30,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:31,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 14:21:33,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83587.023
2025-03-09 14:21:33,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:34,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 14:21:35,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83982.031
2025-03-09 14:21:35,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:36,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 14:21:38,742 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 44 -- loss: tensor([83126.6797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:21:38,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83126.68
2025-03-09 14:21:38,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:39,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 14:21:41,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82927.422
2025-03-09 14:21:41,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:42,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 14:21:44,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82897.414
2025-03-09 14:21:44,386 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:45,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 14:21:47,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81268.727
2025-03-09 14:21:47,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:48,101 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 14:21:50,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81370.32
2025-03-09 14:21:50,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:51,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 14:21:52,962 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 49 -- loss: tensor([82343.7656], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:21:52,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82343.766
2025-03-09 14:21:52,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:53,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 14:21:55,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81048.891
2025-03-09 14:21:55,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:56,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 14:21:58,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81620.078
2025-03-09 14:21:58,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:21:59,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 14:22:01,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80607.805
2025-03-09 14:22:01,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:02,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 14:22:04,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80572.828
2025-03-09 14:22:04,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:05,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 14:22:07,130 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 54 -- loss: tensor([80079.7266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:22:07,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80079.727
2025-03-09 14:22:07,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:08,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 14:22:09,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79987.547
2025-03-09 14:22:09,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:10,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 14:22:12,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80190.664
2025-03-09 14:22:12,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:13,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 14:22:15,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79955.5
2025-03-09 14:22:15,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:16,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 14:22:18,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80060.922
2025-03-09 14:22:18,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:19,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 14:22:21,404 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 59 -- loss: tensor([79917.7812], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:22:21,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79917.781
2025-03-09 14:22:21,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:22,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 14:22:24,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79802.445
2025-03-09 14:22:24,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:25,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 14:22:27,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79536.477
2025-03-09 14:22:27,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:28,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 14:22:29,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79087.383
2025-03-09 14:22:29,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:30,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 14:22:32,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78442.242
2025-03-09 14:22:32,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:33,715 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 14:22:35,653 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 64 -- loss: tensor([78250.2344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:22:35,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78250.234
2025-03-09 14:22:35,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:36,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 14:22:38,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78884.227
2025-03-09 14:22:38,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:39,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 14:22:41,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78138.031
2025-03-09 14:22:41,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:42,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 14:22:44,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77980.68
2025-03-09 14:22:44,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:45,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 14:22:47,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77770.586
2025-03-09 14:22:47,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:47,929 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 14:22:49,901 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 69 -- loss: tensor([78499.3047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:22:49,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78499.305
2025-03-09 14:22:49,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:50,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:22:52,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77925.375
2025-03-09 14:22:52,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:53,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:22:55,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77219.375
2025-03-09 14:22:55,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:56,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:22:58,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77950.047
2025-03-09 14:22:58,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:22:59,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:23:01,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77610.719
2025-03-09 14:23:01,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:02,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:23:04,066 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 74 -- loss: tensor([77043.7188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:23:04,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77043.719
2025-03-09 14:23:04,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:04,979 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:23:06,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77733.984
2025-03-09 14:23:06,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:07,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:23:09,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75942.922
2025-03-09 14:23:09,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:10,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:23:12,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76631.57
2025-03-09 14:23:12,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:13,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:23:15,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76118.672
2025-03-09 14:23:15,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:16,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:23:18,256 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 79 -- loss: tensor([76458.0078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:23:18,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76458.008
2025-03-09 14:23:18,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:19,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:23:21,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76291.0
2025-03-09 14:23:21,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:22,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:23:24,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76050.562
2025-03-09 14:23:24,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:24,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:23:26,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75524.758
2025-03-09 14:23:26,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:27,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:23:29,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76545.484
2025-03-09 14:23:29,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:30,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:23:32,531 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 84 -- loss: tensor([75353.5156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:23:32,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75353.516
2025-03-09 14:23:32,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:33,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:23:35,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75808.656
2025-03-09 14:23:35,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:36,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:23:38,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75184.109
2025-03-09 14:23:38,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:39,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:23:41,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75388.445
2025-03-09 14:23:41,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:41,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:23:43,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74486.133
2025-03-09 14:23:43,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:44,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:23:46,676 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 89 -- loss: tensor([75057.0781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:23:46,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75057.078
2025-03-09 14:23:46,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:47,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:23:49,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74307.688
2025-03-09 14:23:49,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:50,513 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:23:52,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75697.648
2025-03-09 14:23:52,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:53,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:23:55,304 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73345.227
2025-03-09 14:23:55,304 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:56,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:23:58,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75425.945
2025-03-09 14:23:58,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:23:59,054 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:24:00,987 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 94 -- loss: tensor([73627.2344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:24:00,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73627.234
2025-03-09 14:24:00,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:01,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:24:03,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74120.477
2025-03-09 14:24:03,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:04,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:24:06,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75148.125
2025-03-09 14:24:06,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:07,552 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:24:09,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74162.453
2025-03-09 14:24:09,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:10,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:24:12,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74010.117
2025-03-09 14:24:12,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:13,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:24:15,162 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 99 -- loss: tensor([74827.5312], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:24:15,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74827.531
2025-03-09 14:24:15,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:17,041 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 4447.35645
2025-03-09 14:24:17,045 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 14:24:17,046 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:24:17,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:24:18,022 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 3818.04004
2025-03-09 14:24:18,027 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 14:24:18,028 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 
layer8: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :conv1d out_channels:17 kernel_size:4 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :deconv1d out_channels:19 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :deconv1d out_channels:42 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.13213050094522136 alpha:0.8019244656848048 weight_decay:1.540340583675447e-05 batch_size:32 epochs:100
2025-03-09 14:24:18,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:24:18,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:24:20,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 855427.688
2025-03-09 14:24:20,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:21,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:24:23,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1994605.625
2025-03-09 14:24:23,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:24,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:24:26,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:24:26,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:27,007 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:24:28,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1573907.625
2025-03-09 14:24:28,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:29,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:24:31,716 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 4 -- loss: tensor([814862.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:24:31,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814862.812
2025-03-09 14:24:31,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:32,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:24:34,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2331071.25
2025-03-09 14:24:34,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:35,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:24:37,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1336351.25
2025-03-09 14:24:37,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:38,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:24:40,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 830899.688
2025-03-09 14:24:40,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:41,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:24:43,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2191783.75
2025-03-09 14:24:43,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:43,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:24:45,831 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 9 -- loss: tensor([1001067.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:24:45,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1001067.062
2025-03-09 14:24:45,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:46,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:24:48,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1368138.0
2025-03-09 14:24:48,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:49,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:24:51,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1161149.875
2025-03-09 14:24:51,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:52,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:24:54,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1530884.125
2025-03-09 14:24:54,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:55,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:24:57,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1875682.125
2025-03-09 14:24:57,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:24:58,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:25:00,143 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 14 -- loss: tensor([907056.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:25:00,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 907056.375
2025-03-09 14:25:00,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:01,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:25:03,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1398393.25
2025-03-09 14:25:03,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:03,957 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:25:05,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1005721.562
2025-03-09 14:25:05,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:06,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:25:08,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 996920.0
2025-03-09 14:25:08,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:09,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:25:11,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1035277.75
2025-03-09 14:25:11,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:12,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:25:14,371 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 19 -- loss: tensor([990437.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:25:14,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 990437.75
2025-03-09 14:25:14,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:15,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:25:17,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 999115.5
2025-03-09 14:25:17,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:18,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:25:20,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 946756.0
2025-03-09 14:25:20,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:21,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:25:22,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 901149.312
2025-03-09 14:25:22,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:23,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:25:25,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 873315.75
2025-03-09 14:25:25,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:26,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:25:28,596 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 24 -- loss: tensor([946113.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:25:28,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 946113.688
2025-03-09 14:25:28,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:29,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 14:25:31,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 938920.75
2025-03-09 14:25:31,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:32,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 14:25:34,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 893707.312
2025-03-09 14:25:34,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:35,237 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 14:25:37,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 973020.0
2025-03-09 14:25:37,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:38,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 14:25:40,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 918432.562
2025-03-09 14:25:40,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:40,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 14:25:42,883 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 29 -- loss: tensor([909363.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:25:42,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 909363.438
2025-03-09 14:25:42,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:43,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 14:25:45,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 914807.312
2025-03-09 14:25:45,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:46,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 14:25:48,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 920544.312
2025-03-09 14:25:48,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:49,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 14:25:51,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 899362.938
2025-03-09 14:25:51,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:52,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 14:25:54,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 879549.375
2025-03-09 14:25:54,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:55,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 14:25:57,175 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 34 -- loss: tensor([946950.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:25:57,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 946950.938
2025-03-09 14:25:57,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:25:58,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 14:26:00,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 916966.625
2025-03-09 14:26:00,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:00,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 14:26:02,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 901984.438
2025-03-09 14:26:02,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:03,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 14:26:05,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 925534.188
2025-03-09 14:26:05,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:06,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 14:26:08,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 913782.125
2025-03-09 14:26:08,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:09,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 14:26:11,398 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 39 -- loss: tensor([924224.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:26:11,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 924224.312
2025-03-09 14:26:11,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:12,339 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 14:26:14,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 909782.875
2025-03-09 14:26:14,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:15,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 14:26:17,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 922375.875
2025-03-09 14:26:17,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:18,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 14:26:20,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 935022.688
2025-03-09 14:26:20,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:20,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 14:26:22,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 920968.375
2025-03-09 14:26:22,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:23,573 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 14:26:25,429 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 44 -- loss: tensor([928021.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:26:25,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 928021.875
2025-03-09 14:26:25,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:26,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 14:26:28,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 931163.25
2025-03-09 14:26:28,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:29,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 14:26:30,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 925019.375
2025-03-09 14:26:30,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:31,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 14:26:33,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 898642.5
2025-03-09 14:26:33,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:34,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 14:26:36,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 947518.438
2025-03-09 14:26:36,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:37,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 14:26:39,014 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 49 -- loss: tensor([930811.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:26:39,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 930811.25
2025-03-09 14:26:39,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:39,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 14:26:41,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 949370.5
2025-03-09 14:26:41,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:42,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 14:26:44,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 925653.312
2025-03-09 14:26:44,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:45,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 14:26:47,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 870708.0
2025-03-09 14:26:47,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:48,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 14:26:49,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 972956.5
2025-03-09 14:26:49,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:50,786 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 14:26:52,617 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 54 -- loss: tensor([879565.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:26:52,617 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 879565.5
2025-03-09 14:26:52,617 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:53,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 14:26:55,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 859956.062
2025-03-09 14:26:55,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:56,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 14:26:58,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 961301.25
2025-03-09 14:26:58,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:26:58,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 14:27:00,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 912503.625
2025-03-09 14:27:00,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:01,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 14:27:03,392 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1016232.25
2025-03-09 14:27:03,392 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:04,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 14:27:06,119 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 59 -- loss: tensor([877059.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:27:06,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 877059.688
2025-03-09 14:27:06,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:06,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 14:27:08,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 975130.312
2025-03-09 14:27:08,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:09,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 14:27:11,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 912180.25
2025-03-09 14:27:11,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:12,387 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 14:27:14,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 942694.438
2025-03-09 14:27:14,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:15,079 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 14:27:16,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 915839.75
2025-03-09 14:27:16,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:17,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 14:27:19,691 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 64 -- loss: tensor([941872.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:27:19,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 941872.312
2025-03-09 14:27:19,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:20,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 14:27:22,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 936540.5
2025-03-09 14:27:22,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:23,289 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 14:27:25,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 884061.688
2025-03-09 14:27:25,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:26,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 14:27:27,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 947372.812
2025-03-09 14:27:27,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:28,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 14:27:30,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 957046.125
2025-03-09 14:27:30,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:31,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 14:27:33,263 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 69 -- loss: tensor([947582.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:27:33,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 947582.812
2025-03-09 14:27:33,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:34,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:27:35,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 920042.562
2025-03-09 14:27:35,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:36,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:27:38,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 947396.125
2025-03-09 14:27:38,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:39,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:27:41,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 961380.0
2025-03-09 14:27:41,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:42,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:27:44,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 919894.625
2025-03-09 14:27:44,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:44,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:27:46,832 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 74 -- loss: tensor([922299.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:27:46,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 922299.688
2025-03-09 14:27:46,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:47,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:27:49,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 934904.5
2025-03-09 14:27:49,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:50,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:27:52,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 983378.25
2025-03-09 14:27:52,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:53,182 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:27:55,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 863574.625
2025-03-09 14:27:55,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:55,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:27:57,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1055392.875
2025-03-09 14:27:57,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:27:58,617 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:28:00,480 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 79 -- loss: tensor([882806.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:28:00,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 882806.625
2025-03-09 14:28:00,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:01,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:28:03,212 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 927990.75
2025-03-09 14:28:03,212 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:04,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:28:05,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 962403.125
2025-03-09 14:28:05,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:06,798 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:28:08,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1024889.438
2025-03-09 14:28:08,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:09,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:28:11,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 927232.562
2025-03-09 14:28:11,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:12,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:28:14,102 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 84 -- loss: tensor([945876.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:28:14,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 945876.312
2025-03-09 14:28:14,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:14,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:28:16,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 966513.125
2025-03-09 14:28:16,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:17,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:28:19,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 922844.438
2025-03-09 14:28:19,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:20,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:28:22,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 930942.875
2025-03-09 14:28:22,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:23,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:28:25,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 909872.438
2025-03-09 14:28:25,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:25,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:28:27,752 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 89 -- loss: tensor([939301.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:28:27,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 939301.0
2025-03-09 14:28:27,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:28,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:28:30,466 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 956252.688
2025-03-09 14:28:30,466 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:31,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:28:33,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 914271.375
2025-03-09 14:28:33,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:34,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:28:35,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1001627.062
2025-03-09 14:28:35,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:36,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:28:38,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 892240.5
2025-03-09 14:28:38,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:39,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:28:41,399 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 94 -- loss: tensor([928961.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:28:41,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 928961.25
2025-03-09 14:28:41,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:42,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:28:44,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 923370.812
2025-03-09 14:28:44,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:45,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:28:46,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 907480.438
2025-03-09 14:28:46,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:47,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:28:49,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 947716.438
2025-03-09 14:28:49,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:50,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:28:52,433 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 938357.25
2025-03-09 14:28:52,433 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:53,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:28:55,167 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 99 -- loss: tensor([972275.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:28:55,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 972275.188
2025-03-09 14:28:55,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:57,026 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 44541.28125
2025-03-09 14:28:57,030 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 14:28:57,031 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 
layer12: :fc act:selu out_features:200 bias:True input:11 learning:rmsprop lr:0.10330427658126845 alpha:0.9188290896159456 weight_decay:0.00046552104501895747 batch_size:32 epochs:100
2025-03-09 14:28:57,041 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:28:57,041 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:28:58,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1065155.125
2025-03-09 14:28:58,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:28:59,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:29:01,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 808067.875
2025-03-09 14:29:01,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:02,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:29:04,361 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 792652.375
2025-03-09 14:29:04,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:05,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:29:07,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 925906.5
2025-03-09 14:29:07,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:08,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:29:09,856 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 4 -- loss: tensor([836631.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:29:09,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 836631.688
2025-03-09 14:29:09,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:10,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:29:12,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 810423.188
2025-03-09 14:29:12,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:13,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:29:15,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 809550.812
2025-03-09 14:29:15,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:16,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:29:17,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 848682.438
2025-03-09 14:29:17,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:18,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:29:20,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 796615.125
2025-03-09 14:29:20,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:21,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:29:23,351 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 9 -- loss: tensor([912388.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:29:23,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 912388.375
2025-03-09 14:29:23,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:24,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:29:26,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 758872.625
2025-03-09 14:29:26,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:26,929 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:29:28,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 834385.312
2025-03-09 14:29:28,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:29,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:29:31,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 786024.5
2025-03-09 14:29:31,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:32,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:29:34,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 808391.938
2025-03-09 14:29:34,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:34,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:29:36,737 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 14 -- loss: tensor([748985.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:29:36,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 748985.625
2025-03-09 14:29:36,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:37,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:29:39,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 918084.25
2025-03-09 14:29:39,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:40,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:29:42,091 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 851585.375
2025-03-09 14:29:42,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:42,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:29:44,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 858789.938
2025-03-09 14:29:44,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:45,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:29:47,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 731445.312
2025-03-09 14:29:47,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:48,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:29:50,185 :: INFO :: evodenss.train.trainers :: [2051] -- [1.86s] TRAIN epoch 19 -- loss: tensor([795353.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:29:50,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 795353.875
2025-03-09 14:29:50,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:51,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:29:52,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 757837.375
2025-03-09 14:29:52,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:53,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:29:55,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 920272.75
2025-03-09 14:29:55,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:56,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:29:58,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 860253.688
2025-03-09 14:29:58,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:29:59,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:30:00,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 688112.062
2025-03-09 14:30:00,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:01,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:30:03,486 :: INFO :: evodenss.train.trainers :: [2051] -- [1.79s] TRAIN epoch 24 -- loss: tensor([900834.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:30:03,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 900834.625
2025-03-09 14:30:03,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:04,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 14:30:06,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 877744.062
2025-03-09 14:30:06,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:07,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 14:30:08,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 890449.0
2025-03-09 14:30:08,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:09,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 14:30:11,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 892822.062
2025-03-09 14:30:11,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:12,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 14:30:14,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 867784.812
2025-03-09 14:30:14,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:15,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 14:30:16,820 :: INFO :: evodenss.train.trainers :: [2051] -- [1.78s] TRAIN epoch 29 -- loss: tensor([888966.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:30:16,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 888966.875
2025-03-09 14:30:16,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:17,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 14:30:19,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 831281.75
2025-03-09 14:30:19,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:20,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 14:30:22,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1042858.0
2025-03-09 14:30:22,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:23,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 14:30:24,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 826897.938
2025-03-09 14:30:24,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:25,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 14:30:27,623 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 864421.5
2025-03-09 14:30:27,623 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:28,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 14:30:30,293 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 34 -- loss: tensor([872841.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:30:30,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 872841.875
2025-03-09 14:30:30,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:31,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 14:30:32,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 876068.562
2025-03-09 14:30:32,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:33,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 14:30:35,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 965683.562
2025-03-09 14:30:35,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:36,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 14:30:38,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 818970.25
2025-03-09 14:30:38,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:39,207 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 14:30:41,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 857425.375
2025-03-09 14:30:41,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:41,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 14:30:43,673 :: INFO :: evodenss.train.trainers :: [2051] -- [1.79s] TRAIN epoch 39 -- loss: tensor([899585.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:30:43,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 899585.812
2025-03-09 14:30:43,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:44,511 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 14:30:46,326 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 832590.125
2025-03-09 14:30:46,326 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:47,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 14:30:49,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 841311.438
2025-03-09 14:30:49,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:49,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 14:30:51,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 822945.312
2025-03-09 14:30:51,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:52,594 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 14:30:54,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 869886.062
2025-03-09 14:30:54,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:55,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 14:30:57,087 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 44 -- loss: tensor([857979.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:30:57,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 857979.062
2025-03-09 14:30:57,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:30:57,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 14:30:59,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 843529.938
2025-03-09 14:30:59,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:00,616 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 14:31:02,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 830259.062
2025-03-09 14:31:02,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:03,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 14:31:05,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 891506.438
2025-03-09 14:31:05,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:06,050 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 14:31:07,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 863271.688
2025-03-09 14:31:07,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:08,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 14:31:10,544 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 49 -- loss: tensor([859171.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:31:10,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 859171.5
2025-03-09 14:31:10,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:11,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 14:31:13,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 825057.0
2025-03-09 14:31:13,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:14,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 14:31:15,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 820563.188
2025-03-09 14:31:15,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:16,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 14:31:18,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 866389.125
2025-03-09 14:31:18,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:19,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 14:31:21,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 839187.812
2025-03-09 14:31:21,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:22,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 14:31:23,983 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 54 -- loss: tensor([857607.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:31:23,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 857607.125
2025-03-09 14:31:23,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:24,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 14:31:26,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 932403.188
2025-03-09 14:31:26,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:27,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 14:31:29,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 864249.688
2025-03-09 14:31:29,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:30,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 14:31:32,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 850448.688
2025-03-09 14:31:32,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:32,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 14:31:34,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 971849.562
2025-03-09 14:31:34,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:35,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 14:31:37,546 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 59 -- loss: tensor([815523.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:31:37,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 815523.125
2025-03-09 14:31:37,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:38,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 14:31:40,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 871117.0
2025-03-09 14:31:40,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:41,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 14:31:43,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 898305.312
2025-03-09 14:31:43,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:43,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 14:31:45,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 888446.25
2025-03-09 14:31:45,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:46,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 14:31:48,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 839111.312
2025-03-09 14:31:48,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:49,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 14:31:51,158 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 64 -- loss: tensor([861401.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:31:51,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 861401.812
2025-03-09 14:31:51,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:52,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 14:31:53,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 861219.75
2025-03-09 14:31:53,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:54,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 14:31:56,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 841428.688
2025-03-09 14:31:56,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:31:57,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 14:31:59,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 853044.062
2025-03-09 14:31:59,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:00,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 14:32:02,041 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 884729.938
2025-03-09 14:32:02,041 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:02,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 14:32:04,754 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 69 -- loss: tensor([833410.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:32:04,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 833410.5
2025-03-09 14:32:04,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:05,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:32:07,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 844764.062
2025-03-09 14:32:07,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:08,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:32:10,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 920720.812
2025-03-09 14:32:10,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:11,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:32:12,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 837598.25
2025-03-09 14:32:12,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:13,786 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:32:15,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 807520.375
2025-03-09 14:32:15,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:16,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:32:18,320 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 74 -- loss: tensor([845504.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:32:18,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 845504.625
2025-03-09 14:32:18,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:19,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:32:21,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 881497.188
2025-03-09 14:32:21,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:21,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:32:23,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 863915.5
2025-03-09 14:32:23,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:24,700 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:32:26,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 817893.688
2025-03-09 14:32:26,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:27,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:32:29,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 864726.5
2025-03-09 14:32:29,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:30,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:32:31,992 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 79 -- loss: tensor([875798.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:32:31,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 875798.688
2025-03-09 14:32:31,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:32,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:32:34,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 824847.25
2025-03-09 14:32:34,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:35,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:32:37,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 818926.188
2025-03-09 14:32:37,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:38,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:32:40,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 873015.688
2025-03-09 14:32:40,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:41,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:32:42,994 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814397.938
2025-03-09 14:32:42,994 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:43,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:32:45,706 :: INFO :: evodenss.train.trainers :: [2051] -- [1.84s] TRAIN epoch 84 -- loss: tensor([836803.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:32:45,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 836803.562
2025-03-09 14:32:45,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:46,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:32:48,389 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 855749.625
2025-03-09 14:32:48,389 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:49,289 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:32:51,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 829900.375
2025-03-09 14:32:51,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:51,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:32:53,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 726709.062
2025-03-09 14:32:53,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:54,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:32:56,423 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813522.125
2025-03-09 14:32:56,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:57,290 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:32:59,106 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 89 -- loss: tensor([802046.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:32:59,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 802046.25
2025-03-09 14:32:59,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:32:59,976 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:33:01,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 800497.938
2025-03-09 14:33:01,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:02,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:33:04,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 800074.438
2025-03-09 14:33:04,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:05,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:33:07,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 824461.75
2025-03-09 14:33:07,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:08,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:33:10,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 700816.5
2025-03-09 14:33:10,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:10,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:33:12,801 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 94 -- loss: tensor([788133.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:33:12,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 788133.688
2025-03-09 14:33:12,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:13,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:33:15,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 752042.312
2025-03-09 14:33:15,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:16,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:33:18,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 658274.875
2025-03-09 14:33:18,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:19,116 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:33:20,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 838969.062
2025-03-09 14:33:20,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:21,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:33:23,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 849883.125
2025-03-09 14:33:23,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:24,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:33:26,364 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 99 -- loss: tensor([651858.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:33:26,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 651858.0
2025-03-09 14:33:26,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:28,115 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 26888.01367
2025-03-09 14:33:28,115 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 14:33:28,116 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 1, id: 1
2025-03-09 14:33:28,116 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 2000, 1000, 1000]
2025-03-09 14:33:28,116 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 14:33:28,116 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Individuals trained for the minimum time: [True, False, True, True]
2025-03-09 14:33:28,116 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Min train time parent: idx: 0, id: 0, max fitness detected: 4447.35645
2025-03-09 14:33:28,116 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Fitnesses from min train individuals before selecting best individual: [4447.35645, 44541.28125, 26888.01367]
2025-03-09 14:33:28,116 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Individual 0 has its train extended. Current fitness 4447.35645
2025-03-09 14:33:28,120 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 14:33:28,121 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:33:28,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:33:29,038 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [4447.35645, 3818.04004, 44541.28125, 26888.01367]
2025-03-09 14:33:29,197 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([20730.1777], device='cuda:0')
2025-03-09 14:33:29,198 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 4: 3818.04004
2025-03-09 14:33:29,198 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3818.04004



2025-03-09 14:33:29,276 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 5
2025-03-09 14:33:29,276 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 14:33:29,284 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 total training time extended to 3000
2025-03-09 14:33:29,287 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a layer removed from Module 0: features; Position: 5
2025-03-09 14:33:29,287 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 4. Reused?: True
2025-03-09 14:33:29,288 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 14:33:29,288 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 14:33:29,289 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 14:33:29,290 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 14:33:29,292 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 4
2025-03-09 14:33:29,292 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 4
2025-03-09 14:33:29,293 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 14:33:29,294 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 14:33:29,294 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 14:33:29,295 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 14:33:29,295 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 14:33:29,296 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 14:33:29,296 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 14:33:29,299 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 14:33:29,302 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 2000 secs
2025-03-09 14:33:29,303 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:33:29,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:33:29,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:33:31,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 251562.828
2025-03-09 14:33:31,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:32,067 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:33:33,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 168955.75
2025-03-09 14:33:33,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:34,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:33:36,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 151061.656
2025-03-09 14:33:36,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:37,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:33:39,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 140939.656
2025-03-09 14:33:39,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:40,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:33:42,429 :: INFO :: evodenss.train.trainers :: [2051] -- [2.01s] TRAIN epoch 4 -- loss: tensor([134076.0781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:33:42,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 134076.078
2025-03-09 14:33:42,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:43,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:33:45,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 129721.305
2025-03-09 14:33:45,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:46,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:33:47,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 123849.039
2025-03-09 14:33:47,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:48,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:33:50,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 120166.078
2025-03-09 14:33:50,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:51,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:33:53,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 117648.508
2025-03-09 14:33:53,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:54,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:33:56,308 :: INFO :: evodenss.train.trainers :: [2051] -- [1.95s] TRAIN epoch 9 -- loss: tensor([114960.9219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:33:56,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114960.922
2025-03-09 14:33:56,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:57,181 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:33:59,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 113045.602
2025-03-09 14:33:59,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:33:59,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:34:01,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110670.562
2025-03-09 14:34:01,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:02,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:34:04,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108196.258
2025-03-09 14:34:04,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:05,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:34:07,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107766.508
2025-03-09 14:34:07,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:08,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:34:10,229 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 14 -- loss: tensor([105775.8359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:34:10,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105775.836
2025-03-09 14:34:10,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:11,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:34:13,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103610.148
2025-03-09 14:34:13,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:13,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:34:15,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102021.25
2025-03-09 14:34:15,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:16,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:34:18,623 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100698.812
2025-03-09 14:34:18,623 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:19,535 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:34:21,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100570.672
2025-03-09 14:34:21,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:22,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:34:24,249 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 19 -- loss: tensor([99827.4688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:34:24,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99827.469
2025-03-09 14:34:24,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:25,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:34:27,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97340.828
2025-03-09 14:34:27,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:27,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:34:29,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96769.336
2025-03-09 14:34:29,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:30,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:34:32,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96121.984
2025-03-09 14:34:32,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:33,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:34:35,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95368.766
2025-03-09 14:34:35,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:36,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:34:38,224 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 24 -- loss: tensor([93110.7734], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:34:38,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93110.773
2025-03-09 14:34:38,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:39,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 14:34:40,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93233.914
2025-03-09 14:34:40,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:41,820 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 14:34:43,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92832.328
2025-03-09 14:34:43,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:44,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 14:34:46,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91827.594
2025-03-09 14:34:46,489 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:47,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 14:34:49,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91508.18
2025-03-09 14:34:49,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:50,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 14:34:52,034 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 29 -- loss: tensor([91518.5312], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:34:52,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91518.531
2025-03-09 14:34:52,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:52,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 14:34:54,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90456.492
2025-03-09 14:34:54,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:55,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 14:34:57,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88868.664
2025-03-09 14:34:57,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:34:58,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 14:35:00,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88733.703
2025-03-09 14:35:00,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:01,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 14:35:03,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88861.062
2025-03-09 14:35:03,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:03,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 14:35:05,896 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 34 -- loss: tensor([87980.5078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:35:05,897 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87980.508
2025-03-09 14:35:05,897 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:06,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 14:35:08,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87658.641
2025-03-09 14:35:08,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:09,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 14:35:11,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86416.539
2025-03-09 14:35:11,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:12,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 14:35:14,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86771.125
2025-03-09 14:35:14,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:15,032 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 14:35:16,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86128.953
2025-03-09 14:35:16,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:17,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 14:35:19,856 :: INFO :: evodenss.train.trainers :: [2051] -- [2.0s] TRAIN epoch 39 -- loss: tensor([86513.5156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:35:19,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86513.516
2025-03-09 14:35:19,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:20,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 14:35:22,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85395.562
2025-03-09 14:35:22,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:23,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 14:35:25,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85413.641
2025-03-09 14:35:25,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:26,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 14:35:28,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84800.32
2025-03-09 14:35:28,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:29,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 14:35:31,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84728.523
2025-03-09 14:35:31,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:31,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 14:35:33,883 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 44 -- loss: tensor([84339.5703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:35:33,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84339.57
2025-03-09 14:35:33,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:34,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 14:35:36,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84576.148
2025-03-09 14:35:36,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:37,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 14:35:39,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83318.328
2025-03-09 14:35:39,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:40,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 14:35:42,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83566.297
2025-03-09 14:35:42,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:43,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 14:35:44,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83484.008
2025-03-09 14:35:44,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:45,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 14:35:47,750 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 49 -- loss: tensor([83498.9062], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:35:47,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83498.906
2025-03-09 14:35:47,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:48,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 14:35:50,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82556.836
2025-03-09 14:35:50,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:51,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 14:35:53,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82987.445
2025-03-09 14:35:53,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:54,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 14:35:56,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82827.406
2025-03-09 14:35:56,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:57,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 14:35:58,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82498.32
2025-03-09 14:35:58,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:35:59,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 14:36:01,701 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 54 -- loss: tensor([82534.7891], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:36:01,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82534.789
2025-03-09 14:36:01,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:02,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 14:36:04,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81311.773
2025-03-09 14:36:04,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:05,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 14:36:07,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81111.055
2025-03-09 14:36:07,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:08,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 14:36:10,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80896.57
2025-03-09 14:36:10,079 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:10,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 14:36:12,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80331.898
2025-03-09 14:36:12,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:13,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 14:36:15,694 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 59 -- loss: tensor([80196.8047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:36:15,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80196.805
2025-03-09 14:36:15,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:16,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 14:36:18,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80549.203
2025-03-09 14:36:18,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:19,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 14:36:21,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80450.156
2025-03-09 14:36:21,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:22,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 14:36:24,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79943.82
2025-03-09 14:36:24,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:25,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 14:36:26,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79128.898
2025-03-09 14:36:26,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:27,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 14:36:29,733 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 64 -- loss: tensor([80006.0469], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:36:29,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80006.047
2025-03-09 14:36:29,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:30,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 14:36:32,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79314.344
2025-03-09 14:36:32,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:33,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 14:36:35,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79343.578
2025-03-09 14:36:35,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:36,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 14:36:38,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78832.047
2025-03-09 14:36:38,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:39,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 14:36:40,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78690.914
2025-03-09 14:36:40,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:41,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 14:36:43,719 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 69 -- loss: tensor([78454.1641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:36:43,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78454.164
2025-03-09 14:36:43,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:44,602 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:36:46,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79187.805
2025-03-09 14:36:46,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:47,386 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:36:49,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78161.297
2025-03-09 14:36:49,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:50,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:36:52,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78012.164
2025-03-09 14:36:52,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:52,926 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:36:54,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78442.859
2025-03-09 14:36:54,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:55,700 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:36:57,582 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 74 -- loss: tensor([76993.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:36:57,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76993.75
2025-03-09 14:36:57,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:36:58,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:37:00,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76642.211
2025-03-09 14:37:00,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:01,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:37:03,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77895.125
2025-03-09 14:37:03,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:04,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:37:05,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77007.742
2025-03-09 14:37:05,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:06,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:37:08,734 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77678.977
2025-03-09 14:37:08,734 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:09,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:37:11,531 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 79 -- loss: tensor([76726.8281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:37:11,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76726.828
2025-03-09 14:37:11,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:12,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:37:14,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76431.031
2025-03-09 14:37:14,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:15,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:37:17,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76246.039
2025-03-09 14:37:17,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:17,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:37:19,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76517.617
2025-03-09 14:37:19,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:20,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:37:22,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75585.344
2025-03-09 14:37:22,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:23,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:37:25,535 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 84 -- loss: tensor([75942.5391], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:37:25,535 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75942.539
2025-03-09 14:37:25,535 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:26,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:37:28,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75625.352
2025-03-09 14:37:28,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:29,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:37:31,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76025.359
2025-03-09 14:37:31,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:32,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:37:33,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75751.695
2025-03-09 14:37:33,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:34,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:37:36,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75445.023
2025-03-09 14:37:36,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:37,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:37:39,550 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 89 -- loss: tensor([75213.6641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:37:39,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75213.664
2025-03-09 14:37:39,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:40,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:37:42,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74754.414
2025-03-09 14:37:42,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:43,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:37:45,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75567.484
2025-03-09 14:37:45,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:46,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:37:47,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74768.023
2025-03-09 14:37:47,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:48,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:37:50,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74222.648
2025-03-09 14:37:50,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:51,580 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:37:53,498 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 94 -- loss: tensor([74557.4219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:37:53,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74557.422
2025-03-09 14:37:53,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:54,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:37:56,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73958.336
2025-03-09 14:37:56,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:57,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:37:59,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73690.969
2025-03-09 14:37:59,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:37:59,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:38:01,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74245.141
2025-03-09 14:38:01,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:02,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:38:04,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73593.086
2025-03-09 14:38:04,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:05,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:38:07,449 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 99 -- loss: tensor([74160.6641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:38:07,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74160.664
2025-03-09 14:38:07,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:09,213 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 4048.47729
2025-03-09 14:38:09,217 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 14:38:09,218 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:38:09,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:38:10,141 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 3818.04004
2025-03-09 14:38:10,146 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 14:38:10,147 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:38:10,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:38:10,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:38:12,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 693125.625
2025-03-09 14:38:12,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:13,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:38:15,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 343091.25
2025-03-09 14:38:15,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:16,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:38:18,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 253088.188
2025-03-09 14:38:18,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:19,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:38:21,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 194008.562
2025-03-09 14:38:21,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:22,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:38:24,969 :: INFO :: evodenss.train.trainers :: [2051] -- [2.16s] TRAIN epoch 4 -- loss: tensor([144761.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:38:24,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 144761.297
2025-03-09 14:38:24,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:25,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:38:28,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 131332.547
2025-03-09 14:38:28,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:28,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:38:31,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 121636.664
2025-03-09 14:38:31,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:31,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:38:34,106 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 115783.766
2025-03-09 14:38:34,106 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:35,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:38:37,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110531.203
2025-03-09 14:38:37,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:38,073 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:38:40,229 :: INFO :: evodenss.train.trainers :: [2051] -- [2.15s] TRAIN epoch 9 -- loss: tensor([106918.2188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:38:40,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106918.219
2025-03-09 14:38:40,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:41,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:38:43,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103750.922
2025-03-09 14:38:43,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:44,187 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:38:46,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101093.07
2025-03-09 14:38:46,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:47,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:38:49,500 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99336.391
2025-03-09 14:38:49,500 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:50,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:38:52,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96796.406
2025-03-09 14:38:52,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:53,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:38:55,623 :: INFO :: evodenss.train.trainers :: [2051] -- [2.15s] TRAIN epoch 14 -- loss: tensor([95476.9688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:38:55,623 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95476.969
2025-03-09 14:38:55,623 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:56,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:38:58,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93507.203
2025-03-09 14:38:58,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:38:59,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:39:01,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92316.797
2025-03-09 14:39:01,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:02,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:39:04,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91831.688
2025-03-09 14:39:04,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:05,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:39:07,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89859.008
2025-03-09 14:39:07,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:08,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:39:10,796 :: INFO :: evodenss.train.trainers :: [2051] -- [2.14s] TRAIN epoch 19 -- loss: tensor([89897.2109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:39:10,796 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89897.211
2025-03-09 14:39:10,796 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:11,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:39:13,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88683.438
2025-03-09 14:39:13,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:14,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:39:16,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86757.773
2025-03-09 14:39:16,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:17,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:39:20,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86764.836
2025-03-09 14:39:20,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:20,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:39:23,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86794.195
2025-03-09 14:39:23,101 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:24,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:39:26,170 :: INFO :: evodenss.train.trainers :: [2051] -- [2.15s] TRAIN epoch 24 -- loss: tensor([85732.8047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:39:26,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85732.805
2025-03-09 14:39:26,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:27,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 14:39:29,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84569.344
2025-03-09 14:39:29,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:30,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 14:39:32,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84655.906
2025-03-09 14:39:32,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:33,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 14:39:35,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83971.516
2025-03-09 14:39:35,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:36,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 14:39:38,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83339.125
2025-03-09 14:39:38,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:39,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 14:39:41,556 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 29 -- loss: tensor([82266.8906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:39:41,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82266.891
2025-03-09 14:39:41,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:42,466 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 14:39:44,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82398.164
2025-03-09 14:39:44,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:45,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 14:39:47,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81781.18
2025-03-09 14:39:47,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:48,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 14:39:50,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80887.672
2025-03-09 14:39:50,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:51,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 14:39:53,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80341.094
2025-03-09 14:39:53,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:54,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 14:39:57,004 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 34 -- loss: tensor([81224.9688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:39:57,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81224.969
2025-03-09 14:39:57,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:39:57,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 14:40:00,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80160.375
2025-03-09 14:40:00,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:01,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 14:40:03,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79224.273
2025-03-09 14:40:03,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:04,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 14:40:06,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80116.32
2025-03-09 14:40:06,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:07,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 14:40:09,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79978.875
2025-03-09 14:40:09,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:10,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 14:40:12,475 :: INFO :: evodenss.train.trainers :: [2051] -- [2.16s] TRAIN epoch 39 -- loss: tensor([78058.5391], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:40:12,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78058.539
2025-03-09 14:40:12,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:13,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 14:40:15,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77600.062
2025-03-09 14:40:15,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:16,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 14:40:18,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78689.094
2025-03-09 14:40:18,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:19,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 14:40:21,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77047.562
2025-03-09 14:40:21,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:22,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 14:40:24,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77690.297
2025-03-09 14:40:24,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:25,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 14:40:28,038 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 44 -- loss: tensor([78181.3516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:40:28,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78181.352
2025-03-09 14:40:28,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:28,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 14:40:31,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76989.148
2025-03-09 14:40:31,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:32,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 14:40:34,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76838.344
2025-03-09 14:40:34,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:35,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 14:40:37,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75999.82
2025-03-09 14:40:37,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:38,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 14:40:40,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76317.484
2025-03-09 14:40:40,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:41,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 14:40:43,487 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 49 -- loss: tensor([76272.3516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:40:43,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76272.352
2025-03-09 14:40:43,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:44,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 14:40:46,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76175.68
2025-03-09 14:40:46,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:47,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 14:40:49,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74508.688
2025-03-09 14:40:49,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:50,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 14:40:52,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76717.055
2025-03-09 14:40:52,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:53,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 14:40:56,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75739.852
2025-03-09 14:40:56,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:40:57,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 14:40:59,239 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 54 -- loss: tensor([73238.3281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:40:59,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73238.328
2025-03-09 14:40:59,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:00,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 14:41:02,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75406.617
2025-03-09 14:41:02,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:03,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 14:41:05,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75447.352
2025-03-09 14:41:05,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:06,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 14:41:08,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73362.945
2025-03-09 14:41:08,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:09,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 14:41:11,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73467.453
2025-03-09 14:41:11,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:12,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 14:41:14,873 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 59 -- loss: tensor([74101.7422], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:41:14,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74101.742
2025-03-09 14:41:14,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:15,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 14:41:17,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74336.641
2025-03-09 14:41:17,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:18,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 14:41:21,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73396.188
2025-03-09 14:41:21,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:22,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 14:41:24,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74154.141
2025-03-09 14:41:24,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:25,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 14:41:27,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72276.594
2025-03-09 14:41:27,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:28,206 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 14:41:30,370 :: INFO :: evodenss.train.trainers :: [2051] -- [2.16s] TRAIN epoch 64 -- loss: tensor([73853.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:41:30,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73853.25
2025-03-09 14:41:30,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:31,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 14:41:33,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73268.469
2025-03-09 14:41:33,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:34,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 14:41:36,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72352.188
2025-03-09 14:41:36,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:37,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 14:41:39,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73141.586
2025-03-09 14:41:39,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:40,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 14:41:42,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71581.859
2025-03-09 14:41:42,533 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:43,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 14:41:45,588 :: INFO :: evodenss.train.trainers :: [2051] -- [2.15s] TRAIN epoch 69 -- loss: tensor([71978.2422], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:41:45,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71978.242
2025-03-09 14:41:45,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:46,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:41:48,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71865.0
2025-03-09 14:41:48,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:49,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:41:52,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72210.43
2025-03-09 14:41:52,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:52,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:41:55,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72468.242
2025-03-09 14:41:55,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:55,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:41:58,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71808.344
2025-03-09 14:41:58,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:41:58,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:42:01,161 :: INFO :: evodenss.train.trainers :: [2051] -- [2.16s] TRAIN epoch 74 -- loss: tensor([71085.0234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:42:01,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71085.023
2025-03-09 14:42:01,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:02,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:42:04,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71225.32
2025-03-09 14:42:04,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:05,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:42:07,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71316.305
2025-03-09 14:42:07,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:08,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:42:10,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72484.094
2025-03-09 14:42:10,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:11,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:42:13,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70609.57
2025-03-09 14:42:13,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:14,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:42:16,428 :: INFO :: evodenss.train.trainers :: [2051] -- [2.15s] TRAIN epoch 79 -- loss: tensor([70983.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:42:16,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70983.562
2025-03-09 14:42:16,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:17,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:42:19,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71100.125
2025-03-09 14:42:19,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:20,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:42:22,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69968.305
2025-03-09 14:42:22,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:23,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:42:25,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70945.773
2025-03-09 14:42:25,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:26,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:42:28,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70068.109
2025-03-09 14:42:28,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:29,623 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:42:31,789 :: INFO :: evodenss.train.trainers :: [2051] -- [2.16s] TRAIN epoch 84 -- loss: tensor([70457.6797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:42:31,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70457.68
2025-03-09 14:42:31,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:32,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:42:34,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70132.414
2025-03-09 14:42:34,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:35,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:42:37,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71142.977
2025-03-09 14:42:37,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:38,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:42:41,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69027.82
2025-03-09 14:42:41,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:41,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:42:44,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68782.547
2025-03-09 14:42:44,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:45,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:42:47,183 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 89 -- loss: tensor([70250.9141], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:42:47,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70250.914
2025-03-09 14:42:47,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:48,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:42:50,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69549.039
2025-03-09 14:42:50,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:51,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:42:53,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70761.719
2025-03-09 14:42:53,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:54,207 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:42:56,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69964.75
2025-03-09 14:42:56,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:42:57,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:42:59,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68378.32
2025-03-09 14:42:59,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:00,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:43:02,422 :: INFO :: evodenss.train.trainers :: [2051] -- [2.14s] TRAIN epoch 94 -- loss: tensor([69077.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:43:02,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69077.375
2025-03-09 14:43:02,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:03,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:43:05,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69600.352
2025-03-09 14:43:05,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:06,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:43:08,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68633.875
2025-03-09 14:43:08,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:09,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:43:11,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68109.906
2025-03-09 14:43:11,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:12,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:43:14,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68946.992
2025-03-09 14:43:14,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:15,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:43:17,673 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 99 -- loss: tensor([69539.7266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:43:17,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69539.727
2025-03-09 14:43:17,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:19,500 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 3758.75586
2025-03-09 14:43:19,504 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 14:43:19,505 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:9 
layer11: :fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:43:19,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:43:19,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:43:21,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 307409.812
2025-03-09 14:43:21,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:22,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:43:23,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 169062.891
2025-03-09 14:43:23,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:24,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:43:26,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 154457.938
2025-03-09 14:43:26,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:27,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:43:29,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 146206.906
2025-03-09 14:43:29,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:30,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:43:31,948 :: INFO :: evodenss.train.trainers :: [2051] -- [1.79s] TRAIN epoch 4 -- loss: tensor([138060.4531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:43:31,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 138060.453
2025-03-09 14:43:31,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:32,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:43:34,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 131427.938
2025-03-09 14:43:34,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:35,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:43:37,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 126332.617
2025-03-09 14:43:37,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:38,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:43:39,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 123014.445
2025-03-09 14:43:39,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:40,763 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:43:42,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 118863.414
2025-03-09 14:43:42,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:43,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:43:45,196 :: INFO :: evodenss.train.trainers :: [2051] -- [1.78s] TRAIN epoch 9 -- loss: tensor([116457.1641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:43:45,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 116457.164
2025-03-09 14:43:45,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:46,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:43:47,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114710.156
2025-03-09 14:43:47,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:48,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:43:50,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 111882.172
2025-03-09 14:43:50,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:51,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:43:53,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109087.312
2025-03-09 14:43:53,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:54,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:43:55,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107571.336
2025-03-09 14:43:55,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:56,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:43:58,686 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 14 -- loss: tensor([106861.3516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:43:58,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106861.352
2025-03-09 14:43:58,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:43:59,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:44:01,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104775.188
2025-03-09 14:44:01,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:02,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:44:04,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103540.711
2025-03-09 14:44:04,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:04,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:44:06,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102210.117
2025-03-09 14:44:06,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:07,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:44:09,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101414.445
2025-03-09 14:44:09,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:10,436 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:44:12,249 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 19 -- loss: tensor([100325.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:44:12,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100325.875
2025-03-09 14:44:12,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:13,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:44:14,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99961.328
2025-03-09 14:44:14,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:15,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:44:17,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99041.078
2025-03-09 14:44:17,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:18,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:44:20,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98409.281
2025-03-09 14:44:20,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:21,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:44:23,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97572.734
2025-03-09 14:44:23,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:24,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:44:25,957 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 24 -- loss: tensor([97562.4844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:44:25,957 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97562.484
2025-03-09 14:44:25,957 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:26,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 14:44:28,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97257.367
2025-03-09 14:44:28,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:29,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 14:44:31,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96824.07
2025-03-09 14:44:31,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:32,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 14:44:34,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95519.688
2025-03-09 14:44:34,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:35,054 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 14:44:36,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94355.555
2025-03-09 14:44:36,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:37,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 14:44:39,603 :: INFO :: evodenss.train.trainers :: [2051] -- [1.83s] TRAIN epoch 29 -- loss: tensor([93663.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:44:39,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93663.562
2025-03-09 14:44:39,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:40,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 14:44:42,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93622.203
2025-03-09 14:44:42,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:43,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 14:44:44,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93262.0
2025-03-09 14:44:44,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:45,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 14:44:47,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92660.602
2025-03-09 14:44:47,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:48,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 14:44:50,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92250.828
2025-03-09 14:44:50,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:51,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 14:44:52,866 :: INFO :: evodenss.train.trainers :: [2051] -- [1.77s] TRAIN epoch 34 -- loss: tensor([91550.1484], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:44:52,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91550.148
2025-03-09 14:44:52,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:53,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 14:44:55,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90389.117
2025-03-09 14:44:55,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:56,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 14:44:58,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90271.469
2025-03-09 14:44:58,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:44:59,041 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 14:45:00,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89994.75
2025-03-09 14:45:00,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:01,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 14:45:03,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89687.445
2025-03-09 14:45:03,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:04,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 14:45:06,158 :: INFO :: evodenss.train.trainers :: [2051] -- [1.79s] TRAIN epoch 39 -- loss: tensor([89177.8594], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:45:06,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89177.859
2025-03-09 14:45:06,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:07,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 14:45:08,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88483.688
2025-03-09 14:45:08,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:09,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 14:45:11,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89064.328
2025-03-09 14:45:11,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:12,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 14:45:14,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87472.367
2025-03-09 14:45:14,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:15,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 14:45:16,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87530.734
2025-03-09 14:45:16,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:17,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 14:45:19,558 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 44 -- loss: tensor([87192.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:45:19,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87192.688
2025-03-09 14:45:19,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:20,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 14:45:22,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86559.609
2025-03-09 14:45:22,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:23,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 14:45:24,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87112.789
2025-03-09 14:45:24,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:25,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 14:45:27,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85608.039
2025-03-09 14:45:27,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:28,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 14:45:30,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86221.336
2025-03-09 14:45:30,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:31,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 14:45:32,810 :: INFO :: evodenss.train.trainers :: [2051] -- [1.78s] TRAIN epoch 49 -- loss: tensor([85455.5312], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:45:32,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85455.531
2025-03-09 14:45:32,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:33,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 14:45:35,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84594.477
2025-03-09 14:45:35,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:36,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 14:45:38,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85364.656
2025-03-09 14:45:38,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:39,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 14:45:40,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83918.719
2025-03-09 14:45:40,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:41,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 14:45:43,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83373.062
2025-03-09 14:45:43,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:44,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 14:45:46,112 :: INFO :: evodenss.train.trainers :: [2051] -- [1.78s] TRAIN epoch 54 -- loss: tensor([83298.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:45:46,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83298.297
2025-03-09 14:45:46,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:46,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 14:45:48,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83353.492
2025-03-09 14:45:48,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:49,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 14:45:51,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82916.008
2025-03-09 14:45:51,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:52,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 14:45:54,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82691.508
2025-03-09 14:45:54,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:54,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 14:45:56,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82451.219
2025-03-09 14:45:56,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:45:57,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 14:45:59,444 :: INFO :: evodenss.train.trainers :: [2051] -- [1.78s] TRAIN epoch 59 -- loss: tensor([81604.3672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:45:59,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81604.367
2025-03-09 14:45:59,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:00,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 14:46:02,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81925.078
2025-03-09 14:46:02,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:02,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 14:46:04,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81040.297
2025-03-09 14:46:04,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:05,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 14:46:07,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80762.828
2025-03-09 14:46:07,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:08,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 14:46:10,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80812.781
2025-03-09 14:46:10,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:11,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 14:46:12,845 :: INFO :: evodenss.train.trainers :: [2051] -- [1.81s] TRAIN epoch 64 -- loss: tensor([81172.2422], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:46:12,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81172.242
2025-03-09 14:46:12,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:13,734 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 14:46:15,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80940.32
2025-03-09 14:46:15,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:16,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 14:46:18,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79907.094
2025-03-09 14:46:18,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:19,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 14:46:20,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80272.258
2025-03-09 14:46:20,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:21,769 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 14:46:23,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80325.25
2025-03-09 14:46:23,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:24,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 14:46:26,291 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 69 -- loss: tensor([80129.3359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:46:26,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80129.336
2025-03-09 14:46:26,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:27,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:46:28,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79419.406
2025-03-09 14:46:28,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:29,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:46:31,596 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79077.773
2025-03-09 14:46:31,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:32,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:46:34,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78842.938
2025-03-09 14:46:34,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:35,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:46:36,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78716.703
2025-03-09 14:46:36,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:37,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:46:39,649 :: INFO :: evodenss.train.trainers :: [2051] -- [1.82s] TRAIN epoch 74 -- loss: tensor([80011.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:46:39,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80011.875
2025-03-09 14:46:39,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:40,539 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:46:42,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78653.273
2025-03-09 14:46:42,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:43,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:46:45,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78103.719
2025-03-09 14:46:45,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:45,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:46:47,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78187.922
2025-03-09 14:46:47,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:48,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:46:50,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77985.75
2025-03-09 14:46:50,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:51,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:46:53,139 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 79 -- loss: tensor([77578.4219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:46:53,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77578.422
2025-03-09 14:46:53,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:54,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:46:55,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78086.367
2025-03-09 14:46:55,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:56,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:46:58,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79034.211
2025-03-09 14:46:58,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:46:59,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:47:01,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78003.648
2025-03-09 14:47:01,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:02,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:47:04,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76285.297
2025-03-09 14:47:04,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:04,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:47:06,716 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 84 -- loss: tensor([76822.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:47:06,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76822.562
2025-03-09 14:47:06,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:07,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:47:09,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76667.172
2025-03-09 14:47:09,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:10,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:47:12,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77559.75
2025-03-09 14:47:12,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:12,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:47:14,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75914.156
2025-03-09 14:47:14,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:15,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:47:17,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76700.578
2025-03-09 14:47:17,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:18,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:47:20,332 :: INFO :: evodenss.train.trainers :: [2051] -- [1.85s] TRAIN epoch 89 -- loss: tensor([77525.1719], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:47:20,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77525.172
2025-03-09 14:47:20,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:21,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:47:23,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75373.914
2025-03-09 14:47:23,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:23,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:47:25,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76481.641
2025-03-09 14:47:25,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:26,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:47:28,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75455.188
2025-03-09 14:47:28,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:29,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:47:31,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75386.75
2025-03-09 14:47:31,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:32,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:47:33,851 :: INFO :: evodenss.train.trainers :: [2051] -- [1.79s] TRAIN epoch 94 -- loss: tensor([76019.5078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:47:33,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76019.508
2025-03-09 14:47:33,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:34,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:47:36,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76650.453
2025-03-09 14:47:36,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:37,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:47:39,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75729.281
2025-03-09 14:47:39,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:40,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:47:41,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75343.07
2025-03-09 14:47:41,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:42,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:47:44,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75079.18
2025-03-09 14:47:44,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:45,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:47:47,389 :: INFO :: evodenss.train.trainers :: [2051] -- [1.8s] TRAIN epoch 99 -- loss: tensor([75378.0859], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:47:47,389 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75378.086
2025-03-09 14:47:47,389 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:49,269 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 3927.66016
2025-03-09 14:47:49,270 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 14:47:49,270 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 2, id: 2
2025-03-09 14:47:49,270 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [2000, 3000, 1000, 1000]
2025-03-09 14:47:49,270 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 14:47:49,273 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [4048.47729, 3818.04004, 3758.75586, 3927.66016]
2025-03-09 14:47:49,768 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([19622.5703], device='cuda:0')
2025-03-09 14:47:49,768 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 5: 3758.75586
2025-03-09 14:47:49,768 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3758.75586



2025-03-09 14:47:49,839 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 6
2025-03-09 14:47:49,839 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 14:47:49,848 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a layer removed from Module 0: features; Position: 5
2025-03-09 14:47:49,848 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 14:47:49,849 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 14:47:49,850 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 14:47:49,851 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 14:47:49,852 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 14:47:49,852 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 14:47:49,855 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a layer removed from Module 0: features; Position: 10
2025-03-09 14:47:49,856 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 10. Reused?: False
2025-03-09 14:47:49,856 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 14:47:49,857 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 14:47:49,858 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 14:47:49,858 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 14:47:49,859 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 14:47:49,860 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 14:47:49,860 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 14:47:49,863 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 12
2025-03-09 14:47:49,863 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 10
2025-03-09 14:47:49,864 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have an extra layer at Module 0: features; Position: 10. Reused?: False
2025-03-09 14:47:49,864 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 14:47:49,865 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 14:47:49,867 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 14:47:49,870 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 14:47:49,871 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 14:47:49,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:47:49,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:47:52,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 765254.938
2025-03-09 14:47:52,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:52,979 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:47:55,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 320873.5
2025-03-09 14:47:55,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:56,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:47:58,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 210512.984
2025-03-09 14:47:58,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:47:59,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:48:01,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 175085.016
2025-03-09 14:48:01,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:02,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:48:04,358 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 4 -- loss: tensor([152402.0781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:48:04,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 152402.078
2025-03-09 14:48:04,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:05,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:48:07,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 140366.797
2025-03-09 14:48:07,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:08,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:48:10,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 131075.406
2025-03-09 14:48:10,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:11,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:48:13,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 124104.258
2025-03-09 14:48:13,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:14,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:48:16,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 117578.523
2025-03-09 14:48:16,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:17,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:48:19,861 :: INFO :: evodenss.train.trainers :: [2051] -- [2.25s] TRAIN epoch 9 -- loss: tensor([114158.5391], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:48:19,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114158.539
2025-03-09 14:48:19,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:20,767 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:48:22,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110267.352
2025-03-09 14:48:22,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:23,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:48:26,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107840.5
2025-03-09 14:48:26,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:26,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:48:29,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105854.641
2025-03-09 14:48:29,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:30,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:48:32,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101849.914
2025-03-09 14:48:32,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:33,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:48:35,379 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 14 -- loss: tensor([101503.8516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:48:35,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101503.852
2025-03-09 14:48:35,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:36,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:48:38,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98732.891
2025-03-09 14:48:38,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:39,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:48:41,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98546.547
2025-03-09 14:48:41,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:42,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:48:44,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97309.18
2025-03-09 14:48:44,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:45,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:48:47,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95703.938
2025-03-09 14:48:47,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:48,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:48:50,854 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 19 -- loss: tensor([95377.8359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:48:50,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95377.836
2025-03-09 14:48:50,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:51,766 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:48:53,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94497.203
2025-03-09 14:48:53,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:54,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:48:57,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92884.758
2025-03-09 14:48:57,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:48:58,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:49:00,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92792.742
2025-03-09 14:49:00,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:01,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:49:03,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91393.336
2025-03-09 14:49:03,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:04,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:49:06,379 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 24 -- loss: tensor([91096.9219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:49:06,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91096.922
2025-03-09 14:49:06,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:07,303 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 14:49:09,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89955.734
2025-03-09 14:49:09,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:10,389 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 14:49:12,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89673.18
2025-03-09 14:49:12,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:13,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 14:49:15,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88541.344
2025-03-09 14:49:15,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:16,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 14:49:18,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88569.898
2025-03-09 14:49:18,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:19,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 14:49:21,901 :: INFO :: evodenss.train.trainers :: [2051] -- [2.2s] TRAIN epoch 29 -- loss: tensor([87617.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:49:21,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87617.688
2025-03-09 14:49:21,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:22,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 14:49:24,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87085.891
2025-03-09 14:49:24,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:25,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 14:49:28,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86823.016
2025-03-09 14:49:28,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:29,006 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 14:49:31,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87052.648
2025-03-09 14:49:31,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:32,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 14:49:34,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86684.047
2025-03-09 14:49:34,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:35,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 14:49:37,360 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 34 -- loss: tensor([86898.2578], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:49:37,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86898.258
2025-03-09 14:49:37,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:38,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 14:49:40,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85457.008
2025-03-09 14:49:40,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:41,338 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 14:49:43,511 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85825.977
2025-03-09 14:49:43,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:44,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 14:49:46,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84999.062
2025-03-09 14:49:46,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:47,535 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 14:49:49,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84840.758
2025-03-09 14:49:49,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:50,720 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 14:49:52,916 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 39 -- loss: tensor([83501.8203], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:49:52,916 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83501.82
2025-03-09 14:49:52,916 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:53,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 14:49:56,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83743.969
2025-03-09 14:49:56,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:49:56,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 14:49:59,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83559.438
2025-03-09 14:49:59,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:00,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 14:50:02,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83678.172
2025-03-09 14:50:02,212 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:03,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 14:50:05,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82315.008
2025-03-09 14:50:05,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:06,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 14:50:08,431 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 44 -- loss: tensor([83164.4688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:50:08,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83164.469
2025-03-09 14:50:08,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:09,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 14:50:11,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83029.938
2025-03-09 14:50:11,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:12,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 14:50:14,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81652.578
2025-03-09 14:50:14,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:15,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 14:50:17,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82369.844
2025-03-09 14:50:17,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:18,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 14:50:20,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81546.984
2025-03-09 14:50:20,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:21,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 14:50:24,131 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 49 -- loss: tensor([82255.5078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:50:24,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82255.508
2025-03-09 14:50:24,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:25,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 14:50:27,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81392.562
2025-03-09 14:50:27,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:28,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 14:50:30,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81752.727
2025-03-09 14:50:30,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:31,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 14:50:33,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81525.984
2025-03-09 14:50:33,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:34,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 14:50:36,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81547.836
2025-03-09 14:50:36,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:37,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 14:50:39,845 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 54 -- loss: tensor([80450.8281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:50:39,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80450.828
2025-03-09 14:50:39,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:40,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 14:50:42,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81035.414
2025-03-09 14:50:42,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:43,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 14:50:46,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80407.125
2025-03-09 14:50:46,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:47,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 14:50:49,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80545.664
2025-03-09 14:50:49,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:50,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 14:50:52,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79766.461
2025-03-09 14:50:52,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:53,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 14:50:55,612 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 59 -- loss: tensor([79731.3984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:50:55,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79731.398
2025-03-09 14:50:55,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:56,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 14:50:58,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79596.555
2025-03-09 14:50:58,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:50:59,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 14:51:01,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80200.727
2025-03-09 14:51:01,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:03,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 14:51:05,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79896.602
2025-03-09 14:51:05,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:06,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 14:51:08,511 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80044.812
2025-03-09 14:51:08,511 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:09,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 14:51:11,648 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 64 -- loss: tensor([79567.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:51:11,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79567.25
2025-03-09 14:51:11,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:12,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 14:51:14,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79803.281
2025-03-09 14:51:14,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:15,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 14:51:17,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78358.625
2025-03-09 14:51:17,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:18,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 14:51:21,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78988.219
2025-03-09 14:51:21,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:22,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 14:51:24,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78678.508
2025-03-09 14:51:24,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:25,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 14:51:27,494 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 69 -- loss: tensor([78287.5156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:51:27,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78287.516
2025-03-09 14:51:27,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:28,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:51:30,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78243.805
2025-03-09 14:51:30,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:31,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:51:33,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77502.727
2025-03-09 14:51:33,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:34,766 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:51:37,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77895.211
2025-03-09 14:51:37,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:37,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:51:40,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77771.555
2025-03-09 14:51:40,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:41,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:51:43,384 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 74 -- loss: tensor([78696.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:51:43,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78696.062
2025-03-09 14:51:43,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:44,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:51:46,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77443.547
2025-03-09 14:51:46,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:47,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:51:49,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78523.852
2025-03-09 14:51:49,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:50,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:51:53,008 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77555.836
2025-03-09 14:51:53,008 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:53,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:51:56,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77177.633
2025-03-09 14:51:56,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:51:57,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:51:59,352 :: INFO :: evodenss.train.trainers :: [2051] -- [2.2s] TRAIN epoch 79 -- loss: tensor([78037.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:51:59,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78037.312
2025-03-09 14:51:59,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:00,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:52:02,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77097.109
2025-03-09 14:52:02,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:03,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:52:05,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77792.461
2025-03-09 14:52:05,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:06,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:52:08,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76349.883
2025-03-09 14:52:08,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:09,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:52:12,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77263.781
2025-03-09 14:52:12,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:13,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:52:15,262 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 84 -- loss: tensor([76611.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:52:15,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76611.477
2025-03-09 14:52:15,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:16,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:52:18,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76234.914
2025-03-09 14:52:18,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:19,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:52:21,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76946.453
2025-03-09 14:52:21,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:22,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:52:24,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75589.297
2025-03-09 14:52:24,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:25,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:52:28,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76688.516
2025-03-09 14:52:28,058 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:29,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:52:31,233 :: INFO :: evodenss.train.trainers :: [2051] -- [2.2s] TRAIN epoch 89 -- loss: tensor([76189.9219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:52:31,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76189.922
2025-03-09 14:52:31,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:32,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:52:34,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75712.711
2025-03-09 14:52:34,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:35,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:52:37,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75134.609
2025-03-09 14:52:37,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:38,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:52:40,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75386.164
2025-03-09 14:52:40,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:41,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:52:43,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75087.359
2025-03-09 14:52:43,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:44,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:52:47,155 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 94 -- loss: tensor([75381.0156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:52:47,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75381.016
2025-03-09 14:52:47,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:48,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:52:50,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75568.68
2025-03-09 14:52:50,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:51,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:52:53,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74903.953
2025-03-09 14:52:53,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:54,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:52:56,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75879.625
2025-03-09 14:52:56,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:52:57,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:52:59,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74538.297
2025-03-09 14:52:59,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:00,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:53:03,159 :: INFO :: evodenss.train.trainers :: [2051] -- [2.2s] TRAIN epoch 99 -- loss: tensor([75156.9609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:53:03,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75156.961
2025-03-09 14:53:03,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:05,125 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 4168.11719
2025-03-09 14:53:05,129 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 14:53:05,130 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 
layer8: :deconv1d out_channels:12 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:91 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :deconv1d out_channels:77 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 
layer12: :fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.10134321967434042 beta1:0.8757456768906996 beta2:0.9185131468757451 weight_decay:7.996517212786006e-05 batch_size:32 epochs:100
2025-03-09 14:53:05,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:53:05,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:53:07,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 916670.812
2025-03-09 14:53:07,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:08,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:53:11,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:53:11,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:12,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:53:14,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:53:14,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:15,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:53:18,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 4803381.5
2025-03-09 14:53:18,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:19,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:53:21,631 :: INFO :: evodenss.train.trainers :: [2051] -- [2.46s] TRAIN epoch 4 -- loss: tensor([804812.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:53:21,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804812.062
2025-03-09 14:53:21,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:22,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:53:25,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 818853.562
2025-03-09 14:53:25,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:26,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:53:28,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 931315.875
2025-03-09 14:53:28,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:29,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:53:31,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3501521.5
2025-03-09 14:53:31,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:32,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:53:35,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 833478.062
2025-03-09 14:53:35,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:36,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:53:38,833 :: INFO :: evodenss.train.trainers :: [2051] -- [2.48s] TRAIN epoch 9 -- loss: tensor([842501.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:53:38,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 842501.938
2025-03-09 14:53:38,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:39,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:53:42,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1134832.25
2025-03-09 14:53:42,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:43,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:53:45,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2864791.75
2025-03-09 14:53:45,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:46,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:53:49,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:53:49,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:50,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:53:52,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1058648.125
2025-03-09 14:53:52,589 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:53,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:53:56,002 :: INFO :: evodenss.train.trainers :: [2051] -- [2.46s] TRAIN epoch 14 -- loss: tensor([1187249.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:53:56,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1187249.5
2025-03-09 14:53:56,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:53:56,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:53:59,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2371651.75
2025-03-09 14:53:59,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:00,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:54:02,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1567333.75
2025-03-09 14:54:02,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:03,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:54:06,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1267671.625
2025-03-09 14:54:06,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:07,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:54:09,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 871830.688
2025-03-09 14:54:09,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:10,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:54:13,200 :: INFO :: evodenss.train.trainers :: [2051] -- [2.46s] TRAIN epoch 19 -- loss: tensor([1635234.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:54:13,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1635234.875
2025-03-09 14:54:13,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:14,179 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:54:16,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1412862.875
2025-03-09 14:54:16,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:17,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:54:20,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1520228.75
2025-03-09 14:54:20,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:21,101 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:54:23,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804746.312
2025-03-09 14:54:23,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:24,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:54:26,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 970007.25
2025-03-09 14:54:26,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:27,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:54:30,366 :: INFO :: evodenss.train.trainers :: [2051] -- [2.44s] TRAIN epoch 24 -- loss: tensor([2320586.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:54:30,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2320586.25
2025-03-09 14:54:30,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:31,339 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 14:54:33,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1564767.25
2025-03-09 14:54:33,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:34,767 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 14:54:37,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1445744.5
2025-03-09 14:54:37,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:38,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 14:54:40,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 924401.25
2025-03-09 14:54:40,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:41,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 14:54:44,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1707206.0
2025-03-09 14:54:44,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:45,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 14:54:47,559 :: INFO :: evodenss.train.trainers :: [2051] -- [2.46s] TRAIN epoch 29 -- loss: tensor([1813917.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:54:47,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1813917.75
2025-03-09 14:54:47,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:48,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 14:54:51,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1158733.375
2025-03-09 14:54:51,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:52,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 14:54:54,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 816327.188
2025-03-09 14:54:54,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:55,466 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 14:54:57,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1172340.0
2025-03-09 14:54:57,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:54:58,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 14:55:01,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1928776.875
2025-03-09 14:55:01,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:02,339 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 14:55:04,803 :: INFO :: evodenss.train.trainers :: [2051] -- [2.46s] TRAIN epoch 34 -- loss: tensor([1229229.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:55:04,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1229229.5
2025-03-09 14:55:04,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:05,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 14:55:08,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 944945.938
2025-03-09 14:55:08,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:09,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 14:55:11,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1079400.75
2025-03-09 14:55:11,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:12,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 14:55:14,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1714544.25
2025-03-09 14:55:14,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:15,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 14:55:18,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1061495.125
2025-03-09 14:55:18,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:19,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 14:55:21,504 :: INFO :: evodenss.train.trainers :: [2051] -- [2.41s] TRAIN epoch 39 -- loss: tensor([1098806.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:55:21,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1098806.5
2025-03-09 14:55:21,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:22,387 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 14:55:24,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 876180.75
2025-03-09 14:55:24,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:25,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 14:55:28,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1112748.25
2025-03-09 14:55:28,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:29,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 14:55:31,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1156922.0
2025-03-09 14:55:31,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:32,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 14:55:34,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1061566.875
2025-03-09 14:55:34,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:35,616 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 14:55:38,028 :: INFO :: evodenss.train.trainers :: [2051] -- [2.41s] TRAIN epoch 44 -- loss: tensor([835341.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:55:38,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 835341.75
2025-03-09 14:55:38,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:38,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 14:55:41,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1285087.875
2025-03-09 14:55:41,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:42,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 14:55:44,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1393284.625
2025-03-09 14:55:44,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:45,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 14:55:47,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1243783.0
2025-03-09 14:55:47,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:48,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 14:55:51,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 900691.688
2025-03-09 14:55:51,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:52,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 14:55:54,541 :: INFO :: evodenss.train.trainers :: [2051] -- [2.44s] TRAIN epoch 49 -- loss: tensor([1055973.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:55:54,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1055973.375
2025-03-09 14:55:54,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:55,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 14:55:57,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1544676.125
2025-03-09 14:55:57,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:55:58,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 14:56:01,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1134703.75
2025-03-09 14:56:01,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:02,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 14:56:04,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 988094.625
2025-03-09 14:56:04,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:05,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 14:56:07,734 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 906096.25
2025-03-09 14:56:07,734 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:08,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 14:56:11,068 :: INFO :: evodenss.train.trainers :: [2051] -- [2.43s] TRAIN epoch 54 -- loss: tensor([1152307.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:56:11,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1152307.25
2025-03-09 14:56:11,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:11,951 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 14:56:14,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1578522.625
2025-03-09 14:56:14,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:15,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 14:56:17,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 990796.875
2025-03-09 14:56:17,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:18,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 14:56:20,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1003346.625
2025-03-09 14:56:20,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:21,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 14:56:24,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1065011.625
2025-03-09 14:56:24,260 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:25,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 14:56:27,590 :: INFO :: evodenss.train.trainers :: [2051] -- [2.44s] TRAIN epoch 59 -- loss: tensor([1757635.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:56:27,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1757635.0
2025-03-09 14:56:27,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:28,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 14:56:30,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1628336.125
2025-03-09 14:56:30,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:31,797 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 14:56:34,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 842196.438
2025-03-09 14:56:34,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:35,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 14:56:37,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1019801.125
2025-03-09 14:56:37,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:38,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 14:56:40,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1167408.625
2025-03-09 14:56:40,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:41,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 14:56:44,251 :: INFO :: evodenss.train.trainers :: [2051] -- [2.45s] TRAIN epoch 64 -- loss: tensor([1539838.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:56:44,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1539838.875
2025-03-09 14:56:44,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:45,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 14:56:47,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1560187.25
2025-03-09 14:56:47,539 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:48,423 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 14:56:50,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 966606.0
2025-03-09 14:56:50,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:51,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 14:56:54,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1031118.0
2025-03-09 14:56:54,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:55,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 14:56:57,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1512713.0
2025-03-09 14:56:57,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:56:58,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 14:57:00,744 :: INFO :: evodenss.train.trainers :: [2051] -- [2.39s] TRAIN epoch 69 -- loss: tensor([1172376.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:57:00,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1172376.25
2025-03-09 14:57:00,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:01,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 14:57:04,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1004721.938
2025-03-09 14:57:04,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:04,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 14:57:07,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1054288.5
2025-03-09 14:57:07,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:08,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 14:57:10,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1107232.625
2025-03-09 14:57:10,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:11,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 14:57:13,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1283976.375
2025-03-09 14:57:13,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:14,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 14:57:17,177 :: INFO :: evodenss.train.trainers :: [2051] -- [2.37s] TRAIN epoch 74 -- loss: tensor([1415350.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:57:17,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1415350.5
2025-03-09 14:57:17,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:18,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 14:57:20,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 988802.438
2025-03-09 14:57:20,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:21,489 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 14:57:23,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1180059.75
2025-03-09 14:57:23,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:24,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 14:57:27,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1343890.25
2025-03-09 14:57:27,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:28,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 14:57:30,594 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1248552.5
2025-03-09 14:57:30,594 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:31,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 14:57:33,887 :: INFO :: evodenss.train.trainers :: [2051] -- [2.4s] TRAIN epoch 79 -- loss: tensor([1259201.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:57:33,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1259201.25
2025-03-09 14:57:33,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:34,797 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 14:57:37,206 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1014053.875
2025-03-09 14:57:37,206 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:38,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 14:57:40,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1252152.625
2025-03-09 14:57:40,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:41,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 14:57:43,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1397855.875
2025-03-09 14:57:43,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:44,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 14:57:47,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1091194.5
2025-03-09 14:57:47,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:48,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 14:57:50,523 :: INFO :: evodenss.train.trainers :: [2051] -- [2.46s] TRAIN epoch 84 -- loss: tensor([968647.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:57:50,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 968647.688
2025-03-09 14:57:50,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:51,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 14:57:53,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1037391.312
2025-03-09 14:57:53,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:54,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 14:57:57,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1333785.75
2025-03-09 14:57:57,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:57:57,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 14:58:00,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1217124.5
2025-03-09 14:58:00,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:01,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 14:58:03,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1209654.75
2025-03-09 14:58:03,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:04,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 14:58:06,925 :: INFO :: evodenss.train.trainers :: [2051] -- [2.38s] TRAIN epoch 89 -- loss: tensor([931346.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:58:06,926 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 931346.938
2025-03-09 14:58:06,926 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:07,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 14:58:10,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1226004.0
2025-03-09 14:58:10,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:11,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 14:58:13,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1314655.5
2025-03-09 14:58:13,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:14,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 14:58:16,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1052353.875
2025-03-09 14:58:16,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:17,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 14:58:20,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1093931.0
2025-03-09 14:58:20,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:21,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 14:58:23,555 :: INFO :: evodenss.train.trainers :: [2051] -- [2.45s] TRAIN epoch 94 -- loss: tensor([1057024.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:58:23,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1057024.25
2025-03-09 14:58:23,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:24,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 14:58:26,806 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1179607.875
2025-03-09 14:58:26,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:27,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 14:58:30,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1097080.0
2025-03-09 14:58:30,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:30,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 14:58:33,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1159134.0
2025-03-09 14:58:33,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:34,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 14:58:36,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 906876.375
2025-03-09 14:58:36,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:37,513 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 14:58:39,922 :: INFO :: evodenss.train.trainers :: [2051] -- [2.41s] TRAIN epoch 99 -- loss: tensor([1041044.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:58:39,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1041044.938
2025-03-09 14:58:39,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:42,170 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 44567.78906
2025-03-09 14:58:42,174 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 14:58:42,176 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 
layer10: :deconv1d out_channels:52 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.24135323220063165 alpha:0.8661623565077308 weight_decay:5.1621231932745314e-05 batch_size:32 epochs:100
2025-03-09 14:58:42,187 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 14:58:42,187 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 14:58:44,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1102239.25
2025-03-09 14:58:44,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:45,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 14:58:47,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1291857.75
2025-03-09 14:58:47,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:48,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 14:58:50,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 830711.938
2025-03-09 14:58:50,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:51,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 14:58:53,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 4160031.5
2025-03-09 14:58:53,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:54,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 14:58:56,845 :: INFO :: evodenss.train.trainers :: [2051] -- [2.16s] TRAIN epoch 4 -- loss: tensor([805935.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:58:56,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805935.312
2025-03-09 14:58:56,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:58:57,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 14:58:59,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2333978.5
2025-03-09 14:58:59,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:00,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 14:59:02,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:59:02,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:03,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 14:59:06,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2467569.75
2025-03-09 14:59:06,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:06,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 14:59:09,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:59:09,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:10,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 14:59:12,337 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 9 -- loss: tensor([1152271.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:59:12,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1152271.125
2025-03-09 14:59:12,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:13,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 14:59:15,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1392017.75
2025-03-09 14:59:15,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:16,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 14:59:18,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1971294.625
2025-03-09 14:59:18,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:19,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 14:59:21,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 970427.812
2025-03-09 14:59:21,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:22,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 14:59:24,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:59:24,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:25,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 14:59:27,944 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 14 -- loss: tensor([1369050.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:59:27,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1369050.25
2025-03-09 14:59:27,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:28,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 14:59:31,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:59:31,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:31,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 14:59:34,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2586036.75
2025-03-09 14:59:34,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:35,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 14:59:37,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:59:37,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:38,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 14:59:40,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2276230.75
2025-03-09 14:59:40,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:41,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 14:59:43,448 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 19 -- loss: tensor([2748880.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:59:43,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2748880.0
2025-03-09 14:59:43,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:44,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 14:59:46,576 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 14:59:46,576 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:47,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 14:59:49,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 810129.375
2025-03-09 14:59:49,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:50,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 14:59:52,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1847972.75
2025-03-09 14:59:52,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:53,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 14:59:55,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2524805.25
2025-03-09 14:59:55,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:56,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 14:59:59,013 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 24 -- loss: tensor([3025667.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 14:59:59,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3025667.75
2025-03-09 14:59:59,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 14:59:59,916 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 15:00:02,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 860648.188
2025-03-09 15:00:02,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:03,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 15:00:05,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:00:05,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:06,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 15:00:08,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 4673301.0
2025-03-09 15:00:08,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:09,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 15:00:11,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1368765.625
2025-03-09 15:00:11,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:12,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 15:00:14,523 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 29 -- loss: tensor([871837.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:00:14,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 871837.812
2025-03-09 15:00:14,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:15,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 15:00:17,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2380018.25
2025-03-09 15:00:17,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:18,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 15:00:20,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:00:20,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:21,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 15:00:23,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2263582.75
2025-03-09 15:00:23,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:24,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 15:00:27,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1275946.5
2025-03-09 15:00:27,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:27,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 15:00:30,115 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 34 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:00:30,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:00:30,116 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:31,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 15:00:33,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2583008.75
2025-03-09 15:00:33,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:34,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 15:00:36,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1997196.125
2025-03-09 15:00:36,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:37,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 15:00:39,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 823590.25
2025-03-09 15:00:39,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:40,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 15:00:42,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2214056.5
2025-03-09 15:00:42,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:43,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 15:00:45,508 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 39 -- loss: tensor([938890.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:00:45,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 938890.938
2025-03-09 15:00:45,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:46,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 15:00:48,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2340382.5
2025-03-09 15:00:48,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:49,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 15:00:51,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1056190.5
2025-03-09 15:00:51,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:52,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 15:00:54,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 846585.438
2025-03-09 15:00:54,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:55,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 15:00:57,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1182955.375
2025-03-09 15:00:57,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:00:58,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 15:01:00,996 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 44 -- loss: tensor([1651020.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:01:00,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1651020.0
2025-03-09 15:01:00,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:01,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 15:01:04,060 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 960783.375
2025-03-09 15:01:04,060 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:04,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 15:01:07,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1262077.625
2025-03-09 15:01:07,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:08,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 15:01:10,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1066704.625
2025-03-09 15:01:10,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:11,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 15:01:13,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1362200.125
2025-03-09 15:01:13,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:14,187 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 15:01:16,345 :: INFO :: evodenss.train.trainers :: [2051] -- [2.16s] TRAIN epoch 49 -- loss: tensor([1106733.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:01:16,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1106733.875
2025-03-09 15:01:16,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:17,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 15:01:19,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1043999.375
2025-03-09 15:01:19,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:20,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 15:01:22,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1322851.375
2025-03-09 15:01:22,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:23,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 15:01:25,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1228229.5
2025-03-09 15:01:25,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:26,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 15:01:28,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1173103.375
2025-03-09 15:01:28,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:29,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 15:01:31,789 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 54 -- loss: tensor([1231617.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:01:31,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1231617.625
2025-03-09 15:01:31,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:32,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 15:01:34,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1206747.625
2025-03-09 15:01:34,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:35,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 15:01:37,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1239222.875
2025-03-09 15:01:37,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:38,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 15:01:41,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1214131.375
2025-03-09 15:01:41,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:41,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 15:01:44,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1082976.75
2025-03-09 15:01:44,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:45,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 15:01:47,198 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 59 -- loss: tensor([1430369.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:01:47,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1430369.875
2025-03-09 15:01:47,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:48,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 15:01:50,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1022701.062
2025-03-09 15:01:50,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:51,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 15:01:53,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1258057.625
2025-03-09 15:01:53,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:54,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 15:01:56,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1350301.375
2025-03-09 15:01:56,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:01:57,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 15:01:59,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1306006.875
2025-03-09 15:01:59,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:00,596 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 15:02:02,805 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 64 -- loss: tensor([1010820.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:02:02,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1010820.312
2025-03-09 15:02:02,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:03,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 15:02:05,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1292384.375
2025-03-09 15:02:05,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:06,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 15:02:10,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1345117.75
2025-03-09 15:02:10,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:11,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 15:02:13,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1231184.0
2025-03-09 15:02:13,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:14,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 15:02:16,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1302530.25
2025-03-09 15:02:16,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:17,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 15:02:20,045 :: INFO :: evodenss.train.trainers :: [2051] -- [2.26s] TRAIN epoch 69 -- loss: tensor([1335556.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:02:20,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1335556.75
2025-03-09 15:02:20,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:20,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 15:02:23,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1227055.125
2025-03-09 15:02:23,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:24,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 15:02:26,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1070598.75
2025-03-09 15:02:26,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:27,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 15:02:29,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1217709.75
2025-03-09 15:02:29,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:30,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 15:02:32,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1556927.125
2025-03-09 15:02:32,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:33,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 15:02:35,703 :: INFO :: evodenss.train.trainers :: [2051] -- [2.2s] TRAIN epoch 74 -- loss: tensor([1144413.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:02:35,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1144413.875
2025-03-09 15:02:35,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:36,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 15:02:38,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1094077.25
2025-03-09 15:02:38,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:39,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 15:02:41,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1547603.125
2025-03-09 15:02:41,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:42,925 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 15:02:45,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1259160.0
2025-03-09 15:02:45,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:46,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 15:02:48,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1002196.938
2025-03-09 15:02:48,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:49,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 15:02:51,421 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 79 -- loss: tensor([1355243.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:02:51,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1355243.875
2025-03-09 15:02:51,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:52,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 15:02:54,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1383133.375
2025-03-09 15:02:54,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:55,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 15:02:57,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1117799.125
2025-03-09 15:02:57,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:02:58,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 15:03:00,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1445263.25
2025-03-09 15:03:00,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:01,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 15:03:03,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1236412.625
2025-03-09 15:03:03,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:04,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 15:03:07,053 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 84 -- loss: tensor([1063738.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:03:07,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1063738.875
2025-03-09 15:03:07,054 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:07,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 15:03:10,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1241618.375
2025-03-09 15:03:10,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:11,101 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 15:03:13,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1311614.125
2025-03-09 15:03:13,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:14,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 15:03:16,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1192441.125
2025-03-09 15:03:16,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:17,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 15:03:19,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1094483.625
2025-03-09 15:03:19,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:20,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 15:03:22,708 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 89 -- loss: tensor([1325608.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:03:22,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1325608.5
2025-03-09 15:03:22,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:23,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 15:03:25,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1196562.875
2025-03-09 15:03:25,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:26,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 15:03:28,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1091476.75
2025-03-09 15:03:28,994 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:29,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 15:03:32,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1414214.5
2025-03-09 15:03:32,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:33,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 15:03:35,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1093803.25
2025-03-09 15:03:35,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:36,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 15:03:38,450 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 94 -- loss: tensor([1075311.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:03:38,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1075311.0
2025-03-09 15:03:38,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:39,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 15:03:41,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1277317.125
2025-03-09 15:03:41,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:42,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 15:03:44,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1480764.125
2025-03-09 15:03:44,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:45,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 15:03:47,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1318987.625
2025-03-09 15:03:47,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:48,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 15:03:50,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1043109.312
2025-03-09 15:03:50,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:51,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 15:03:54,118 :: INFO :: evodenss.train.trainers :: [2051] -- [2.2s] TRAIN epoch 99 -- loss: tensor([1204451.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:03:54,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1204451.625
2025-03-09 15:03:54,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:56,013 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 47737.57031
2025-03-09 15:03:56,017 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 15:03:56,018 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :conv1d out_channels:16 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100
2025-03-09 15:03:56,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 15:03:56,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 15:03:58,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1284107.625
2025-03-09 15:03:58,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:03:59,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 15:04:01,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:01,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:02,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 15:04:04,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:04,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:04,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 15:04:07,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:07,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:07,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 15:04:09,983 :: INFO :: evodenss.train.trainers :: [2051] -- [2.04s] TRAIN epoch 4 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:04:09,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:09,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:10,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 15:04:12,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:12,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:13,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 15:04:15,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:15,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:16,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 15:04:18,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:18,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:19,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 15:04:21,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:21,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:22,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 15:04:24,871 :: INFO :: evodenss.train.trainers :: [2051] -- [2.06s] TRAIN epoch 9 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:04:24,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:24,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:25,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 15:04:27,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:27,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:28,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 15:04:30,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:30,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:31,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 15:04:33,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:33,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:34,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 15:04:36,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:36,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:37,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 15:04:39,728 :: INFO :: evodenss.train.trainers :: [2051] -- [2.04s] TRAIN epoch 14 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:04:39,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:39,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:40,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 15:04:42,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:42,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:43,616 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 15:04:45,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:45,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:46,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 15:04:48,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:48,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:49,592 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 15:04:51,638 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:51,638 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:52,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 15:04:54,590 :: INFO :: evodenss.train.trainers :: [2051] -- [2.03s] TRAIN epoch 19 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:04:54,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:54,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:55,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 15:04:57,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:04:57,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:04:58,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 15:05:00,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:00,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:01,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 15:05:03,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:03,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:04,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 15:05:06,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:06,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:07,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 15:05:09,441 :: INFO :: evodenss.train.trainers :: [2051] -- [2.04s] TRAIN epoch 24 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:05:09,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:09,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:10,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 15:05:12,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:12,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:13,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 15:05:15,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:15,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:16,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 15:05:18,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:18,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:19,304 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 15:05:21,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:21,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:22,238 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 15:05:24,300 :: INFO :: evodenss.train.trainers :: [2051] -- [2.06s] TRAIN epoch 29 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:05:24,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:24,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:25,202 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 15:05:27,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:27,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:28,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 15:05:30,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:30,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:31,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 15:05:33,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:33,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:33,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 15:05:35,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:35,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:36,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 15:05:38,884 :: INFO :: evodenss.train.trainers :: [2051] -- [2.02s] TRAIN epoch 34 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:05:38,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:38,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:39,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 15:05:41,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:41,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:42,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 15:05:44,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:44,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:45,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 15:05:47,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:47,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:48,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 15:05:50,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:50,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:51,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 15:05:53,658 :: INFO :: evodenss.train.trainers :: [2051] -- [2.03s] TRAIN epoch 39 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:05:53,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:53,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:54,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 15:05:56,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:56,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:05:57,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 15:05:59,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:05:59,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:00,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 15:06:02,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:02,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:03,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 15:06:05,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:05,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:06,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 15:06:08,239 :: INFO :: evodenss.train.trainers :: [2051] -- [2.01s] TRAIN epoch 44 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:06:08,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:08,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:09,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 15:06:11,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:11,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:12,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 15:06:14,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:14,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:14,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 15:06:17,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:17,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:17,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 15:06:19,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:19,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:20,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 15:06:22,842 :: INFO :: evodenss.train.trainers :: [2051] -- [2.0s] TRAIN epoch 49 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:06:22,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:22,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:23,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 15:06:25,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:25,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:26,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 15:06:28,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:28,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:29,602 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 15:06:31,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:31,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:32,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 15:06:34,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:34,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:35,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 15:06:37,528 :: INFO :: evodenss.train.trainers :: [2051] -- [2.05s] TRAIN epoch 54 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:06:37,528 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:37,528 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:38,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 15:06:40,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:40,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:41,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 15:06:43,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:43,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:44,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 15:06:46,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:46,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:47,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 15:06:49,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:49,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:50,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 15:06:52,275 :: INFO :: evodenss.train.trainers :: [2051] -- [2.08s] TRAIN epoch 59 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:06:52,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:52,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:53,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 15:06:55,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:55,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:56,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 15:06:58,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:06:58,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:06:59,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 15:07:01,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:01,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:02,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 15:07:04,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:04,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:04,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 15:07:07,030 :: INFO :: evodenss.train.trainers :: [2051] -- [2.03s] TRAIN epoch 64 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:07:07,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:07,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:07,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 15:07:10,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:10,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:10,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 15:07:12,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:12,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:13,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 15:07:15,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:15,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:16,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 15:07:18,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:18,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:19,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 15:07:21,889 :: INFO :: evodenss.train.trainers :: [2051] -- [2.05s] TRAIN epoch 69 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:07:21,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:21,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:22,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 15:07:24,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:24,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:25,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 15:07:27,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:27,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:28,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 15:07:30,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:30,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:31,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 15:07:33,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:33,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:34,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 15:07:36,844 :: INFO :: evodenss.train.trainers :: [2051] -- [2.05s] TRAIN epoch 74 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:07:36,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:36,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:37,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 15:07:39,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:39,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:40,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 15:07:42,897 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:42,897 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:43,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 15:07:45,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:45,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:46,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 15:07:48,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:48,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:49,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 15:07:51,950 :: INFO :: evodenss.train.trainers :: [2051] -- [2.06s] TRAIN epoch 79 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:07:51,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:51,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:52,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 15:07:54,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:54,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:55,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 15:07:57,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:07:57,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:07:58,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 15:08:00,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:00,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:01,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 15:08:03,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:03,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:04,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 15:08:06,846 :: INFO :: evodenss.train.trainers :: [2051] -- [2.06s] TRAIN epoch 84 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:08:06,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:06,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:07,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 15:08:09,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:09,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:10,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 15:08:12,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:12,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:13,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 15:08:15,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:15,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:16,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 15:08:18,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:18,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:19,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 15:08:21,946 :: INFO :: evodenss.train.trainers :: [2051] -- [2.08s] TRAIN epoch 89 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:08:21,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:21,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:22,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 15:08:24,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:24,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:25,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 15:08:27,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:27,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:28,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 15:08:30,979 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:30,979 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:31,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 15:08:33,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:33,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:34,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 15:08:36,975 :: INFO :: evodenss.train.trainers :: [2051] -- [2.06s] TRAIN epoch 94 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:08:36,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:36,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:37,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 15:08:39,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:39,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:40,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 15:08:42,978 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:42,978 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:43,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 15:08:45,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:45,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:46,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 15:08:48,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:48,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:49,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 15:08:52,016 :: INFO :: evodenss.train.trainers :: [2051] -- [2.07s] TRAIN epoch 99 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:08:52,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:08:52,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:53,987 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 44538.54297
2025-03-09 15:08:53,988 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 15:08:53,988 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 0, id: 0
2025-03-09 15:08:53,988 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 1000, 1000]
2025-03-09 15:08:53,988 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 15:08:53,991 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [4168.11719, 44567.78906, 47737.57031, 44538.54297]
2025-03-09 15:08:54,194 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([20844.1387], device='cuda:0')
2025-03-09 15:08:54,195 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 6: 4168.11719
2025-03-09 15:08:54,195 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3758.75586



2025-03-09 15:08:54,296 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 7
2025-03-09 15:08:54,296 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 15:08:54,305 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have an extra layer at Module 0: features; Position: 9. Reused?: False
2025-03-09 15:08:54,306 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 15:08:54,307 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 15:08:54,307 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 15:08:54,308 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 15:08:54,309 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 15:08:54,310 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 15:08:54,310 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 15:08:54,311 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 15:08:54,313 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a layer removed from Module 0: features; Position: 8
2025-03-09 15:08:54,314 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 8. Reused?: False
2025-03-09 15:08:54,314 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 15:08:54,315 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 15:08:54,316 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 15:08:54,317 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 15:08:54,317 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 15:08:54,318 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 15:08:54,319 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 15:08:54,319 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 15:08:54,322 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have an extra layer at Module 0: features; Position: 8. Reused?: False
2025-03-09 15:08:54,323 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 13
2025-03-09 15:08:54,323 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have an extra layer at Module 0: features; Position: 5. Reused?: False
2025-03-09 15:08:54,324 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 15:08:54,325 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 15:08:54,325 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 15:08:54,326 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 15:08:54,326 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 15:08:54,329 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 15:08:54,332 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 15:08:54,333 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 15:08:54,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 15:08:54,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 15:08:56,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 692320.312
2025-03-09 15:08:56,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:08:57,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 15:08:59,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 326408.625
2025-03-09 15:08:59,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:00,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 15:09:02,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 238977.688
2025-03-09 15:09:02,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:03,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 15:09:05,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 205425.453
2025-03-09 15:09:05,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:06,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 15:09:08,785 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 4 -- loss: tensor([150738.0156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:09:08,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 150738.016
2025-03-09 15:09:08,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:09,663 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 15:09:11,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 132594.297
2025-03-09 15:09:11,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:12,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 15:09:14,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 121095.734
2025-03-09 15:09:14,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:15,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 15:09:18,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 115033.031
2025-03-09 15:09:18,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:18,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 15:09:21,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109937.398
2025-03-09 15:09:21,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:22,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 15:09:24,176 :: INFO :: evodenss.train.trainers :: [2051] -- [2.16s] TRAIN epoch 9 -- loss: tensor([106089.5312], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:09:24,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106089.531
2025-03-09 15:09:24,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:25,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 15:09:27,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103391.891
2025-03-09 15:09:27,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:28,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 15:09:30,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101687.805
2025-03-09 15:09:30,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:31,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 15:09:33,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99162.391
2025-03-09 15:09:33,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:34,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 15:09:36,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98755.133
2025-03-09 15:09:36,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:37,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 15:09:39,648 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 14 -- loss: tensor([96511.3359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:09:39,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96511.336
2025-03-09 15:09:39,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:40,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 15:09:42,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94907.328
2025-03-09 15:09:42,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:43,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 15:09:45,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94525.969
2025-03-09 15:09:45,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:46,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 15:09:48,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93101.984
2025-03-09 15:09:48,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:49,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 15:09:52,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91792.797
2025-03-09 15:09:52,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:52,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 15:09:55,148 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 19 -- loss: tensor([90917.0703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:09:55,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90917.07
2025-03-09 15:09:55,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:56,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 15:09:58,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90917.352
2025-03-09 15:09:58,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:09:59,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 15:10:01,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89744.617
2025-03-09 15:10:01,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:02,212 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 15:10:04,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88260.898
2025-03-09 15:10:04,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:05,289 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 15:10:07,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88943.094
2025-03-09 15:10:07,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:08,355 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 15:10:10,561 :: INFO :: evodenss.train.trainers :: [2051] -- [2.2s] TRAIN epoch 24 -- loss: tensor([87453.9766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:10:10,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87453.977
2025-03-09 15:10:10,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:11,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 15:10:13,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86940.93
2025-03-09 15:10:13,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:14,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 15:10:16,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87016.75
2025-03-09 15:10:16,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:17,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 15:10:19,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86279.07
2025-03-09 15:10:19,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:20,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 15:10:22,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85471.633
2025-03-09 15:10:22,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:23,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 15:10:25,971 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 29 -- loss: tensor([85111.1328], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:10:25,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85111.133
2025-03-09 15:10:25,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:26,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 15:10:29,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84403.391
2025-03-09 15:10:29,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:29,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 15:10:32,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84138.32
2025-03-09 15:10:32,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:33,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 15:10:35,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83603.891
2025-03-09 15:10:35,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:36,091 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 15:10:38,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84002.961
2025-03-09 15:10:38,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:39,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 15:10:41,343 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 34 -- loss: tensor([82650.5312], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:10:41,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82650.531
2025-03-09 15:10:41,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:42,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 15:10:44,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83292.594
2025-03-09 15:10:44,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:45,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 15:10:47,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83571.008
2025-03-09 15:10:47,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:48,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 15:10:51,067 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82345.406
2025-03-09 15:10:51,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:51,976 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 15:10:54,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81395.812
2025-03-09 15:10:54,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:55,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 15:10:57,238 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 39 -- loss: tensor([81111.7422], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:10:57,238 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81111.742
2025-03-09 15:10:57,238 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:10:58,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 15:11:00,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80211.023
2025-03-09 15:11:00,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:01,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 15:11:03,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80752.719
2025-03-09 15:11:03,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:04,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 15:11:06,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80368.039
2025-03-09 15:11:06,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:07,413 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 15:11:09,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80795.156
2025-03-09 15:11:09,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:10,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 15:11:12,692 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 44 -- loss: tensor([79931.3906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:11:12,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79931.391
2025-03-09 15:11:12,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:13,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 15:11:15,769 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78982.586
2025-03-09 15:11:15,769 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:16,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 15:11:18,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80520.781
2025-03-09 15:11:18,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:19,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 15:11:22,025 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79296.391
2025-03-09 15:11:22,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:22,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 15:11:25,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79616.422
2025-03-09 15:11:25,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:26,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 15:11:28,217 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 49 -- loss: tensor([78686.3984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:11:28,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78686.398
2025-03-09 15:11:28,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:29,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 15:11:31,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78967.453
2025-03-09 15:11:31,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:32,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 15:11:34,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78361.781
2025-03-09 15:11:34,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:35,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 15:11:37,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77848.617
2025-03-09 15:11:37,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:38,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 15:11:40,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77368.109
2025-03-09 15:11:40,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:41,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 15:11:43,669 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 54 -- loss: tensor([77232.8672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:11:43,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77232.867
2025-03-09 15:11:43,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:44,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 15:11:46,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77132.258
2025-03-09 15:11:46,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:47,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 15:11:49,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77342.109
2025-03-09 15:11:49,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:50,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 15:11:52,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77299.227
2025-03-09 15:11:52,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:53,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 15:11:56,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77117.336
2025-03-09 15:11:56,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:11:57,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 15:11:59,190 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 59 -- loss: tensor([77436.5234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:11:59,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77436.523
2025-03-09 15:11:59,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:00,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 15:12:02,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76698.594
2025-03-09 15:12:02,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:03,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 15:12:05,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76038.242
2025-03-09 15:12:05,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:06,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 15:12:08,454 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76635.68
2025-03-09 15:12:08,454 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:09,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 15:12:11,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77195.812
2025-03-09 15:12:11,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:12,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 15:12:14,630 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 64 -- loss: tensor([76999.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:12:14,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76999.312
2025-03-09 15:12:14,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:15,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 15:12:17,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73927.023
2025-03-09 15:12:17,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:18,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 15:12:20,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76174.844
2025-03-09 15:12:20,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:21,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 15:12:23,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75230.797
2025-03-09 15:12:23,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:24,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 15:12:26,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74898.758
2025-03-09 15:12:26,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:27,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 15:12:30,002 :: INFO :: evodenss.train.trainers :: [2051] -- [2.13s] TRAIN epoch 69 -- loss: tensor([74963.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:12:30,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74963.477
2025-03-09 15:12:30,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:30,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 15:12:33,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75712.781
2025-03-09 15:12:33,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:34,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 15:12:36,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76208.766
2025-03-09 15:12:36,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:37,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 15:12:39,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74035.977
2025-03-09 15:12:39,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:40,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 15:12:42,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74393.781
2025-03-09 15:12:42,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:43,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 15:12:45,385 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 74 -- loss: tensor([74708.7812], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:12:45,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74708.781
2025-03-09 15:12:45,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:46,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 15:12:48,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73688.57
2025-03-09 15:12:48,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:49,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 15:12:51,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74274.672
2025-03-09 15:12:51,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:52,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 15:12:54,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74729.211
2025-03-09 15:12:54,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:55,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 15:12:57,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74639.984
2025-03-09 15:12:57,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:12:58,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 15:13:00,803 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 79 -- loss: tensor([73090.0391], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:13:00,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73090.039
2025-03-09 15:13:00,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:01,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 15:13:03,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73166.922
2025-03-09 15:13:03,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:04,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 15:13:06,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74229.984
2025-03-09 15:13:06,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:07,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 15:13:10,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73809.531
2025-03-09 15:13:10,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:10,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 15:13:13,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71999.008
2025-03-09 15:13:13,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:13,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 15:13:16,167 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 84 -- loss: tensor([72540.8281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:13:16,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72540.828
2025-03-09 15:13:16,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:17,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 15:13:19,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74663.188
2025-03-09 15:13:19,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:20,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 15:13:22,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72545.484
2025-03-09 15:13:22,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:23,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 15:13:25,433 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71987.133
2025-03-09 15:13:25,433 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:26,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 15:13:28,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72824.578
2025-03-09 15:13:28,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:29,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 15:13:31,598 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 89 -- loss: tensor([74365.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:13:31,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74365.562
2025-03-09 15:13:31,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:32,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 15:13:34,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71719.211
2025-03-09 15:13:34,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:35,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 15:13:37,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70840.25
2025-03-09 15:13:37,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:38,662 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 15:13:40,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71201.219
2025-03-09 15:13:40,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:41,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 15:13:43,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72997.078
2025-03-09 15:13:43,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:44,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 15:13:47,033 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 94 -- loss: tensor([71207.6172], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:13:47,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71207.617
2025-03-09 15:13:47,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:47,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 15:13:50,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71092.078
2025-03-09 15:13:50,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:51,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 15:13:53,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71888.781
2025-03-09 15:13:53,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:54,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 15:13:56,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71701.523
2025-03-09 15:13:56,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:13:57,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 15:13:59,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71988.312
2025-03-09 15:13:59,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:00,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 15:14:02,456 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 99 -- loss: tensor([70923.6797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:14:02,456 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70923.68
2025-03-09 15:14:02,456 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:04,299 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 4082.02271
2025-03-09 15:14:04,303 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 15:14:04,304 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :deconv1d out_channels:92 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 15:14:04,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 15:14:04,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 15:14:07,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 935192.312
2025-03-09 15:14:07,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:08,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 15:14:11,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:14:11,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:12,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 15:14:16,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 829170.625
2025-03-09 15:14:16,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:17,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 15:14:20,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 847364.688
2025-03-09 15:14:20,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:21,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 15:14:24,481 :: INFO :: evodenss.train.trainers :: [2051] -- [3.19s] TRAIN epoch 4 -- loss: tensor([808524.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:14:24,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 808524.812
2025-03-09 15:14:24,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:25,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 15:14:28,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 839887.625
2025-03-09 15:14:28,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:29,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 15:14:32,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 831578.938
2025-03-09 15:14:32,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:33,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 15:14:36,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 824242.062
2025-03-09 15:14:36,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:37,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 15:14:41,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 855400.125
2025-03-09 15:14:41,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:42,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 15:14:45,306 :: INFO :: evodenss.train.trainers :: [2051] -- [3.21s] TRAIN epoch 9 -- loss: tensor([590912.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:14:45,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 590912.938
2025-03-09 15:14:45,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:46,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 15:14:49,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 283004.188
2025-03-09 15:14:49,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:50,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 15:14:53,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 180693.938
2025-03-09 15:14:53,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:54,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 15:14:58,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 149598.406
2025-03-09 15:14:58,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:14:59,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 15:15:02,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 131257.359
2025-03-09 15:15:02,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:03,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 15:15:06,398 :: INFO :: evodenss.train.trainers :: [2051] -- [3.2s] TRAIN epoch 14 -- loss: tensor([122397.2344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:15:06,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 122397.234
2025-03-09 15:15:06,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:07,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 15:15:10,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 118376.461
2025-03-09 15:15:10,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:11,592 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 15:15:14,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114379.18
2025-03-09 15:15:14,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:15,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 15:15:18,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109503.602
2025-03-09 15:15:18,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:19,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 15:15:23,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108906.203
2025-03-09 15:15:23,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:24,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 15:15:27,376 :: INFO :: evodenss.train.trainers :: [2051] -- [3.21s] TRAIN epoch 19 -- loss: tensor([105820.1094], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:15:27,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105820.109
2025-03-09 15:15:27,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:28,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 15:15:31,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103442.07
2025-03-09 15:15:31,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:32,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 15:15:35,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102533.094
2025-03-09 15:15:35,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:36,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 15:15:39,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99841.055
2025-03-09 15:15:39,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:40,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 15:15:44,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97240.156
2025-03-09 15:15:44,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:45,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 15:15:48,425 :: INFO :: evodenss.train.trainers :: [2051] -- [3.21s] TRAIN epoch 24 -- loss: tensor([98869.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:15:48,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98869.688
2025-03-09 15:15:48,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:49,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 15:15:52,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95885.781
2025-03-09 15:15:52,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:53,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 15:15:56,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94984.883
2025-03-09 15:15:56,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:15:57,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 15:16:01,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93718.805
2025-03-09 15:16:01,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:02,207 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 15:16:05,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92526.742
2025-03-09 15:16:05,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:06,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 15:16:09,661 :: INFO :: evodenss.train.trainers :: [2051] -- [3.23s] TRAIN epoch 29 -- loss: tensor([91654.7891], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:16:09,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91654.789
2025-03-09 15:16:09,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:10,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 15:16:13,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94620.023
2025-03-09 15:16:13,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:14,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 15:16:18,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90589.602
2025-03-09 15:16:18,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:19,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 15:16:22,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89853.633
2025-03-09 15:16:22,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:23,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 15:16:26,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88137.547
2025-03-09 15:16:26,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:27,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 15:16:30,841 :: INFO :: evodenss.train.trainers :: [2051] -- [3.22s] TRAIN epoch 34 -- loss: tensor([88084.3984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:16:30,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88084.398
2025-03-09 15:16:30,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:31,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 15:16:35,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89934.625
2025-03-09 15:16:35,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:36,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 15:16:39,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85975.07
2025-03-09 15:16:39,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:40,338 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 15:16:43,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87279.055
2025-03-09 15:16:43,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:44,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 15:16:47,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86280.961
2025-03-09 15:16:47,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:48,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 15:16:52,038 :: INFO :: evodenss.train.trainers :: [2051] -- [3.26s] TRAIN epoch 39 -- loss: tensor([86869.6641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:16:52,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86869.664
2025-03-09 15:16:52,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:53,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 15:16:56,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85709.234
2025-03-09 15:16:56,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:16:57,234 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 15:17:00,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88785.812
2025-03-09 15:17:00,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:01,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 15:17:04,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85565.297
2025-03-09 15:17:04,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:05,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 15:17:08,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85331.18
2025-03-09 15:17:08,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:09,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 15:17:13,201 :: INFO :: evodenss.train.trainers :: [2051] -- [3.28s] TRAIN epoch 44 -- loss: tensor([85460.0547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:17:13,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85460.055
2025-03-09 15:17:13,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:14,191 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 15:17:17,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84907.008
2025-03-09 15:17:17,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:18,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 15:17:21,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82730.828
2025-03-09 15:17:21,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:22,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 15:17:25,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83842.586
2025-03-09 15:17:25,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:26,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 15:17:30,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83664.688
2025-03-09 15:17:30,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:31,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 15:17:34,455 :: INFO :: evodenss.train.trainers :: [2051] -- [3.26s] TRAIN epoch 49 -- loss: tensor([82656.5703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:17:34,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82656.57
2025-03-09 15:17:34,456 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:35,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 15:17:38,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81823.477
2025-03-09 15:17:38,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:39,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 15:17:42,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82263.531
2025-03-09 15:17:42,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:43,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 15:17:47,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82372.484
2025-03-09 15:17:47,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:48,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 15:17:51,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82797.82
2025-03-09 15:17:51,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:52,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 15:17:55,772 :: INFO :: evodenss.train.trainers :: [2051] -- [3.22s] TRAIN epoch 54 -- loss: tensor([82989.9062], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:17:55,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82989.906
2025-03-09 15:17:55,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:17:56,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 15:18:00,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82471.336
2025-03-09 15:18:00,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:01,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 15:18:04,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81801.102
2025-03-09 15:18:04,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:05,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 15:18:08,511 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81190.766
2025-03-09 15:18:08,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:09,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 15:18:12,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82392.219
2025-03-09 15:18:12,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:13,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 15:18:17,070 :: INFO :: evodenss.train.trainers :: [2051] -- [3.28s] TRAIN epoch 59 -- loss: tensor([82017.3203], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:18:17,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82017.32
2025-03-09 15:18:17,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:18,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 15:18:21,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80909.727
2025-03-09 15:18:21,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:22,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 15:18:25,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78835.398
2025-03-09 15:18:25,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:26,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 15:18:29,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80947.766
2025-03-09 15:18:29,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:30,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 15:18:34,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80084.617
2025-03-09 15:18:34,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:35,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 15:18:38,382 :: INFO :: evodenss.train.trainers :: [2051] -- [3.25s] TRAIN epoch 64 -- loss: tensor([82201.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:18:38,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82201.953
2025-03-09 15:18:38,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:39,393 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 15:18:42,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78249.82
2025-03-09 15:18:42,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:43,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 15:18:46,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80359.945
2025-03-09 15:18:46,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:47,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 15:18:51,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77534.859
2025-03-09 15:18:51,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:52,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 15:18:55,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80638.469
2025-03-09 15:18:55,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:18:56,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 15:18:59,765 :: INFO :: evodenss.train.trainers :: [2051] -- [3.25s] TRAIN epoch 69 -- loss: tensor([78651.3359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:18:59,766 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78651.336
2025-03-09 15:18:59,766 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:00,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 15:19:04,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79316.328
2025-03-09 15:19:04,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:05,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 15:19:08,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79464.945
2025-03-09 15:19:08,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:09,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 15:19:12,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76995.359
2025-03-09 15:19:12,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:13,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 15:19:16,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79008.055
2025-03-09 15:19:16,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:17,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 15:19:21,122 :: INFO :: evodenss.train.trainers :: [2051] -- [3.32s] TRAIN epoch 74 -- loss: tensor([79454.2891], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:19:21,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79454.289
2025-03-09 15:19:21,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:22,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 15:19:25,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78494.305
2025-03-09 15:19:25,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:26,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 15:19:29,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77689.172
2025-03-09 15:19:29,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:30,584 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 15:19:33,829 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79439.867
2025-03-09 15:19:33,829 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:34,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 15:19:38,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76867.234
2025-03-09 15:19:38,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:39,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 15:19:42,316 :: INFO :: evodenss.train.trainers :: [2051] -- [3.22s] TRAIN epoch 79 -- loss: tensor([77806.3828], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:19:42,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77806.383
2025-03-09 15:19:42,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:43,355 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 15:19:46,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76922.43
2025-03-09 15:19:46,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:47,643 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 15:19:50,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77365.477
2025-03-09 15:19:50,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:51,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 15:19:55,179 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77701.484
2025-03-09 15:19:55,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:19:56,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 15:19:59,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75290.586
2025-03-09 15:19:59,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:00,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 15:20:03,703 :: INFO :: evodenss.train.trainers :: [2051] -- [3.25s] TRAIN epoch 84 -- loss: tensor([76885.6641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:20:03,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76885.664
2025-03-09 15:20:03,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:04,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 15:20:07,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76493.141
2025-03-09 15:20:07,953 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:08,994 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 15:20:12,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75914.547
2025-03-09 15:20:12,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:13,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 15:20:16,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75820.266
2025-03-09 15:20:16,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:17,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 15:20:20,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76001.32
2025-03-09 15:20:20,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:21,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 15:20:24,983 :: INFO :: evodenss.train.trainers :: [2051] -- [3.15s] TRAIN epoch 89 -- loss: tensor([76499.3047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:20:24,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76499.305
2025-03-09 15:20:24,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:25,916 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 15:20:29,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75782.695
2025-03-09 15:20:29,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:30,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 15:20:33,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75821.32
2025-03-09 15:20:33,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:34,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 15:20:37,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77567.43
2025-03-09 15:20:37,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:38,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 15:20:41,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77760.609
2025-03-09 15:20:41,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:42,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 15:20:45,549 :: INFO :: evodenss.train.trainers :: [2051] -- [3.16s] TRAIN epoch 94 -- loss: tensor([76255.8906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:20:45,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76255.891
2025-03-09 15:20:45,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:46,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 15:20:49,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75633.5
2025-03-09 15:20:49,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:50,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 15:20:53,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76378.062
2025-03-09 15:20:53,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:54,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 15:20:58,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72639.328
2025-03-09 15:20:58,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:20:58,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 15:21:02,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74320.008
2025-03-09 15:21:02,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:03,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 15:21:06,382 :: INFO :: evodenss.train.trainers :: [2051] -- [3.24s] TRAIN epoch 99 -- loss: tensor([73642.6953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:21:06,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73642.695
2025-03-09 15:21:06,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:08,348 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 4183.57715
2025-03-09 15:21:08,352 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 15:21:08,354 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :deconv1d out_channels:48 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:15 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:95 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.13608389601433019 alpha:0.9633416312369782 weight_decay:1.9178584481764503e-05 batch_size:32 epochs:100
2025-03-09 15:21:08,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 15:21:08,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 15:21:11,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 963840.938
2025-03-09 15:21:11,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:12,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 15:21:14,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:21:14,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:15,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 15:21:18,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:21:18,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:19,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 15:21:22,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:21:22,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:23,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 15:21:25,883 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 4 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:21:25,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:21:25,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:26,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 15:21:29,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:21:29,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:30,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 15:21:33,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 4757229.5
2025-03-09 15:21:33,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:34,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 15:21:36,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:21:36,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:37,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 15:21:40,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:21:40,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:41,355 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 15:21:44,037 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 9 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:21:44,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:21:44,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:45,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 15:21:47,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 7595153.0
2025-03-09 15:21:47,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:48,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 15:21:51,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:21:51,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:52,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 15:21:55,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:21:55,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:55,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 15:21:58,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:21:58,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:21:59,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 15:22:02,301 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 14 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:22:02,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:22:02,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:03,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 15:22:05,978 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:22:05,978 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:06,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 15:22:09,663 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:22:09,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:10,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 15:22:13,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:22:13,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:14,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 15:22:16,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3661812.5
2025-03-09 15:22:16,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:17,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 15:22:20,614 :: INFO :: evodenss.train.trainers :: [2051] -- [2.76s] TRAIN epoch 19 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:22:20,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:22:20,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:21,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 15:22:24,237 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813041.75
2025-03-09 15:22:24,237 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:25,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 15:22:27,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2947863.75
2025-03-09 15:22:27,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:28,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 15:22:31,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:22:31,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:32,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 15:22:35,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:22:35,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:36,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 15:22:38,836 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 24 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:22:38,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:22:38,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:39,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 15:22:42,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:22:42,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:43,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 15:22:46,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1963062.625
2025-03-09 15:22:46,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:47,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 15:22:49,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:22:49,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:50,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 15:22:53,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:22:53,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:54,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 15:22:57,226 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 29 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:22:57,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:22:57,227 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:22:58,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 15:23:00,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:00,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:01,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 15:23:04,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:04,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:05,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 15:23:08,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:08,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:09,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 15:23:11,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:11,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:12,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 15:23:15,447 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 34 -- loss: tensor([2946813.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:23:15,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2946813.25
2025-03-09 15:23:15,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:16,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 15:23:19,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:19,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:20,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 15:23:22,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:22,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:23,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 15:23:26,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:26,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:27,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 15:23:30,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:30,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:31,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 15:23:33,743 :: INFO :: evodenss.train.trainers :: [2051] -- [2.67s] TRAIN epoch 39 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:23:33,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:33,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:34,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 15:23:37,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:37,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:38,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 15:23:41,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:41,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:42,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 15:23:44,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3811725.75
2025-03-09 15:23:44,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:45,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 15:23:48,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 836448.438
2025-03-09 15:23:48,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:49,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 15:23:52,328 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 44 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:23:52,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:52,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:53,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 15:23:55,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:55,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:23:56,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 15:23:59,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:23:59,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:00,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 15:24:03,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:24:03,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:04,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 15:24:06,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3276342.25
2025-03-09 15:24:06,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:07,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 15:24:10,616 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 49 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:24:10,616 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:24:10,616 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:11,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 15:24:14,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:24:14,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:15,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 15:24:17,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:24:17,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:18,763 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 15:24:21,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 829656.062
2025-03-09 15:24:21,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:22,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 15:24:25,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:24:25,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:26,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 15:24:28,650 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 54 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:24:28,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:24:28,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:29,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 15:24:32,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:24:32,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:33,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 15:24:35,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2159583.5
2025-03-09 15:24:35,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:36,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 15:24:39,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 810289.688
2025-03-09 15:24:39,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:40,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 15:24:43,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:24:43,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:43,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 15:24:46,652 :: INFO :: evodenss.train.trainers :: [2051] -- [2.66s] TRAIN epoch 59 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:24:46,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:24:46,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:47,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 15:24:50,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:24:50,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:51,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 15:24:53,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2783587.0
2025-03-09 15:24:53,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:54,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 15:24:57,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:24:57,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:24:58,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 15:25:01,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:25:01,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:02,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 15:25:04,935 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 64 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:25:04,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:25:04,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:05,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 15:25:08,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:25:08,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:09,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 15:25:12,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:25:12,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:13,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 15:25:15,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3180827.75
2025-03-09 15:25:15,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:16,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 15:25:19,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:25:19,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:20,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 15:25:23,036 :: INFO :: evodenss.train.trainers :: [2051] -- [2.67s] TRAIN epoch 69 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:25:23,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:25:23,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:23,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 15:25:26,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:25:26,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:27,637 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 15:25:30,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:25:30,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:31,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 15:25:33,951 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:25:33,951 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:34,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 15:25:37,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:25:37,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:38,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 15:25:41,298 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 74 -- loss: tensor([804485.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:25:41,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804485.812
2025-03-09 15:25:41,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:42,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 15:25:44,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 934121.688
2025-03-09 15:25:44,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:45,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 15:25:48,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:25:48,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:49,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 15:25:52,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:25:52,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:53,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 15:25:56,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 995490.438
2025-03-09 15:25:56,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:25:57,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 15:25:59,745 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 79 -- loss: tensor([857475.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:25:59,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 857475.688
2025-03-09 15:25:59,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:00,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 15:26:03,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 835489.938
2025-03-09 15:26:03,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:04,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 15:26:07,058 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1895943.25
2025-03-09 15:26:07,058 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:08,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 15:26:10,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:26:10,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:11,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 15:26:14,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:26:14,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:15,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 15:26:18,084 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 84 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:26:18,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:26:18,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:19,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 15:26:21,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:26:21,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:22,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 15:26:25,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:26:25,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:26,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 15:26:29,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 899181.562
2025-03-09 15:26:29,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:30,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 15:26:32,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1916854.75
2025-03-09 15:26:32,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:33,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 15:26:36,587 :: INFO :: evodenss.train.trainers :: [2051] -- [2.7s] TRAIN epoch 89 -- loss: tensor([808914.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:26:36,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 808914.938
2025-03-09 15:26:36,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:37,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 15:26:40,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:26:40,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:41,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 15:26:43,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 816750.188
2025-03-09 15:26:43,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:44,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 15:26:47,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:26:47,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:48,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 15:26:51,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 894929.25
2025-03-09 15:26:51,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:52,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 15:26:55,045 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 94 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:26:55,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:26:55,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:56,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 15:26:58,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:26:58,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:26:59,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 15:27:02,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:27:02,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:27:03,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 15:27:06,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1270300.5
2025-03-09 15:27:06,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:27:07,101 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 15:27:09,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 892266.562
2025-03-09 15:27:09,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:27:10,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 15:27:13,520 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 99 -- loss: tensor([2600977.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:27:13,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2600977.5
2025-03-09 15:27:13,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:27:15,502 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 44537.41797
2025-03-09 15:27:15,506 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 15:27:15,508 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:54 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:10 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:57 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 15:27:15,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 15:27:15,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 15:27:20,796 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1066973.625
2025-03-09 15:27:20,796 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:27:21,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 15:27:26,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:27:26,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:27:27,797 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 15:27:32,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:27:32,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:27:33,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 15:27:38,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:27:38,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:27:39,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 15:27:44,524 :: INFO :: evodenss.train.trainers :: [2051] -- [4.89s] TRAIN epoch 4 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:27:44,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:27:44,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:27:45,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 15:27:50,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:27:50,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:27:51,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 15:27:56,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:27:56,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:27:57,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 15:28:02,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:28:02,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:28:03,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 15:28:08,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:28:08,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:28:09,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 15:28:14,324 :: INFO :: evodenss.train.trainers :: [2051] -- [4.92s] TRAIN epoch 9 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:28:14,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:28:14,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:28:15,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 15:28:20,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:28:20,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:28:21,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 15:28:26,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:28:26,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:28:27,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 15:28:32,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:28:32,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:28:33,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 15:28:38,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:28:38,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:28:39,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 15:28:44,262 :: INFO :: evodenss.train.trainers :: [2051] -- [4.91s] TRAIN epoch 14 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:28:44,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:28:44,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:28:45,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 15:28:50,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:28:50,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:28:51,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 15:28:56,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:28:56,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:28:57,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 15:29:02,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:29:02,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:29:03,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 15:29:08,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:29:08,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:29:09,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 15:29:14,156 :: INFO :: evodenss.train.trainers :: [2051] -- [4.94s] TRAIN epoch 19 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:29:14,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:29:14,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:29:15,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 15:29:20,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:29:20,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:29:21,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 15:29:26,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:29:26,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:29:27,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 15:29:32,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:29:32,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:29:33,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 15:29:38,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:29:38,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:29:39,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 15:29:44,076 :: INFO :: evodenss.train.trainers :: [2051] -- [4.9s] TRAIN epoch 24 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:29:44,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:29:44,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:29:45,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 15:29:50,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:29:50,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:29:51,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 15:29:56,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:29:56,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:29:57,206 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 15:30:02,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:30:02,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:30:03,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 15:30:08,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:30:08,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:30:09,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 15:30:13,965 :: INFO :: evodenss.train.trainers :: [2051] -- [4.87s] TRAIN epoch 29 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:30:13,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:30:13,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:30:15,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 15:30:19,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:30:19,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:30:21,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 15:30:25,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:30:25,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:30:26,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 15:30:31,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:30:31,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:30:32,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 15:30:37,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:30:37,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:30:38,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 15:30:43,681 :: INFO :: evodenss.train.trainers :: [2051] -- [4.88s] TRAIN epoch 34 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:30:43,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:30:43,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:30:44,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 15:30:49,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:30:49,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:30:50,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 15:30:55,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:30:55,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:30:56,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 15:31:01,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:31:01,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:31:02,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 15:31:07,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:31:07,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:31:08,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 15:31:13,316 :: INFO :: evodenss.train.trainers :: [2051] -- [4.89s] TRAIN epoch 39 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:31:13,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:31:13,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:31:14,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 15:31:19,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:31:19,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:31:20,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 15:31:25,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:31:25,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:31:26,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 15:31:30,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:31:30,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:31:31,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 15:31:36,795 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:31:36,795 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:31:37,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 15:31:42,699 :: INFO :: evodenss.train.trainers :: [2051] -- [4.87s] TRAIN epoch 44 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:31:42,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:31:42,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:31:43,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 15:31:48,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:31:48,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:31:49,763 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 15:31:54,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:31:54,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:31:55,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 15:32:00,616 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:32:00,616 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:32:01,623 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 15:32:06,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:32:06,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:32:07,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 15:32:12,335 :: INFO :: evodenss.train.trainers :: [2051] -- [4.85s] TRAIN epoch 49 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:32:12,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:32:12,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:32:13,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 15:32:18,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:32:18,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:32:19,212 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 15:32:24,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:32:24,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:32:25,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 15:32:29,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:32:29,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:32:30,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 15:32:35,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:32:35,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:32:36,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 15:32:41,684 :: INFO :: evodenss.train.trainers :: [2051] -- [4.84s] TRAIN epoch 54 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:32:41,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:32:41,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:32:42,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 15:32:47,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:32:47,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:32:48,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 15:32:53,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:32:53,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:32:54,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 15:32:59,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:32:59,389 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:33:00,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 15:33:05,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:33:05,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:33:06,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 15:33:11,159 :: INFO :: evodenss.train.trainers :: [2051] -- [4.84s] TRAIN epoch 59 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:33:11,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:33:11,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:33:12,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 15:33:17,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:33:17,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:33:18,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 15:33:23,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:33:23,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:33:24,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 15:33:28,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:33:28,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:33:29,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 15:33:34,662 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:33:34,662 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:33:35,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 15:33:40,505 :: INFO :: evodenss.train.trainers :: [2051] -- [4.87s] TRAIN epoch 64 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:33:40,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:33:40,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:33:41,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 15:33:46,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:33:46,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:33:47,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 15:33:52,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:33:52,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:33:53,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 15:33:58,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:33:58,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:33:59,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 15:34:04,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:34:04,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:34:05,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 15:34:09,959 :: INFO :: evodenss.train.trainers :: [2051] -- [4.88s] TRAIN epoch 69 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:34:09,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:34:09,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:34:10,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 15:34:15,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:34:15,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:34:16,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 15:34:21,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:34:21,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:34:22,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 15:34:27,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:34:27,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:34:28,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 15:34:33,596 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:34:33,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:34:34,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 15:34:39,471 :: INFO :: evodenss.train.trainers :: [2051] -- [4.84s] TRAIN epoch 74 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:34:39,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:34:39,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:34:40,489 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 15:34:45,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:34:45,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:34:46,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 15:34:51,309 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:34:51,309 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:34:52,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 15:34:57,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:34:57,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:34:58,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 15:35:03,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:35:03,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:35:04,106 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 15:35:08,971 :: INFO :: evodenss.train.trainers :: [2051] -- [4.86s] TRAIN epoch 79 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:35:08,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:35:08,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:35:09,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 15:35:14,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:35:14,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:35:15,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 15:35:20,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:35:20,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:35:21,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 15:35:26,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:35:26,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:35:27,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 15:35:32,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:35:32,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:35:33,637 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 15:35:38,511 :: INFO :: evodenss.train.trainers :: [2051] -- [4.87s] TRAIN epoch 84 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:35:38,511 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:35:38,511 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:35:39,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 15:35:44,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805475.625
2025-03-09 15:35:44,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:35:45,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 15:35:50,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:35:50,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:35:51,260 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 15:35:56,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:35:56,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:35:57,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 15:36:02,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:36:02,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:36:03,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 15:36:07,911 :: INFO :: evodenss.train.trainers :: [2051] -- [4.89s] TRAIN epoch 89 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:36:07,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:36:07,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:36:08,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 15:36:13,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:36:13,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:36:14,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 15:36:19,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:36:19,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:36:20,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 15:36:25,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:36:25,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:36:26,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 15:36:31,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:36:31,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:36:32,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 15:36:37,440 :: INFO :: evodenss.train.trainers :: [2051] -- [4.86s] TRAIN epoch 94 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:36:37,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:36:37,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:36:38,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 15:36:43,304 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:36:43,304 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:36:44,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 15:36:49,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:36:49,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:36:50,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 15:36:55,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:36:55,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:36:56,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 15:37:01,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:37:01,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:02,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 15:37:06,941 :: INFO :: evodenss.train.trainers :: [2051] -- [4.88s] TRAIN epoch 99 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:37:06,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:37:06,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:11,447 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 44673.72266
2025-03-09 15:37:11,447 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 15:37:11,447 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 0, id: 0
2025-03-09 15:37:11,447 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 1000, 1000]
2025-03-09 15:37:11,447 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 15:37:11,450 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [4082.02271, 4183.57715, 44537.41797, 44673.72266]
2025-03-09 15:37:11,684 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([21541.8867], device='cuda:0')
2025-03-09 15:37:11,684 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 7: 4082.02271
2025-03-09 15:37:11,684 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3758.75586



2025-03-09 15:37:11,919 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 8
2025-03-09 15:37:11,919 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 15:37:11,928 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a layer removed from Module 0: features; Position: 11
2025-03-09 15:37:11,929 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have an extra layer at Module 0: features; Position: 4. Reused?: False
2025-03-09 15:37:11,930 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have an extra layer at Module 0: features; Position: 11. Reused?: False
2025-03-09 15:37:11,930 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 15:37:11,931 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 15:37:11,932 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 15:37:11,932 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 15:37:11,933 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 15:37:11,934 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 15:37:11,936 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a layer removed from Module 0: features; Position: 8
2025-03-09 15:37:11,937 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 5. Reused?: True
2025-03-09 15:37:11,937 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 9. Reused?: False
2025-03-09 15:37:11,938 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 15:37:11,939 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 15:37:11,939 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 15:37:11,940 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 15:37:11,941 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 15:37:11,941 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 15:37:11,944 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 10
2025-03-09 15:37:11,944 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 10
2025-03-09 15:37:11,945 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 15:37:11,945 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 15:37:11,946 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 15:37:11,946 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 15:37:11,949 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 15:37:11,952 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 15:37:11,953 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 15:37:11,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 15:37:11,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 15:37:14,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 653905.312
2025-03-09 15:37:14,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:15,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 15:37:17,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 345124.094
2025-03-09 15:37:17,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:18,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 15:37:20,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 256841.922
2025-03-09 15:37:20,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:21,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 15:37:23,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 216752.938
2025-03-09 15:37:23,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:24,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 15:37:26,917 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 4 -- loss: tensor([189313.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:37:26,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 189313.688
2025-03-09 15:37:26,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:27,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 15:37:30,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 144258.297
2025-03-09 15:37:30,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:31,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 15:37:33,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 129197.039
2025-03-09 15:37:33,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:34,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 15:37:36,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 119385.375
2025-03-09 15:37:36,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:37,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 15:37:39,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114304.82
2025-03-09 15:37:39,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:40,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 15:37:42,673 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 9 -- loss: tensor([111157.8984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:37:42,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 111157.898
2025-03-09 15:37:42,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:43,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 15:37:45,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106769.586
2025-03-09 15:37:45,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:46,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 15:37:49,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104421.523
2025-03-09 15:37:49,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:50,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 15:37:52,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102125.562
2025-03-09 15:37:52,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:53,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 15:37:55,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100477.914
2025-03-09 15:37:55,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:56,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 15:37:58,673 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 14 -- loss: tensor([98183.9844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:37:58,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98183.984
2025-03-09 15:37:58,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:37:59,638 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 15:38:01,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96423.516
2025-03-09 15:38:01,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:02,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 15:38:05,006 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95206.391
2025-03-09 15:38:05,006 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:05,951 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 15:38:08,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94055.578
2025-03-09 15:38:08,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:09,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 15:38:11,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94161.359
2025-03-09 15:38:11,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:12,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 15:38:14,512 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 19 -- loss: tensor([91208.7969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:38:14,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91208.797
2025-03-09 15:38:14,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:15,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 15:38:17,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90244.18
2025-03-09 15:38:17,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:18,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 15:38:21,058 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89454.391
2025-03-09 15:38:21,058 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:22,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 15:38:24,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89258.492
2025-03-09 15:38:24,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:25,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 15:38:27,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88423.211
2025-03-09 15:38:27,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:28,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 15:38:30,573 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 24 -- loss: tensor([87679.8828], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:38:30,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87679.883
2025-03-09 15:38:30,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:31,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 15:38:33,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87056.391
2025-03-09 15:38:33,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:34,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 15:38:36,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85662.289
2025-03-09 15:38:36,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:37,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 15:38:40,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85929.289
2025-03-09 15:38:40,106 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:41,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 15:38:43,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85303.492
2025-03-09 15:38:43,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:44,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 15:38:46,479 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 29 -- loss: tensor([86122.8594], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:38:46,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86122.859
2025-03-09 15:38:46,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:47,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 15:38:49,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84655.508
2025-03-09 15:38:49,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:50,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 15:38:52,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84241.797
2025-03-09 15:38:52,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:53,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 15:38:56,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82982.219
2025-03-09 15:38:56,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:38:57,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 15:38:59,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82840.172
2025-03-09 15:38:59,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:00,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 15:39:02,543 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 34 -- loss: tensor([82714.8281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:39:02,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82714.828
2025-03-09 15:39:02,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:03,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 15:39:05,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81726.305
2025-03-09 15:39:05,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:06,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 15:39:08,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81508.875
2025-03-09 15:39:08,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:09,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 15:39:12,181 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81187.75
2025-03-09 15:39:12,181 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:13,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 15:39:15,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80755.781
2025-03-09 15:39:15,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:16,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 15:39:18,519 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 39 -- loss: tensor([80872.4609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:39:18,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80872.461
2025-03-09 15:39:18,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:19,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 15:39:21,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79701.766
2025-03-09 15:39:21,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:22,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 15:39:24,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79629.383
2025-03-09 15:39:24,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:25,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 15:39:28,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79758.938
2025-03-09 15:39:28,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:29,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 15:39:31,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78529.828
2025-03-09 15:39:31,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:32,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 15:39:34,440 :: INFO :: evodenss.train.trainers :: [2051] -- [2.2s] TRAIN epoch 44 -- loss: tensor([80007.3594], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:39:34,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80007.359
2025-03-09 15:39:34,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:35,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 15:39:37,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77855.234
2025-03-09 15:39:37,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:38,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 15:39:40,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79173.328
2025-03-09 15:39:40,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:41,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 15:39:43,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77183.812
2025-03-09 15:39:43,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:44,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 15:39:47,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78123.469
2025-03-09 15:39:47,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:47,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 15:39:50,263 :: INFO :: evodenss.train.trainers :: [2051] -- [2.27s] TRAIN epoch 49 -- loss: tensor([76810.6094], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:39:50,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76810.609
2025-03-09 15:39:50,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:51,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 15:39:53,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76580.047
2025-03-09 15:39:53,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:54,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 15:39:56,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77310.023
2025-03-09 15:39:56,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:39:57,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 15:39:59,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77007.211
2025-03-09 15:39:59,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:00,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 15:40:02,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76634.25
2025-03-09 15:40:02,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:03,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 15:40:06,157 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 54 -- loss: tensor([77061.1016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:40:06,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77061.102
2025-03-09 15:40:06,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:07,116 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 15:40:09,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75546.68
2025-03-09 15:40:09,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:10,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 15:40:12,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77344.953
2025-03-09 15:40:12,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:13,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 15:40:15,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76354.867
2025-03-09 15:40:15,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:16,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 15:40:18,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76230.406
2025-03-09 15:40:18,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:19,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 15:40:22,098 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 59 -- loss: tensor([75432.3203], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:40:22,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75432.32
2025-03-09 15:40:22,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:23,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 15:40:25,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76060.461
2025-03-09 15:40:25,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:26,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 15:40:28,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75117.109
2025-03-09 15:40:28,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:29,423 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 15:40:31,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73674.883
2025-03-09 15:40:31,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:32,602 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 15:40:34,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74815.867
2025-03-09 15:40:34,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:35,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 15:40:38,014 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 64 -- loss: tensor([75535.2891], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:40:38,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75535.289
2025-03-09 15:40:38,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:38,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 15:40:41,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75331.602
2025-03-09 15:40:41,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:42,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 15:40:44,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74538.789
2025-03-09 15:40:44,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:45,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 15:40:47,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72953.336
2025-03-09 15:40:47,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:48,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 15:40:50,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74334.352
2025-03-09 15:40:50,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:51,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 15:40:53,998 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 69 -- loss: tensor([73298.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:40:53,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73298.688
2025-03-09 15:40:53,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:54,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 15:40:57,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73110.273
2025-03-09 15:40:57,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:40:58,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 15:41:00,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73894.922
2025-03-09 15:41:00,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:01,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 15:41:03,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72811.023
2025-03-09 15:41:03,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:04,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 15:41:06,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73658.93
2025-03-09 15:41:06,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:07,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 15:41:09,930 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 74 -- loss: tensor([73029.3828], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:41:09,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73029.383
2025-03-09 15:41:09,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:10,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 15:41:13,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72317.398
2025-03-09 15:41:13,116 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:14,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 15:41:16,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72041.32
2025-03-09 15:41:16,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:17,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 15:41:19,593 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72754.516
2025-03-09 15:41:19,593 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:20,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 15:41:22,769 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73391.102
2025-03-09 15:41:22,769 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:23,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 15:41:25,961 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 79 -- loss: tensor([72093.2812], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:41:25,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72093.281
2025-03-09 15:41:25,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:26,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 15:41:29,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72317.289
2025-03-09 15:41:29,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:30,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 15:41:32,386 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72033.266
2025-03-09 15:41:32,386 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:33,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 15:41:35,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71292.805
2025-03-09 15:41:35,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:36,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 15:41:38,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71740.359
2025-03-09 15:41:38,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:39,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 15:41:41,944 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 84 -- loss: tensor([73230.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:41:41,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73230.938
2025-03-09 15:41:41,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:42,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 15:41:45,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72829.031
2025-03-09 15:41:45,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:46,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 15:41:48,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73739.719
2025-03-09 15:41:48,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:49,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 15:41:51,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71775.789
2025-03-09 15:41:51,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:52,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 15:41:54,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71764.953
2025-03-09 15:41:54,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:55,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 15:41:58,150 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 89 -- loss: tensor([73152.7969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:41:58,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73152.797
2025-03-09 15:41:58,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:41:59,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 15:42:01,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72689.227
2025-03-09 15:42:01,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:02,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 15:42:04,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72088.422
2025-03-09 15:42:04,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:05,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 15:42:07,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71214.742
2025-03-09 15:42:07,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:08,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 15:42:10,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69975.734
2025-03-09 15:42:10,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:11,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 15:42:14,216 :: INFO :: evodenss.train.trainers :: [2051] -- [2.26s] TRAIN epoch 94 -- loss: tensor([70029.1328], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:42:14,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70029.133
2025-03-09 15:42:14,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:15,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 15:42:17,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71687.547
2025-03-09 15:42:17,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:18,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 15:42:20,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70135.812
2025-03-09 15:42:20,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:21,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 15:42:23,994 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71365.539
2025-03-09 15:42:23,994 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:24,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 15:42:27,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70543.414
2025-03-09 15:42:27,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:28,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 15:42:30,389 :: INFO :: evodenss.train.trainers :: [2051] -- [2.26s] TRAIN epoch 99 -- loss: tensor([69417.7422], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:42:30,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69417.742
2025-03-09 15:42:30,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:32,390 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 3986.04321
2025-03-09 15:42:32,394 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 15:42:32,395 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 15:42:32,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 15:42:32,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 15:42:35,348 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 487407.406
2025-03-09 15:42:35,348 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:36,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 15:42:39,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 206545.938
2025-03-09 15:42:39,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:40,073 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 15:42:42,715 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 161781.453
2025-03-09 15:42:42,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:43,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 15:42:46,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 143412.141
2025-03-09 15:42:46,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:47,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 15:42:50,153 :: INFO :: evodenss.train.trainers :: [2051] -- [2.8s] TRAIN epoch 4 -- loss: tensor([129347.9844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:42:50,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 129347.984
2025-03-09 15:42:50,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:51,181 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 15:42:53,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 120017.852
2025-03-09 15:42:53,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:54,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 15:42:57,593 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112579.906
2025-03-09 15:42:57,593 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:42:58,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 15:43:01,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109149.891
2025-03-09 15:43:01,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:02,266 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 15:43:04,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105489.109
2025-03-09 15:43:04,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:05,951 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 15:43:08,650 :: INFO :: evodenss.train.trainers :: [2051] -- [2.7s] TRAIN epoch 9 -- loss: tensor([102658.8438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:43:08,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102658.844
2025-03-09 15:43:08,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:09,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 15:43:12,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101242.398
2025-03-09 15:43:12,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:13,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 15:43:16,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97822.328
2025-03-09 15:43:16,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:17,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 15:43:19,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96474.219
2025-03-09 15:43:19,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:20,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 15:43:23,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94198.297
2025-03-09 15:43:23,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:24,533 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 15:43:27,194 :: INFO :: evodenss.train.trainers :: [2051] -- [2.66s] TRAIN epoch 14 -- loss: tensor([93210.1016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:43:27,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93210.102
2025-03-09 15:43:27,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:28,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 15:43:30,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91307.094
2025-03-09 15:43:30,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:31,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 15:43:34,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90203.695
2025-03-09 15:43:34,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:35,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 15:43:38,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89491.43
2025-03-09 15:43:38,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:39,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 15:43:41,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88304.805
2025-03-09 15:43:41,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:42,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 15:43:45,642 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 19 -- loss: tensor([87029.6016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:43:45,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87029.602
2025-03-09 15:43:45,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:46,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 15:43:49,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86266.742
2025-03-09 15:43:49,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:50,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 15:43:53,055 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84760.867
2025-03-09 15:43:53,055 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:54,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 15:43:56,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84919.883
2025-03-09 15:43:56,766 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:43:57,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 15:44:00,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83353.797
2025-03-09 15:44:00,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:01,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 15:44:04,189 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 24 -- loss: tensor([83016.3359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:44:04,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83016.336
2025-03-09 15:44:04,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:05,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 15:44:07,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82685.156
2025-03-09 15:44:07,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:08,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 15:44:11,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81661.25
2025-03-09 15:44:11,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:12,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 15:44:15,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80675.562
2025-03-09 15:44:15,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:16,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 15:44:18,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79871.195
2025-03-09 15:44:18,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:19,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 15:44:22,672 :: INFO :: evodenss.train.trainers :: [2051] -- [2.7s] TRAIN epoch 29 -- loss: tensor([80358.8359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:44:22,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80358.836
2025-03-09 15:44:22,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:23,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 15:44:26,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79559.203
2025-03-09 15:44:26,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:27,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 15:44:30,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78409.305
2025-03-09 15:44:30,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:31,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 15:44:33,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78654.469
2025-03-09 15:44:33,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:34,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 15:44:37,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77275.531
2025-03-09 15:44:37,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:38,580 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 15:44:41,267 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 34 -- loss: tensor([77216.1094], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:44:41,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77216.109
2025-03-09 15:44:41,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:42,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 15:44:44,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76691.719
2025-03-09 15:44:44,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:46,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 15:44:48,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76513.828
2025-03-09 15:44:48,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:49,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 15:44:52,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76087.398
2025-03-09 15:44:52,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:53,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 15:44:56,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75214.305
2025-03-09 15:44:56,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:44:57,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 15:44:59,875 :: INFO :: evodenss.train.trainers :: [2051] -- [2.7s] TRAIN epoch 39 -- loss: tensor([75263.9609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:44:59,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75263.961
2025-03-09 15:44:59,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:00,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 15:45:03,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74322.43
2025-03-09 15:45:03,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:04,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 15:45:07,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74169.023
2025-03-09 15:45:07,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:08,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 15:45:10,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74288.898
2025-03-09 15:45:10,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:11,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 15:45:14,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74077.242
2025-03-09 15:45:14,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:15,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 15:45:18,041 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 44 -- loss: tensor([73523.8984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:45:18,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73523.898
2025-03-09 15:45:18,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:19,006 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 15:45:21,688 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73276.172
2025-03-09 15:45:21,688 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:22,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 15:45:25,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72609.875
2025-03-09 15:45:25,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:26,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 15:45:28,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72849.648
2025-03-09 15:45:28,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:29,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 15:45:32,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72593.805
2025-03-09 15:45:32,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:33,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 15:45:36,099 :: INFO :: evodenss.train.trainers :: [2051] -- [2.62s] TRAIN epoch 49 -- loss: tensor([72604.3516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:45:36,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72604.352
2025-03-09 15:45:36,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:37,091 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 15:45:39,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72370.695
2025-03-09 15:45:39,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:40,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 15:45:43,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70695.172
2025-03-09 15:45:43,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:44,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 15:45:46,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71478.594
2025-03-09 15:45:46,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:47,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 15:45:50,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71325.805
2025-03-09 15:45:50,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:51,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 15:45:54,226 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 54 -- loss: tensor([71000.6797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:45:54,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71000.68
2025-03-09 15:45:54,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:55,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 15:45:57,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70106.047
2025-03-09 15:45:57,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:45:58,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 15:46:01,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69832.734
2025-03-09 15:46:01,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:02,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 15:46:05,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69137.43
2025-03-09 15:46:05,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:05,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 15:46:08,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69834.234
2025-03-09 15:46:08,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:09,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 15:46:12,310 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 59 -- loss: tensor([69719.5859], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:46:12,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69719.586
2025-03-09 15:46:12,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:13,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 15:46:15,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69040.914
2025-03-09 15:46:15,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:16,929 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 15:46:19,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69345.711
2025-03-09 15:46:19,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:20,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 15:46:23,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69309.594
2025-03-09 15:46:23,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:24,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 15:46:26,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68827.883
2025-03-09 15:46:26,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:27,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 15:46:30,513 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 64 -- loss: tensor([68583.0547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:46:30,513 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68583.055
2025-03-09 15:46:30,513 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:31,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 15:46:34,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67926.688
2025-03-09 15:46:34,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:35,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 15:46:37,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67180.719
2025-03-09 15:46:37,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:38,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 15:46:41,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67899.828
2025-03-09 15:46:41,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:42,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 15:46:45,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69244.359
2025-03-09 15:46:45,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:46,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 15:46:48,698 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 69 -- loss: tensor([67529.5703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:46:48,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67529.57
2025-03-09 15:46:48,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:49,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 15:46:52,393 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67940.586
2025-03-09 15:46:52,393 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:53,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 15:46:56,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67366.055
2025-03-09 15:46:56,006 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:46:56,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 15:46:59,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67738.211
2025-03-09 15:46:59,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:00,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 15:47:03,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67142.328
2025-03-09 15:47:03,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:04,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 15:47:06,886 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 74 -- loss: tensor([67060.7344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:47:06,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67060.734
2025-03-09 15:47:06,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:07,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 15:47:10,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65892.234
2025-03-09 15:47:10,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:11,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 15:47:14,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66433.594
2025-03-09 15:47:14,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:15,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 15:47:17,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65970.695
2025-03-09 15:47:17,820 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:18,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 15:47:21,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66544.758
2025-03-09 15:47:21,511 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:22,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 15:47:25,132 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 79 -- loss: tensor([66252.3672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:47:25,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66252.367
2025-03-09 15:47:25,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:26,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 15:47:28,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65157.066
2025-03-09 15:47:28,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:29,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 15:47:32,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65933.562
2025-03-09 15:47:32,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:33,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 15:47:36,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66029.961
2025-03-09 15:47:36,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:37,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 15:47:39,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65517.09
2025-03-09 15:47:39,688 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:40,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 15:47:43,322 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 84 -- loss: tensor([65505.6562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:47:43,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65505.656
2025-03-09 15:47:43,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:44,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 15:47:46,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66082.938
2025-03-09 15:47:46,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:47,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 15:47:50,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64517.039
2025-03-09 15:47:50,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:51,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 15:47:54,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65548.492
2025-03-09 15:47:54,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:55,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 15:47:58,008 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64890.242
2025-03-09 15:47:58,008 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:47:58,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 15:48:01,648 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 89 -- loss: tensor([65848.5859], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:48:01,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65848.586
2025-03-09 15:48:01,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:02,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 15:48:05,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64145.219
2025-03-09 15:48:05,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:06,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 15:48:08,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64957.754
2025-03-09 15:48:08,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:09,957 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 15:48:12,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63954.672
2025-03-09 15:48:12,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:13,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 15:48:16,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63975.066
2025-03-09 15:48:16,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:17,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 15:48:20,036 :: INFO :: evodenss.train.trainers :: [2051] -- [2.81s] TRAIN epoch 94 -- loss: tensor([65590.6562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:48:20,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65590.656
2025-03-09 15:48:20,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:21,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 15:48:23,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63762.035
2025-03-09 15:48:23,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:24,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 15:48:27,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 62667.82
2025-03-09 15:48:27,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:28,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 15:48:31,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65182.121
2025-03-09 15:48:31,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:32,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 15:48:34,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64533.449
2025-03-09 15:48:34,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:35,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 15:48:38,392 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 99 -- loss: tensor([63528.4492], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:48:38,392 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63528.449
2025-03-09 15:48:38,392 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:40,436 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 3675.68726
2025-03-09 15:48:40,441 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 15:48:40,442 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:81 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 
layer11: :deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.18217518027729646 alpha:0.9528526802138942 weight_decay:0.0009608786234735189 batch_size:32 epochs:100
2025-03-09 15:48:40,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 15:48:40,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 15:48:42,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1484723.375
2025-03-09 15:48:42,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:43,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 15:48:47,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:48:47,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:48,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 15:48:50,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 826108.062
2025-03-09 15:48:50,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:51,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 15:48:54,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 5540940.5
2025-03-09 15:48:54,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:55,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 15:48:57,308 :: INFO :: evodenss.train.trainers :: [2051] -- [2.27s] TRAIN epoch 4 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:48:57,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:48:57,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:48:58,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 15:49:00,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:49:00,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:01,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 15:49:03,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:49:03,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:04,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 15:49:06,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 5197190.5
2025-03-09 15:49:06,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:07,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 15:49:10,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 865546.438
2025-03-09 15:49:10,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:11,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 15:49:13,284 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 9 -- loss: tensor([804769.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:49:13,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804769.562
2025-03-09 15:49:13,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:14,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 15:49:16,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:49:16,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:17,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 15:49:19,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:49:19,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:20,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 15:49:22,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2080590.5
2025-03-09 15:49:22,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:23,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 15:49:26,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:49:26,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:27,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 15:49:29,381 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 14 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:49:29,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:49:29,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:30,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 15:49:32,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:49:32,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:33,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 15:49:35,786 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2928079.5
2025-03-09 15:49:35,786 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:36,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 15:49:38,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 966455.438
2025-03-09 15:49:38,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:39,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 15:49:42,227 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:49:42,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:43,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 15:49:45,442 :: INFO :: evodenss.train.trainers :: [2051] -- [2.22s] TRAIN epoch 19 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:49:45,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:49:45,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:46,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 15:49:48,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:49:48,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:49,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 15:49:51,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 6692867.0
2025-03-09 15:49:51,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:52,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 15:49:55,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 809353.688
2025-03-09 15:49:55,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:56,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 15:49:58,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806279.25
2025-03-09 15:49:58,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:49:59,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 15:50:01,670 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 24 -- loss: tensor([5382647.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:50:01,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 5382647.5
2025-03-09 15:50:01,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:02,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 15:50:04,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:50:04,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:05,916 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 15:50:08,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:50:08,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:09,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 15:50:11,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:50:11,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:12,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 15:50:14,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 5196698.0
2025-03-09 15:50:14,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:15,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 15:50:17,862 :: INFO :: evodenss.train.trainers :: [2051] -- [2.25s] TRAIN epoch 29 -- loss: tensor([804486.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:50:17,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804486.688
2025-03-09 15:50:17,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:18,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 15:50:21,120 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:50:21,120 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:22,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 15:50:24,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805576.438
2025-03-09 15:50:24,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:25,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 15:50:27,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1380964.125
2025-03-09 15:50:27,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:28,663 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 15:50:30,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1060414.25
2025-03-09 15:50:30,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:31,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 15:50:34,141 :: INFO :: evodenss.train.trainers :: [2051] -- [2.25s] TRAIN epoch 34 -- loss: tensor([1026350.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:50:34,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1026350.375
2025-03-09 15:50:34,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:35,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 15:50:37,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 5108249.5
2025-03-09 15:50:37,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:38,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 15:50:40,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:50:40,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:41,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 15:50:43,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 987462.188
2025-03-09 15:50:43,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:44,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 15:50:47,182 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.938
2025-03-09 15:50:47,182 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:48,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 15:50:50,379 :: INFO :: evodenss.train.trainers :: [2051] -- [2.27s] TRAIN epoch 39 -- loss: tensor([3785223.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:50:50,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3785223.5
2025-03-09 15:50:50,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:51,303 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 15:50:53,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805756.188
2025-03-09 15:50:53,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:54,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 15:50:56,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:50:56,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:50:57,592 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 15:50:59,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 807135.125
2025-03-09 15:50:59,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:00,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 15:51:02,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804694.062
2025-03-09 15:51:02,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:03,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 15:51:06,073 :: INFO :: evodenss.train.trainers :: [2051] -- [2.2s] TRAIN epoch 44 -- loss: tensor([3926275.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:51:06,073 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3926275.75
2025-03-09 15:51:06,073 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:07,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 15:51:09,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 827969.562
2025-03-09 15:51:09,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:10,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 15:51:12,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 829210.812
2025-03-09 15:51:12,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:13,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 15:51:15,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:51:15,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:16,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 15:51:18,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 5160698.0
2025-03-09 15:51:18,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:19,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 15:51:21,777 :: INFO :: evodenss.train.trainers :: [2051] -- [2.2s] TRAIN epoch 49 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:51:21,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:51:21,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:22,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 15:51:24,926 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:51:24,926 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:25,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 15:51:28,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2473418.25
2025-03-09 15:51:28,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:29,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 15:51:31,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804490.812
2025-03-09 15:51:31,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:32,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 15:51:34,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:51:34,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:35,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 15:51:37,469 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 54 -- loss: tensor([818468.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:51:37,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 818468.625
2025-03-09 15:51:37,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:38,392 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 15:51:40,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3552641.25
2025-03-09 15:51:40,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:41,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 15:51:43,720 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 831808.688
2025-03-09 15:51:43,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:44,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 15:51:46,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:51:46,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:47,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 15:51:50,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:51:50,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:50,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 15:51:53,154 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 59 -- loss: tensor([4998513.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:51:53,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 4998513.5
2025-03-09 15:51:53,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:54,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 15:51:56,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:51:56,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:51:57,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 15:51:59,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:51:59,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:00,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 15:52:02,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:52:02,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:03,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 15:52:05,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1251312.75
2025-03-09 15:52:05,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:06,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 15:52:08,833 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 64 -- loss: tensor([804516.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:52:08,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804516.062
2025-03-09 15:52:08,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:09,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 15:52:11,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1744345.5
2025-03-09 15:52:11,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:12,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 15:52:15,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:52:15,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:16,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 15:52:18,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:52:18,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:19,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 15:52:21,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 4633257.5
2025-03-09 15:52:21,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:22,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 15:52:24,666 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 69 -- loss: tensor([986738.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:52:24,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 986738.312
2025-03-09 15:52:24,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:25,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 15:52:27,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:52:27,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:28,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 15:52:31,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 929941.438
2025-03-09 15:52:31,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:31,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 15:52:34,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:52:34,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:35,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 15:52:37,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3573057.75
2025-03-09 15:52:37,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:38,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 15:52:40,473 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 74 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:52:40,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:52:40,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:41,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 15:52:43,623 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:52:43,623 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:44,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 15:52:46,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3644247.25
2025-03-09 15:52:46,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:47,786 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 15:52:50,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:52:50,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:51,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 15:52:53,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:52:53,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:54,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 15:52:56,436 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 79 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:52:56,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:52:56,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:52:57,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 15:52:59,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 4406545.5
2025-03-09 15:52:59,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:00,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 15:53:02,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:53:02,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:03,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 15:53:06,006 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:53:06,007 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:06,926 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 15:53:09,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:53:09,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:10,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 15:53:12,356 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 84 -- loss: tensor([5916527.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:53:12,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 5916527.0
2025-03-09 15:53:12,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:13,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 15:53:15,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:53:15,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:16,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 15:53:18,734 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:53:18,734 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:19,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 15:53:21,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:53:21,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:22,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 15:53:25,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3336176.25
2025-03-09 15:53:25,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:26,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 15:53:28,334 :: INFO :: evodenss.train.trainers :: [2051] -- [2.21s] TRAIN epoch 89 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:53:28,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:53:28,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:29,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 15:53:31,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:53:31,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:32,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 15:53:34,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:53:34,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:35,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 15:53:37,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813384.438
2025-03-09 15:53:37,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:38,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 15:53:41,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1217271.875
2025-03-09 15:53:41,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:41,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 15:53:44,182 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 94 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:53:44,182 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:53:44,182 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:45,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 15:53:47,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:53:47,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:48,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 15:53:50,638 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:53:50,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:51,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 15:53:53,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1574616.0
2025-03-09 15:53:53,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:54,797 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 15:53:57,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805864.938
2025-03-09 15:53:57,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:53:58,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 15:54:00,278 :: INFO :: evodenss.train.trainers :: [2051] -- [2.25s] TRAIN epoch 99 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:54:00,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 15:54:00,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:02,243 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 44535.45703
2025-03-09 15:54:02,247 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 15:54:02,248 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 
layer11: :fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100
2025-03-09 15:54:02,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 15:54:02,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 15:54:04,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 361900.875
2025-03-09 15:54:04,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:05,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 15:54:07,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 163739.891
2025-03-09 15:54:07,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:08,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 15:54:09,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 144241.625
2025-03-09 15:54:09,996 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:10,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 15:54:12,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 133271.031
2025-03-09 15:54:12,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:13,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 15:54:15,821 :: INFO :: evodenss.train.trainers :: [2051] -- [1.96s] TRAIN epoch 4 -- loss: tensor([129699.5547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:54:15,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 129699.555
2025-03-09 15:54:15,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:16,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 15:54:18,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 122691.891
2025-03-09 15:54:18,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:19,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 15:54:21,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 118789.203
2025-03-09 15:54:21,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:22,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 15:54:24,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 116031.984
2025-03-09 15:54:24,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:25,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 15:54:27,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 113379.609
2025-03-09 15:54:27,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:28,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 15:54:30,395 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 9 -- loss: tensor([110498.8516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:54:30,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110498.852
2025-03-09 15:54:30,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:31,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 15:54:33,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108167.82
2025-03-09 15:54:33,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:34,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 15:54:36,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105658.258
2025-03-09 15:54:36,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:37,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 15:54:39,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103542.32
2025-03-09 15:54:39,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:40,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 15:54:42,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102894.383
2025-03-09 15:54:42,179 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:43,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 15:54:45,151 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 14 -- loss: tensor([100953.4922], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:54:45,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100953.492
2025-03-09 15:54:45,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:46,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 15:54:48,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99414.891
2025-03-09 15:54:48,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:49,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 15:54:51,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98295.68
2025-03-09 15:54:51,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:52,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 15:54:54,025 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97050.891
2025-03-09 15:54:54,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:54,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 15:54:56,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96431.703
2025-03-09 15:54:56,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:54:57,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 15:54:59,913 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 19 -- loss: tensor([95335.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:54:59,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95335.312
2025-03-09 15:54:59,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:00,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 15:55:02,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94214.383
2025-03-09 15:55:02,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:03,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 15:55:05,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93658.164
2025-03-09 15:55:05,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:06,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 15:55:08,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92870.5
2025-03-09 15:55:08,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:09,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 15:55:11,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92670.945
2025-03-09 15:55:11,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:12,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 15:55:14,652 :: INFO :: evodenss.train.trainers :: [2051] -- [1.95s] TRAIN epoch 24 -- loss: tensor([92387.3359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:55:14,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92387.336
2025-03-09 15:55:14,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:15,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 15:55:17,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91565.539
2025-03-09 15:55:17,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:18,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 15:55:20,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90024.375
2025-03-09 15:55:20,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:21,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 15:55:23,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89276.133
2025-03-09 15:55:23,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:24,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 15:55:26,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89189.32
2025-03-09 15:55:26,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:27,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 15:55:29,639 :: INFO :: evodenss.train.trainers :: [2051] -- [2.01s] TRAIN epoch 29 -- loss: tensor([88713.9219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:55:29,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88713.922
2025-03-09 15:55:29,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:30,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 15:55:32,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87433.969
2025-03-09 15:55:32,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:33,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 15:55:35,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87851.078
2025-03-09 15:55:35,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:36,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 15:55:38,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86991.32
2025-03-09 15:55:38,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:39,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 15:55:41,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87185.672
2025-03-09 15:55:41,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:42,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 15:55:44,438 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 34 -- loss: tensor([86036.1562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:55:44,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86036.156
2025-03-09 15:55:44,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:45,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 15:55:47,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85453.148
2025-03-09 15:55:47,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:48,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 15:55:50,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86412.781
2025-03-09 15:55:50,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:51,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 15:55:53,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85062.914
2025-03-09 15:55:53,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:54,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 15:55:56,413 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84942.617
2025-03-09 15:55:56,413 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:55:57,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 15:55:59,369 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 39 -- loss: tensor([85143.4844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:55:59,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85143.484
2025-03-09 15:55:59,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:00,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 15:56:02,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84175.711
2025-03-09 15:56:02,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:03,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 15:56:05,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83340.602
2025-03-09 15:56:05,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:06,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 15:56:08,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83791.531
2025-03-09 15:56:08,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:09,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 15:56:11,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83519.781
2025-03-09 15:56:11,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:12,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 15:56:14,213 :: INFO :: evodenss.train.trainers :: [2051] -- [1.96s] TRAIN epoch 44 -- loss: tensor([82758.3672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:56:14,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82758.367
2025-03-09 15:56:14,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:15,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 15:56:17,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82496.367
2025-03-09 15:56:17,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:18,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 15:56:20,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82765.938
2025-03-09 15:56:20,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:21,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 15:56:23,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82084.664
2025-03-09 15:56:23,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:24,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 15:56:26,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81872.977
2025-03-09 15:56:26,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:27,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 15:56:29,226 :: INFO :: evodenss.train.trainers :: [2051] -- [1.96s] TRAIN epoch 49 -- loss: tensor([81511.7891], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:56:29,227 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81511.789
2025-03-09 15:56:29,227 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:30,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 15:56:32,181 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81900.938
2025-03-09 15:56:32,181 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:33,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 15:56:35,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80470.945
2025-03-09 15:56:35,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:36,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 15:56:38,041 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81029.602
2025-03-09 15:56:38,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:39,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 15:56:41,032 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79768.672
2025-03-09 15:56:41,032 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:42,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 15:56:43,998 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 54 -- loss: tensor([80663.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:56:43,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80663.75
2025-03-09 15:56:43,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:44,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 15:56:46,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79681.633
2025-03-09 15:56:46,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:47,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 15:56:49,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80142.688
2025-03-09 15:56:49,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:50,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 15:56:52,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79985.234
2025-03-09 15:56:52,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:53,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 15:56:55,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78473.305
2025-03-09 15:56:55,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:56,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 15:56:58,830 :: INFO :: evodenss.train.trainers :: [2051] -- [1.98s] TRAIN epoch 59 -- loss: tensor([79804.7266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:56:58,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79804.727
2025-03-09 15:56:58,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:56:59,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 15:57:01,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78926.211
2025-03-09 15:57:01,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:02,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 15:57:04,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78482.797
2025-03-09 15:57:04,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:05,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 15:57:07,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78480.188
2025-03-09 15:57:07,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:08,734 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 15:57:10,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78494.25
2025-03-09 15:57:10,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:11,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 15:57:13,665 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 64 -- loss: tensor([77660.5703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:57:13,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77660.57
2025-03-09 15:57:13,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:14,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 15:57:16,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77658.719
2025-03-09 15:57:16,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:17,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 15:57:19,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77789.023
2025-03-09 15:57:19,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:20,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 15:57:22,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78045.484
2025-03-09 15:57:22,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:23,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 15:57:25,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77464.445
2025-03-09 15:57:25,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:26,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 15:57:28,651 :: INFO :: evodenss.train.trainers :: [2051] -- [1.98s] TRAIN epoch 69 -- loss: tensor([77962.0234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:57:28,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77962.023
2025-03-09 15:57:28,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:29,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 15:57:31,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77557.977
2025-03-09 15:57:31,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:32,593 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 15:57:34,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77879.734
2025-03-09 15:57:34,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:35,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 15:57:37,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76425.977
2025-03-09 15:57:37,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:38,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 15:57:40,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76830.875
2025-03-09 15:57:40,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:41,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 15:57:43,458 :: INFO :: evodenss.train.trainers :: [2051] -- [1.96s] TRAIN epoch 74 -- loss: tensor([77332.3438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:57:43,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77332.344
2025-03-09 15:57:43,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:44,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 15:57:46,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76624.695
2025-03-09 15:57:46,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:47,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 15:57:49,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76065.773
2025-03-09 15:57:49,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:50,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 15:57:52,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76685.469
2025-03-09 15:57:52,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:53,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 15:57:55,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75333.422
2025-03-09 15:57:55,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:56,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 15:57:58,023 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 79 -- loss: tensor([76364.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:57:58,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76364.953
2025-03-09 15:57:58,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:57:58,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 15:58:00,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75703.352
2025-03-09 15:58:00,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:01,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 15:58:03,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75804.469
2025-03-09 15:58:03,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:04,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 15:58:06,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75404.375
2025-03-09 15:58:06,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:07,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 15:58:09,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74421.664
2025-03-09 15:58:09,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:10,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 15:58:12,244 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 84 -- loss: tensor([75506.0078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:58:12,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75506.008
2025-03-09 15:58:12,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:13,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 15:58:15,058 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75371.734
2025-03-09 15:58:15,058 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:15,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 15:58:17,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75199.93
2025-03-09 15:58:17,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:18,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 15:58:20,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74733.633
2025-03-09 15:58:20,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:21,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 15:58:23,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74223.953
2025-03-09 15:58:23,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:24,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 15:58:26,472 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 89 -- loss: tensor([73514.7969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:58:26,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73514.797
2025-03-09 15:58:26,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:27,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 15:58:29,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74940.297
2025-03-09 15:58:29,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:30,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 15:58:32,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74537.43
2025-03-09 15:58:32,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:33,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 15:58:34,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73650.883
2025-03-09 15:58:34,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:35,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 15:58:37,767 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74768.703
2025-03-09 15:58:37,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:38,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 15:58:40,614 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 94 -- loss: tensor([74259.6641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:58:40,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74259.664
2025-03-09 15:58:40,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:41,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 15:58:43,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73421.242
2025-03-09 15:58:43,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:44,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 15:58:46,309 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73623.695
2025-03-09 15:58:46,309 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:47,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 15:58:49,181 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72976.969
2025-03-09 15:58:49,181 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:50,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 15:58:52,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73061.281
2025-03-09 15:58:52,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:52,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 15:58:54,850 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 99 -- loss: tensor([73904.1797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:58:54,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73904.18
2025-03-09 15:58:54,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:58:56,745 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 4256.84766
2025-03-09 15:58:56,746 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 15:58:56,746 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 1, id: 1
2025-03-09 15:58:56,746 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 1000, 1000]
2025-03-09 15:58:56,746 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 15:58:56,749 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [3986.04321, 3675.68726, 44535.45703, 4256.84766]
2025-03-09 15:58:57,327 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([18809.8477], device='cuda:0')
2025-03-09 15:58:57,327 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 8: 3675.68726
2025-03-09 15:58:57,327 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3675.68726



2025-03-09 15:58:57,425 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 9
2025-03-09 15:58:57,425 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 15:58:57,552 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have an extra layer at Module 0: features; Position: 6. Reused?: False
2025-03-09 15:58:57,553 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 15:58:57,553 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 15:58:57,554 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 15:58:57,555 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 15:58:57,555 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 15:58:57,556 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 15:58:57,557 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 14
2025-03-09 15:58:57,557 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 15:58:57,560 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a layer removed from Module 0: features; Position: 9
2025-03-09 15:58:57,561 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 15:58:57,561 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 15:58:57,562 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 15:58:57,563 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 15:58:57,563 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 15:58:57,564 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 15:58:57,565 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 15:58:57,565 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 15:58:57,568 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 15:58:57,569 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 15:58:57,570 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 15:58:57,571 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 15:58:57,571 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 15:58:57,572 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 15:58:57,573 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 15:58:57,575 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 15:58:57,579 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 15:58:57,580 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 15:58:57,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 15:58:57,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 15:59:00,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 491947.125
2025-03-09 15:59:00,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:01,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 15:59:03,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 214926.312
2025-03-09 15:59:03,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:04,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 15:59:07,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 168618.719
2025-03-09 15:59:07,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:08,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 15:59:10,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 147184.281
2025-03-09 15:59:10,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:11,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 15:59:14,385 :: INFO :: evodenss.train.trainers :: [2051] -- [2.62s] TRAIN epoch 4 -- loss: tensor([132856.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:59:14,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 132856.562
2025-03-09 15:59:14,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:15,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 15:59:17,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 122673.922
2025-03-09 15:59:17,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:18,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 15:59:21,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 116227.391
2025-03-09 15:59:21,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:22,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 15:59:25,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 111045.883
2025-03-09 15:59:25,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:26,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 15:59:28,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107550.305
2025-03-09 15:59:28,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:29,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 15:59:32,205 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 9 -- loss: tensor([103709.5391], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:59:32,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103709.539
2025-03-09 15:59:32,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:33,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 15:59:35,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100086.602
2025-03-09 15:59:35,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:36,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 15:59:39,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98432.523
2025-03-09 15:59:39,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:40,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 15:59:42,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96395.203
2025-03-09 15:59:42,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:43,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 15:59:46,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93963.914
2025-03-09 15:59:46,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:47,290 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 15:59:49,971 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 14 -- loss: tensor([92208.3516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 15:59:49,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92208.352
2025-03-09 15:59:49,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:50,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 15:59:53,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91940.367
2025-03-09 15:59:53,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:54,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 15:59:57,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89095.438
2025-03-09 15:59:57,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 15:59:58,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 16:00:00,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88174.969
2025-03-09 16:00:00,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:01,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 16:00:04,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87817.164
2025-03-09 16:00:04,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:05,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 16:00:07,751 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 19 -- loss: tensor([85664.4609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:00:07,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85664.461
2025-03-09 16:00:07,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:08,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 16:00:11,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85937.25
2025-03-09 16:00:11,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:12,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 16:00:14,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84391.391
2025-03-09 16:00:14,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:15,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 16:00:18,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84707.266
2025-03-09 16:00:18,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:19,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 16:00:21,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83070.156
2025-03-09 16:00:21,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:22,929 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 16:00:25,515 :: INFO :: evodenss.train.trainers :: [2051] -- [2.58s] TRAIN epoch 24 -- loss: tensor([83589.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:00:25,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83589.438
2025-03-09 16:00:25,516 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:26,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 16:00:29,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81443.367
2025-03-09 16:00:29,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:29,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 16:00:32,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81452.195
2025-03-09 16:00:32,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:33,454 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 16:00:36,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81076.945
2025-03-09 16:00:36,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:37,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 16:00:39,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80140.477
2025-03-09 16:00:39,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:40,573 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 16:00:43,194 :: INFO :: evodenss.train.trainers :: [2051] -- [2.62s] TRAIN epoch 29 -- loss: tensor([80553.4141], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:00:43,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80553.414
2025-03-09 16:00:43,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:44,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 16:00:46,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78217.055
2025-03-09 16:00:46,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:47,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 16:00:50,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79060.32
2025-03-09 16:00:50,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:51,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 16:00:53,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77928.75
2025-03-09 16:00:53,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:54,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 16:00:57,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78285.227
2025-03-09 16:00:57,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:00:58,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 16:01:01,023 :: INFO :: evodenss.train.trainers :: [2051] -- [2.62s] TRAIN epoch 34 -- loss: tensor([77315.6953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:01:01,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77315.695
2025-03-09 16:01:01,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:01,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 16:01:04,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77375.172
2025-03-09 16:01:04,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:05,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 16:01:08,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76325.898
2025-03-09 16:01:08,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:09,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 16:01:11,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75932.062
2025-03-09 16:01:11,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:12,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 16:01:15,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76470.375
2025-03-09 16:01:15,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:16,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 16:01:18,767 :: INFO :: evodenss.train.trainers :: [2051] -- [2.61s] TRAIN epoch 39 -- loss: tensor([76154.6406], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:01:18,767 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76154.641
2025-03-09 16:01:18,767 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:19,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 16:01:22,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75284.648
2025-03-09 16:01:22,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:23,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 16:01:25,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74482.055
2025-03-09 16:01:25,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:26,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 16:01:29,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74297.25
2025-03-09 16:01:29,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:30,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 16:01:33,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73222.32
2025-03-09 16:01:33,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:33,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 16:01:36,601 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 44 -- loss: tensor([73336.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:01:36,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73336.25
2025-03-09 16:01:36,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:37,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 16:01:40,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72957.406
2025-03-09 16:01:40,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:41,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 16:01:43,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72635.453
2025-03-09 16:01:43,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:44,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 16:01:47,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71947.234
2025-03-09 16:01:47,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:48,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 16:01:50,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72129.609
2025-03-09 16:01:50,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:51,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 16:01:54,330 :: INFO :: evodenss.train.trainers :: [2051] -- [2.6s] TRAIN epoch 49 -- loss: tensor([70773.0938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:01:54,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70773.094
2025-03-09 16:01:54,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:55,260 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 16:01:57,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70901.797
2025-03-09 16:01:57,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:01:58,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 16:02:01,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70343.18
2025-03-09 16:02:01,392 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:02,338 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 16:02:04,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70370.328
2025-03-09 16:02:04,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:05,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 16:02:08,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70206.68
2025-03-09 16:02:08,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:09,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 16:02:12,037 :: INFO :: evodenss.train.trainers :: [2051] -- [2.59s] TRAIN epoch 54 -- loss: tensor([69305.9297], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:02:12,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69305.93
2025-03-09 16:02:12,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:12,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 16:02:15,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70274.508
2025-03-09 16:02:15,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:16,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 16:02:19,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68928.297
2025-03-09 16:02:19,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:20,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 16:02:22,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69030.039
2025-03-09 16:02:22,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:23,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 16:02:26,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70577.656
2025-03-09 16:02:26,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:27,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 16:02:29,844 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 59 -- loss: tensor([68358.0469], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:02:29,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68358.047
2025-03-09 16:02:29,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:30,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 16:02:33,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68339.383
2025-03-09 16:02:33,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:34,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 16:02:36,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68129.508
2025-03-09 16:02:36,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:37,925 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 16:02:40,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68454.523
2025-03-09 16:02:40,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:41,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 16:02:44,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67709.07
2025-03-09 16:02:44,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:45,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 16:02:47,758 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 64 -- loss: tensor([68559.6016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:02:47,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68559.602
2025-03-09 16:02:47,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:48,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 16:02:51,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68217.047
2025-03-09 16:02:51,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:52,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 16:02:54,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67348.258
2025-03-09 16:02:54,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:55,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 16:02:58,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67003.094
2025-03-09 16:02:58,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:02:59,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 16:03:02,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67333.844
2025-03-09 16:03:02,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:03,101 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 16:03:05,697 :: INFO :: evodenss.train.trainers :: [2051] -- [2.59s] TRAIN epoch 69 -- loss: tensor([67076.5859], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:03:05,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67076.586
2025-03-09 16:03:05,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:06,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 16:03:09,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67497.367
2025-03-09 16:03:09,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:10,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 16:03:12,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67632.578
2025-03-09 16:03:12,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:13,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 16:03:16,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66643.531
2025-03-09 16:03:16,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:17,500 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 16:03:20,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65767.711
2025-03-09 16:03:20,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:21,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 16:03:23,828 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 74 -- loss: tensor([66297.4219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:03:23,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66297.422
2025-03-09 16:03:23,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:24,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 16:03:27,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66867.266
2025-03-09 16:03:27,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:28,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 16:03:31,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66274.406
2025-03-09 16:03:31,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:31,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 16:03:34,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65606.992
2025-03-09 16:03:34,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:35,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 16:03:38,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66250.086
2025-03-09 16:03:38,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:39,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 16:03:41,927 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 79 -- loss: tensor([65055.3555], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:03:41,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65055.355
2025-03-09 16:03:41,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:42,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 16:03:45,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66776.672
2025-03-09 16:03:45,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:46,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 16:03:49,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65210.215
2025-03-09 16:03:49,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:50,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 16:03:52,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64875.238
2025-03-09 16:03:52,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:53,951 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 16:03:56,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65800.562
2025-03-09 16:03:56,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:03:57,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 16:04:00,253 :: INFO :: evodenss.train.trainers :: [2051] -- [2.62s] TRAIN epoch 84 -- loss: tensor([64876.6797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:04:00,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64876.68
2025-03-09 16:04:00,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:01,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 16:04:03,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64425.199
2025-03-09 16:04:03,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:04,870 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 16:04:07,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65172.422
2025-03-09 16:04:07,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:08,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 16:04:11,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64725.0
2025-03-09 16:04:11,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:12,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 16:04:14,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64640.164
2025-03-09 16:04:14,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:15,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 16:04:18,427 :: INFO :: evodenss.train.trainers :: [2051] -- [2.67s] TRAIN epoch 89 -- loss: tensor([64173.5742], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:04:18,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64173.574
2025-03-09 16:04:18,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:19,489 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 16:04:22,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64792.988
2025-03-09 16:04:22,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:23,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 16:04:25,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64007.113
2025-03-09 16:04:25,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:26,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 16:04:29,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64520.004
2025-03-09 16:04:29,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:30,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 16:04:33,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63422.746
2025-03-09 16:04:33,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:34,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 16:04:36,730 :: INFO :: evodenss.train.trainers :: [2051] -- [2.66s] TRAIN epoch 94 -- loss: tensor([63940.8047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:04:36,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63940.805
2025-03-09 16:04:36,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:37,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 16:04:40,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64903.117
2025-03-09 16:04:40,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:41,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 16:04:44,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63116.559
2025-03-09 16:04:44,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:45,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 16:04:47,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64066.832
2025-03-09 16:04:47,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:48,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 16:04:51,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63135.504
2025-03-09 16:04:51,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:52,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 16:04:55,104 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 99 -- loss: tensor([63078.2070], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:04:55,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63078.207
2025-03-09 16:04:55,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:04:57,125 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 3683.25952
2025-03-09 16:04:57,130 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 16:04:57,131 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :deconv1d out_channels:51 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 
layer12: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 
layer13: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 
layer15: :fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:32 epochs:100
2025-03-09 16:04:57,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 16:04:57,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 16:05:00,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 562683.188
2025-03-09 16:05:00,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:01,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 16:05:03,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 358686.156
2025-03-09 16:05:03,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:04,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 16:05:07,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 194502.312
2025-03-09 16:05:07,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:08,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 16:05:11,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 154489.578
2025-03-09 16:05:11,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:12,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 16:05:14,798 :: INFO :: evodenss.train.trainers :: [2051] -- [2.62s] TRAIN epoch 4 -- loss: tensor([135826.8438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:05:14,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 135826.844
2025-03-09 16:05:14,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:15,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 16:05:18,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 123528.977
2025-03-09 16:05:18,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:19,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 16:05:22,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 119269.664
2025-03-09 16:05:22,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:23,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 16:05:25,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114173.242
2025-03-09 16:05:25,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:26,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 16:05:29,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110959.406
2025-03-09 16:05:29,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:30,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 16:05:33,114 :: INFO :: evodenss.train.trainers :: [2051] -- [2.62s] TRAIN epoch 9 -- loss: tensor([108281.8516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:05:33,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108281.852
2025-03-09 16:05:33,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:34,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 16:05:36,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106093.672
2025-03-09 16:05:36,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:37,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 16:05:40,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104601.859
2025-03-09 16:05:40,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:41,454 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 16:05:44,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102185.992
2025-03-09 16:05:44,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:45,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 16:05:47,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101270.922
2025-03-09 16:05:47,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:48,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 16:05:51,483 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 14 -- loss: tensor([100088.5156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:05:51,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100088.516
2025-03-09 16:05:51,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:52,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 16:05:55,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99570.539
2025-03-09 16:05:55,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:56,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 16:05:58,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97959.789
2025-03-09 16:05:58,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:05:59,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 16:06:02,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96287.938
2025-03-09 16:06:02,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:03,594 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 16:06:06,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95429.516
2025-03-09 16:06:06,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:07,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 16:06:09,912 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 19 -- loss: tensor([95172.7344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:06:09,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95172.734
2025-03-09 16:06:09,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:10,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 16:06:13,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93077.922
2025-03-09 16:06:13,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:14,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 16:06:17,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92713.945
2025-03-09 16:06:17,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:18,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 16:06:21,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91440.664
2025-03-09 16:06:21,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:22,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 16:06:24,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91411.672
2025-03-09 16:06:24,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:25,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 16:06:28,373 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 24 -- loss: tensor([89660.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:06:28,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89660.062
2025-03-09 16:06:28,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:29,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 16:06:32,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89040.195
2025-03-09 16:06:32,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:33,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 16:06:35,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89192.898
2025-03-09 16:06:35,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:36,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 16:06:39,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87992.852
2025-03-09 16:06:39,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:40,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 16:06:43,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86788.953
2025-03-09 16:06:43,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:44,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 16:06:46,777 :: INFO :: evodenss.train.trainers :: [2051] -- [2.66s] TRAIN epoch 29 -- loss: tensor([86911.8594], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:06:46,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86911.859
2025-03-09 16:06:46,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:47,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 16:06:50,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85022.773
2025-03-09 16:06:50,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:51,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 16:06:54,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86120.414
2025-03-09 16:06:54,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:55,289 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 16:06:57,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85436.188
2025-03-09 16:06:57,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:06:59,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 16:07:01,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85653.953
2025-03-09 16:07:01,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:02,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 16:07:05,313 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 34 -- loss: tensor([83207.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:07:05,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83207.562
2025-03-09 16:07:05,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:06,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 16:07:09,007 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82853.383
2025-03-09 16:07:09,008 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:10,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 16:07:12,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82055.688
2025-03-09 16:07:12,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:13,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 16:07:16,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82092.266
2025-03-09 16:07:16,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:17,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 16:07:20,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81347.398
2025-03-09 16:07:20,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:21,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 16:07:23,863 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 39 -- loss: tensor([82705.1406], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:07:23,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82705.141
2025-03-09 16:07:23,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:24,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 16:07:27,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81363.969
2025-03-09 16:07:27,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:28,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 16:07:31,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80716.742
2025-03-09 16:07:31,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:32,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 16:07:35,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79163.148
2025-03-09 16:07:35,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:36,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 16:07:38,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80977.516
2025-03-09 16:07:38,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:39,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 16:07:42,381 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 44 -- loss: tensor([79401.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:07:42,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79401.938
2025-03-09 16:07:42,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:43,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 16:07:46,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78969.391
2025-03-09 16:07:46,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:47,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 16:07:49,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78960.203
2025-03-09 16:07:49,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:50,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 16:07:53,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79716.461
2025-03-09 16:07:53,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:54,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 16:07:57,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78431.18
2025-03-09 16:07:57,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:07:58,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 16:08:00,887 :: INFO :: evodenss.train.trainers :: [2051] -- [2.67s] TRAIN epoch 49 -- loss: tensor([78619.1016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:08:00,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78619.102
2025-03-09 16:08:00,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:01,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 16:08:04,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77237.891
2025-03-09 16:08:04,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:05,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 16:08:08,202 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76909.812
2025-03-09 16:08:08,202 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:09,237 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 16:08:11,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77699.539
2025-03-09 16:08:11,894 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:12,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 16:08:15,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77221.273
2025-03-09 16:08:15,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:16,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 16:08:19,363 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 54 -- loss: tensor([76268.7109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:08:19,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76268.711
2025-03-09 16:08:19,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:20,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 16:08:23,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76382.461
2025-03-09 16:08:23,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:24,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 16:08:26,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76213.984
2025-03-09 16:08:26,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:27,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 16:08:30,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75023.125
2025-03-09 16:08:30,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:31,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 16:08:34,067 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76553.68
2025-03-09 16:08:34,067 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:35,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 16:08:37,745 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 59 -- loss: tensor([75837.1484], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:08:37,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75837.148
2025-03-09 16:08:37,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:38,796 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 16:08:41,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76092.789
2025-03-09 16:08:41,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:42,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 16:08:45,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76130.805
2025-03-09 16:08:45,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:46,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 16:08:48,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75589.883
2025-03-09 16:08:48,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:49,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 16:08:52,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76252.695
2025-03-09 16:08:52,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:53,389 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 16:08:55,993 :: INFO :: evodenss.train.trainers :: [2051] -- [2.6s] TRAIN epoch 64 -- loss: tensor([76207.4453], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:08:55,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76207.445
2025-03-09 16:08:55,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:08:56,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 16:08:59,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75717.07
2025-03-09 16:08:59,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:00,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 16:09:03,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74905.539
2025-03-09 16:09:03,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:04,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 16:09:06,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74576.289
2025-03-09 16:09:06,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:07,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 16:09:10,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74165.672
2025-03-09 16:09:10,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:11,238 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 16:09:13,830 :: INFO :: evodenss.train.trainers :: [2051] -- [2.59s] TRAIN epoch 69 -- loss: tensor([74015.1641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:09:13,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74015.164
2025-03-09 16:09:13,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:14,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 16:09:17,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73903.234
2025-03-09 16:09:17,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:18,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 16:09:21,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74643.57
2025-03-09 16:09:21,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:22,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 16:09:24,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72574.641
2025-03-09 16:09:24,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:25,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 16:09:28,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73374.891
2025-03-09 16:09:28,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:29,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 16:09:31,803 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 74 -- loss: tensor([72533.7109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:09:31,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72533.711
2025-03-09 16:09:31,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:32,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 16:09:36,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72984.656
2025-03-09 16:09:36,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:37,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 16:09:40,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72626.609
2025-03-09 16:09:40,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:41,535 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 16:09:44,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72471.469
2025-03-09 16:09:44,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:45,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 16:09:47,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72127.859
2025-03-09 16:09:47,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:48,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 16:09:51,469 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 79 -- loss: tensor([72355.3203], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:09:51,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72355.32
2025-03-09 16:09:51,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:52,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 16:09:55,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72198.438
2025-03-09 16:09:55,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:56,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 16:09:58,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71138.578
2025-03-09 16:09:58,688 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:09:59,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 16:10:02,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71711.469
2025-03-09 16:10:02,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:03,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 16:10:05,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72502.789
2025-03-09 16:10:05,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:06,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 16:10:09,558 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 84 -- loss: tensor([71353.2109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:10:09,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71353.211
2025-03-09 16:10:09,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:10,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 16:10:13,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71093.609
2025-03-09 16:10:13,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:14,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 16:10:16,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72321.836
2025-03-09 16:10:16,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:17,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 16:10:20,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71606.133
2025-03-09 16:10:20,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:21,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 16:10:24,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70828.695
2025-03-09 16:10:24,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:25,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 16:10:27,805 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 89 -- loss: tensor([71059.6562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:10:27,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71059.656
2025-03-09 16:10:27,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:28,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 16:10:31,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71634.547
2025-03-09 16:10:31,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:32,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 16:10:35,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70940.133
2025-03-09 16:10:35,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:36,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 16:10:38,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70535.766
2025-03-09 16:10:38,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:39,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 16:10:42,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71730.438
2025-03-09 16:10:42,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:43,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 16:10:45,919 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 94 -- loss: tensor([70168.2734], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:10:45,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70168.273
2025-03-09 16:10:45,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:46,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 16:10:49,637 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69619.188
2025-03-09 16:10:49,637 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:50,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 16:10:53,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70258.812
2025-03-09 16:10:53,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:54,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 16:10:56,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71346.477
2025-03-09 16:10:56,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:10:57,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 16:11:00,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70732.289
2025-03-09 16:11:00,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:01,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 16:11:04,221 :: INFO :: evodenss.train.trainers :: [2051] -- [2.66s] TRAIN epoch 99 -- loss: tensor([69841.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:11:04,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69841.312
2025-03-09 16:11:04,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:06,276 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 4103.36133
2025-03-09 16:11:06,280 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 16:11:06,281 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 16:11:06,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 16:11:06,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 16:11:08,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 347604.125
2025-03-09 16:11:08,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:09,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 16:11:11,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 171842.109
2025-03-09 16:11:11,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:12,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 16:11:15,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 144309.141
2025-03-09 16:11:15,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:16,207 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 16:11:18,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 129782.016
2025-03-09 16:11:18,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:19,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 16:11:21,847 :: INFO :: evodenss.train.trainers :: [2051] -- [2.27s] TRAIN epoch 4 -- loss: tensor([122469.7109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:11:21,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 122469.711
2025-03-09 16:11:21,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:22,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 16:11:25,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 116777.125
2025-03-09 16:11:25,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:26,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 16:11:28,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 113613.133
2025-03-09 16:11:28,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:29,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 16:11:31,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110119.328
2025-03-09 16:11:31,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:32,593 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 16:11:34,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105656.898
2025-03-09 16:11:34,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:35,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 16:11:38,115 :: INFO :: evodenss.train.trainers :: [2051] -- [2.25s] TRAIN epoch 9 -- loss: tensor([103617.2188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:11:38,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103617.219
2025-03-09 16:11:38,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:39,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 16:11:41,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101877.484
2025-03-09 16:11:41,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:42,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 16:11:44,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99991.781
2025-03-09 16:11:44,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:45,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 16:11:47,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98730.156
2025-03-09 16:11:47,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:48,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 16:11:51,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96959.641
2025-03-09 16:11:51,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:51,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 16:11:54,219 :: INFO :: evodenss.train.trainers :: [2051] -- [2.25s] TRAIN epoch 14 -- loss: tensor([96806.2578], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:11:54,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96806.258
2025-03-09 16:11:54,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:55,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 16:11:57,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95349.461
2025-03-09 16:11:57,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:11:58,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 16:12:00,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94249.539
2025-03-09 16:12:00,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:01,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 16:12:03,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93087.469
2025-03-09 16:12:03,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:04,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 16:12:06,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92546.82
2025-03-09 16:12:06,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:07,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 16:12:10,149 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 19 -- loss: tensor([91760.0938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:12:10,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91760.094
2025-03-09 16:12:10,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:11,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 16:12:13,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91087.547
2025-03-09 16:12:13,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:14,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 16:12:16,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90016.414
2025-03-09 16:12:16,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:17,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 16:12:19,767 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90498.625
2025-03-09 16:12:19,767 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:20,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 16:12:22,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87973.891
2025-03-09 16:12:22,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:23,894 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 16:12:26,153 :: INFO :: evodenss.train.trainers :: [2051] -- [2.26s] TRAIN epoch 24 -- loss: tensor([88474.9453], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:12:26,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88474.945
2025-03-09 16:12:26,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:27,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 16:12:29,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87614.914
2025-03-09 16:12:29,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:30,279 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 16:12:32,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87396.531
2025-03-09 16:12:32,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:33,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 16:12:35,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86184.547
2025-03-09 16:12:35,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:36,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 16:12:38,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85112.242
2025-03-09 16:12:38,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:39,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 16:12:42,031 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 29 -- loss: tensor([85722.6016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:12:42,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85722.602
2025-03-09 16:12:42,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:42,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 16:12:45,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84610.219
2025-03-09 16:12:45,260 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:46,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 16:12:48,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84133.031
2025-03-09 16:12:48,456 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:49,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 16:12:51,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83389.156
2025-03-09 16:12:51,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:52,700 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 16:12:54,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83973.992
2025-03-09 16:12:54,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:55,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 16:12:58,115 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 34 -- loss: tensor([81913.3359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:12:58,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81913.336
2025-03-09 16:12:58,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:12:59,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 16:13:01,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81522.695
2025-03-09 16:13:01,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:02,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 16:13:04,465 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80466.461
2025-03-09 16:13:04,465 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:05,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 16:13:07,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81459.68
2025-03-09 16:13:07,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:08,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 16:13:10,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79703.594
2025-03-09 16:13:10,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:11,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 16:13:14,030 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 39 -- loss: tensor([80349.5312], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:13:14,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80349.531
2025-03-09 16:13:14,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:14,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 16:13:17,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79281.188
2025-03-09 16:13:17,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:18,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 16:13:20,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78275.023
2025-03-09 16:13:20,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:21,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 16:13:23,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78088.484
2025-03-09 16:13:23,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:24,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 16:13:26,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78175.172
2025-03-09 16:13:26,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:27,897 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 16:13:30,129 :: INFO :: evodenss.train.trainers :: [2051] -- [2.23s] TRAIN epoch 44 -- loss: tensor([76872.5703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:13:30,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76872.57
2025-03-09 16:13:30,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:31,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 16:13:33,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76983.062
2025-03-09 16:13:33,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:34,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 16:13:36,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75681.227
2025-03-09 16:13:36,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:37,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 16:13:39,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75671.633
2025-03-09 16:13:39,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:40,602 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 16:13:42,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75870.312
2025-03-09 16:13:42,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:43,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 16:13:46,083 :: INFO :: evodenss.train.trainers :: [2051] -- [2.27s] TRAIN epoch 49 -- loss: tensor([74846.5312], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:13:46,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74846.531
2025-03-09 16:13:46,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:47,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 16:13:49,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74587.938
2025-03-09 16:13:49,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:50,289 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 16:13:52,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74647.492
2025-03-09 16:13:52,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:53,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 16:13:55,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73696.641
2025-03-09 16:13:55,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:56,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 16:13:58,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73500.109
2025-03-09 16:13:58,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:13:59,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 16:14:02,131 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 54 -- loss: tensor([73859.5938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:14:02,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73859.594
2025-03-09 16:14:02,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:03,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 16:14:05,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73502.102
2025-03-09 16:14:05,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:06,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 16:14:08,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72291.969
2025-03-09 16:14:08,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:09,594 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 16:14:11,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72462.336
2025-03-09 16:14:11,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:12,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 16:14:15,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73332.891
2025-03-09 16:14:15,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:16,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 16:14:18,286 :: INFO :: evodenss.train.trainers :: [2051] -- [2.27s] TRAIN epoch 59 -- loss: tensor([71532.0469], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:14:18,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71532.047
2025-03-09 16:14:18,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:19,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 16:14:21,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71777.25
2025-03-09 16:14:21,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:22,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 16:14:24,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72422.516
2025-03-09 16:14:24,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:25,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 16:14:28,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71894.25
2025-03-09 16:14:28,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:29,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 16:14:31,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71271.234
2025-03-09 16:14:31,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:32,386 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 16:14:34,645 :: INFO :: evodenss.train.trainers :: [2051] -- [2.26s] TRAIN epoch 64 -- loss: tensor([71193.7344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:14:34,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71193.734
2025-03-09 16:14:34,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:35,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 16:14:37,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70577.805
2025-03-09 16:14:37,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:38,829 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 16:14:41,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70957.898
2025-03-09 16:14:41,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:42,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 16:14:44,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71058.461
2025-03-09 16:14:44,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:45,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 16:14:47,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71204.641
2025-03-09 16:14:47,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:48,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 16:14:50,732 :: INFO :: evodenss.train.trainers :: [2051] -- [2.33s] TRAIN epoch 69 -- loss: tensor([71140.0781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:14:50,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71140.078
2025-03-09 16:14:50,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:51,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 16:14:53,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70958.312
2025-03-09 16:14:53,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:54,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 16:14:57,106 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71185.117
2025-03-09 16:14:57,106 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:14:58,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 16:15:00,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68838.133
2025-03-09 16:15:00,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:01,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 16:15:03,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70428.828
2025-03-09 16:15:03,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:04,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 16:15:06,652 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 74 -- loss: tensor([69247.8047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:15:06,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69247.805
2025-03-09 16:15:06,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:07,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 16:15:09,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69870.797
2025-03-09 16:15:09,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:10,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 16:15:13,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69186.672
2025-03-09 16:15:13,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:13,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 16:15:16,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69470.641
2025-03-09 16:15:16,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:17,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 16:15:19,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68115.055
2025-03-09 16:15:19,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:20,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 16:15:22,691 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 79 -- loss: tensor([69932.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:15:22,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69932.25
2025-03-09 16:15:22,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:23,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 16:15:25,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68559.023
2025-03-09 16:15:25,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:26,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 16:15:29,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68982.586
2025-03-09 16:15:29,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:30,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 16:15:32,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68468.078
2025-03-09 16:15:32,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:33,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 16:15:35,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68081.859
2025-03-09 16:15:35,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:36,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 16:15:38,757 :: INFO :: evodenss.train.trainers :: [2051] -- [2.27s] TRAIN epoch 84 -- loss: tensor([68380.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:15:38,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68380.562
2025-03-09 16:15:38,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:39,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 16:15:41,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68886.758
2025-03-09 16:15:41,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:42,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 16:15:45,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68016.062
2025-03-09 16:15:45,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:46,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 16:15:48,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68328.406
2025-03-09 16:15:48,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:49,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 16:15:51,643 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67120.109
2025-03-09 16:15:51,643 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:52,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 16:15:54,906 :: INFO :: evodenss.train.trainers :: [2051] -- [2.28s] TRAIN epoch 89 -- loss: tensor([68000.0469], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:15:54,907 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68000.047
2025-03-09 16:15:54,907 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:55,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 16:15:58,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67714.859
2025-03-09 16:15:58,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:15:59,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 16:16:01,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67758.266
2025-03-09 16:16:01,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:02,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 16:16:04,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67237.57
2025-03-09 16:16:04,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:05,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 16:16:07,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67811.281
2025-03-09 16:16:07,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:08,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 16:16:11,061 :: INFO :: evodenss.train.trainers :: [2051] -- [2.28s] TRAIN epoch 94 -- loss: tensor([66955.9141], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:16:11,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66955.914
2025-03-09 16:16:11,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:12,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 16:16:14,290 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66474.641
2025-03-09 16:16:14,290 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:15,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 16:16:17,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67023.281
2025-03-09 16:16:17,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:18,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 16:16:20,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66477.188
2025-03-09 16:16:20,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:21,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 16:16:24,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66350.398
2025-03-09 16:16:24,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:25,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 16:16:27,293 :: INFO :: evodenss.train.trainers :: [2051] -- [2.24s] TRAIN epoch 99 -- loss: tensor([66591.7656], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:16:27,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66591.766
2025-03-09 16:16:27,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:29,273 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 3974.46729
2025-03-09 16:16:29,278 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 16:16:29,281 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:100 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :conv1d out_channels:100 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:23 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.17086469976195787 alpha:0.9852932025065296 weight_decay:0.00038315889612095384 batch_size:32 epochs:100
2025-03-09 16:16:29,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 16:16:29,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 16:16:31,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 897797.938
2025-03-09 16:16:31,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:32,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 16:16:34,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:16:34,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:35,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 16:16:37,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804458.188
2025-03-09 16:16:37,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:38,454 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 16:16:40,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:16:40,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:41,387 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 16:16:43,336 :: INFO :: evodenss.train.trainers :: [2051] -- [1.95s] TRAIN epoch 4 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:16:43,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:16:43,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:44,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 16:16:46,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804523.938
2025-03-09 16:16:46,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:47,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 16:16:49,266 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:16:49,266 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:50,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 16:16:52,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 854251.938
2025-03-09 16:16:52,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:53,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 16:16:55,106 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:16:55,106 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:56,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 16:16:58,016 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 9 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:16:58,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:16:58,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:16:58,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 16:17:00,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 807686.812
2025-03-09 16:17:00,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:01,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 16:17:03,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 807620.688
2025-03-09 16:17:03,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:04,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 16:17:06,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 862719.5
2025-03-09 16:17:06,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:07,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 16:17:09,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804837.188
2025-03-09 16:17:09,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:10,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 16:17:12,673 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 14 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:17:12,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:17:12,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:13,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 16:17:15,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:17:15,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:16,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 16:17:18,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1194882.0
2025-03-09 16:17:18,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:19,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 16:17:21,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806129.125
2025-03-09 16:17:21,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:22,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 16:17:24,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806370.5
2025-03-09 16:17:24,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:25,535 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 16:17:27,505 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 19 -- loss: tensor([806681.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:17:27,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806681.688
2025-03-09 16:17:27,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:28,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 16:17:30,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804918.375
2025-03-09 16:17:30,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:31,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 16:17:33,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1046976.562
2025-03-09 16:17:33,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:34,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 16:17:36,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 807302.5
2025-03-09 16:17:36,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:37,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 16:17:39,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805666.062
2025-03-09 16:17:39,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:40,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 16:17:41,941 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 24 -- loss: tensor([866449.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:17:41,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 866449.875
2025-03-09 16:17:41,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:42,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 16:17:44,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804480.625
2025-03-09 16:17:44,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:45,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 16:17:47,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804479.375
2025-03-09 16:17:47,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:48,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 16:17:50,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 832666.0
2025-03-09 16:17:50,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:51,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 16:17:53,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811507.75
2025-03-09 16:17:53,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:54,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 16:17:56,529 :: INFO :: evodenss.train.trainers :: [2051] -- [1.95s] TRAIN epoch 29 -- loss: tensor([805648.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:17:56,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805648.375
2025-03-09 16:17:56,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:17:57,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 16:17:59,386 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 838442.938
2025-03-09 16:17:59,387 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:00,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 16:18:02,290 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 808204.5
2025-03-09 16:18:02,290 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:03,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 16:18:05,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804699.312
2025-03-09 16:18:05,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:06,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 16:18:08,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 859867.438
2025-03-09 16:18:08,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:09,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 16:18:11,065 :: INFO :: evodenss.train.trainers :: [2051] -- [1.96s] TRAIN epoch 34 -- loss: tensor([808776.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:18:11,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 808776.438
2025-03-09 16:18:11,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:12,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 16:18:13,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 809040.938
2025-03-09 16:18:13,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:14,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 16:18:16,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 809064.688
2025-03-09 16:18:16,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:17,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 16:18:19,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 878457.75
2025-03-09 16:18:19,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:20,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 16:18:22,796 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805359.375
2025-03-09 16:18:22,796 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:23,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 16:18:25,751 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 39 -- loss: tensor([808666.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:18:25,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 808666.562
2025-03-09 16:18:25,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:26,720 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 16:18:28,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 819942.312
2025-03-09 16:18:28,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:29,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 16:18:31,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805446.938
2025-03-09 16:18:31,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:32,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 16:18:34,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 807338.188
2025-03-09 16:18:34,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:35,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 16:18:37,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 809243.688
2025-03-09 16:18:37,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:38,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 16:18:40,346 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 44 -- loss: tensor([947542.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:18:40,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 947542.688
2025-03-09 16:18:40,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:41,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 16:18:43,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804715.75
2025-03-09 16:18:43,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:44,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 16:18:46,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806411.5
2025-03-09 16:18:46,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:47,179 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 16:18:49,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814829.938
2025-03-09 16:18:49,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:50,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 16:18:52,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805235.312
2025-03-09 16:18:52,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:53,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 16:18:55,014 :: INFO :: evodenss.train.trainers :: [2051] -- [1.95s] TRAIN epoch 49 -- loss: tensor([851484.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:18:55,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 851484.062
2025-03-09 16:18:55,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:55,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 16:18:57,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805207.188
2025-03-09 16:18:57,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:18:58,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 16:19:00,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805087.375
2025-03-09 16:19:00,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:01,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 16:19:03,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 823500.312
2025-03-09 16:19:03,829 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:04,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 16:19:06,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 812730.188
2025-03-09 16:19:06,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:07,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 16:19:09,721 :: INFO :: evodenss.train.trainers :: [2051] -- [1.94s] TRAIN epoch 54 -- loss: tensor([804567.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:19:09,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804567.812
2025-03-09 16:19:09,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:10,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 16:19:12,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806759.188
2025-03-09 16:19:12,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:13,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 16:19:15,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804458.438
2025-03-09 16:19:15,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:16,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 16:19:18,576 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 843772.062
2025-03-09 16:19:18,576 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:19,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 16:19:21,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806013.312
2025-03-09 16:19:21,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:22,589 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 16:19:24,585 :: INFO :: evodenss.train.trainers :: [2051] -- [1.99s] TRAIN epoch 59 -- loss: tensor([810230.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:19:24,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 810230.688
2025-03-09 16:19:24,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:25,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 16:19:27,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 807343.812
2025-03-09 16:19:27,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:28,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 16:19:30,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 953516.125
2025-03-09 16:19:30,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:31,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 16:19:33,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804901.25
2025-03-09 16:19:33,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:34,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 16:19:36,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805122.875
2025-03-09 16:19:36,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:37,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 16:19:39,487 :: INFO :: evodenss.train.trainers :: [2051] -- [1.98s] TRAIN epoch 64 -- loss: tensor([804606.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:19:39,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804606.875
2025-03-09 16:19:39,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:40,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 16:19:42,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805586.875
2025-03-09 16:19:42,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:43,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 16:19:45,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 844797.75
2025-03-09 16:19:45,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:46,433 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 16:19:48,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805884.812
2025-03-09 16:19:48,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:49,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 16:19:51,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 845537.25
2025-03-09 16:19:51,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:52,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 16:19:54,449 :: INFO :: evodenss.train.trainers :: [2051] -- [1.95s] TRAIN epoch 69 -- loss: tensor([806891.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:19:54,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806891.0
2025-03-09 16:19:54,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:55,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 16:19:57,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806786.188
2025-03-09 16:19:57,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:19:58,462 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 16:20:00,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 810860.125
2025-03-09 16:20:00,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:01,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 16:20:03,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 835490.75
2025-03-09 16:20:03,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:04,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 16:20:06,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805010.375
2025-03-09 16:20:06,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:07,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 16:20:09,450 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 74 -- loss: tensor([906298.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:20:09,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 906298.562
2025-03-09 16:20:09,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:10,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 16:20:12,433 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804690.875
2025-03-09 16:20:12,433 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:13,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 16:20:16,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804515.875
2025-03-09 16:20:16,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:17,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 16:20:19,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804779.125
2025-03-09 16:20:19,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:20,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 16:20:22,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805488.5
2025-03-09 16:20:22,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:23,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 16:20:25,742 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 79 -- loss: tensor([804470.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:20:25,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804470.438
2025-03-09 16:20:25,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:26,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 16:20:28,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 817447.562
2025-03-09 16:20:28,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:29,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 16:20:31,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804932.75
2025-03-09 16:20:31,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:32,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 16:20:34,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:20:34,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:35,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 16:20:37,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 825299.75
2025-03-09 16:20:37,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:38,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 16:20:40,728 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 84 -- loss: tensor([806104.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:20:40,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806104.5
2025-03-09 16:20:40,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:41,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 16:20:43,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1026685.375
2025-03-09 16:20:43,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:44,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 16:20:46,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 787626.25
2025-03-09 16:20:46,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:47,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 16:20:49,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 770986.938
2025-03-09 16:20:49,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:50,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 16:20:52,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 722169.25
2025-03-09 16:20:52,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:53,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 16:20:55,800 :: INFO :: evodenss.train.trainers :: [2051] -- [1.98s] TRAIN epoch 89 -- loss: tensor([798629.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:20:55,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 798629.75
2025-03-09 16:20:55,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:56,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 16:20:58,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 698916.812
2025-03-09 16:20:58,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:20:59,766 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 16:21:01,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 772232.125
2025-03-09 16:21:01,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:02,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 16:21:04,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813450.75
2025-03-09 16:21:04,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:05,798 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 16:21:07,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 819799.438
2025-03-09 16:21:07,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:08,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 16:21:10,778 :: INFO :: evodenss.train.trainers :: [2051] -- [1.98s] TRAIN epoch 94 -- loss: tensor([826800.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:21:10,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 826800.562
2025-03-09 16:21:10,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:11,796 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 16:21:13,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804849.0
2025-03-09 16:21:13,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:14,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 16:21:16,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 882153.062
2025-03-09 16:21:16,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:17,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 16:21:19,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805758.562
2025-03-09 16:21:19,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:20,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 16:21:22,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804590.0
2025-03-09 16:21:22,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:23,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 16:21:25,902 :: INFO :: evodenss.train.trainers :: [2051] -- [1.99s] TRAIN epoch 99 -- loss: tensor([806556.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:21:25,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806556.125
2025-03-09 16:21:25,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:27,947 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 44534.8125
2025-03-09 16:21:27,947 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 16:21:27,947 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 0, id: 0
2025-03-09 16:21:27,948 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 1000, 1000]
2025-03-09 16:21:27,948 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 16:21:27,951 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [3683.25952, 4103.36133, 3974.46729, 44534.8125]
2025-03-09 16:21:28,204 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([19318.1699], device='cuda:0')
2025-03-09 16:21:28,204 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 9: 3683.25952
2025-03-09 16:21:28,204 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3675.68726



2025-03-09 16:21:28,411 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 10
2025-03-09 16:21:28,411 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 16:21:28,422 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a layer removed from Module 0: features; Position: 10
2025-03-09 16:21:28,423 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 16:21:28,423 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 16:21:28,424 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 16:21:28,427 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 16:21:28,427 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 16:21:28,428 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 16:21:28,431 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 12
2025-03-09 16:21:28,431 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 8
2025-03-09 16:21:28,432 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 16:21:28,433 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 16:21:28,433 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 16:21:28,436 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 16:21:28,440 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 16:21:28,453 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 16:21:28,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 16:21:28,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 16:21:31,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 407441.281
2025-03-09 16:21:31,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:32,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 16:21:34,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 210198.438
2025-03-09 16:21:34,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:36,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 16:21:38,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 189037.938
2025-03-09 16:21:38,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:39,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 16:21:42,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 168418.922
2025-03-09 16:21:42,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:43,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 16:21:46,409 :: INFO :: evodenss.train.trainers :: [2051] -- [2.8s] TRAIN epoch 4 -- loss: tensor([154485.1094], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:21:46,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 154485.109
2025-03-09 16:21:46,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:47,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 16:21:50,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 143029.719
2025-03-09 16:21:50,309 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:51,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 16:21:54,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 131676.109
2025-03-09 16:21:54,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:55,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 16:21:57,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 123712.289
2025-03-09 16:21:57,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:21:58,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 16:22:01,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 117022.68
2025-03-09 16:22:01,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:02,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 16:22:05,419 :: INFO :: evodenss.train.trainers :: [2051] -- [2.66s] TRAIN epoch 9 -- loss: tensor([110395.1016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:22:05,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110395.102
2025-03-09 16:22:05,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:06,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 16:22:09,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109048.125
2025-03-09 16:22:09,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:10,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 16:22:12,766 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104576.891
2025-03-09 16:22:12,767 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:13,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 16:22:16,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102709.508
2025-03-09 16:22:16,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:17,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 16:22:20,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100652.164
2025-03-09 16:22:20,194 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:21,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 16:22:23,779 :: INFO :: evodenss.train.trainers :: [2051] -- [2.61s] TRAIN epoch 14 -- loss: tensor([101048.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:22:23,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101048.125
2025-03-09 16:22:23,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:24,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 16:22:27,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97612.062
2025-03-09 16:22:27,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:28,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 16:22:31,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97014.047
2025-03-09 16:22:31,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:32,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 16:22:34,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95737.32
2025-03-09 16:22:34,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:35,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 16:22:38,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95159.258
2025-03-09 16:22:38,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:39,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 16:22:42,121 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 19 -- loss: tensor([94557.2109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:22:42,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94557.211
2025-03-09 16:22:42,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:43,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 16:22:45,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94405.242
2025-03-09 16:22:45,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:46,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 16:22:49,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93445.398
2025-03-09 16:22:49,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:50,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 16:22:53,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91178.523
2025-03-09 16:22:53,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:54,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 16:22:56,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90527.812
2025-03-09 16:22:56,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:22:57,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 16:23:00,418 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 24 -- loss: tensor([89183.0078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:23:00,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89183.008
2025-03-09 16:23:00,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:01,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 16:23:04,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88403.953
2025-03-09 16:23:04,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:05,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 16:23:07,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88713.102
2025-03-09 16:23:07,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:08,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 16:23:11,393 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87747.203
2025-03-09 16:23:11,393 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:12,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 16:23:15,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87415.156
2025-03-09 16:23:15,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:16,008 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 16:23:18,700 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 29 -- loss: tensor([86305.9766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:23:18,700 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86305.977
2025-03-09 16:23:18,700 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:19,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 16:23:22,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85827.602
2025-03-09 16:23:22,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:23,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 16:23:26,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85486.633
2025-03-09 16:23:26,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:27,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 16:23:29,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84920.648
2025-03-09 16:23:29,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:30,806 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 16:23:33,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83532.117
2025-03-09 16:23:33,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:34,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 16:23:37,157 :: INFO :: evodenss.train.trainers :: [2051] -- [2.7s] TRAIN epoch 34 -- loss: tensor([83507.7734], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:23:37,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83507.773
2025-03-09 16:23:37,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:38,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 16:23:40,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83430.25
2025-03-09 16:23:40,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:41,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 16:23:44,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83067.766
2025-03-09 16:23:44,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:45,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 16:23:48,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82918.852
2025-03-09 16:23:48,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:49,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 16:23:51,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81983.82
2025-03-09 16:23:51,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:52,994 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 16:23:55,689 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 39 -- loss: tensor([81604.5156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:23:55,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81604.516
2025-03-09 16:23:55,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:23:56,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 16:23:59,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85500.047
2025-03-09 16:23:59,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:00,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 16:24:03,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80196.047
2025-03-09 16:24:03,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:03,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 16:24:06,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80161.82
2025-03-09 16:24:06,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:07,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 16:24:10,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79204.805
2025-03-09 16:24:10,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:11,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 16:24:13,868 :: INFO :: evodenss.train.trainers :: [2051] -- [2.58s] TRAIN epoch 44 -- loss: tensor([79456.8281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:24:13,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79456.828
2025-03-09 16:24:13,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:14,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 16:24:17,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79575.789
2025-03-09 16:24:17,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:18,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 16:24:21,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78659.633
2025-03-09 16:24:21,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:22,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 16:24:24,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78276.297
2025-03-09 16:24:24,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:25,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 16:24:28,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77196.828
2025-03-09 16:24:28,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:29,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 16:24:32,208 :: INFO :: evodenss.train.trainers :: [2051] -- [2.67s] TRAIN epoch 49 -- loss: tensor([78397.1953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:24:32,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78397.195
2025-03-09 16:24:32,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:33,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 16:24:35,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76573.852
2025-03-09 16:24:35,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:36,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 16:24:39,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76692.453
2025-03-09 16:24:39,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:40,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 16:24:43,055 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75421.586
2025-03-09 16:24:43,055 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:44,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 16:24:46,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75709.484
2025-03-09 16:24:46,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:47,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 16:24:50,427 :: INFO :: evodenss.train.trainers :: [2051] -- [2.79s] TRAIN epoch 54 -- loss: tensor([74974.8438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:24:50,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74974.844
2025-03-09 16:24:50,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:51,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 16:24:54,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74516.812
2025-03-09 16:24:54,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:55,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 16:24:57,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74602.609
2025-03-09 16:24:57,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:24:58,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 16:25:01,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74402.164
2025-03-09 16:25:01,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:02,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 16:25:05,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74849.734
2025-03-09 16:25:05,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:06,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 16:25:08,813 :: INFO :: evodenss.train.trainers :: [2051] -- [2.7s] TRAIN epoch 59 -- loss: tensor([74233.7031], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:25:08,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74233.703
2025-03-09 16:25:08,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:09,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 16:25:12,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74246.539
2025-03-09 16:25:12,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:13,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 16:25:16,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73236.367
2025-03-09 16:25:16,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:17,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 16:25:19,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73456.25
2025-03-09 16:25:19,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:20,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 16:25:23,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72548.445
2025-03-09 16:25:23,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:24,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 16:25:27,325 :: INFO :: evodenss.train.trainers :: [2051] -- [2.75s] TRAIN epoch 64 -- loss: tensor([72846.2344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:25:27,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72846.234
2025-03-09 16:25:27,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:28,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 16:25:31,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73286.438
2025-03-09 16:25:31,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:32,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 16:25:34,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72217.242
2025-03-09 16:25:34,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:35,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 16:25:38,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71455.547
2025-03-09 16:25:38,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:39,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 16:25:42,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71383.82
2025-03-09 16:25:42,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:43,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 16:25:45,773 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 69 -- loss: tensor([71025.8281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:25:45,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71025.828
2025-03-09 16:25:45,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:46,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 16:25:49,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71824.219
2025-03-09 16:25:49,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:50,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 16:25:53,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70892.031
2025-03-09 16:25:53,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:54,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 16:25:57,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70819.445
2025-03-09 16:25:57,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:25:58,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 16:26:00,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70337.539
2025-03-09 16:26:00,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:01,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 16:26:04,451 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 74 -- loss: tensor([70471.9141], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:26:04,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70471.914
2025-03-09 16:26:04,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:05,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 16:26:08,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69879.789
2025-03-09 16:26:08,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:09,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 16:26:11,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70808.523
2025-03-09 16:26:11,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:12,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 16:26:15,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68976.031
2025-03-09 16:26:15,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:16,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 16:26:19,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69707.844
2025-03-09 16:26:19,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:20,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 16:26:23,148 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 79 -- loss: tensor([69738.5469], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:26:23,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69738.547
2025-03-09 16:26:23,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:24,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 16:26:26,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70451.406
2025-03-09 16:26:26,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:27,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 16:26:30,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69210.82
2025-03-09 16:26:30,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:31,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 16:26:34,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70082.25
2025-03-09 16:26:34,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:35,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 16:26:38,191 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69192.984
2025-03-09 16:26:38,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:39,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 16:26:41,938 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 84 -- loss: tensor([68987.8672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:26:41,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68987.867
2025-03-09 16:26:41,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:42,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 16:26:45,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68380.797
2025-03-09 16:26:45,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:46,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 16:26:49,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69288.859
2025-03-09 16:26:49,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:50,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 16:26:53,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68543.891
2025-03-09 16:26:53,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:54,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 16:26:56,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68432.812
2025-03-09 16:26:56,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:26:58,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 16:27:00,735 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 89 -- loss: tensor([67965.5547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:27:00,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67965.555
2025-03-09 16:27:00,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:01,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 16:27:04,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66774.867
2025-03-09 16:27:04,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:05,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 16:27:08,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67808.836
2025-03-09 16:27:08,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:09,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 16:27:12,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67553.086
2025-03-09 16:27:12,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:13,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 16:27:15,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67326.797
2025-03-09 16:27:15,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:16,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 16:27:19,748 :: INFO :: evodenss.train.trainers :: [2051] -- [2.86s] TRAIN epoch 94 -- loss: tensor([67416.3438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:27:19,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67416.344
2025-03-09 16:27:19,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:20,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 16:27:23,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66692.453
2025-03-09 16:27:23,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:24,584 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 16:27:27,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68529.578
2025-03-09 16:27:27,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:28,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 16:27:31,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65998.602
2025-03-09 16:27:31,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:32,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 16:27:34,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66909.469
2025-03-09 16:27:34,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:35,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 16:27:38,466 :: INFO :: evodenss.train.trainers :: [2051] -- [2.66s] TRAIN epoch 99 -- loss: tensor([67505.5938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:27:38,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67505.594
2025-03-09 16:27:38,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:40,492 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 3794.05249
2025-03-09 16:27:40,496 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 16:27:40,497 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 16:27:40,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 16:27:40,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 16:27:43,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 433636.75
2025-03-09 16:27:43,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:44,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 16:27:46,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 223179.781
2025-03-09 16:27:46,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:47,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 16:27:50,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 177967.766
2025-03-09 16:27:50,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:51,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 16:27:54,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 154436.219
2025-03-09 16:27:54,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:55,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 16:27:57,754 :: INFO :: evodenss.train.trainers :: [2051] -- [2.56s] TRAIN epoch 4 -- loss: tensor([138757.8281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:27:57,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 138757.828
2025-03-09 16:27:57,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:27:58,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 16:28:01,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 129847.609
2025-03-09 16:28:01,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:02,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 16:28:05,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 121573.383
2025-03-09 16:28:05,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:06,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 16:28:08,596 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 115496.906
2025-03-09 16:28:08,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:09,643 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 16:28:12,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 111087.289
2025-03-09 16:28:12,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:13,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 16:28:15,810 :: INFO :: evodenss.train.trainers :: [2051] -- [2.55s] TRAIN epoch 9 -- loss: tensor([108321.5156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:28:15,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108321.516
2025-03-09 16:28:15,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:16,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 16:28:19,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105842.367
2025-03-09 16:28:19,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:20,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 16:28:23,120 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103443.984
2025-03-09 16:28:23,120 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:24,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 16:28:26,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101973.961
2025-03-09 16:28:26,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:27,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 16:28:30,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99838.141
2025-03-09 16:28:30,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:31,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 16:28:33,929 :: INFO :: evodenss.train.trainers :: [2051] -- [2.6s] TRAIN epoch 14 -- loss: tensor([98476.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:28:33,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98476.75
2025-03-09 16:28:33,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:34,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 16:28:37,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98160.289
2025-03-09 16:28:37,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:38,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 16:28:41,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96615.828
2025-03-09 16:28:41,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:42,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 16:28:44,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95665.867
2025-03-09 16:28:44,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:45,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 16:28:48,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94183.891
2025-03-09 16:28:48,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:49,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 16:28:52,037 :: INFO :: evodenss.train.trainers :: [2051] -- [2.57s] TRAIN epoch 19 -- loss: tensor([94275.1719], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:28:52,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94275.172
2025-03-09 16:28:52,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:53,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 16:28:55,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92525.453
2025-03-09 16:28:55,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:28:56,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 16:28:59,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90456.812
2025-03-09 16:28:59,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:00,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 16:29:02,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91985.047
2025-03-09 16:29:02,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:03,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 16:29:06,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89941.938
2025-03-09 16:29:06,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:07,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 16:29:10,127 :: INFO :: evodenss.train.trainers :: [2051] -- [2.57s] TRAIN epoch 24 -- loss: tensor([88707.7422], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:29:10,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88707.742
2025-03-09 16:29:10,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:11,181 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 16:29:13,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88870.93
2025-03-09 16:29:13,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:14,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 16:29:17,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88416.898
2025-03-09 16:29:17,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:18,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 16:29:21,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87795.469
2025-03-09 16:29:21,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:22,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 16:29:24,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86735.203
2025-03-09 16:29:24,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:25,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 16:29:28,364 :: INFO :: evodenss.train.trainers :: [2051] -- [2.59s] TRAIN epoch 29 -- loss: tensor([86267.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:29:28,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86267.188
2025-03-09 16:29:28,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:29,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 16:29:31,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84994.305
2025-03-09 16:29:32,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:33,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 16:29:35,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84906.672
2025-03-09 16:29:35,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:36,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 16:29:39,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83997.656
2025-03-09 16:29:39,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:40,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 16:29:42,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83955.484
2025-03-09 16:29:42,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:43,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 16:29:46,488 :: INFO :: evodenss.train.trainers :: [2051] -- [2.53s] TRAIN epoch 34 -- loss: tensor([83419.8359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:29:46,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83419.836
2025-03-09 16:29:46,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:47,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 16:29:50,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82943.188
2025-03-09 16:29:50,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:51,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 16:29:53,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82955.812
2025-03-09 16:29:53,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:54,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 16:29:57,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81674.969
2025-03-09 16:29:57,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:29:58,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 16:30:01,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81750.805
2025-03-09 16:30:01,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:02,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 16:30:04,736 :: INFO :: evodenss.train.trainers :: [2051] -- [2.62s] TRAIN epoch 39 -- loss: tensor([81046.5703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:30:04,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81046.57
2025-03-09 16:30:04,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:05,795 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 16:30:08,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81490.75
2025-03-09 16:30:08,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:09,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 16:30:11,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80917.875
2025-03-09 16:30:11,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:12,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 16:30:15,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80802.141
2025-03-09 16:30:15,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:16,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 16:30:18,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80062.109
2025-03-09 16:30:18,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:19,869 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 16:30:22,376 :: INFO :: evodenss.train.trainers :: [2051] -- [2.51s] TRAIN epoch 44 -- loss: tensor([80830.4609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:30:22,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80830.461
2025-03-09 16:30:22,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:23,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 16:30:25,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79306.234
2025-03-09 16:30:25,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:26,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 16:30:29,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78210.633
2025-03-09 16:30:29,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:30,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 16:30:32,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79389.461
2025-03-09 16:30:32,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:33,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 16:30:36,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77894.211
2025-03-09 16:30:36,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:37,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 16:30:39,892 :: INFO :: evodenss.train.trainers :: [2051] -- [2.51s] TRAIN epoch 49 -- loss: tensor([78188.5859], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:30:39,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78188.586
2025-03-09 16:30:39,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:40,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 16:30:43,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78398.859
2025-03-09 16:30:43,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:44,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 16:30:46,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77110.0
2025-03-09 16:30:46,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:47,720 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 16:30:50,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77555.117
2025-03-09 16:30:50,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:51,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 16:30:53,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77430.352
2025-03-09 16:30:53,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:54,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 16:30:57,329 :: INFO :: evodenss.train.trainers :: [2051] -- [2.52s] TRAIN epoch 54 -- loss: tensor([77541.9922], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:30:57,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77541.992
2025-03-09 16:30:57,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:30:58,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 16:31:00,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77228.18
2025-03-09 16:31:00,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:01,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 16:31:04,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76859.266
2025-03-09 16:31:04,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:05,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 16:31:07,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76199.227
2025-03-09 16:31:07,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:08,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 16:31:11,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75708.703
2025-03-09 16:31:11,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:12,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 16:31:14,642 :: INFO :: evodenss.train.trainers :: [2051] -- [2.5s] TRAIN epoch 59 -- loss: tensor([76202.8906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:31:14,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76202.891
2025-03-09 16:31:14,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:15,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 16:31:18,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76474.0
2025-03-09 16:31:18,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:19,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 16:31:21,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75301.961
2025-03-09 16:31:21,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:22,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 16:31:25,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75803.773
2025-03-09 16:31:25,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:26,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 16:31:28,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74846.672
2025-03-09 16:31:28,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:29,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 16:31:32,257 :: INFO :: evodenss.train.trainers :: [2051] -- [2.52s] TRAIN epoch 64 -- loss: tensor([74987.0938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:31:32,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74987.094
2025-03-09 16:31:32,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:33,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 16:31:35,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74685.75
2025-03-09 16:31:35,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:36,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 16:31:39,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75246.383
2025-03-09 16:31:39,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:40,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 16:31:42,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74306.93
2025-03-09 16:31:42,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:43,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 16:31:46,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74566.508
2025-03-09 16:31:46,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:47,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 16:31:49,914 :: INFO :: evodenss.train.trainers :: [2051] -- [2.62s] TRAIN epoch 69 -- loss: tensor([73744.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:31:49,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73744.562
2025-03-09 16:31:49,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:50,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 16:31:53,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73144.938
2025-03-09 16:31:53,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:54,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 16:31:56,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73639.773
2025-03-09 16:31:56,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:31:57,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 16:32:00,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73325.555
2025-03-09 16:32:00,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:01,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 16:32:03,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73524.812
2025-03-09 16:32:03,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:04,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 16:32:07,327 :: INFO :: evodenss.train.trainers :: [2051] -- [2.56s] TRAIN epoch 74 -- loss: tensor([72798.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:32:07,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72798.297
2025-03-09 16:32:07,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:08,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 16:32:10,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72453.18
2025-03-09 16:32:10,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:11,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 16:32:14,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72862.328
2025-03-09 16:32:14,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:15,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 16:32:17,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73473.391
2025-03-09 16:32:17,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:18,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 16:32:21,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72815.25
2025-03-09 16:32:21,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:22,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 16:32:24,926 :: INFO :: evodenss.train.trainers :: [2051] -- [2.43s] TRAIN epoch 79 -- loss: tensor([71655.2109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:32:24,926 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71655.211
2025-03-09 16:32:24,926 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:25,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 16:32:28,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71431.211
2025-03-09 16:32:28,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:29,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 16:32:31,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71305.422
2025-03-09 16:32:31,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:32,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 16:32:35,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71096.852
2025-03-09 16:32:35,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:36,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 16:32:38,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71855.773
2025-03-09 16:32:38,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:39,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 16:32:42,499 :: INFO :: evodenss.train.trainers :: [2051] -- [2.54s] TRAIN epoch 84 -- loss: tensor([72182.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:32:42,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72182.438
2025-03-09 16:32:42,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:43,511 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 16:32:46,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71334.836
2025-03-09 16:32:46,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:47,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 16:32:49,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71213.156
2025-03-09 16:32:49,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:50,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 16:32:53,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71268.766
2025-03-09 16:32:53,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:54,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 16:32:56,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70886.602
2025-03-09 16:32:56,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:32:57,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 16:33:00,267 :: INFO :: evodenss.train.trainers :: [2051] -- [2.45s] TRAIN epoch 89 -- loss: tensor([70543.7734], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:33:00,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70543.773
2025-03-09 16:33:00,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:01,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 16:33:03,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70717.688
2025-03-09 16:33:03,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:04,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 16:33:07,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70364.914
2025-03-09 16:33:07,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:08,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 16:33:10,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70269.734
2025-03-09 16:33:10,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:11,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 16:33:14,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70381.125
2025-03-09 16:33:14,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:15,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 16:33:18,117 :: INFO :: evodenss.train.trainers :: [2051] -- [2.57s] TRAIN epoch 94 -- loss: tensor([70033.1562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:33:18,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70033.156
2025-03-09 16:33:18,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:19,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 16:33:21,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69710.266
2025-03-09 16:33:21,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:22,769 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 16:33:25,338 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70894.484
2025-03-09 16:33:25,338 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:26,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 16:33:28,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69407.898
2025-03-09 16:33:28,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:29,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 16:33:32,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69559.0
2025-03-09 16:33:32,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:33,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 16:33:36,094 :: INFO :: evodenss.train.trainers :: [2051] -- [2.54s] TRAIN epoch 99 -- loss: tensor([69618.3047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:33:36,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69618.305
2025-03-09 16:33:36,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:38,185 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 4233.76758
2025-03-09 16:33:38,190 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 16:33:38,191 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:84 kernel_size:5 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.15325615523249037 alpha:0.9472563778469602 weight_decay:3.033766983682965e-05 batch_size:32 epochs:100
2025-03-09 16:33:38,202 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 16:33:38,202 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 16:33:41,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 887237.312
2025-03-09 16:33:41,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:42,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 16:33:44,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:33:44,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:45,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 16:33:48,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:33:48,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:49,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 16:33:52,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1319543.5
2025-03-09 16:33:52,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:53,414 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 16:33:56,165 :: INFO :: evodenss.train.trainers :: [2051] -- [2.75s] TRAIN epoch 4 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:33:56,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:33:56,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:33:57,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 16:33:59,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:33:59,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:00,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 16:34:03,602 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:34:03,602 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:04,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 16:34:07,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:34:07,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:08,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 16:34:11,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1708279.375
2025-03-09 16:34:11,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:11,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 16:34:14,735 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 9 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:34:14,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:34:14,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:15,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 16:34:18,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:34:18,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:19,528 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 16:34:22,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:34:22,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:23,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 16:34:25,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:34:25,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:26,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 16:34:29,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804466.688
2025-03-09 16:34:29,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:30,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 16:34:33,352 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 14 -- loss: tensor([1324179.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:34:33,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1324179.375
2025-03-09 16:34:33,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:34,348 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 16:34:37,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:34:37,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:38,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 16:34:40,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:34:40,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:41,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 16:34:44,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:34:44,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:45,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 16:34:48,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:34:48,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:49,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 16:34:52,063 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 19 -- loss: tensor([1247429.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:34:52,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1247429.25
2025-03-09 16:34:52,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:53,055 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 16:34:55,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811087.562
2025-03-09 16:34:55,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:34:56,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 16:34:59,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804557.938
2025-03-09 16:34:59,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:00,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 16:35:03,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:35:03,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:04,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 16:35:06,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3409000.75
2025-03-09 16:35:06,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:07,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 16:35:10,658 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 24 -- loss: tensor([815686.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:35:10,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 815686.125
2025-03-09 16:35:10,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:11,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 16:35:14,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:35:14,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:15,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 16:35:18,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1165307.25
2025-03-09 16:35:18,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:19,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 16:35:21,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:35:21,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:22,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 16:35:25,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:35:25,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:26,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 16:35:29,290 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 29 -- loss: tensor([910146.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:35:29,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 910146.062
2025-03-09 16:35:29,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:30,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 16:35:33,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:35:33,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:34,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 16:35:36,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:35:36,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:37,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 16:35:40,465 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:35:40,465 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:41,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 16:35:44,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 826912.188
2025-03-09 16:35:44,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:45,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 16:35:47,923 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 34 -- loss: tensor([1412636.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:35:47,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1412636.5
2025-03-09 16:35:47,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:48,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 16:35:51,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:35:51,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:52,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 16:35:55,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:35:55,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:35:56,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 16:35:59,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:35:59,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:00,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 16:36:02,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2050709.0
2025-03-09 16:36:02,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:03,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 16:36:06,567 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 39 -- loss: tensor([872656.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:36:06,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 872656.188
2025-03-09 16:36:06,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:07,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 16:36:10,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1637625.0
2025-03-09 16:36:10,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:11,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 16:36:14,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:36:14,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:15,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 16:36:17,786 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:36:17,786 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:18,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 16:36:21,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:36:21,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:22,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 16:36:25,272 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 44 -- loss: tensor([1430517.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:36:25,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1430517.75
2025-03-09 16:36:25,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:26,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 16:36:28,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 985223.688
2025-03-09 16:36:28,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:29,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 16:36:32,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1048689.625
2025-03-09 16:36:32,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:33,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 16:36:36,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:36:36,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:37,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 16:36:40,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:36:40,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:41,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 16:36:43,864 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 49 -- loss: tensor([1116363.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:36:43,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1116363.5
2025-03-09 16:36:43,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:44,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 16:36:47,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 865119.062
2025-03-09 16:36:47,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:48,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 16:36:51,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 892455.812
2025-03-09 16:36:51,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:52,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 16:36:55,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1000706.812
2025-03-09 16:36:55,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:56,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 16:36:58,829 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805044.5
2025-03-09 16:36:58,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:36:59,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 16:37:02,548 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 54 -- loss: tensor([951468.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:37:02,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 951468.438
2025-03-09 16:37:02,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:03,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 16:37:06,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 955496.562
2025-03-09 16:37:06,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:07,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 16:37:09,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1076685.25
2025-03-09 16:37:09,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:10,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 16:37:13,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 903096.562
2025-03-09 16:37:13,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:14,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 16:37:17,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 879421.375
2025-03-09 16:37:17,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:18,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 16:37:21,236 :: INFO :: evodenss.train.trainers :: [2051] -- [2.85s] TRAIN epoch 59 -- loss: tensor([849515.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:37:21,237 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 849515.562
2025-03-09 16:37:21,237 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:22,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 16:37:24,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 904048.562
2025-03-09 16:37:24,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:25,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 16:37:28,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 944736.125
2025-03-09 16:37:28,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:29,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 16:37:32,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 922596.812
2025-03-09 16:37:32,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:33,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 16:37:36,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 952520.438
2025-03-09 16:37:36,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:37,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 16:37:39,909 :: INFO :: evodenss.train.trainers :: [2051] -- [2.76s] TRAIN epoch 64 -- loss: tensor([913932.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:37:39,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 913932.25
2025-03-09 16:37:39,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:40,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 16:37:43,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 834224.25
2025-03-09 16:37:43,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:44,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 16:37:47,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 851156.75
2025-03-09 16:37:47,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:48,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 16:37:51,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 934307.875
2025-03-09 16:37:51,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:52,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 16:37:54,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 964010.688
2025-03-09 16:37:54,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:55,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 16:37:58,551 :: INFO :: evodenss.train.trainers :: [2051] -- [2.7s] TRAIN epoch 69 -- loss: tensor([890826.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:37:58,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 890826.438
2025-03-09 16:37:58,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:37:59,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 16:38:02,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 922029.0
2025-03-09 16:38:02,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:03,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 16:38:06,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 881789.938
2025-03-09 16:38:06,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:06,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 16:38:09,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 872742.125
2025-03-09 16:38:09,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:10,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 16:38:13,454 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 835388.062
2025-03-09 16:38:13,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:14,456 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 16:38:17,160 :: INFO :: evodenss.train.trainers :: [2051] -- [2.7s] TRAIN epoch 74 -- loss: tensor([855122.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:38:17,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 855122.062
2025-03-09 16:38:17,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:18,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 16:38:21,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 847988.312
2025-03-09 16:38:21,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:22,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 16:38:24,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1034154.75
2025-03-09 16:38:24,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:25,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 16:38:28,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 883384.438
2025-03-09 16:38:28,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:29,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 16:38:32,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 856273.25
2025-03-09 16:38:32,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:33,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 16:38:35,878 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 79 -- loss: tensor([878125.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:38:35,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 878125.562
2025-03-09 16:38:35,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:36,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 16:38:39,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 930978.875
2025-03-09 16:38:39,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:40,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 16:38:43,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 863062.562
2025-03-09 16:38:43,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:44,338 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 16:38:47,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 873036.312
2025-03-09 16:38:47,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:48,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 16:38:50,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 921188.812
2025-03-09 16:38:50,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:51,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 16:38:54,571 :: INFO :: evodenss.train.trainers :: [2051] -- [2.67s] TRAIN epoch 84 -- loss: tensor([928581.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:38:54,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 928581.375
2025-03-09 16:38:54,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:55,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 16:38:58,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 877782.688
2025-03-09 16:38:58,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:38:59,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 16:39:02,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 864855.375
2025-03-09 16:39:02,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:02,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 16:39:05,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 884713.438
2025-03-09 16:39:05,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:06,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 16:39:09,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 846091.75
2025-03-09 16:39:09,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:10,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 16:39:13,158 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 89 -- loss: tensor([845957.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:39:13,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 845957.5
2025-03-09 16:39:13,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:14,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 16:39:16,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 956934.812
2025-03-09 16:39:16,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:17,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 16:39:20,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 944923.875
2025-03-09 16:39:20,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:21,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 16:39:24,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 863438.438
2025-03-09 16:39:24,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:25,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 16:39:28,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 942476.312
2025-03-09 16:39:28,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:29,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 16:39:31,805 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 94 -- loss: tensor([898389.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:39:31,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 898389.812
2025-03-09 16:39:31,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:32,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 16:39:35,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 942996.688
2025-03-09 16:39:35,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:36,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 16:39:39,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 847626.438
2025-03-09 16:39:39,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:40,238 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 16:39:42,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 827020.75
2025-03-09 16:39:42,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:43,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 16:39:46,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 898586.0
2025-03-09 16:39:46,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:47,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 16:39:50,499 :: INFO :: evodenss.train.trainers :: [2051] -- [2.82s] TRAIN epoch 99 -- loss: tensor([878824.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:39:50,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 878824.125
2025-03-09 16:39:50,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:52,534 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 74907.34375
2025-03-09 16:39:52,538 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 16:39:52,539 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:33 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:33 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :deconv1d out_channels:88 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 
layer12: :fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.21392777113391248 beta1:0.9520928483562265 beta2:0.996180483672492 weight_decay:0.0001621548771194498 batch_size:32 epochs:100
2025-03-09 16:39:52,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 16:39:52,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 16:39:54,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 830282.312
2025-03-09 16:39:54,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:55,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 16:39:57,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:39:57,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:39:58,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 16:40:00,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:00,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:01,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 16:40:03,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:03,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:04,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 16:40:06,248 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 4 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:40:06,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:06,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:07,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 16:40:09,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:09,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:10,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 16:40:12,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:12,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:12,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 16:40:14,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:14,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:15,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 16:40:17,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:17,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:18,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 16:40:20,762 :: INFO :: evodenss.train.trainers :: [2051] -- [1.95s] TRAIN epoch 9 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:40:20,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:20,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:21,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 16:40:23,637 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:23,637 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:24,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 16:40:26,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:26,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:27,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 16:40:29,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:29,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:30,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 16:40:32,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:32,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:33,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 16:40:35,211 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 14 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:40:35,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:35,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:36,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 16:40:38,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:38,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:39,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 16:40:40,978 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:40,978 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:41,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 16:40:43,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:43,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:44,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 16:40:46,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:46,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:47,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 16:40:49,795 :: INFO :: evodenss.train.trainers :: [2051] -- [2.02s] TRAIN epoch 19 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:40:49,795 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:49,795 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:50,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 16:40:52,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:52,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:53,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 16:40:55,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:55,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:56,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 16:40:58,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:40:58,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:40:59,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 16:41:01,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:41:01,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:02,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 16:41:04,395 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 24 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:41:04,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:41:04,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:05,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 16:41:07,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:41:07,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:08,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 16:41:10,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:41:10,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:11,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 16:41:13,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804501.688
2025-03-09 16:41:13,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:14,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 16:41:16,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:41:16,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:17,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 16:41:19,150 :: INFO :: evodenss.train.trainers :: [2051] -- [1.96s] TRAIN epoch 29 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:41:19,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:41:19,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:20,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 16:41:22,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:41:22,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:23,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 16:41:25,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:41:25,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:26,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 16:41:28,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:41:28,060 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:29,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 16:41:31,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:41:31,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:32,025 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 16:41:33,948 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 34 -- loss: tensor([2014073.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:41:33,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2014073.125
2025-03-09 16:41:33,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:34,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 16:41:36,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1630881.25
2025-03-09 16:41:36,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:37,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 16:41:39,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:41:39,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:40,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 16:41:42,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 988490.938
2025-03-09 16:41:42,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:43,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 16:41:45,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1009376.062
2025-03-09 16:41:45,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:46,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 16:41:48,786 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 39 -- loss: tensor([1052831.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:41:48,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1052831.75
2025-03-09 16:41:48,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:49,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 16:41:51,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 922801.562
2025-03-09 16:41:51,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:52,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 16:41:54,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 845349.312
2025-03-09 16:41:54,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:55,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 16:41:57,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 943182.688
2025-03-09 16:41:57,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:41:58,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 16:42:00,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:00,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:01,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 16:42:03,745 :: INFO :: evodenss.train.trainers :: [2051] -- [1.91s] TRAIN epoch 44 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:42:03,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:03,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:04,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 16:42:06,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:06,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:07,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 16:42:09,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:09,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:10,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 16:42:12,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:12,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:13,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 16:42:15,593 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:15,593 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:16,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 16:42:18,599 :: INFO :: evodenss.train.trainers :: [2051] -- [1.98s] TRAIN epoch 49 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:42:18,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:18,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:20,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 16:42:21,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804549.312
2025-03-09 16:42:21,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:22,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 16:42:24,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804857.188
2025-03-09 16:42:24,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:25,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 16:42:27,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:27,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:28,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 16:42:30,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:30,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:31,870 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 16:42:33,793 :: INFO :: evodenss.train.trainers :: [2051] -- [1.92s] TRAIN epoch 54 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:42:33,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:33,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:34,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 16:42:36,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:36,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:37,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 16:42:39,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:39,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:40,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 16:42:42,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:42,700 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:43,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 16:42:45,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:45,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:46,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 16:42:48,657 :: INFO :: evodenss.train.trainers :: [2051] -- [1.93s] TRAIN epoch 59 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:42:48,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:48,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:49,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 16:42:51,715 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:51,715 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:52,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 16:42:54,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804484.812
2025-03-09 16:42:54,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:55,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 16:42:57,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:42:57,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:42:58,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 16:43:00,592 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804502.812
2025-03-09 16:43:00,592 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:01,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 16:43:03,608 :: INFO :: evodenss.train.trainers :: [2051] -- [1.97s] TRAIN epoch 64 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:43:03,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:03,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:04,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 16:43:06,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:06,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:07,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 16:43:09,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:09,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:10,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 16:43:12,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:12,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:13,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 16:43:15,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:15,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:16,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 16:43:18,527 :: INFO :: evodenss.train.trainers :: [2051] -- [1.99s] TRAIN epoch 69 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:43:18,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:18,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:19,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 16:43:21,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:21,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:22,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 16:43:24,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804595.438
2025-03-09 16:43:24,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:25,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 16:43:27,355 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 875623.062
2025-03-09 16:43:27,355 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:28,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 16:43:30,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:30,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:31,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 16:43:33,080 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 74 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:43:33,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:33,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:34,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 16:43:35,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:35,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:36,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 16:43:38,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 808191.312
2025-03-09 16:43:38,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:39,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 16:43:41,663 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:41,663 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:42,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 16:43:44,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 923409.562
2025-03-09 16:43:44,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:45,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 16:43:47,408 :: INFO :: evodenss.train.trainers :: [2051] -- [1.89s] TRAIN epoch 79 -- loss: tensor([804525.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:43:47,408 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804525.688
2025-03-09 16:43:47,408 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:48,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 16:43:50,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:50,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:51,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 16:43:53,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:53,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:54,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 16:43:56,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:43:56,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:43:57,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 16:43:59,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 827446.688
2025-03-09 16:43:59,050 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:00,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 16:44:01,934 :: INFO :: evodenss.train.trainers :: [2051] -- [1.9s] TRAIN epoch 84 -- loss: tensor([907474.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:44:01,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 907474.062
2025-03-09 16:44:01,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:02,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 16:44:04,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 910992.625
2025-03-09 16:44:04,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:05,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 16:44:07,580 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:44:07,580 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:08,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 16:44:10,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811740.188
2025-03-09 16:44:10,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:11,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 16:44:13,260 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 812163.812
2025-03-09 16:44:13,260 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:14,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 16:44:16,080 :: INFO :: evodenss.train.trainers :: [2051] -- [1.87s] TRAIN epoch 89 -- loss: tensor([3410328.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:44:16,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3410328.25
2025-03-09 16:44:16,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:17,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 16:44:18,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1132731.75
2025-03-09 16:44:18,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:19,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 16:44:21,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 944342.438
2025-03-09 16:44:21,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:22,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 16:44:24,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1419886.625
2025-03-09 16:44:24,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:25,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 16:44:27,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 895964.812
2025-03-09 16:44:27,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:28,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 16:44:30,401 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 94 -- loss: tensor([899645.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:44:30,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 899645.188
2025-03-09 16:44:30,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:31,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 16:44:33,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 16:44:33,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:34,195 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 16:44:36,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 856862.062
2025-03-09 16:44:36,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:37,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 16:44:38,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804506.938
2025-03-09 16:44:38,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:39,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 16:44:41,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 824987.438
2025-03-09 16:44:41,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:42,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 16:44:44,652 :: INFO :: evodenss.train.trainers :: [2051] -- [1.88s] TRAIN epoch 99 -- loss: tensor([934243.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:44:44,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 934243.812
2025-03-09 16:44:44,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:46,630 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 44624.05078
2025-03-09 16:44:46,630 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 16:44:46,631 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 0, id: 0
2025-03-09 16:44:46,631 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 1000, 1000]
2025-03-09 16:44:46,631 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 16:44:46,634 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [3794.05249, 4233.76758, 74907.34375, 44624.05078]
2025-03-09 16:44:46,885 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([18616.0684], device='cuda:0')
2025-03-09 16:44:46,886 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 10: 3794.05249
2025-03-09 16:44:46,886 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3675.68726



2025-03-09 16:44:46,967 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 11
2025-03-09 16:44:46,967 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 16:44:46,978 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a layer removed from Module 0: features; Position: 12
2025-03-09 16:44:46,979 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 16:44:46,979 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 16:44:46,980 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 16:44:46,980 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 16:44:46,981 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 16:44:46,982 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 16:44:46,983 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 16:44:46,983 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 16:44:46,986 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 16:44:46,987 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 16:44:46,988 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 16:44:46,988 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 16:44:46,989 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 16:44:46,990 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 16:44:46,990 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 16:44:46,991 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 16:44:46,993 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 5
2025-03-09 16:44:46,994 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have an extra layer at Module 0: features; Position: 9. Reused?: False
2025-03-09 16:44:46,995 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have an extra layer at Module 0: features; Position: 13. Reused?: True
2025-03-09 16:44:46,995 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 16:44:46,996 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 16:44:46,997 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 16:44:46,997 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 16:44:46,998 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 16:44:47,000 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 16:44:47,004 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 16:44:47,005 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 16:44:47,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 16:44:47,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 16:44:49,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 524067.375
2025-03-09 16:44:49,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:50,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 16:44:53,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 189744.906
2025-03-09 16:44:53,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:54,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 16:44:57,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 156704.828
2025-03-09 16:44:57,024 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:44:58,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 16:45:00,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 129719.266
2025-03-09 16:45:00,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:01,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 16:45:04,373 :: INFO :: evodenss.train.trainers :: [2051] -- [2.67s] TRAIN epoch 4 -- loss: tensor([120239.4609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:45:04,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 120239.461
2025-03-09 16:45:04,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:05,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 16:45:08,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109623.164
2025-03-09 16:45:08,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:09,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 16:45:11,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108693.32
2025-03-09 16:45:11,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:12,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 16:45:15,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101985.133
2025-03-09 16:45:15,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:16,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 16:45:19,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101700.906
2025-03-09 16:45:19,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:20,120 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 16:45:22,776 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 9 -- loss: tensor([97313.0703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:45:22,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97313.07
2025-03-09 16:45:22,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:23,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 16:45:26,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96283.82
2025-03-09 16:45:26,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:27,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 16:45:30,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93871.242
2025-03-09 16:45:30,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:31,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 16:45:33,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93408.266
2025-03-09 16:45:33,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:34,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 16:45:37,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89566.102
2025-03-09 16:45:37,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:38,513 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 16:45:41,193 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 14 -- loss: tensor([89301.7891], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:45:41,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89301.789
2025-03-09 16:45:41,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:42,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 16:45:44,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90434.133
2025-03-09 16:45:44,916 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:45,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 16:45:48,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87633.562
2025-03-09 16:45:48,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:49,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 16:45:52,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86493.43
2025-03-09 16:45:52,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:53,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 16:45:56,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85419.32
2025-03-09 16:45:56,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:45:57,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 16:45:59,825 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 19 -- loss: tensor([83979.5234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:45:59,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83979.523
2025-03-09 16:45:59,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:00,820 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 16:46:03,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84048.234
2025-03-09 16:46:03,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:04,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 16:46:07,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82762.492
2025-03-09 16:46:07,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:08,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 16:46:10,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82158.281
2025-03-09 16:46:10,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:11,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 16:46:14,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81773.227
2025-03-09 16:46:14,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:15,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 16:46:18,209 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 24 -- loss: tensor([80685.9062], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:46:18,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80685.906
2025-03-09 16:46:18,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:19,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 16:46:21,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78870.062
2025-03-09 16:46:21,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:23,007 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 16:46:25,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78418.422
2025-03-09 16:46:25,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:26,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 16:46:29,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77961.445
2025-03-09 16:46:29,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:30,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 16:46:33,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78648.508
2025-03-09 16:46:33,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:34,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 16:46:36,843 :: INFO :: evodenss.train.trainers :: [2051] -- [2.67s] TRAIN epoch 29 -- loss: tensor([77610.1953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:46:36,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77610.195
2025-03-09 16:46:36,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:37,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 16:46:40,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77157.078
2025-03-09 16:46:40,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:41,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 16:46:44,206 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75839.773
2025-03-09 16:46:44,206 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:45,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 16:46:47,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76605.391
2025-03-09 16:46:47,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:48,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 16:46:51,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76364.906
2025-03-09 16:46:51,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:52,726 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 16:46:55,431 :: INFO :: evodenss.train.trainers :: [2051] -- [2.7s] TRAIN epoch 34 -- loss: tensor([75322.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:46:55,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75322.938
2025-03-09 16:46:55,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:46:56,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 16:46:59,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74323.945
2025-03-09 16:46:59,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:00,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 16:47:02,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75318.391
2025-03-09 16:47:02,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:03,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 16:47:06,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73742.281
2025-03-09 16:47:06,566 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:07,580 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 16:47:10,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72814.336
2025-03-09 16:47:10,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:11,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 16:47:13,975 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 39 -- loss: tensor([72571.9766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:47:13,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72571.977
2025-03-09 16:47:13,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:15,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 16:47:17,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72453.258
2025-03-09 16:47:17,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:18,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 16:47:21,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73556.93
2025-03-09 16:47:21,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:22,552 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 16:47:25,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71776.484
2025-03-09 16:47:25,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:26,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 16:47:28,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72254.391
2025-03-09 16:47:28,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:29,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 16:47:32,720 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 44 -- loss: tensor([72259.9062], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:47:32,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72259.906
2025-03-09 16:47:32,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:33,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 16:47:36,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71020.617
2025-03-09 16:47:36,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:37,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 16:47:40,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70858.234
2025-03-09 16:47:40,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:41,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 16:47:43,897 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70615.328
2025-03-09 16:47:43,897 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:44,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 16:47:47,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70563.531
2025-03-09 16:47:47,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:48,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 16:47:51,448 :: INFO :: evodenss.train.trainers :: [2051] -- [2.77s] TRAIN epoch 49 -- loss: tensor([69600.0781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:47:51,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69600.078
2025-03-09 16:47:51,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:52,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 16:47:55,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69727.523
2025-03-09 16:47:55,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:56,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 16:47:58,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70981.711
2025-03-09 16:47:58,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:47:59,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 16:48:02,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69572.219
2025-03-09 16:48:02,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:03,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 16:48:06,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68798.625
2025-03-09 16:48:06,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:07,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 16:48:10,188 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 54 -- loss: tensor([68701.4453], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:48:10,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68701.445
2025-03-09 16:48:10,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:11,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 16:48:13,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69014.375
2025-03-09 16:48:13,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:14,979 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 16:48:17,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68216.031
2025-03-09 16:48:17,697 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:18,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 16:48:21,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67714.297
2025-03-09 16:48:21,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:22,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 16:48:25,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68007.266
2025-03-09 16:48:25,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:26,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 16:48:29,031 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 59 -- loss: tensor([67433.7656], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:48:29,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67433.766
2025-03-09 16:48:29,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:30,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 16:48:32,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67110.859
2025-03-09 16:48:32,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:33,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 16:48:36,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67057.711
2025-03-09 16:48:36,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:37,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 16:48:40,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65952.469
2025-03-09 16:48:40,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:41,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 16:48:44,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66512.305
2025-03-09 16:48:44,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:45,050 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 16:48:47,805 :: INFO :: evodenss.train.trainers :: [2051] -- [2.75s] TRAIN epoch 64 -- loss: tensor([66660.7734], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:48:47,806 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66660.773
2025-03-09 16:48:47,806 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:48,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 16:48:51,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65993.156
2025-03-09 16:48:51,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:52,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 16:48:55,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66641.125
2025-03-09 16:48:55,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:48:56,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 16:48:59,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66064.992
2025-03-09 16:48:59,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:00,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 16:49:02,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65981.102
2025-03-09 16:49:02,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:03,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 16:49:06,667 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 69 -- loss: tensor([66221.4297], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:49:06,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66221.43
2025-03-09 16:49:06,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:07,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 16:49:10,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66053.883
2025-03-09 16:49:10,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:11,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 16:49:14,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65830.703
2025-03-09 16:49:14,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:15,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 16:49:17,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65300.582
2025-03-09 16:49:17,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:19,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 16:49:21,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65292.559
2025-03-09 16:49:21,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:22,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 16:49:25,553 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 74 -- loss: tensor([64680.0977], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:49:25,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64680.098
2025-03-09 16:49:25,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:26,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 16:49:29,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64643.441
2025-03-09 16:49:29,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:30,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 16:49:33,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65219.691
2025-03-09 16:49:33,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:34,174 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 16:49:36,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63864.551
2025-03-09 16:49:36,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:37,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 16:49:40,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64679.648
2025-03-09 16:49:40,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:41,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 16:49:44,373 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 79 -- loss: tensor([64474.7461], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:49:44,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64474.746
2025-03-09 16:49:44,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:45,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 16:49:48,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64118.188
2025-03-09 16:49:48,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:49,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 16:49:52,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64725.0
2025-03-09 16:49:52,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:53,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 16:49:55,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63466.621
2025-03-09 16:49:55,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:49:56,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 16:49:59,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64622.738
2025-03-09 16:49:59,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:00,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 16:50:03,287 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 84 -- loss: tensor([63593.5078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:50:03,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63593.508
2025-03-09 16:50:03,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:04,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 16:50:07,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63915.977
2025-03-09 16:50:07,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:08,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 16:50:10,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 62579.879
2025-03-09 16:50:10,763 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:11,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 16:50:14,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63712.668
2025-03-09 16:50:14,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:15,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 16:50:18,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 62882.16
2025-03-09 16:50:18,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:19,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 16:50:22,166 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 89 -- loss: tensor([62691.0469], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:50:22,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 62691.047
2025-03-09 16:50:22,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:23,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 16:50:25,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63866.871
2025-03-09 16:50:25,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:26,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 16:50:29,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63133.555
2025-03-09 16:50:29,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:30,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 16:50:33,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 62728.082
2025-03-09 16:50:33,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:34,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 16:50:37,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 62668.227
2025-03-09 16:50:37,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:38,116 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 16:50:40,835 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 94 -- loss: tensor([62220.3906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:50:40,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 62220.391
2025-03-09 16:50:40,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:41,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 16:50:44,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 61628.371
2025-03-09 16:50:44,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:45,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 16:50:48,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 62053.707
2025-03-09 16:50:48,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:49,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 16:50:51,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63077.055
2025-03-09 16:50:51,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:52,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 16:50:55,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 62839.758
2025-03-09 16:50:55,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:50:56,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 16:50:59,175 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 99 -- loss: tensor([63036.4688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:50:59,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63036.469
2025-03-09 16:50:59,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:01,147 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 3575.71436
2025-03-09 16:51:01,152 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 16:51:01,153 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :deconv1d out_channels:35 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 16:51:01,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 16:51:01,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 16:51:03,662 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 825558.688
2025-03-09 16:51:03,662 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:04,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 16:51:07,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 333790.375
2025-03-09 16:51:07,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:08,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 16:51:10,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 251052.641
2025-03-09 16:51:10,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:11,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 16:51:13,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 212514.375
2025-03-09 16:51:13,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:14,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 16:51:17,197 :: INFO :: evodenss.train.trainers :: [2051] -- [2.41s] TRAIN epoch 4 -- loss: tensor([181927.4062], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:51:17,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 181927.406
2025-03-09 16:51:17,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:18,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 16:51:20,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 167914.75
2025-03-09 16:51:20,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:21,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 16:51:24,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 153608.062
2025-03-09 16:51:24,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:25,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 16:51:27,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 140526.266
2025-03-09 16:51:27,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:28,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 16:51:30,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 135035.781
2025-03-09 16:51:30,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:31,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 16:51:34,164 :: INFO :: evodenss.train.trainers :: [2051] -- [2.39s] TRAIN epoch 9 -- loss: tensor([128950.6719], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:51:34,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 128950.672
2025-03-09 16:51:34,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:35,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 16:51:37,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 121499.141
2025-03-09 16:51:37,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:38,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 16:51:40,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 116228.961
2025-03-09 16:51:40,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:41,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 16:51:44,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 113588.086
2025-03-09 16:51:44,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:45,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 16:51:47,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110219.547
2025-03-09 16:51:47,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:48,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 16:51:52,684 :: INFO :: evodenss.train.trainers :: [2051] -- [4.08s] TRAIN epoch 14 -- loss: tensor([108541.3047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:51:52,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108541.305
2025-03-09 16:51:52,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:53,643 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 16:51:56,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105886.547
2025-03-09 16:51:56,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:51:57,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 16:51:59,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103741.266
2025-03-09 16:51:59,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:00,466 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 16:52:02,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102762.312
2025-03-09 16:52:02,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:03,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 16:52:06,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101064.133
2025-03-09 16:52:06,216 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:07,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 16:52:09,596 :: INFO :: evodenss.train.trainers :: [2051] -- [2.38s] TRAIN epoch 19 -- loss: tensor([100343.6484], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:52:09,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100343.648
2025-03-09 16:52:09,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:10,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 16:52:12,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98205.664
2025-03-09 16:52:12,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:13,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 16:52:16,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98127.445
2025-03-09 16:52:16,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:17,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 16:52:19,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97748.172
2025-03-09 16:52:19,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:20,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 16:52:23,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96670.141
2025-03-09 16:52:23,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:24,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 16:52:26,654 :: INFO :: evodenss.train.trainers :: [2051] -- [2.4s] TRAIN epoch 24 -- loss: tensor([96640.3984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:52:26,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96640.398
2025-03-09 16:52:26,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:27,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 16:52:30,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95892.055
2025-03-09 16:52:30,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:31,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 16:52:33,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93752.68
2025-03-09 16:52:33,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:34,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 16:52:36,894 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94018.633
2025-03-09 16:52:36,894 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:37,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 16:52:40,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94160.328
2025-03-09 16:52:40,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:41,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 16:52:43,722 :: INFO :: evodenss.train.trainers :: [2051] -- [2.41s] TRAIN epoch 29 -- loss: tensor([92302.5469], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:52:43,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92302.547
2025-03-09 16:52:43,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:44,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 16:52:47,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91428.453
2025-03-09 16:52:47,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:48,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 16:52:50,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91907.695
2025-03-09 16:52:50,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:51,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 16:52:54,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92358.641
2025-03-09 16:52:54,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:55,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 16:52:57,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90087.562
2025-03-09 16:52:57,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:52:58,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 16:53:00,850 :: INFO :: evodenss.train.trainers :: [2051] -- [2.4s] TRAIN epoch 34 -- loss: tensor([90238.0234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:53:00,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90238.023
2025-03-09 16:53:00,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:01,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 16:53:04,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90944.328
2025-03-09 16:53:04,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:05,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 16:53:07,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89655.383
2025-03-09 16:53:07,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:08,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 16:53:10,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89210.586
2025-03-09 16:53:10,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:11,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 16:53:14,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88807.211
2025-03-09 16:53:14,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:15,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 16:53:17,594 :: INFO :: evodenss.train.trainers :: [2051] -- [2.36s] TRAIN epoch 39 -- loss: tensor([89128.0312], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:53:17,594 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89128.031
2025-03-09 16:53:17,594 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:18,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 16:53:21,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88592.281
2025-03-09 16:53:21,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:22,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 16:53:24,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87169.188
2025-03-09 16:53:24,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:25,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 16:53:27,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88235.359
2025-03-09 16:53:27,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:28,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 16:53:31,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86992.406
2025-03-09 16:53:31,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:32,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 16:53:34,567 :: INFO :: evodenss.train.trainers :: [2051] -- [2.38s] TRAIN epoch 44 -- loss: tensor([86679.7969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:53:34,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86679.797
2025-03-09 16:53:34,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:35,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 16:53:37,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85987.945
2025-03-09 16:53:37,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:38,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 16:53:41,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84765.117
2025-03-09 16:53:41,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:42,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 16:53:44,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85326.438
2025-03-09 16:53:44,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:45,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 16:53:48,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84536.562
2025-03-09 16:53:48,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:49,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 16:53:51,619 :: INFO :: evodenss.train.trainers :: [2051] -- [2.44s] TRAIN epoch 49 -- loss: tensor([83768.4844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:53:51,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83768.484
2025-03-09 16:53:51,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:52,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 16:53:55,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83317.016
2025-03-09 16:53:55,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:56,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 16:53:58,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82948.219
2025-03-09 16:53:58,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:53:59,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 16:54:01,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82962.836
2025-03-09 16:54:01,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:02,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 16:54:05,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82592.695
2025-03-09 16:54:05,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:06,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 16:54:08,518 :: INFO :: evodenss.train.trainers :: [2051] -- [2.38s] TRAIN epoch 54 -- loss: tensor([82017.0781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:54:08,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82017.078
2025-03-09 16:54:08,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:09,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 16:54:11,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82227.844
2025-03-09 16:54:11,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:12,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 16:54:15,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81275.414
2025-03-09 16:54:15,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:16,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 16:54:18,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81305.898
2025-03-09 16:54:18,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:19,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 16:54:22,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79856.547
2025-03-09 16:54:22,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:23,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 16:54:25,557 :: INFO :: evodenss.train.trainers :: [2051] -- [2.36s] TRAIN epoch 59 -- loss: tensor([79780.3828], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:54:25,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79780.383
2025-03-09 16:54:25,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:26,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 16:54:28,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80420.422
2025-03-09 16:54:28,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:29,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 16:54:32,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78019.273
2025-03-09 16:54:32,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:33,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 16:54:35,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78645.828
2025-03-09 16:54:35,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:36,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 16:54:39,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79246.844
2025-03-09 16:54:39,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:40,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 16:54:42,526 :: INFO :: evodenss.train.trainers :: [2051] -- [2.39s] TRAIN epoch 64 -- loss: tensor([76961.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:54:42,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76961.812
2025-03-09 16:54:42,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:43,513 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 16:54:45,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77359.734
2025-03-09 16:54:45,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:46,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 16:54:49,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77354.539
2025-03-09 16:54:49,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:50,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 16:54:52,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77316.375
2025-03-09 16:54:52,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:53,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 16:54:56,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76720.703
2025-03-09 16:54:56,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:54:57,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 16:54:59,610 :: INFO :: evodenss.train.trainers :: [2051] -- [2.39s] TRAIN epoch 69 -- loss: tensor([77005.0938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:54:59,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77005.094
2025-03-09 16:54:59,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:00,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 16:55:02,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75789.328
2025-03-09 16:55:02,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:04,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 16:55:06,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75809.656
2025-03-09 16:55:06,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:07,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 16:55:09,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74868.023
2025-03-09 16:55:09,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:10,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 16:55:13,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75786.359
2025-03-09 16:55:13,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:14,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 16:55:16,754 :: INFO :: evodenss.train.trainers :: [2051] -- [2.43s] TRAIN epoch 74 -- loss: tensor([75648.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:55:16,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75648.312
2025-03-09 16:55:16,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:17,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 16:55:20,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74221.547
2025-03-09 16:55:20,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:21,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 16:55:23,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74871.984
2025-03-09 16:55:23,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:24,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 16:55:27,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74110.68
2025-03-09 16:55:27,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:28,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 16:55:30,592 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74625.414
2025-03-09 16:55:30,592 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:31,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 16:55:34,079 :: INFO :: evodenss.train.trainers :: [2051] -- [2.46s] TRAIN epoch 79 -- loss: tensor([73851.9219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:55:34,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73851.922
2025-03-09 16:55:34,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:35,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 16:55:37,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74243.875
2025-03-09 16:55:37,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:38,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 16:55:41,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73002.844
2025-03-09 16:55:41,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:42,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 16:55:44,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72358.477
2025-03-09 16:55:44,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:45,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 16:55:47,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73341.281
2025-03-09 16:55:47,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:48,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 16:55:51,385 :: INFO :: evodenss.train.trainers :: [2051] -- [2.47s] TRAIN epoch 84 -- loss: tensor([72619.7422], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:55:51,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72619.742
2025-03-09 16:55:51,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:52,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 16:55:54,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72626.891
2025-03-09 16:55:54,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:55,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 16:55:58,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73636.477
2025-03-09 16:55:58,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:55:59,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 16:56:01,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72516.891
2025-03-09 16:56:01,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:02,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 16:56:05,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72389.219
2025-03-09 16:56:05,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:06,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 16:56:08,629 :: INFO :: evodenss.train.trainers :: [2051] -- [2.4s] TRAIN epoch 89 -- loss: tensor([73114.3906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:56:08,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73114.391
2025-03-09 16:56:08,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:09,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 16:56:12,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72495.383
2025-03-09 16:56:12,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:13,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 16:56:15,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71367.914
2025-03-09 16:56:15,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:16,535 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 16:56:18,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71390.922
2025-03-09 16:56:18,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:20,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 16:56:22,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71184.438
2025-03-09 16:56:22,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:23,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 16:56:25,960 :: INFO :: evodenss.train.trainers :: [2051] -- [2.41s] TRAIN epoch 94 -- loss: tensor([70860.4453], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:56:25,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70860.445
2025-03-09 16:56:25,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:26,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 16:56:29,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71882.664
2025-03-09 16:56:29,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:30,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 16:56:32,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71823.336
2025-03-09 16:56:32,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:33,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 16:56:36,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70503.172
2025-03-09 16:56:36,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:37,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 16:56:39,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72277.109
2025-03-09 16:56:39,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:40,433 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 16:56:42,819 :: INFO :: evodenss.train.trainers :: [2051] -- [2.38s] TRAIN epoch 99 -- loss: tensor([68965.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:56:42,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68965.25
2025-03-09 16:56:42,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:44,835 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 4118.32861
2025-03-09 16:56:44,840 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 16:56:44,841 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 16:56:44,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 16:56:44,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 16:56:47,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 445558.281
2025-03-09 16:56:47,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:48,573 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 16:56:51,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 222234.406
2025-03-09 16:56:51,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:52,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 16:56:54,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 170624.484
2025-03-09 16:56:55,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:56,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 16:56:58,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 146490.016
2025-03-09 16:56:58,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:56:59,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 16:57:02,377 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 4 -- loss: tensor([132334.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:57:02,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 132334.297
2025-03-09 16:57:02,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:03,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 16:57:06,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 119415.125
2025-03-09 16:57:06,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:07,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 16:57:09,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 111457.188
2025-03-09 16:57:09,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:10,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 16:57:13,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106064.695
2025-03-09 16:57:13,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:14,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 16:57:17,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103022.391
2025-03-09 16:57:17,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:18,206 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 16:57:21,018 :: INFO :: evodenss.train.trainers :: [2051] -- [2.81s] TRAIN epoch 9 -- loss: tensor([98984.7812], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:57:21,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98984.781
2025-03-09 16:57:21,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:22,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 16:57:24,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96862.953
2025-03-09 16:57:24,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:25,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 16:57:28,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94961.0
2025-03-09 16:57:28,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:29,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 16:57:32,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93735.391
2025-03-09 16:57:32,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:33,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 16:57:35,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91932.672
2025-03-09 16:57:35,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:36,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 16:57:39,594 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 14 -- loss: tensor([90390.0391], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:57:39,594 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90390.039
2025-03-09 16:57:39,594 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:40,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 16:57:43,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90157.445
2025-03-09 16:57:43,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:44,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 16:57:47,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88459.789
2025-03-09 16:57:47,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:48,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 16:57:50,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87689.539
2025-03-09 16:57:50,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:51,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 16:57:54,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87715.695
2025-03-09 16:57:54,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:55,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 16:57:58,388 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 19 -- loss: tensor([86111.0938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:57:58,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86111.094
2025-03-09 16:57:58,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:57:59,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 16:58:02,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84808.469
2025-03-09 16:58:02,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:03,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 16:58:05,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85818.43
2025-03-09 16:58:05,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:06,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 16:58:09,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84555.156
2025-03-09 16:58:09,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:10,663 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 16:58:13,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83101.438
2025-03-09 16:58:13,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:14,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 16:58:17,088 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 24 -- loss: tensor([83016.4219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:58:17,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83016.422
2025-03-09 16:58:17,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:18,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 16:58:20,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81528.492
2025-03-09 16:58:20,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:21,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 16:58:24,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81738.141
2025-03-09 16:58:24,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:25,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 16:58:28,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80582.258
2025-03-09 16:58:28,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:29,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 16:58:32,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81087.727
2025-03-09 16:58:32,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:33,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 16:58:35,844 :: INFO :: evodenss.train.trainers :: [2051] -- [2.66s] TRAIN epoch 29 -- loss: tensor([80016.1953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:58:35,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80016.195
2025-03-09 16:58:35,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:36,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 16:58:39,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80087.633
2025-03-09 16:58:39,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:40,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 16:58:43,304 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78816.281
2025-03-09 16:58:43,304 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:44,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 16:58:47,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79137.781
2025-03-09 16:58:47,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:48,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 16:58:50,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78024.938
2025-03-09 16:58:50,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:51,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 16:58:54,497 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 34 -- loss: tensor([77645.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:58:54,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77645.562
2025-03-09 16:58:54,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:55,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 16:58:58,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77118.055
2025-03-09 16:58:58,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:58:59,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 16:59:01,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76731.375
2025-03-09 16:59:01,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:03,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 16:59:05,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77197.109
2025-03-09 16:59:05,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:06,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 16:59:09,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76420.016
2025-03-09 16:59:09,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:10,528 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 16:59:13,222 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 39 -- loss: tensor([75551.7188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:59:13,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75551.719
2025-03-09 16:59:13,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:14,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 16:59:16,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75451.594
2025-03-09 16:59:16,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:18,058 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 16:59:20,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74852.859
2025-03-09 16:59:20,916 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:21,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 16:59:24,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74591.055
2025-03-09 16:59:24,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:25,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 16:59:28,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73791.508
2025-03-09 16:59:28,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:29,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 16:59:32,200 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 44 -- loss: tensor([74326.3984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:59:32,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74326.398
2025-03-09 16:59:32,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:33,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 16:59:35,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73676.25
2025-03-09 16:59:35,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:37,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 16:59:39,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73255.203
2025-03-09 16:59:39,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:40,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 16:59:43,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72504.523
2025-03-09 16:59:43,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:44,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 16:59:47,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73482.992
2025-03-09 16:59:47,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:48,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 16:59:51,164 :: INFO :: evodenss.train.trainers :: [2051] -- [2.84s] TRAIN epoch 49 -- loss: tensor([72459.3594], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 16:59:51,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72459.359
2025-03-09 16:59:51,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:52,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 16:59:54,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72049.68
2025-03-09 16:59:54,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:55,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 16:59:58,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71568.141
2025-03-09 16:59:58,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 16:59:59,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 17:00:02,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71730.039
2025-03-09 17:00:02,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:03,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 17:00:06,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71290.594
2025-03-09 17:00:06,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:07,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 17:00:09,954 :: INFO :: evodenss.train.trainers :: [2051] -- [2.67s] TRAIN epoch 54 -- loss: tensor([71753.5078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:00:09,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71753.508
2025-03-09 17:00:09,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:10,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 17:00:13,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71438.68
2025-03-09 17:00:13,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:14,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 17:00:17,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70390.945
2025-03-09 17:00:17,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:18,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 17:00:21,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71368.703
2025-03-09 17:00:21,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:22,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 17:00:24,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69782.859
2025-03-09 17:00:24,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:25,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 17:00:28,639 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 59 -- loss: tensor([70723.6719], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:00:28,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70723.672
2025-03-09 17:00:28,639 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:29,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 17:00:32,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70236.5
2025-03-09 17:00:32,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:33,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 17:00:36,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70466.016
2025-03-09 17:00:36,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:37,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 17:00:39,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70290.914
2025-03-09 17:00:39,715 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:40,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 17:00:43,361 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69863.766
2025-03-09 17:00:43,361 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:44,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 17:00:46,951 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 64 -- loss: tensor([69262.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:00:46,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69262.938
2025-03-09 17:00:46,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:47,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 17:00:50,700 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69168.031
2025-03-09 17:00:50,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:51,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 17:00:54,340 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68284.75
2025-03-09 17:00:54,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:55,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 17:00:57,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68962.109
2025-03-09 17:00:57,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:00:58,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 17:01:01,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68051.07
2025-03-09 17:01:01,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:02,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 17:01:05,343 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 69 -- loss: tensor([68510.8203], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:01:05,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68510.82
2025-03-09 17:01:05,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:06,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 17:01:08,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68678.539
2025-03-09 17:01:08,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:09,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 17:01:12,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66929.547
2025-03-09 17:01:12,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:13,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 17:01:16,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67945.086
2025-03-09 17:01:16,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:17,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 17:01:19,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67658.5
2025-03-09 17:01:19,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:20,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 17:01:23,634 :: INFO :: evodenss.train.trainers :: [2051] -- [2.67s] TRAIN epoch 74 -- loss: tensor([67636.6797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:01:23,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67636.68
2025-03-09 17:01:23,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:24,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 17:01:27,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66829.086
2025-03-09 17:01:27,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:28,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 17:01:30,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67872.641
2025-03-09 17:01:30,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:31,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 17:01:34,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67205.219
2025-03-09 17:01:34,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:35,580 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 17:01:38,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66934.336
2025-03-09 17:01:38,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:39,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 17:01:41,854 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 79 -- loss: tensor([67081.7891], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:01:41,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67081.789
2025-03-09 17:01:41,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:42,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 17:01:45,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67091.227
2025-03-09 17:01:45,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:46,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 17:01:49,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67183.156
2025-03-09 17:01:49,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:50,191 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 17:01:52,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66521.492
2025-03-09 17:01:52,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:53,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 17:01:56,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66435.617
2025-03-09 17:01:56,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:01:57,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 17:02:00,113 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 84 -- loss: tensor([65974.3906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:02:00,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65974.391
2025-03-09 17:02:00,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:01,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 17:02:03,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66049.953
2025-03-09 17:02:03,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:04,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 17:02:07,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65422.82
2025-03-09 17:02:07,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:08,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 17:02:11,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65172.57
2025-03-09 17:02:11,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:12,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 17:02:14,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64800.793
2025-03-09 17:02:14,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:15,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 17:02:18,312 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 89 -- loss: tensor([66134.4844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:02:18,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66134.484
2025-03-09 17:02:18,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:19,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 17:02:22,101 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66127.828
2025-03-09 17:02:22,101 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:23,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 17:02:25,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65307.305
2025-03-09 17:02:25,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:26,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 17:02:29,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66574.812
2025-03-09 17:02:29,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:30,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 17:02:33,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65747.469
2025-03-09 17:02:33,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:34,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 17:02:36,697 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 94 -- loss: tensor([65382.8164], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:02:36,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65382.816
2025-03-09 17:02:36,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:37,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 17:02:40,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65520.805
2025-03-09 17:02:40,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:41,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 17:02:43,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65064.809
2025-03-09 17:02:43,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:44,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 17:02:47,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64814.5
2025-03-09 17:02:47,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:48,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 17:02:51,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64802.484
2025-03-09 17:02:51,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:52,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 17:02:55,140 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 99 -- loss: tensor([64271.7539], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:02:55,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64271.754
2025-03-09 17:02:55,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:02:57,457 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 3898.19165
2025-03-09 17:02:57,462 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 17:02:57,463 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:73 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 
layer11: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:111 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:111 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 
layer15: :fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.15625491466532437 beta1:0.8611698301838286 beta2:0.8442834887728108 weight_decay:0.0001057198629418226 batch_size:32 epochs:100
2025-03-09 17:02:57,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 17:02:57,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 17:03:00,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 786721.688
2025-03-09 17:03:00,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:01,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 17:03:03,289 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 620984.625
2025-03-09 17:03:03,289 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:04,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 17:03:06,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 313801.25
2025-03-09 17:03:06,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:07,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 17:03:09,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 276816.156
2025-03-09 17:03:09,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:10,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 17:03:12,626 :: INFO :: evodenss.train.trainers :: [2051] -- [2.12s] TRAIN epoch 4 -- loss: tensor([264546.2188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:03:12,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 264546.219
2025-03-09 17:03:12,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:13,617 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 17:03:15,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 282174.844
2025-03-09 17:03:15,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:16,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 17:03:18,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 267706.812
2025-03-09 17:03:18,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:20,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 17:03:22,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 275609.469
2025-03-09 17:03:22,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:23,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 17:03:25,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 260675.188
2025-03-09 17:03:25,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:26,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 17:03:28,439 :: INFO :: evodenss.train.trainers :: [2051] -- [2.13s] TRAIN epoch 9 -- loss: tensor([250084.5312], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:03:28,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 250084.531
2025-03-09 17:03:28,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:29,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 17:03:31,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 251038.969
2025-03-09 17:03:31,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:32,628 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 17:03:34,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 241237.406
2025-03-09 17:03:34,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:35,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 17:03:37,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 241758.047
2025-03-09 17:03:37,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:38,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 17:03:41,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 250453.094
2025-03-09 17:03:41,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:42,179 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 17:03:44,320 :: INFO :: evodenss.train.trainers :: [2051] -- [2.14s] TRAIN epoch 14 -- loss: tensor([242521.8906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:03:44,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 242521.891
2025-03-09 17:03:44,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:45,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 17:03:47,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 213433.719
2025-03-09 17:03:47,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:48,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 17:03:50,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 227673.297
2025-03-09 17:03:50,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:51,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 17:03:54,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 214845.141
2025-03-09 17:03:54,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:55,050 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 17:03:57,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 234922.797
2025-03-09 17:03:57,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:03:58,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 17:04:00,374 :: INFO :: evodenss.train.trainers :: [2051] -- [2.15s] TRAIN epoch 19 -- loss: tensor([228960.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:04:00,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 228960.875
2025-03-09 17:04:00,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:01,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 17:04:03,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 229496.156
2025-03-09 17:04:03,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:04,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 17:04:06,796 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 249916.125
2025-03-09 17:04:06,796 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:07,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 17:04:09,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 240162.484
2025-03-09 17:04:09,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:11,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 17:04:13,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 214735.688
2025-03-09 17:04:13,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:14,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 17:04:16,390 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 24 -- loss: tensor([210550.6406], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:04:16,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 210550.641
2025-03-09 17:04:16,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:17,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 17:04:19,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 225743.984
2025-03-09 17:04:19,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:20,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 17:04:22,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 215770.859
2025-03-09 17:04:22,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:23,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 17:04:26,032 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 220909.797
2025-03-09 17:04:26,032 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:27,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 17:04:29,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 220177.125
2025-03-09 17:04:29,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:30,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 17:04:32,246 :: INFO :: evodenss.train.trainers :: [2051] -- [2.1s] TRAIN epoch 29 -- loss: tensor([223460.4531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:04:32,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 223460.453
2025-03-09 17:04:32,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:33,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 17:04:35,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 223868.797
2025-03-09 17:04:35,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:36,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 17:04:38,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 227207.156
2025-03-09 17:04:38,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:39,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 17:04:41,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 229753.844
2025-03-09 17:04:41,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:42,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 17:04:44,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 207898.344
2025-03-09 17:04:44,704 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:45,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 17:04:47,830 :: INFO :: evodenss.train.trainers :: [2051] -- [2.13s] TRAIN epoch 34 -- loss: tensor([232965.3281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:04:47,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 232965.328
2025-03-09 17:04:47,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:48,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 17:04:51,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 217818.125
2025-03-09 17:04:51,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:52,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 17:04:54,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 290342.938
2025-03-09 17:04:54,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:55,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 17:04:57,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 278895.062
2025-03-09 17:04:57,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:04:58,237 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 17:05:00,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 247895.094
2025-03-09 17:05:00,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:01,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 17:05:03,545 :: INFO :: evodenss.train.trainers :: [2051] -- [2.16s] TRAIN epoch 39 -- loss: tensor([258899.9062], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:05:03,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 258899.906
2025-03-09 17:05:03,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:04,539 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 17:05:06,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 247375.547
2025-03-09 17:05:06,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:07,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 17:05:09,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 258817.969
2025-03-09 17:05:09,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:10,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 17:05:12,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 233783.828
2025-03-09 17:05:12,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:13,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 17:05:15,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 242877.094
2025-03-09 17:05:15,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:16,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 17:05:19,149 :: INFO :: evodenss.train.trainers :: [2051] -- [2.15s] TRAIN epoch 44 -- loss: tensor([238800.2812], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:05:19,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 238800.281
2025-03-09 17:05:19,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:20,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 17:05:22,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 221810.328
2025-03-09 17:05:22,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:23,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 17:05:25,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 221330.625
2025-03-09 17:05:25,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:26,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 17:05:28,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 236964.859
2025-03-09 17:05:28,565 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:29,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 17:05:31,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 222889.062
2025-03-09 17:05:31,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:32,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 17:05:34,798 :: INFO :: evodenss.train.trainers :: [2051] -- [2.1s] TRAIN epoch 49 -- loss: tensor([219881.2344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:05:34,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 219881.234
2025-03-09 17:05:34,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:35,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 17:05:37,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 225404.516
2025-03-09 17:05:37,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:38,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 17:05:41,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 221528.062
2025-03-09 17:05:41,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:42,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 17:05:44,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 251518.062
2025-03-09 17:05:44,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:45,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 17:05:47,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 237538.391
2025-03-09 17:05:47,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:48,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 17:05:50,643 :: INFO :: evodenss.train.trainers :: [2051] -- [2.27s] TRAIN epoch 54 -- loss: tensor([251574.2812], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:05:50,643 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 251574.281
2025-03-09 17:05:50,643 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:51,652 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 17:05:53,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 262380.219
2025-03-09 17:05:53,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:54,820 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 17:05:56,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 235489.797
2025-03-09 17:05:56,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:05:57,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 17:06:00,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 218541.391
2025-03-09 17:06:00,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:01,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 17:06:03,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 262888.844
2025-03-09 17:06:03,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:04,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 17:06:06,457 :: INFO :: evodenss.train.trainers :: [2051] -- [2.18s] TRAIN epoch 59 -- loss: tensor([215291.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:06:06,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 215291.312
2025-03-09 17:06:06,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:07,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 17:06:09,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 216629.969
2025-03-09 17:06:09,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:10,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 17:06:12,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 259535.094
2025-03-09 17:06:12,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:13,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 17:06:15,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 212862.016
2025-03-09 17:06:15,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:16,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 17:06:19,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 213975.531
2025-03-09 17:06:19,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:20,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 17:06:22,379 :: INFO :: evodenss.train.trainers :: [2051] -- [2.16s] TRAIN epoch 64 -- loss: tensor([199608.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:06:22,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 199608.5
2025-03-09 17:06:22,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:23,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 17:06:25,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 222176.328
2025-03-09 17:06:25,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:26,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 17:06:28,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 223172.312
2025-03-09 17:06:28,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:29,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 17:06:31,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 213519.453
2025-03-09 17:06:31,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:32,979 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 17:06:35,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 229977.281
2025-03-09 17:06:35,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:36,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 17:06:38,276 :: INFO :: evodenss.train.trainers :: [2051] -- [2.13s] TRAIN epoch 69 -- loss: tensor([210589.2188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:06:38,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 210589.219
2025-03-09 17:06:38,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:39,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 17:06:41,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 198380.438
2025-03-09 17:06:41,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:42,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 17:06:44,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 213372.5
2025-03-09 17:06:44,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:45,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 17:06:47,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 236141.016
2025-03-09 17:06:47,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:48,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 17:06:50,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 222039.531
2025-03-09 17:06:50,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:51,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 17:06:53,938 :: INFO :: evodenss.train.trainers :: [2051] -- [2.12s] TRAIN epoch 74 -- loss: tensor([237589.0156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:06:53,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 237589.016
2025-03-09 17:06:53,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:54,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 17:06:57,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 248586.828
2025-03-09 17:06:57,050 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:06:58,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 17:07:00,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 230112.5
2025-03-09 17:07:00,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:01,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 17:07:03,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 217322.375
2025-03-09 17:07:03,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:04,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 17:07:06,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 218446.953
2025-03-09 17:07:06,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:07,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 17:07:09,474 :: INFO :: evodenss.train.trainers :: [2051] -- [2.12s] TRAIN epoch 79 -- loss: tensor([210847.7031], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:07:09,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 210847.703
2025-03-09 17:07:09,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:10,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 17:07:12,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 210812.844
2025-03-09 17:07:12,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:13,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 17:07:15,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 200985.062
2025-03-09 17:07:15,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:16,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 17:07:18,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 195717.844
2025-03-09 17:07:18,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:20,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 17:07:22,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 244268.047
2025-03-09 17:07:22,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:23,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 17:07:25,272 :: INFO :: evodenss.train.trainers :: [2051] -- [2.13s] TRAIN epoch 84 -- loss: tensor([211753.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:07:25,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 211753.953
2025-03-09 17:07:25,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:26,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 17:07:28,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 207589.297
2025-03-09 17:07:28,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:29,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 17:07:31,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 213432.656
2025-03-09 17:07:31,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:32,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 17:07:34,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 215238.922
2025-03-09 17:07:34,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:35,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 17:07:37,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 205315.125
2025-03-09 17:07:37,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:38,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 17:07:41,071 :: INFO :: evodenss.train.trainers :: [2051] -- [2.15s] TRAIN epoch 89 -- loss: tensor([211055.5781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:07:41,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 211055.578
2025-03-09 17:07:41,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:42,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 17:07:44,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 204254.547
2025-03-09 17:07:44,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:45,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 17:07:47,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 222170.703
2025-03-09 17:07:47,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:48,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 17:07:50,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 213709.156
2025-03-09 17:07:50,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:51,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 17:07:53,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 215853.781
2025-03-09 17:07:53,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:54,976 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 17:07:57,147 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 94 -- loss: tensor([197970.1406], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:07:57,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 197970.141
2025-03-09 17:07:57,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:07:58,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 17:08:00,338 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 216594.094
2025-03-09 17:08:00,338 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:01,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 17:08:03,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 242157.578
2025-03-09 17:08:03,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:04,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 17:08:06,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 212317.312
2025-03-09 17:08:06,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:07,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 17:08:09,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 195842.953
2025-03-09 17:08:09,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:10,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 17:08:13,121 :: INFO :: evodenss.train.trainers :: [2051] -- [2.15s] TRAIN epoch 99 -- loss: tensor([228240.6562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:08:13,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 228240.656
2025-03-09 17:08:13,121 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:15,182 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 23986.4082
2025-03-09 17:08:15,183 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 17:08:15,183 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 0, id: 0
2025-03-09 17:08:15,183 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 1000, 1000]
2025-03-09 17:08:15,183 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 17:08:15,187 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [3575.71436, 4118.32861, 3898.19165, 23986.4082]
2025-03-09 17:08:15,618 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([18575.4551], device='cuda:0')
2025-03-09 17:08:15,619 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 11: 3575.71436
2025-03-09 17:08:15,619 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3575.71436



2025-03-09 17:08:15,700 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 12
2025-03-09 17:08:15,700 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 17:08:15,710 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a layer removed from Module 0: features; Position: 12
2025-03-09 17:08:15,711 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 17:08:15,712 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 17:08:15,712 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 17:08:15,713 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 17:08:15,716 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 9. Reused?: False
2025-03-09 17:08:15,717 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 6. Reused?: False
2025-03-09 17:08:15,718 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 17:08:15,718 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 17:08:15,719 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 17:08:15,720 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 17:08:15,720 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 17:08:15,721 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 15
2025-03-09 17:08:15,721 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 17:08:15,724 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 10
2025-03-09 17:08:15,725 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 17:08:15,726 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 17:08:15,726 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 17:08:15,727 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 17:08:15,728 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 17:08:15,728 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 17:08:15,731 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 17:08:15,734 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 17:08:15,735 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 17:08:15,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 17:08:15,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 17:08:18,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 535767.062
2025-03-09 17:08:18,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:19,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 17:08:22,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 226549.953
2025-03-09 17:08:22,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:23,371 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 17:08:26,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 186367.141
2025-03-09 17:08:26,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:27,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 17:08:29,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 157811.453
2025-03-09 17:08:29,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:30,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 17:08:33,531 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 4 -- loss: tensor([141432.1562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:08:33,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 141432.156
2025-03-09 17:08:33,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:34,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 17:08:37,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 130046.844
2025-03-09 17:08:37,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:38,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 17:08:41,060 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 121335.398
2025-03-09 17:08:41,060 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:42,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 17:08:44,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 116856.938
2025-03-09 17:08:44,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:45,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 17:08:48,580 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112629.484
2025-03-09 17:08:48,580 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:49,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 17:08:52,432 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 9 -- loss: tensor([108316.7422], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:08:52,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108316.742
2025-03-09 17:08:52,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:53,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 17:08:56,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106426.18
2025-03-09 17:08:56,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:08:57,206 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 17:08:59,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103660.219
2025-03-09 17:08:59,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:00,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 17:09:03,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102894.906
2025-03-09 17:09:03,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:04,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 17:09:07,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101059.711
2025-03-09 17:09:07,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:08,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 17:09:11,135 :: INFO :: evodenss.train.trainers :: [2051] -- [2.69s] TRAIN epoch 14 -- loss: tensor([99488.0234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:09:11,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99488.023
2025-03-09 17:09:11,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:12,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 17:09:14,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99060.156
2025-03-09 17:09:14,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:15,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 17:09:18,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97680.594
2025-03-09 17:09:18,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:19,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 17:09:22,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96748.773
2025-03-09 17:09:22,502 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:23,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 17:09:26,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94982.914
2025-03-09 17:09:26,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:27,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 17:09:30,019 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 19 -- loss: tensor([94640.0547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:09:30,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94640.055
2025-03-09 17:09:30,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:31,073 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 17:09:33,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93794.648
2025-03-09 17:09:33,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:34,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 17:09:37,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94248.234
2025-03-09 17:09:37,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:38,580 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 17:09:41,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94377.172
2025-03-09 17:09:41,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:42,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 17:09:45,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92924.758
2025-03-09 17:09:45,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:46,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 17:09:48,828 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 24 -- loss: tensor([90638.1797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:09:48,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90638.18
2025-03-09 17:09:48,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:49,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 17:09:52,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91361.602
2025-03-09 17:09:52,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:53,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 17:09:56,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90158.742
2025-03-09 17:09:56,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:09:57,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 17:10:00,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88974.047
2025-03-09 17:10:00,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:01,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 17:10:04,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88798.758
2025-03-09 17:10:04,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:05,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 17:10:07,855 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 29 -- loss: tensor([88208.8672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:10:07,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88208.867
2025-03-09 17:10:07,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:08,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 17:10:11,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87190.602
2025-03-09 17:10:11,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:12,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 17:10:15,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87500.109
2025-03-09 17:10:15,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:16,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 17:10:19,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86828.656
2025-03-09 17:10:19,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:20,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 17:10:23,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85434.914
2025-03-09 17:10:23,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:24,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 17:10:27,028 :: INFO :: evodenss.train.trainers :: [2051] -- [2.75s] TRAIN epoch 34 -- loss: tensor([85298.2266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:10:27,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85298.227
2025-03-09 17:10:27,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:28,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 17:10:30,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84612.781
2025-03-09 17:10:30,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:31,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 17:10:34,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84489.461
2025-03-09 17:10:34,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:35,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 17:10:38,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83608.727
2025-03-09 17:10:38,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:39,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 17:10:42,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83831.188
2025-03-09 17:10:42,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:43,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 17:10:46,149 :: INFO :: evodenss.train.trainers :: [2051] -- [2.8s] TRAIN epoch 39 -- loss: tensor([83960.8672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:10:46,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83960.867
2025-03-09 17:10:46,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:47,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 17:10:50,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81520.438
2025-03-09 17:10:50,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:51,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 17:10:53,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82465.555
2025-03-09 17:10:53,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:54,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 17:10:57,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82094.633
2025-03-09 17:10:57,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:10:58,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 17:11:01,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81633.055
2025-03-09 17:11:01,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:02,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 17:11:05,382 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 44 -- loss: tensor([80401.3594], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:11:05,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80401.359
2025-03-09 17:11:05,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:06,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 17:11:09,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81474.828
2025-03-09 17:11:09,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:10,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 17:11:13,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80204.523
2025-03-09 17:11:13,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:14,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 17:11:16,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80047.641
2025-03-09 17:11:16,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:17,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 17:11:20,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80357.938
2025-03-09 17:11:20,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:21,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 17:11:24,583 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 49 -- loss: tensor([80010.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:11:24,584 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80010.375
2025-03-09 17:11:24,584 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:25,663 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 17:11:28,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80333.781
2025-03-09 17:11:28,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:29,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 17:11:32,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79175.922
2025-03-09 17:11:32,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:33,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 17:11:35,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79603.766
2025-03-09 17:11:35,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:37,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 17:11:39,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80047.445
2025-03-09 17:11:39,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:40,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 17:11:43,646 :: INFO :: evodenss.train.trainers :: [2051] -- [2.75s] TRAIN epoch 54 -- loss: tensor([78337.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:11:43,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78337.477
2025-03-09 17:11:43,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:44,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 17:11:47,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79111.641
2025-03-09 17:11:47,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:48,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 17:11:51,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78885.508
2025-03-09 17:11:51,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:52,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 17:11:55,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78125.992
2025-03-09 17:11:55,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:11:56,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 17:11:59,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78367.75
2025-03-09 17:11:59,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:00,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 17:12:02,878 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 59 -- loss: tensor([78735.5312], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:12:02,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78735.531
2025-03-09 17:12:02,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:03,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 17:12:06,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77681.578
2025-03-09 17:12:06,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:07,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 17:12:10,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77775.711
2025-03-09 17:12:10,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:11,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 17:12:14,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76703.539
2025-03-09 17:12:14,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:15,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 17:12:18,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76221.406
2025-03-09 17:12:18,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:19,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 17:12:22,097 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 64 -- loss: tensor([77326.5234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:12:22,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77326.523
2025-03-09 17:12:22,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:23,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 17:12:25,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76565.016
2025-03-09 17:12:25,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:27,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 17:12:29,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76414.305
2025-03-09 17:12:29,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:30,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 17:12:33,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76215.57
2025-03-09 17:12:33,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:34,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 17:12:37,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76303.461
2025-03-09 17:12:37,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:38,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 17:12:41,189 :: INFO :: evodenss.train.trainers :: [2051] -- [2.7s] TRAIN epoch 69 -- loss: tensor([75226.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:12:41,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75226.938
2025-03-09 17:12:41,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:42,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 17:12:44,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75732.023
2025-03-09 17:12:44,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:46,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 17:12:50,436 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76609.398
2025-03-09 17:12:50,436 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:51,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 17:12:54,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74609.445
2025-03-09 17:12:54,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:55,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 17:12:58,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74831.5
2025-03-09 17:12:58,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:12:59,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 17:13:01,841 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 74 -- loss: tensor([74935.7188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:13:01,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74935.719
2025-03-09 17:13:01,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:02,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 17:13:05,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75146.75
2025-03-09 17:13:05,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:06,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 17:13:09,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74598.391
2025-03-09 17:13:09,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:10,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 17:13:13,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75004.469
2025-03-09 17:13:13,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:14,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 17:13:16,957 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74658.883
2025-03-09 17:13:16,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:18,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 17:13:20,922 :: INFO :: evodenss.train.trainers :: [2051] -- [2.87s] TRAIN epoch 79 -- loss: tensor([73999.0234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:13:20,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73999.023
2025-03-09 17:13:20,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:22,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 17:13:24,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75266.422
2025-03-09 17:13:24,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:25,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 17:13:28,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74789.117
2025-03-09 17:13:28,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:29,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 17:13:32,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73078.703
2025-03-09 17:13:32,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:33,459 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 17:13:36,206 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73671.523
2025-03-09 17:13:36,206 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:37,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 17:13:40,009 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 84 -- loss: tensor([74096.2344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:13:40,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74096.234
2025-03-09 17:13:40,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:41,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 17:13:43,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73182.891
2025-03-09 17:13:43,795 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:44,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 17:13:47,602 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74389.328
2025-03-09 17:13:47,602 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:48,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 17:13:51,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73443.344
2025-03-09 17:13:51,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:52,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 17:13:55,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73628.828
2025-03-09 17:13:55,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:13:56,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 17:13:59,107 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 89 -- loss: tensor([73710.9844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:13:59,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73710.984
2025-03-09 17:13:59,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:00,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 17:14:02,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73707.156
2025-03-09 17:14:02,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:03,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 17:14:06,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72742.531
2025-03-09 17:14:06,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:07,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 17:14:10,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72917.852
2025-03-09 17:14:10,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:11,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 17:14:14,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73064.391
2025-03-09 17:14:14,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:15,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 17:14:18,148 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 94 -- loss: tensor([73031.2344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:14:18,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73031.234
2025-03-09 17:14:18,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:19,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 17:14:22,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72485.344
2025-03-09 17:14:22,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:23,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 17:14:25,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71515.938
2025-03-09 17:14:25,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:26,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 17:14:29,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71932.805
2025-03-09 17:14:29,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:30,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 17:14:33,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71580.266
2025-03-09 17:14:33,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:34,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 17:14:37,311 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 99 -- loss: tensor([70949.0781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:14:37,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70949.078
2025-03-09 17:14:37,312 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:39,471 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 4151.00586
2025-03-09 17:14:39,476 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 17:14:39,477 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.17498681609496702 beta1:0.8438161378069726 beta2:0.9055656386308496 weight_decay:9.30774889297765e-05 batch_size:32 epochs:100
2025-03-09 17:14:39,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 17:14:39,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 17:14:42,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811840.438
2025-03-09 17:14:42,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:43,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 17:14:45,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:14:45,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:47,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 17:14:49,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1080172.375
2025-03-09 17:14:49,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:50,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 17:14:53,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814706.812
2025-03-09 17:14:53,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:54,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 17:14:57,277 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 4 -- loss: tensor([808697.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:14:57,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 808697.188
2025-03-09 17:14:57,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:14:58,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 17:15:00,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 917229.188
2025-03-09 17:15:00,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:02,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 17:15:04,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 822242.812
2025-03-09 17:15:04,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:05,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 17:15:08,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 857293.25
2025-03-09 17:15:08,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:09,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 17:15:12,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 836963.75
2025-03-09 17:15:12,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:13,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 17:15:15,863 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 9 -- loss: tensor([817118.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:15:15,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 817118.188
2025-03-09 17:15:15,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:16,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 17:15:19,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 812486.625
2025-03-09 17:15:19,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:20,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 17:15:23,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 839640.688
2025-03-09 17:15:23,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:24,389 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 17:15:27,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 818216.875
2025-03-09 17:15:27,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:28,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 17:15:30,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811849.875
2025-03-09 17:15:30,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:31,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 17:15:34,456 :: INFO :: evodenss.train.trainers :: [2051] -- [2.67s] TRAIN epoch 14 -- loss: tensor([810470.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:15:34,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 810470.625
2025-03-09 17:15:34,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:35,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 17:15:38,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 819109.938
2025-03-09 17:15:38,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:39,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 17:15:41,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 816966.125
2025-03-09 17:15:41,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:42,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 17:15:45,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811694.562
2025-03-09 17:15:45,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:46,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 17:15:49,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 810178.75
2025-03-09 17:15:49,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:50,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 17:15:52,982 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 19 -- loss: tensor([814967.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:15:52,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814967.875
2025-03-09 17:15:52,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:54,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 17:15:56,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 830148.562
2025-03-09 17:15:56,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:15:57,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 17:16:00,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 817073.25
2025-03-09 17:16:00,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:01,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 17:16:04,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811756.625
2025-03-09 17:16:04,032 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:05,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 17:16:07,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 815817.562
2025-03-09 17:16:07,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:08,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 17:16:11,361 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 24 -- loss: tensor([827412.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:16:11,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 827412.312
2025-03-09 17:16:11,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:12,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 17:16:14,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 819341.125
2025-03-09 17:16:14,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:16,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 17:16:18,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813692.812
2025-03-09 17:16:18,703 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:19,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 17:16:22,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813768.125
2025-03-09 17:16:22,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:23,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 17:16:26,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813096.938
2025-03-09 17:16:26,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:27,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 17:16:29,845 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 29 -- loss: tensor([830860.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:16:29,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 830860.312
2025-03-09 17:16:29,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:30,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 17:16:33,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 815745.688
2025-03-09 17:16:33,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:34,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 17:16:37,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 809695.938
2025-03-09 17:16:37,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:38,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 17:16:40,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813183.938
2025-03-09 17:16:40,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:41,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 17:16:44,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 817903.0
2025-03-09 17:16:44,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:45,584 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 17:16:48,230 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 34 -- loss: tensor([816199.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:16:48,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 816199.188
2025-03-09 17:16:48,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:49,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 17:16:52,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811583.625
2025-03-09 17:16:52,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:53,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 17:16:55,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811338.375
2025-03-09 17:16:55,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:16:56,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 17:16:59,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 815409.125
2025-03-09 17:16:59,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:00,489 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 17:17:03,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 827547.625
2025-03-09 17:17:03,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:04,202 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 17:17:06,877 :: INFO :: evodenss.train.trainers :: [2051] -- [2.67s] TRAIN epoch 39 -- loss: tensor([812646.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:17:06,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 812646.188
2025-03-09 17:17:06,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:07,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 17:17:10,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 810494.5
2025-03-09 17:17:10,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:11,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 17:17:14,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813695.438
2025-03-09 17:17:14,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:15,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 17:17:17,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 820195.938
2025-03-09 17:17:17,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:19,032 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 17:17:21,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 823651.375
2025-03-09 17:17:21,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:22,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 17:17:25,482 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 44 -- loss: tensor([813787.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:17:25,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813787.688
2025-03-09 17:17:25,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:26,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 17:17:29,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 810330.25
2025-03-09 17:17:29,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:30,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 17:17:32,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 821580.938
2025-03-09 17:17:32,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:33,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 17:17:36,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 817965.375
2025-03-09 17:17:36,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:37,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 17:17:40,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 819177.125
2025-03-09 17:17:40,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:41,436 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 17:17:44,094 :: INFO :: evodenss.train.trainers :: [2051] -- [2.66s] TRAIN epoch 49 -- loss: tensor([811673.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:17:44,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811673.562
2025-03-09 17:17:44,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:45,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 17:17:47,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 816610.625
2025-03-09 17:17:47,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:48,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 17:17:51,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 815959.062
2025-03-09 17:17:51,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:52,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 17:17:55,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 819240.062
2025-03-09 17:17:55,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:56,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 17:17:58,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811915.625
2025-03-09 17:17:58,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:17:59,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 17:18:02,537 :: INFO :: evodenss.train.trainers :: [2051] -- [2.62s] TRAIN epoch 54 -- loss: tensor([814231.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:18:02,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814231.25
2025-03-09 17:18:02,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:03,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 17:18:06,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 817360.5
2025-03-09 17:18:06,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:07,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 17:18:09,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 817993.625
2025-03-09 17:18:09,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:10,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 17:18:13,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 820258.062
2025-03-09 17:18:13,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:14,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 17:18:16,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811632.75
2025-03-09 17:18:16,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:17,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 17:18:20,727 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 59 -- loss: tensor([814724.0625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:18:20,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814724.062
2025-03-09 17:18:20,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:21,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 17:18:24,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 821911.188
2025-03-09 17:18:24,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:25,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 17:18:28,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 819984.375
2025-03-09 17:18:28,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:28,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 17:18:31,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 811434.625
2025-03-09 17:18:31,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:32,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 17:18:35,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 809226.875
2025-03-09 17:18:35,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:36,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 17:18:38,898 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 64 -- loss: tensor([822304.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:18:38,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 822304.312
2025-03-09 17:18:38,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:39,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 17:18:42,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814738.375
2025-03-09 17:18:42,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:43,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 17:18:46,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 822193.875
2025-03-09 17:18:46,147 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:47,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 17:18:49,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814297.5
2025-03-09 17:18:49,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:50,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 17:18:53,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814745.312
2025-03-09 17:18:53,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:54,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 17:18:57,086 :: INFO :: evodenss.train.trainers :: [2051] -- [2.59s] TRAIN epoch 69 -- loss: tensor([817705.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:18:57,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 817705.188
2025-03-09 17:18:57,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:18:58,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 17:19:00,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 822284.312
2025-03-09 17:19:00,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:01,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 17:19:04,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814767.438
2025-03-09 17:19:04,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:05,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 17:19:07,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814819.75
2025-03-09 17:19:07,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:08,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 17:19:11,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 820288.0
2025-03-09 17:19:11,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:12,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 17:19:15,101 :: INFO :: evodenss.train.trainers :: [2051] -- [2.61s] TRAIN epoch 74 -- loss: tensor([823121.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:19:15,101 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 823121.625
2025-03-09 17:19:15,101 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:16,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 17:19:18,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 819769.312
2025-03-09 17:19:18,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:19,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 17:19:22,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813732.562
2025-03-09 17:19:22,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:23,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 17:19:26,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 815057.312
2025-03-09 17:19:26,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:27,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 17:19:29,643 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 820831.438
2025-03-09 17:19:29,644 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:30,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 17:19:33,251 :: INFO :: evodenss.train.trainers :: [2051] -- [2.59s] TRAIN epoch 79 -- loss: tensor([818577.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:19:33,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 818577.75
2025-03-09 17:19:33,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:34,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 17:19:36,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 818059.812
2025-03-09 17:19:36,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:37,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 17:19:40,513 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814230.0
2025-03-09 17:19:40,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:41,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 17:19:44,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 816975.375
2025-03-09 17:19:44,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:45,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 17:19:47,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 817949.75
2025-03-09 17:19:47,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:48,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 17:19:51,551 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 84 -- loss: tensor([816480.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:19:51,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 816480.938
2025-03-09 17:19:51,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:52,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 17:19:55,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 815373.938
2025-03-09 17:19:55,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:56,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 17:19:58,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 817115.5
2025-03-09 17:19:58,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:19:59,870 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 17:20:02,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 815412.062
2025-03-09 17:20:02,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:03,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 17:20:06,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 817016.0
2025-03-09 17:20:06,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:07,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 17:20:09,842 :: INFO :: evodenss.train.trainers :: [2051] -- [2.64s] TRAIN epoch 89 -- loss: tensor([816197.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:20:09,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 816197.875
2025-03-09 17:20:09,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:10,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 17:20:13,525 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 822129.25
2025-03-09 17:20:13,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:14,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 17:20:17,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 818625.375
2025-03-09 17:20:17,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:18,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 17:20:20,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 819543.062
2025-03-09 17:20:20,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:21,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 17:20:24,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 815231.688
2025-03-09 17:20:24,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:25,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 17:20:28,198 :: INFO :: evodenss.train.trainers :: [2051] -- [2.63s] TRAIN epoch 94 -- loss: tensor([819196.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:20:28,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 819196.562
2025-03-09 17:20:28,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:29,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 17:20:31,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 823151.625
2025-03-09 17:20:31,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:32,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 17:20:35,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 815170.438
2025-03-09 17:20:35,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:36,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 17:20:39,187 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 816720.0
2025-03-09 17:20:39,187 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:40,187 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 17:20:42,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 814715.188
2025-03-09 17:20:42,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:43,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 17:20:46,504 :: INFO :: evodenss.train.trainers :: [2051] -- [2.66s] TRAIN epoch 99 -- loss: tensor([819205.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:20:46,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 819205.125
2025-03-09 17:20:46,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:48,566 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 45176.64844
2025-03-09 17:20:48,572 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 17:20:48,573 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :deconv1d out_channels:81 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 
layer7: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :deconv1d out_channels:42 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 
layer13: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 
layer14: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 
layer15: :deconv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:14 
layer16: :fc act:selu out_features:200 bias:True input:15 learning:adadelta batch_size:32 epochs:100
2025-03-09 17:20:48,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 17:20:48,585 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 17:20:52,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 919371.312
2025-03-09 17:20:52,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:53,397 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 17:20:56,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:20:56,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:20:57,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 17:21:00,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804908.438
2025-03-09 17:21:00,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:01,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 17:21:05,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:05,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:06,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 17:21:09,517 :: INFO :: evodenss.train.trainers :: [2051] -- [3.24s] TRAIN epoch 4 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:21:09,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:09,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:10,564 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 17:21:13,797 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:13,797 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:14,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 17:21:18,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:18,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:19,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 17:21:22,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:22,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:23,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 17:21:26,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:26,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:27,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 17:21:31,275 :: INFO :: evodenss.train.trainers :: [2051] -- [3.28s] TRAIN epoch 9 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:21:31,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:31,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:32,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 17:21:35,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:35,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:36,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 17:21:39,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:39,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:41,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 17:21:44,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:44,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:45,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 17:21:48,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:48,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:49,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 17:21:53,187 :: INFO :: evodenss.train.trainers :: [2051] -- [3.26s] TRAIN epoch 14 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:21:53,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:53,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:54,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 17:21:57,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:21:57,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:21:58,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 17:22:01,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:01,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:03,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 17:22:06,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:06,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:07,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 17:22:10,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:10,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:11,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 17:22:15,081 :: INFO :: evodenss.train.trainers :: [2051] -- [3.31s] TRAIN epoch 19 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:22:15,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:15,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:16,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 17:22:19,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:19,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:20,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 17:22:23,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:23,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:25,054 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 17:22:28,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:28,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:29,389 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 17:22:32,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:32,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:33,766 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 17:22:37,040 :: INFO :: evodenss.train.trainers :: [2051] -- [3.27s] TRAIN epoch 24 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:22:37,041 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:37,041 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:38,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 17:22:41,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:41,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:42,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 17:22:45,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:45,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:46,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 17:22:50,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:50,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:51,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 17:22:54,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:54,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:22:55,798 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 17:22:59,086 :: INFO :: evodenss.train.trainers :: [2051] -- [3.29s] TRAIN epoch 29 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:22:59,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:22:59,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:00,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 17:23:03,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:03,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:04,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 17:23:07,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:07,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:08,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 17:23:12,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:12,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:13,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 17:23:16,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:16,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:17,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 17:23:21,109 :: INFO :: evodenss.train.trainers :: [2051] -- [3.4s] TRAIN epoch 34 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:23:21,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:21,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:22,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 17:23:25,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:25,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:26,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 17:23:29,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:29,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:30,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 17:23:34,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:34,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:35,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 17:23:38,562 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:38,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:39,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 17:23:42,918 :: INFO :: evodenss.train.trainers :: [2051] -- [3.27s] TRAIN epoch 39 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:23:42,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:42,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:44,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 17:23:47,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:47,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:48,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 17:23:51,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:51,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:52,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 17:23:56,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:23:56,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:23:57,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 17:24:00,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:00,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:01,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 17:24:04,926 :: INFO :: evodenss.train.trainers :: [2051] -- [3.29s] TRAIN epoch 44 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:24:04,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:04,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:06,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 17:24:09,326 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:09,326 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:10,402 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 17:24:13,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:13,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:14,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 17:24:18,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:18,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:19,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 17:24:22,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:22,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:23,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 17:24:26,988 :: INFO :: evodenss.train.trainers :: [2051] -- [3.29s] TRAIN epoch 49 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:24:26,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:26,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:28,106 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 17:24:31,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:31,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:32,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 17:24:35,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:35,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:36,870 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 17:24:40,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:40,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:41,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 17:24:44,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:44,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:45,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 17:24:48,902 :: INFO :: evodenss.train.trainers :: [2051] -- [3.27s] TRAIN epoch 54 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:24:48,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:48,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:50,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 17:24:53,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:53,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:54,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 17:24:57,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:24:57,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:24:59,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 17:25:02,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:02,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:03,392 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 17:25:06,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:06,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:07,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 17:25:11,031 :: INFO :: evodenss.train.trainers :: [2051] -- [3.27s] TRAIN epoch 59 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:25:11,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:11,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:12,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 17:25:15,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:15,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:16,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 17:25:19,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:19,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:20,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 17:25:24,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:24,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:25,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 17:25:28,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:28,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:29,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 17:25:33,029 :: INFO :: evodenss.train.trainers :: [2051] -- [3.28s] TRAIN epoch 64 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:25:33,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:33,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:34,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 17:25:37,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:37,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:38,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 17:25:41,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:41,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:42,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 17:25:46,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:46,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:47,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 17:25:50,663 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:50,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:51,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 17:25:54,897 :: INFO :: evodenss.train.trainers :: [2051] -- [3.21s] TRAIN epoch 69 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:25:54,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:54,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:25:55,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 17:25:59,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:25:59,136 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:00,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 17:26:03,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:03,382 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:04,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 17:26:07,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:07,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:08,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 17:26:11,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:11,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:12,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 17:26:16,138 :: INFO :: evodenss.train.trainers :: [2051] -- [3.23s] TRAIN epoch 74 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:26:16,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:16,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:17,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 17:26:20,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:20,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:21,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 17:26:24,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:24,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:25,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 17:26:28,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:28,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:29,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 17:26:33,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:33,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:34,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 17:26:37,500 :: INFO :: evodenss.train.trainers :: [2051] -- [3.26s] TRAIN epoch 79 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:26:37,500 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:37,500 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:38,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 17:26:41,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:41,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:42,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 17:26:45,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:45,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:46,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 17:26:50,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:50,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:51,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 17:26:54,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:54,576 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:55,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 17:26:58,842 :: INFO :: evodenss.train.trainers :: [2051] -- [3.22s] TRAIN epoch 84 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:26:58,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:26:58,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:26:59,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 17:27:03,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:03,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:04,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 17:27:07,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:07,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:08,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 17:27:11,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:11,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:12,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 17:27:15,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:15,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:16,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 17:27:20,137 :: INFO :: evodenss.train.trainers :: [2051] -- [3.32s] TRAIN epoch 89 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:27:20,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:20,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:21,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 17:27:24,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:24,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:25,387 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 17:27:28,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:28,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:29,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 17:27:32,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:32,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:33,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 17:27:37,060 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:37,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:38,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 17:27:41,319 :: INFO :: evodenss.train.trainers :: [2051] -- [3.23s] TRAIN epoch 94 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:27:41,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:41,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:42,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 17:27:45,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:45,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:46,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 17:27:49,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:49,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:50,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 17:27:54,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:54,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:55,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 17:27:58,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:27:58,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:27:59,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 17:28:02,708 :: INFO :: evodenss.train.trainers :: [2051] -- [3.23s] TRAIN epoch 99 -- loss: tensor([804482.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:28:02,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.438
2025-03-09 17:28:02,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:05,038 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 44550.28906
2025-03-09 17:28:05,042 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 17:28:05,044 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:40 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 17:28:05,055 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 17:28:05,055 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 17:28:07,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 429008.812
2025-03-09 17:28:07,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:08,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 17:28:10,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 384534.375
2025-03-09 17:28:10,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:11,617 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 17:28:13,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 247340.266
2025-03-09 17:28:13,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:14,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 17:28:16,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 177701.5
2025-03-09 17:28:16,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:17,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 17:28:20,151 :: INFO :: evodenss.train.trainers :: [2051] -- [2.29s] TRAIN epoch 4 -- loss: tensor([136230.4531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:28:20,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 136230.453
2025-03-09 17:28:20,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:21,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 17:28:23,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 124229.961
2025-03-09 17:28:23,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:24,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 17:28:26,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 118562.406
2025-03-09 17:28:26,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:27,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 17:28:29,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 113257.844
2025-03-09 17:28:29,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:30,489 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 17:28:32,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109186.094
2025-03-09 17:28:32,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:33,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 17:28:35,741 :: INFO :: evodenss.train.trainers :: [2051] -- [2.14s] TRAIN epoch 9 -- loss: tensor([106653.4297], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:28:35,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106653.43
2025-03-09 17:28:35,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:36,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 17:28:38,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104984.867
2025-03-09 17:28:38,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:39,850 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 17:28:41,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102991.555
2025-03-09 17:28:41,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:42,958 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 17:28:45,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101545.555
2025-03-09 17:28:45,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:46,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 17:28:48,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100269.945
2025-03-09 17:28:48,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:49,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 17:28:51,400 :: INFO :: evodenss.train.trainers :: [2051] -- [2.19s] TRAIN epoch 14 -- loss: tensor([98468.4844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:28:51,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98468.484
2025-03-09 17:28:51,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:52,423 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 17:28:54,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97286.859
2025-03-09 17:28:54,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:55,573 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 17:28:57,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96629.125
2025-03-09 17:28:57,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:28:58,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 17:29:00,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94506.836
2025-03-09 17:29:00,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:01,862 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 17:29:03,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94551.406
2025-03-09 17:29:03,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:04,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 17:29:07,065 :: INFO :: evodenss.train.trainers :: [2051] -- [2.12s] TRAIN epoch 19 -- loss: tensor([92363.2812], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:29:07,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92363.281
2025-03-09 17:29:07,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:08,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 17:29:10,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92010.719
2025-03-09 17:29:10,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:11,191 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 17:29:13,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90585.609
2025-03-09 17:29:13,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:14,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 17:29:16,462 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90651.289
2025-03-09 17:29:16,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:17,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 17:29:19,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89356.109
2025-03-09 17:29:19,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:20,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 17:29:22,881 :: INFO :: evodenss.train.trainers :: [2051] -- [2.12s] TRAIN epoch 24 -- loss: tensor([88476.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:29:22,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88476.5
2025-03-09 17:29:22,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:23,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 17:29:25,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88152.414
2025-03-09 17:29:25,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:26,976 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 17:29:29,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87272.672
2025-03-09 17:29:29,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:30,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 17:29:32,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85842.867
2025-03-09 17:29:32,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:33,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 17:29:35,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86916.438
2025-03-09 17:29:35,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:36,348 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 17:29:38,480 :: INFO :: evodenss.train.trainers :: [2051] -- [2.13s] TRAIN epoch 29 -- loss: tensor([85491.3281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:29:38,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85491.328
2025-03-09 17:29:38,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:39,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 17:29:41,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85595.828
2025-03-09 17:29:41,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:42,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 17:29:44,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85011.203
2025-03-09 17:29:44,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:45,715 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 17:29:47,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84210.078
2025-03-09 17:29:47,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:48,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 17:29:51,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83770.828
2025-03-09 17:29:51,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:52,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 17:29:54,192 :: INFO :: evodenss.train.trainers :: [2051] -- [2.11s] TRAIN epoch 34 -- loss: tensor([84062.8047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:29:54,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84062.805
2025-03-09 17:29:54,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:55,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 17:29:57,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83132.188
2025-03-09 17:29:57,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:29:58,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 17:30:00,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83420.375
2025-03-09 17:30:00,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:01,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 17:30:03,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82436.406
2025-03-09 17:30:03,497 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:04,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 17:30:06,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82453.188
2025-03-09 17:30:06,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:07,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 17:30:09,735 :: INFO :: evodenss.train.trainers :: [2051] -- [2.12s] TRAIN epoch 39 -- loss: tensor([81644.6328], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:30:09,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81644.633
2025-03-09 17:30:09,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:10,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 17:30:12,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81976.0
2025-03-09 17:30:12,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:13,864 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 17:30:15,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80506.258
2025-03-09 17:30:15,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:16,979 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 17:30:19,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81497.367
2025-03-09 17:30:19,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:20,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 17:30:22,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80340.383
2025-03-09 17:30:22,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:23,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 17:30:25,490 :: INFO :: evodenss.train.trainers :: [2051] -- [2.12s] TRAIN epoch 44 -- loss: tensor([80527.8906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:30:25,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80527.891
2025-03-09 17:30:25,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:26,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 17:30:28,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80553.203
2025-03-09 17:30:28,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:29,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 17:30:31,786 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79606.461
2025-03-09 17:30:31,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:32,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 17:30:34,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80089.633
2025-03-09 17:30:34,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:35,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 17:30:38,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80181.016
2025-03-09 17:30:38,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:39,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 17:30:41,177 :: INFO :: evodenss.train.trainers :: [2051] -- [2.12s] TRAIN epoch 49 -- loss: tensor([78734.2266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:30:41,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78734.227
2025-03-09 17:30:41,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:42,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 17:30:44,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79744.742
2025-03-09 17:30:44,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:45,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 17:30:47,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78588.312
2025-03-09 17:30:47,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:48,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 17:30:50,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79012.023
2025-03-09 17:30:50,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:51,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 17:30:53,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78204.469
2025-03-09 17:30:53,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:54,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 17:30:56,934 :: INFO :: evodenss.train.trainers :: [2051] -- [2.12s] TRAIN epoch 54 -- loss: tensor([78358.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:30:56,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78358.812
2025-03-09 17:30:56,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:30:57,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 17:31:00,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78668.75
2025-03-09 17:31:00,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:01,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 17:31:03,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77208.891
2025-03-09 17:31:03,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:04,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 17:31:06,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77531.328
2025-03-09 17:31:06,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:07,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 17:31:09,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77329.109
2025-03-09 17:31:09,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:10,481 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 17:31:12,615 :: INFO :: evodenss.train.trainers :: [2051] -- [2.13s] TRAIN epoch 59 -- loss: tensor([76465.8672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:31:12,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76465.867
2025-03-09 17:31:12,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:13,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 17:31:15,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76476.383
2025-03-09 17:31:15,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:16,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 17:31:18,848 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76822.828
2025-03-09 17:31:18,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:19,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 17:31:22,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76679.648
2025-03-09 17:31:22,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:23,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 17:31:25,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75475.961
2025-03-09 17:31:25,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:26,202 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 17:31:28,344 :: INFO :: evodenss.train.trainers :: [2051] -- [2.14s] TRAIN epoch 64 -- loss: tensor([76599.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:31:28,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76599.297
2025-03-09 17:31:28,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:29,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 17:31:31,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75755.164
2025-03-09 17:31:31,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:32,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 17:31:34,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75813.945
2025-03-09 17:31:34,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:35,638 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 17:31:37,763 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75995.172
2025-03-09 17:31:37,763 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:38,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 17:31:40,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76049.742
2025-03-09 17:31:40,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:41,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 17:31:44,066 :: INFO :: evodenss.train.trainers :: [2051] -- [2.17s] TRAIN epoch 69 -- loss: tensor([74925.7578], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:31:44,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74925.758
2025-03-09 17:31:44,067 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:45,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 17:31:47,181 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75946.352
2025-03-09 17:31:47,182 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:48,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 17:31:50,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75426.852
2025-03-09 17:31:50,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:51,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 17:31:53,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74891.5
2025-03-09 17:31:53,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:54,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 17:31:56,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74503.383
2025-03-09 17:31:56,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:31:57,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 17:31:59,879 :: INFO :: evodenss.train.trainers :: [2051] -- [2.12s] TRAIN epoch 74 -- loss: tensor([73996.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:31:59,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73996.812
2025-03-09 17:31:59,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:00,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 17:32:03,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73171.07
2025-03-09 17:32:03,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:04,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 17:32:06,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74541.742
2025-03-09 17:32:06,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:07,131 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 17:32:09,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73232.422
2025-03-09 17:32:09,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:10,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 17:32:12,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75084.375
2025-03-09 17:32:12,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:13,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 17:32:15,548 :: INFO :: evodenss.train.trainers :: [2051] -- [2.15s] TRAIN epoch 79 -- loss: tensor([75016.1016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:32:15,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75016.102
2025-03-09 17:32:15,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:16,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 17:32:18,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73220.07
2025-03-09 17:32:18,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:19,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 17:32:21,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73202.523
2025-03-09 17:32:21,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:22,979 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 17:32:25,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73880.594
2025-03-09 17:32:25,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:26,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 17:32:28,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72990.555
2025-03-09 17:32:28,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:29,266 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 17:32:31,411 :: INFO :: evodenss.train.trainers :: [2051] -- [2.14s] TRAIN epoch 84 -- loss: tensor([74312.0078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:32:31,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74312.008
2025-03-09 17:32:31,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:32,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 17:32:34,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73837.062
2025-03-09 17:32:34,535 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:35,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 17:32:37,638 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72533.055
2025-03-09 17:32:37,638 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:38,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 17:32:40,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73140.828
2025-03-09 17:32:40,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:41,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 17:32:43,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72849.398
2025-03-09 17:32:43,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:44,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 17:32:47,027 :: INFO :: evodenss.train.trainers :: [2051] -- [2.13s] TRAIN epoch 89 -- loss: tensor([72369.8281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:32:47,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72369.828
2025-03-09 17:32:47,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:48,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 17:32:50,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72167.5
2025-03-09 17:32:50,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:51,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 17:32:53,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72485.297
2025-03-09 17:32:53,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:54,500 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 17:32:56,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71736.703
2025-03-09 17:32:56,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:32:57,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 17:32:59,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73523.172
2025-03-09 17:32:59,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:00,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 17:33:02,973 :: INFO :: evodenss.train.trainers :: [2051] -- [2.14s] TRAIN epoch 94 -- loss: tensor([72332.5312], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:33:02,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72332.531
2025-03-09 17:33:02,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:03,976 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 17:33:06,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72113.195
2025-03-09 17:33:06,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:07,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 17:33:09,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72294.258
2025-03-09 17:33:09,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:10,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 17:33:12,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71433.375
2025-03-09 17:33:12,454 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:13,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 17:33:15,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71784.945
2025-03-09 17:33:15,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:16,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 17:33:18,817 :: INFO :: evodenss.train.trainers :: [2051] -- [2.15s] TRAIN epoch 99 -- loss: tensor([71892.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:33:18,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71892.953
2025-03-09 17:33:18,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:21,051 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 4319.64062
2025-03-09 17:33:21,051 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 17:33:21,052 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 0, id: 0
2025-03-09 17:33:21,052 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 1000, 1000]
2025-03-09 17:33:21,052 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 17:33:21,055 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [4151.00586, 45176.64844, 44550.28906, 4319.64062]
2025-03-09 17:33:21,306 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([21772.8555], device='cuda:0')
2025-03-09 17:33:21,306 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 12: 4151.00586
2025-03-09 17:33:21,306 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3575.71436



2025-03-09 17:33:21,408 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 13
2025-03-09 17:33:21,408 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 17:33:21,419 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a layer removed from Module 0: features; Position: 4
2025-03-09 17:33:21,420 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have an extra layer at Module 0: features; Position: 9. Reused?: False
2025-03-09 17:33:21,420 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a layer removed from Module 0: features; Position: 5
2025-03-09 17:33:21,421 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 17:33:21,421 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 17:33:21,422 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 17:33:21,423 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 17:33:21,424 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 17:33:21,424 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 17:33:21,427 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a layer removed from Module 0: features; Position: 5
2025-03-09 17:33:21,427 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 17:33:21,428 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 17:33:21,429 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 17:33:21,430 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 17:33:21,430 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 17:33:21,433 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 13
2025-03-09 17:33:21,434 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have an extra layer at Module 0: features; Position: 13. Reused?: False
2025-03-09 17:33:21,434 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 17:33:21,435 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 17:33:21,436 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 17:33:21,437 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 17:33:21,437 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 17:33:21,438 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 17:33:21,440 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 17:33:21,444 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 17:33:21,445 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 17:33:21,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 17:33:21,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 17:33:24,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 476484.0
2025-03-09 17:33:24,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:25,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 17:33:27,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 229088.0
2025-03-09 17:33:27,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:28,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 17:33:31,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 181090.531
2025-03-09 17:33:31,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:32,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 17:33:35,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 161527.641
2025-03-09 17:33:35,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:36,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 17:33:39,375 :: INFO :: evodenss.train.trainers :: [2051] -- [2.77s] TRAIN epoch 4 -- loss: tensor([141731.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:33:39,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 141731.125
2025-03-09 17:33:39,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:40,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 17:33:43,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 130650.625
2025-03-09 17:33:43,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:44,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 17:33:46,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 123735.898
2025-03-09 17:33:46,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:48,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 17:33:50,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 118323.078
2025-03-09 17:33:50,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:51,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 17:33:54,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 113909.062
2025-03-09 17:33:54,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:55,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 17:33:58,453 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 9 -- loss: tensor([109690.7031], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:33:58,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109690.703
2025-03-09 17:33:58,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:33:59,533 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 17:34:02,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109134.695
2025-03-09 17:34:02,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:03,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 17:34:06,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107201.508
2025-03-09 17:34:06,107 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:07,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 17:34:09,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103535.836
2025-03-09 17:34:09,914 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:10,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 17:34:13,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102861.031
2025-03-09 17:34:13,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:14,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 17:34:17,549 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 14 -- loss: tensor([102982.7422], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:34:17,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102982.742
2025-03-09 17:34:17,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:18,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 17:34:21,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99586.539
2025-03-09 17:34:21,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:22,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 17:34:25,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98015.273
2025-03-09 17:34:25,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:26,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 17:34:29,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98152.438
2025-03-09 17:34:29,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:30,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 17:34:32,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96636.055
2025-03-09 17:34:32,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:34,007 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 17:34:36,798 :: INFO :: evodenss.train.trainers :: [2051] -- [2.79s] TRAIN epoch 19 -- loss: tensor([94630.6406], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:34:36,798 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94630.641
2025-03-09 17:34:36,798 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:37,869 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 17:34:40,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95259.031
2025-03-09 17:34:40,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:41,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 17:34:44,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93039.734
2025-03-09 17:34:44,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:45,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 17:34:48,207 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92267.602
2025-03-09 17:34:48,207 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:49,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 17:34:52,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93003.07
2025-03-09 17:34:52,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:53,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 17:34:56,011 :: INFO :: evodenss.train.trainers :: [2051] -- [2.76s] TRAIN epoch 24 -- loss: tensor([90758.1562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:34:56,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90758.156
2025-03-09 17:34:56,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:34:57,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 17:34:59,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90044.648
2025-03-09 17:34:59,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:00,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 17:35:03,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90608.25
2025-03-09 17:35:03,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:04,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 17:35:07,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89363.305
2025-03-09 17:35:07,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:08,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 17:35:11,326 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88206.359
2025-03-09 17:35:11,326 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:12,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 17:35:15,151 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 29 -- loss: tensor([88078.6953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:35:15,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88078.695
2025-03-09 17:35:15,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:16,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 17:35:18,953 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87408.312
2025-03-09 17:35:18,953 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:20,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 17:35:22,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86713.023
2025-03-09 17:35:22,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:23,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 17:35:27,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86588.148
2025-03-09 17:35:27,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:28,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 17:35:31,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85942.727
2025-03-09 17:35:31,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:32,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 17:35:35,561 :: INFO :: evodenss.train.trainers :: [2051] -- [2.77s] TRAIN epoch 34 -- loss: tensor([84844.8594], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:35:35,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84844.859
2025-03-09 17:35:35,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:36,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 17:35:39,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84351.969
2025-03-09 17:35:39,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:40,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 17:35:43,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84323.258
2025-03-09 17:35:43,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:44,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 17:35:46,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84338.68
2025-03-09 17:35:46,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:47,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 17:35:50,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83207.008
2025-03-09 17:35:50,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:51,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 17:35:54,359 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 39 -- loss: tensor([82895.9297], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:35:54,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82895.93
2025-03-09 17:35:54,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:55,387 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 17:35:58,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82449.117
2025-03-09 17:35:58,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:35:59,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 17:36:01,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82439.711
2025-03-09 17:36:01,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:02,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 17:36:05,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82477.328
2025-03-09 17:36:05,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:06,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 17:36:09,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81812.5
2025-03-09 17:36:09,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:10,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 17:36:13,059 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 44 -- loss: tensor([81565.1406], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:36:13,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81565.141
2025-03-09 17:36:13,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:14,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 17:36:16,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80633.086
2025-03-09 17:36:16,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:17,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 17:36:20,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80345.672
2025-03-09 17:36:20,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:21,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 17:36:24,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81332.617
2025-03-09 17:36:24,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:25,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 17:36:28,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79548.023
2025-03-09 17:36:28,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:29,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 17:36:31,863 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 49 -- loss: tensor([79919.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:36:31,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79919.953
2025-03-09 17:36:31,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:32,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 17:36:35,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79090.008
2025-03-09 17:36:35,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:36,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 17:36:39,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78901.602
2025-03-09 17:36:39,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:40,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 17:36:43,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78565.258
2025-03-09 17:36:43,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:44,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 17:36:46,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78390.305
2025-03-09 17:36:46,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:47,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 17:36:50,667 :: INFO :: evodenss.train.trainers :: [2051] -- [2.88s] TRAIN epoch 54 -- loss: tensor([77714.2266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:36:50,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77714.227
2025-03-09 17:36:50,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:51,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 17:36:54,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76954.078
2025-03-09 17:36:54,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:55,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 17:36:58,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77262.609
2025-03-09 17:36:58,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:36:59,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 17:37:01,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77936.547
2025-03-09 17:37:01,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:03,007 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 17:37:05,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76686.789
2025-03-09 17:37:05,706 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:06,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 17:37:09,509 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 59 -- loss: tensor([77330.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:37:09,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77330.297
2025-03-09 17:37:09,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:10,513 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 17:37:13,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76443.562
2025-03-09 17:37:13,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:14,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 17:37:17,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76171.719
2025-03-09 17:37:17,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:18,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 17:37:20,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75461.391
2025-03-09 17:37:20,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:22,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 17:37:24,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75273.891
2025-03-09 17:37:24,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:25,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 17:37:28,578 :: INFO :: evodenss.train.trainers :: [2051] -- [2.76s] TRAIN epoch 64 -- loss: tensor([76165.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:37:28,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76165.812
2025-03-09 17:37:28,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:29,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 17:37:32,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75255.133
2025-03-09 17:37:32,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:33,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 17:37:36,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74784.672
2025-03-09 17:37:36,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:37,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 17:37:39,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74241.188
2025-03-09 17:37:39,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:41,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 17:37:43,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74356.383
2025-03-09 17:37:43,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:44,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 17:37:47,605 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 69 -- loss: tensor([75313.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:37:47,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75313.875
2025-03-09 17:37:47,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:48,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 17:37:51,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73877.406
2025-03-09 17:37:51,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:52,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 17:37:55,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73745.359
2025-03-09 17:37:55,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:37:56,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 17:37:59,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73422.969
2025-03-09 17:37:59,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:00,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 17:38:03,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74358.422
2025-03-09 17:38:03,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:04,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 17:38:06,890 :: INFO :: evodenss.train.trainers :: [2051] -- [2.75s] TRAIN epoch 74 -- loss: tensor([72788.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:38:06,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72788.953
2025-03-09 17:38:06,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:07,978 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 17:38:10,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72656.656
2025-03-09 17:38:10,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:11,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 17:38:14,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72980.125
2025-03-09 17:38:14,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:15,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 17:38:18,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73031.359
2025-03-09 17:38:18,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:19,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 17:38:22,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72611.945
2025-03-09 17:38:22,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:23,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 17:38:26,143 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 79 -- loss: tensor([71766.4688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:38:26,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71766.469
2025-03-09 17:38:26,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:27,237 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 17:38:29,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72126.266
2025-03-09 17:38:29,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:31,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 17:38:33,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71761.875
2025-03-09 17:38:33,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:34,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 17:38:37,589 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71613.109
2025-03-09 17:38:37,589 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:38,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 17:38:41,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71581.891
2025-03-09 17:38:41,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:42,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 17:38:45,210 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 84 -- loss: tensor([71156.5078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:38:45,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71156.508
2025-03-09 17:38:45,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:46,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 17:38:49,055 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70921.773
2025-03-09 17:38:49,055 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:50,289 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 17:38:53,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71676.82
2025-03-09 17:38:53,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:54,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 17:38:56,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71270.609
2025-03-09 17:38:56,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:38:57,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 17:39:00,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71868.672
2025-03-09 17:39:00,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:01,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 17:39:04,479 :: INFO :: evodenss.train.trainers :: [2051] -- [2.77s] TRAIN epoch 89 -- loss: tensor([70554.7188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:39:04,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70554.719
2025-03-09 17:39:04,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:05,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 17:39:08,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71884.742
2025-03-09 17:39:08,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:09,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 17:39:12,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69917.156
2025-03-09 17:39:12,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:13,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 17:39:16,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69662.883
2025-03-09 17:39:16,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:17,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 17:39:19,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71142.203
2025-03-09 17:39:19,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:21,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 17:39:23,752 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 94 -- loss: tensor([71675.9297], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:39:23,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71675.93
2025-03-09 17:39:23,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:24,853 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 17:39:27,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70346.789
2025-03-09 17:39:27,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:28,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 17:39:31,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71171.953
2025-03-09 17:39:31,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:32,573 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 17:39:35,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68893.789
2025-03-09 17:39:35,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:36,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 17:39:39,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69971.297
2025-03-09 17:39:39,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:40,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 17:39:42,972 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 99 -- loss: tensor([69841.4688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:39:42,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69841.469
2025-03-09 17:39:42,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:45,193 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 4077.07715
2025-03-09 17:39:45,197 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 17:39:45,198 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 
layer8: :deconv1d out_channels:112 kernel_size:10 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100
2025-03-09 17:39:45,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 17:39:45,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 17:39:48,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 555818.438
2025-03-09 17:39:48,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:49,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 17:39:52,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 244927.203
2025-03-09 17:39:52,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:53,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 17:39:55,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 206573.844
2025-03-09 17:39:55,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:39:56,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 17:39:59,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 186306.094
2025-03-09 17:39:59,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:00,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 17:40:03,451 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 4 -- loss: tensor([173300.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:40:03,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 173300.953
2025-03-09 17:40:03,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:04,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 17:40:07,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 160686.188
2025-03-09 17:40:07,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:08,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 17:40:10,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 148282.734
2025-03-09 17:40:10,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:12,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 17:40:14,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 141400.953
2025-03-09 17:40:14,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:15,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 17:40:18,602 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 132206.969
2025-03-09 17:40:18,602 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:19,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 17:40:22,573 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 9 -- loss: tensor([130846.6719], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:40:22,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 130846.672
2025-03-09 17:40:22,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:23,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 17:40:26,393 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 123210.898
2025-03-09 17:40:26,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:27,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 17:40:30,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 120539.727
2025-03-09 17:40:30,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:31,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 17:40:34,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 116024.094
2025-03-09 17:40:34,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:35,070 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 17:40:37,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114526.078
2025-03-09 17:40:37,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:38,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 17:40:41,605 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 14 -- loss: tensor([112082.3984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:40:41,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112082.398
2025-03-09 17:40:41,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:42,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 17:40:45,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109274.273
2025-03-09 17:40:45,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:46,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 17:40:49,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108184.656
2025-03-09 17:40:49,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:50,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 17:40:53,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107380.945
2025-03-09 17:40:53,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:54,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 17:40:57,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105147.57
2025-03-09 17:40:57,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:40:58,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 17:41:00,851 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 19 -- loss: tensor([103516.6797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:41:00,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103516.68
2025-03-09 17:41:00,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:01,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 17:41:04,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102645.672
2025-03-09 17:41:04,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:05,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 17:41:08,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100950.773
2025-03-09 17:41:08,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:09,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 17:41:12,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100344.688
2025-03-09 17:41:12,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:13,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 17:41:15,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98971.383
2025-03-09 17:41:15,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:17,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 17:41:19,789 :: INFO :: evodenss.train.trainers :: [2051] -- [2.77s] TRAIN epoch 24 -- loss: tensor([98132.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:41:19,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98132.375
2025-03-09 17:41:19,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:20,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 17:41:23,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96839.133
2025-03-09 17:41:23,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:24,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 17:41:27,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95432.594
2025-03-09 17:41:27,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:28,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 17:41:31,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94689.945
2025-03-09 17:41:31,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:32,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 17:41:35,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94263.859
2025-03-09 17:41:35,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:36,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 17:41:38,933 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 29 -- loss: tensor([93068.6406], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:41:38,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93068.641
2025-03-09 17:41:38,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:40,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 17:41:42,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92598.945
2025-03-09 17:41:42,764 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:43,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 17:41:46,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92439.289
2025-03-09 17:41:46,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:47,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 17:41:50,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91159.453
2025-03-09 17:41:50,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:51,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 17:41:54,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90157.875
2025-03-09 17:41:54,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:55,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 17:41:58,068 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 34 -- loss: tensor([91518.3594], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:41:58,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91518.359
2025-03-09 17:41:58,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:41:59,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 17:42:01,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89490.5
2025-03-09 17:42:01,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:02,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 17:42:05,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87913.359
2025-03-09 17:42:05,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:06,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 17:42:09,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87236.391
2025-03-09 17:42:09,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:10,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 17:42:13,238 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87361.984
2025-03-09 17:42:13,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:14,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 17:42:17,075 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 39 -- loss: tensor([87297.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:42:17,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87297.875
2025-03-09 17:42:17,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:18,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 17:42:20,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85910.656
2025-03-09 17:42:20,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:22,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 17:42:24,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85993.391
2025-03-09 17:42:24,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:25,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 17:42:28,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85263.492
2025-03-09 17:42:28,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:29,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 17:42:32,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85820.844
2025-03-09 17:42:32,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:33,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 17:42:36,149 :: INFO :: evodenss.train.trainers :: [2051] -- [2.73s] TRAIN epoch 44 -- loss: tensor([84700.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:42:36,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84700.625
2025-03-09 17:42:36,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:37,234 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 17:42:39,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85173.875
2025-03-09 17:42:39,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:41,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 17:42:43,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85109.883
2025-03-09 17:42:43,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:44,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 17:42:47,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84672.086
2025-03-09 17:42:47,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:48,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 17:42:51,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84035.891
2025-03-09 17:42:51,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:52,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 17:42:55,415 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 49 -- loss: tensor([83243.2266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:42:55,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83243.227
2025-03-09 17:42:55,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:42:56,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 17:42:59,234 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83893.391
2025-03-09 17:42:59,234 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:00,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 17:43:03,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81924.805
2025-03-09 17:43:03,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:04,120 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 17:43:06,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82484.031
2025-03-09 17:43:06,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:07,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 17:43:10,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82714.227
2025-03-09 17:43:10,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:11,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 17:43:14,399 :: INFO :: evodenss.train.trainers :: [2051] -- [2.71s] TRAIN epoch 54 -- loss: tensor([81715.8438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:43:14,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81715.844
2025-03-09 17:43:14,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:15,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 17:43:18,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80972.078
2025-03-09 17:43:18,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:19,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 17:43:22,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80507.625
2025-03-09 17:43:22,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:23,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 17:43:26,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80972.883
2025-03-09 17:43:26,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:27,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 17:43:29,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80553.492
2025-03-09 17:43:29,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:30,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 17:43:33,691 :: INFO :: evodenss.train.trainers :: [2051] -- [2.72s] TRAIN epoch 59 -- loss: tensor([79306.5703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:43:33,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79306.57
2025-03-09 17:43:33,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:34,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 17:43:37,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80041.617
2025-03-09 17:43:37,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:38,584 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 17:43:41,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79856.352
2025-03-09 17:43:41,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:42,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 17:43:45,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80333.148
2025-03-09 17:43:45,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:46,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 17:43:48,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79071.492
2025-03-09 17:43:48,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:50,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 17:43:52,861 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 64 -- loss: tensor([78943.9297], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:43:52,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78943.93
2025-03-09 17:43:52,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:53,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 17:43:56,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80052.469
2025-03-09 17:43:56,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:43:57,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 17:44:00,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77494.406
2025-03-09 17:44:00,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:01,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 17:44:04,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79359.602
2025-03-09 17:44:04,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:05,413 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 17:44:08,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77756.805
2025-03-09 17:44:08,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:09,238 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 17:44:11,986 :: INFO :: evodenss.train.trainers :: [2051] -- [2.75s] TRAIN epoch 69 -- loss: tensor([78482.2969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:44:11,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78482.297
2025-03-09 17:44:11,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:13,057 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 17:44:15,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76827.617
2025-03-09 17:44:15,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:16,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 17:44:19,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78327.656
2025-03-09 17:44:19,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:20,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 17:44:23,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76743.945
2025-03-09 17:44:23,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:24,712 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 17:44:27,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77506.453
2025-03-09 17:44:27,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:28,539 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 17:44:31,277 :: INFO :: evodenss.train.trainers :: [2051] -- [2.74s] TRAIN epoch 74 -- loss: tensor([77364.9219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:44:31,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77364.922
2025-03-09 17:44:31,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:32,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 17:44:34,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77194.602
2025-03-09 17:44:34,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:36,008 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 17:44:38,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76980.672
2025-03-09 17:44:38,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:39,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 17:44:42,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76880.32
2025-03-09 17:44:42,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:43,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 17:44:46,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75257.227
2025-03-09 17:44:46,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:47,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 17:44:49,855 :: INFO :: evodenss.train.trainers :: [2051] -- [2.8s] TRAIN epoch 79 -- loss: tensor([77464.8984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:44:49,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77464.898
2025-03-09 17:44:49,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:50,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 17:44:53,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77528.781
2025-03-09 17:44:53,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:54,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 17:44:57,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74975.414
2025-03-09 17:44:57,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:44:58,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 17:45:00,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75760.781
2025-03-09 17:45:00,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:01,929 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 17:45:04,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76493.508
2025-03-09 17:45:04,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:05,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 17:45:08,245 :: INFO :: evodenss.train.trainers :: [2051] -- [2.59s] TRAIN epoch 84 -- loss: tensor([75633.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:45:08,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75633.312
2025-03-09 17:45:08,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:09,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 17:45:11,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76170.078
2025-03-09 17:45:11,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:12,927 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 17:45:15,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75375.977
2025-03-09 17:45:15,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:16,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 17:45:19,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76040.461
2025-03-09 17:45:19,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:20,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 17:45:23,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74252.477
2025-03-09 17:45:23,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:24,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 17:45:26,932 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 89 -- loss: tensor([75651.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:45:26,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75651.477
2025-03-09 17:45:26,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:27,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 17:45:30,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75187.008
2025-03-09 17:45:30,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:31,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 17:45:34,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75335.445
2025-03-09 17:45:34,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:35,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 17:45:38,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75395.562
2025-03-09 17:45:38,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:39,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 17:45:41,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75791.125
2025-03-09 17:45:41,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:42,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 17:45:45,582 :: INFO :: evodenss.train.trainers :: [2051] -- [2.65s] TRAIN epoch 94 -- loss: tensor([74643.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:45:45,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74643.312
2025-03-09 17:45:45,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:46,637 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 17:45:49,465 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74169.836
2025-03-09 17:45:49,465 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:50,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 17:45:53,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73937.844
2025-03-09 17:45:53,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:54,179 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 17:45:56,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74201.328
2025-03-09 17:45:56,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:45:57,897 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 17:46:00,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73918.414
2025-03-09 17:46:00,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:46:01,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 17:46:04,357 :: INFO :: evodenss.train.trainers :: [2051] -- [2.68s] TRAIN epoch 99 -- loss: tensor([74808.5234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:46:04,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74808.523
2025-03-09 17:46:04,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:46:06,478 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 4027.01953
2025-03-09 17:46:06,482 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 17:46:06,483 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :conv1d out_channels:120 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:6 epochs:100
2025-03-09 17:46:06,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 17:46:06,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 17:46:12,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 226889.688
2025-03-09 17:46:12,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:46:13,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 17:46:18,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 131475.812
2025-03-09 17:46:18,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:46:20,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 17:46:25,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 116954.25
2025-03-09 17:46:25,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:46:26,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 17:46:32,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109837.531
2025-03-09 17:46:32,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:46:33,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 17:46:39,034 :: INFO :: evodenss.train.trainers :: [2051] -- [5.52s] TRAIN epoch 4 -- loss: tensor([105737.1953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:46:39,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105737.195
2025-03-09 17:46:39,034 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:46:40,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 17:46:45,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102380.562
2025-03-09 17:46:45,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:46:46,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 17:46:52,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98315.234
2025-03-09 17:46:52,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:46:53,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 17:46:59,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95839.938
2025-03-09 17:46:59,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:47:00,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 17:47:05,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95284.898
2025-03-09 17:47:05,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:47:06,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 17:47:12,499 :: INFO :: evodenss.train.trainers :: [2051] -- [5.55s] TRAIN epoch 9 -- loss: tensor([92454.8906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:47:12,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92454.891
2025-03-09 17:47:12,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:47:13,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 17:47:19,348 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91798.305
2025-03-09 17:47:19,348 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:47:20,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 17:47:26,008 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90674.836
2025-03-09 17:47:26,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:47:27,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 17:47:32,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89223.219
2025-03-09 17:47:32,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:47:33,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 17:47:39,304 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88360.734
2025-03-09 17:47:39,304 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:47:40,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 17:47:45,969 :: INFO :: evodenss.train.trainers :: [2051] -- [5.57s] TRAIN epoch 14 -- loss: tensor([87583.5781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:47:45,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87583.578
2025-03-09 17:47:45,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:47:47,073 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 17:47:52,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85774.594
2025-03-09 17:47:52,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:47:53,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 17:47:59,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85401.914
2025-03-09 17:47:59,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:48:00,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 17:48:05,870 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84959.531
2025-03-09 17:48:05,870 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:48:06,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 17:48:12,562 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84097.805
2025-03-09 17:48:12,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:48:13,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 17:48:19,268 :: INFO :: evodenss.train.trainers :: [2051] -- [5.58s] TRAIN epoch 19 -- loss: tensor([83592.3672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:48:19,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83592.367
2025-03-09 17:48:19,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:48:20,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 17:48:25,976 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82278.125
2025-03-09 17:48:25,976 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:48:27,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 17:48:32,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81623.922
2025-03-09 17:48:32,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:48:33,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 17:48:39,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81279.75
2025-03-09 17:48:39,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:48:40,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 17:48:45,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80886.0
2025-03-09 17:48:45,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:48:46,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 17:48:52,578 :: INFO :: evodenss.train.trainers :: [2051] -- [5.62s] TRAIN epoch 24 -- loss: tensor([80126.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:48:52,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80126.875
2025-03-09 17:48:52,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:48:53,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 17:48:59,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80546.156
2025-03-09 17:48:59,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:49:00,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 17:49:05,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80107.242
2025-03-09 17:49:05,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:49:06,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 17:49:12,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79221.281
2025-03-09 17:49:12,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:49:13,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 17:49:19,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78649.016
2025-03-09 17:49:19,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:49:20,433 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 17:49:25,935 :: INFO :: evodenss.train.trainers :: [2051] -- [5.5s] TRAIN epoch 29 -- loss: tensor([78532.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:49:25,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78532.188
2025-03-09 17:49:25,936 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:49:27,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 17:49:32,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77813.453
2025-03-09 17:49:32,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:49:33,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 17:49:39,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77617.719
2025-03-09 17:49:39,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:49:40,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 17:49:45,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77436.562
2025-03-09 17:49:45,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:49:47,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 17:49:52,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76810.852
2025-03-09 17:49:52,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:49:53,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 17:49:59,347 :: INFO :: evodenss.train.trainers :: [2051] -- [5.49s] TRAIN epoch 34 -- loss: tensor([76553.3672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:49:59,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76553.367
2025-03-09 17:49:59,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:50:00,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 17:50:05,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76711.422
2025-03-09 17:50:05,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:50:07,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 17:50:12,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76038.195
2025-03-09 17:50:12,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:50:13,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 17:50:19,462 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76051.852
2025-03-09 17:50:19,462 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:50:20,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 17:50:26,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75424.305
2025-03-09 17:50:26,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:50:27,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 17:50:33,273 :: INFO :: evodenss.train.trainers :: [2051] -- [6.0s] TRAIN epoch 39 -- loss: tensor([75405.8438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:50:33,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75405.844
2025-03-09 17:50:33,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:50:34,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 17:50:39,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74129.617
2025-03-09 17:50:39,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:50:40,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 17:50:46,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74414.75
2025-03-09 17:50:46,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:50:47,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 17:50:53,073 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74750.453
2025-03-09 17:50:53,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:50:54,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 17:50:59,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74172.102
2025-03-09 17:50:59,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:51:00,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 17:51:06,324 :: INFO :: evodenss.train.trainers :: [2051] -- [5.54s] TRAIN epoch 44 -- loss: tensor([74618.6953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:51:06,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74618.695
2025-03-09 17:51:06,325 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:51:07,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 17:51:12,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74376.5
2025-03-09 17:51:12,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:51:14,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 17:51:19,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73407.609
2025-03-09 17:51:19,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:51:20,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 17:51:26,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73392.43
2025-03-09 17:51:26,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:51:27,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 17:51:32,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73515.328
2025-03-09 17:51:32,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:51:33,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 17:51:39,514 :: INFO :: evodenss.train.trainers :: [2051] -- [5.51s] TRAIN epoch 49 -- loss: tensor([72875.7969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:51:39,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72875.797
2025-03-09 17:51:39,514 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:51:40,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 17:51:46,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73691.516
2025-03-09 17:51:46,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:51:47,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 17:51:52,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73121.5
2025-03-09 17:51:52,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:51:54,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 17:51:59,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73284.0
2025-03-09 17:51:59,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:52:00,644 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 17:52:06,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72856.531
2025-03-09 17:52:06,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:52:07,241 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 17:52:12,790 :: INFO :: evodenss.train.trainers :: [2051] -- [5.55s] TRAIN epoch 54 -- loss: tensor([73004.1953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:52:12,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73004.195
2025-03-09 17:52:12,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:52:13,951 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 17:52:19,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72128.203
2025-03-09 17:52:19,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:52:20,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 17:52:26,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71865.172
2025-03-09 17:52:26,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:52:27,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 17:52:32,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72134.258
2025-03-09 17:52:32,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:52:34,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 17:52:39,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72360.133
2025-03-09 17:52:39,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:52:40,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 17:52:46,189 :: INFO :: evodenss.train.trainers :: [2051] -- [5.52s] TRAIN epoch 59 -- loss: tensor([71451.8047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:52:46,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71451.805
2025-03-09 17:52:46,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:52:47,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 17:52:52,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71806.0
2025-03-09 17:52:52,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:52:54,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 17:52:59,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71423.188
2025-03-09 17:52:59,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:53:00,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 17:53:06,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71233.016
2025-03-09 17:53:06,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:53:07,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 17:53:12,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71069.375
2025-03-09 17:53:12,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:53:14,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 17:53:19,752 :: INFO :: evodenss.train.trainers :: [2051] -- [5.75s] TRAIN epoch 64 -- loss: tensor([71064.7578], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:53:19,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71064.758
2025-03-09 17:53:19,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:53:20,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 17:53:26,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70586.172
2025-03-09 17:53:26,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:53:27,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 17:53:33,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70945.133
2025-03-09 17:53:33,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:53:34,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 17:53:39,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70832.617
2025-03-09 17:53:39,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:53:40,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 17:53:46,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70419.102
2025-03-09 17:53:46,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:53:47,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 17:53:53,251 :: INFO :: evodenss.train.trainers :: [2051] -- [5.64s] TRAIN epoch 69 -- loss: tensor([69718.6172], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:53:53,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69718.617
2025-03-09 17:53:53,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:53:54,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 17:53:59,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70803.875
2025-03-09 17:53:59,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:54:01,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 17:54:06,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69807.523
2025-03-09 17:54:06,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:54:07,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 17:54:13,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70092.109
2025-03-09 17:54:13,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:54:14,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 17:54:19,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70069.156
2025-03-09 17:54:19,985 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:54:21,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 17:54:26,713 :: INFO :: evodenss.train.trainers :: [2051] -- [5.57s] TRAIN epoch 74 -- loss: tensor([69804.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:54:26,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69804.938
2025-03-09 17:54:26,713 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:54:27,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 17:54:33,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69456.766
2025-03-09 17:54:33,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:54:34,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 17:54:40,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69293.555
2025-03-09 17:54:40,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:54:41,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 17:54:46,795 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69051.531
2025-03-09 17:54:46,795 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:54:47,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 17:54:53,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68665.188
2025-03-09 17:54:53,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:54:54,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 17:55:00,279 :: INFO :: evodenss.train.trainers :: [2051] -- [5.5s] TRAIN epoch 79 -- loss: tensor([68774.6797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:55:00,279 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68774.68
2025-03-09 17:55:00,279 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:55:01,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 17:55:06,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68193.82
2025-03-09 17:55:06,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:55:08,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 17:55:13,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68869.641
2025-03-09 17:55:13,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:55:14,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 17:55:20,454 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67980.852
2025-03-09 17:55:20,454 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:55:21,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 17:55:27,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68731.047
2025-03-09 17:55:27,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:55:28,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 17:55:33,904 :: INFO :: evodenss.train.trainers :: [2051] -- [5.56s] TRAIN epoch 84 -- loss: tensor([68800.6562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:55:33,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68800.656
2025-03-09 17:55:33,905 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:55:35,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 17:55:40,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68918.812
2025-03-09 17:55:40,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:55:41,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 17:55:47,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68695.516
2025-03-09 17:55:47,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:55:48,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 17:55:53,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68072.742
2025-03-09 17:55:53,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:55:54,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 17:56:00,539 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68431.211
2025-03-09 17:56:00,539 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:56:01,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 17:56:07,154 :: INFO :: evodenss.train.trainers :: [2051] -- [5.5s] TRAIN epoch 89 -- loss: tensor([68421.4453], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:56:07,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68421.445
2025-03-09 17:56:07,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:56:08,279 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 17:56:13,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67508.828
2025-03-09 17:56:13,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:56:14,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 17:56:20,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67190.32
2025-03-09 17:56:20,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:56:21,638 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 17:56:27,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67130.016
2025-03-09 17:56:27,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:56:28,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 17:56:33,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67569.062
2025-03-09 17:56:33,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:56:34,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 17:56:40,428 :: INFO :: evodenss.train.trainers :: [2051] -- [5.49s] TRAIN epoch 94 -- loss: tensor([68045.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:56:40,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68045.188
2025-03-09 17:56:40,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:56:41,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 17:56:46,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67983.844
2025-03-09 17:56:46,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:56:48,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 17:56:53,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67434.5
2025-03-09 17:56:53,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:56:54,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 17:57:00,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67858.852
2025-03-09 17:57:00,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:01,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 17:57:06,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67420.453
2025-03-09 17:57:06,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:08,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 17:57:13,522 :: INFO :: evodenss.train.trainers :: [2051] -- [5.49s] TRAIN epoch 99 -- loss: tensor([67907.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:57:13,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67907.125
2025-03-09 17:57:13,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:15,781 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 4004.8313
2025-03-09 17:57:15,785 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 17:57:15,787 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:109 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :conv1d out_channels:109 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :deconv1d out_channels:37 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100
2025-03-09 17:57:15,798 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 17:57:15,798 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 17:57:18,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 908653.625
2025-03-09 17:57:18,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:19,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 17:57:21,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 810485.25
2025-03-09 17:57:21,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:22,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 17:57:24,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 923091.875
2025-03-09 17:57:24,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:25,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 17:57:27,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813466.875
2025-03-09 17:57:27,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:28,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 17:57:30,819 :: INFO :: evodenss.train.trainers :: [2051] -- [2.06s] TRAIN epoch 4 -- loss: tensor([826561.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:57:30,820 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 826561.25
2025-03-09 17:57:30,820 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:31,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 17:57:33,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 836892.5
2025-03-09 17:57:33,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:34,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 17:57:37,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 813306.062
2025-03-09 17:57:37,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:38,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 17:57:40,120 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 633188.438
2025-03-09 17:57:40,120 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:41,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 17:57:43,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 248115.859
2025-03-09 17:57:43,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:44,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 17:57:46,255 :: INFO :: evodenss.train.trainers :: [2051] -- [2.04s] TRAIN epoch 9 -- loss: tensor([152227.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:57:46,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 152227.0
2025-03-09 17:57:46,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:47,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 17:57:49,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 136177.891
2025-03-09 17:57:49,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:50,528 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 17:57:52,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 127691.328
2025-03-09 17:57:52,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:53,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 17:57:55,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 121898.633
2025-03-09 17:57:55,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:56,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 17:57:58,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 117703.445
2025-03-09 17:57:58,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:57:59,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 17:58:01,984 :: INFO :: evodenss.train.trainers :: [2051] -- [2.1s] TRAIN epoch 14 -- loss: tensor([112577.0703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:58:01,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112577.07
2025-03-09 17:58:01,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:03,025 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 17:58:05,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110537.727
2025-03-09 17:58:05,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:06,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 17:58:08,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107874.102
2025-03-09 17:58:08,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:09,271 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 17:58:11,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105958.656
2025-03-09 17:58:11,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:12,404 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 17:58:14,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104681.391
2025-03-09 17:58:14,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:15,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 17:58:17,600 :: INFO :: evodenss.train.trainers :: [2051] -- [2.09s] TRAIN epoch 19 -- loss: tensor([103545.7578], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:58:17,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103545.758
2025-03-09 17:58:17,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:18,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 17:58:20,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103627.898
2025-03-09 17:58:20,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:21,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 17:58:24,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101332.445
2025-03-09 17:58:24,045 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:25,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 17:58:27,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100571.719
2025-03-09 17:58:27,201 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:28,263 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 17:58:30,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101394.008
2025-03-09 17:58:30,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:31,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 17:58:33,475 :: INFO :: evodenss.train.trainers :: [2051] -- [2.07s] TRAIN epoch 24 -- loss: tensor([101301.6172], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:58:33,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101301.617
2025-03-09 17:58:33,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:34,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 17:58:36,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98615.867
2025-03-09 17:58:36,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:37,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 17:58:39,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96660.609
2025-03-09 17:58:39,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:40,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 17:58:42,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97859.789
2025-03-09 17:58:42,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:43,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 17:58:46,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97854.148
2025-03-09 17:58:46,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:47,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 17:58:49,247 :: INFO :: evodenss.train.trainers :: [2051] -- [2.14s] TRAIN epoch 29 -- loss: tensor([96814.7969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:58:49,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96814.797
2025-03-09 17:58:49,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:50,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 17:58:52,436 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96993.406
2025-03-09 17:58:52,436 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:53,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 17:58:55,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96144.906
2025-03-09 17:58:55,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:56,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 17:58:58,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96709.211
2025-03-09 17:58:58,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:58:59,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 17:59:01,797 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96051.938
2025-03-09 17:59:01,797 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:02,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 17:59:04,961 :: INFO :: evodenss.train.trainers :: [2051] -- [2.08s] TRAIN epoch 34 -- loss: tensor([96415.2266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:59:04,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96415.227
2025-03-09 17:59:04,961 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:06,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 17:59:08,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94593.531
2025-03-09 17:59:08,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:09,182 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 17:59:11,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94286.164
2025-03-09 17:59:11,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:12,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 17:59:14,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94334.234
2025-03-09 17:59:14,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:15,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 17:59:17,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93991.18
2025-03-09 17:59:17,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:18,616 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 17:59:20,886 :: INFO :: evodenss.train.trainers :: [2051] -- [2.27s] TRAIN epoch 39 -- loss: tensor([93916.9688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:59:20,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93916.969
2025-03-09 17:59:20,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:21,953 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 17:59:24,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93401.211
2025-03-09 17:59:24,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:25,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 17:59:27,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93118.805
2025-03-09 17:59:27,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:28,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 17:59:30,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93629.758
2025-03-09 17:59:30,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:31,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 17:59:33,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91448.203
2025-03-09 17:59:33,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:34,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 17:59:36,589 :: INFO :: evodenss.train.trainers :: [2051] -- [2.1s] TRAIN epoch 44 -- loss: tensor([92120.8594], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:59:36,589 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92120.859
2025-03-09 17:59:36,589 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:37,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 17:59:39,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92334.5
2025-03-09 17:59:39,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:40,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 17:59:42,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92242.641
2025-03-09 17:59:42,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:44,007 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 17:59:46,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91963.0
2025-03-09 17:59:46,099 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:47,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 17:59:49,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92304.586
2025-03-09 17:59:49,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:50,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 17:59:52,535 :: INFO :: evodenss.train.trainers :: [2051] -- [2.09s] TRAIN epoch 49 -- loss: tensor([89978.5547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 17:59:52,535 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89978.555
2025-03-09 17:59:52,535 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:53,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 17:59:55,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90440.102
2025-03-09 17:59:55,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:56,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 17:59:58,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90492.297
2025-03-09 17:59:58,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 17:59:59,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 18:00:01,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89846.398
2025-03-09 18:00:01,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:03,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 18:00:05,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90473.469
2025-03-09 18:00:05,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:06,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 18:00:08,347 :: INFO :: evodenss.train.trainers :: [2051] -- [2.07s] TRAIN epoch 54 -- loss: tensor([90752.9688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:00:08,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90752.969
2025-03-09 18:00:08,348 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:09,411 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 18:00:11,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90482.164
2025-03-09 18:00:11,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:12,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 18:00:14,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89590.617
2025-03-09 18:00:14,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:15,691 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 18:00:17,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90843.055
2025-03-09 18:00:17,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:18,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 18:00:21,054 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88661.133
2025-03-09 18:00:21,054 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:22,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 18:00:24,221 :: INFO :: evodenss.train.trainers :: [2051] -- [2.09s] TRAIN epoch 59 -- loss: tensor([89818.3438], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:00:24,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89818.344
2025-03-09 18:00:24,221 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:25,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 18:00:27,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88943.289
2025-03-09 18:00:27,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:28,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 18:00:30,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89489.039
2025-03-09 18:00:30,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:31,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 18:00:33,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88250.758
2025-03-09 18:00:33,665 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:34,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 18:00:36,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89322.398
2025-03-09 18:00:36,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:37,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 18:00:39,997 :: INFO :: evodenss.train.trainers :: [2051] -- [2.08s] TRAIN epoch 64 -- loss: tensor([89421.2656], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:00:39,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89421.266
2025-03-09 18:00:39,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:41,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 18:00:43,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88523.336
2025-03-09 18:00:43,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:44,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 18:00:46,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88247.258
2025-03-09 18:00:46,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:47,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 18:00:49,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88246.125
2025-03-09 18:00:49,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:50,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 18:00:52,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86824.734
2025-03-09 18:00:52,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:53,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 18:00:55,808 :: INFO :: evodenss.train.trainers :: [2051] -- [2.07s] TRAIN epoch 69 -- loss: tensor([89165.5859], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:00:55,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89165.586
2025-03-09 18:00:55,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:00:56,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 18:00:58,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87410.492
2025-03-09 18:00:58,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:00,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 18:01:02,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87977.953
2025-03-09 18:01:02,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:03,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 18:01:05,222 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86406.812
2025-03-09 18:01:05,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:06,279 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 18:01:08,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87101.766
2025-03-09 18:01:08,361 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:09,413 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 18:01:11,526 :: INFO :: evodenss.train.trainers :: [2051] -- [2.11s] TRAIN epoch 74 -- loss: tensor([86936.4219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:01:11,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86936.422
2025-03-09 18:01:11,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:12,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 18:01:14,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87379.328
2025-03-09 18:01:14,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:15,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 18:01:17,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87156.0
2025-03-09 18:01:17,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:18,894 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 18:01:21,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87443.609
2025-03-09 18:01:21,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:22,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 18:01:24,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86393.367
2025-03-09 18:01:24,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:25,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 18:01:27,395 :: INFO :: evodenss.train.trainers :: [2051] -- [2.08s] TRAIN epoch 79 -- loss: tensor([86627.8281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:01:27,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86627.828
2025-03-09 18:01:27,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:28,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 18:01:30,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85757.836
2025-03-09 18:01:30,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:31,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 18:01:33,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84762.711
2025-03-09 18:01:33,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:34,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 18:01:36,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86293.695
2025-03-09 18:01:36,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:37,851 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 18:01:39,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85424.172
2025-03-09 18:01:39,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:40,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 18:01:42,990 :: INFO :: evodenss.train.trainers :: [2051] -- [2.05s] TRAIN epoch 84 -- loss: tensor([86509.7656], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:01:42,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86509.766
2025-03-09 18:01:42,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:43,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 18:01:46,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86860.375
2025-03-09 18:01:46,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:47,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 18:01:49,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84284.266
2025-03-09 18:01:49,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:50,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 18:01:52,413 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85653.453
2025-03-09 18:01:52,413 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:53,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 18:01:55,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85232.812
2025-03-09 18:01:55,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:56,594 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 18:01:58,655 :: INFO :: evodenss.train.trainers :: [2051] -- [2.06s] TRAIN epoch 89 -- loss: tensor([84008.6406], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:01:58,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84008.641
2025-03-09 18:01:58,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:01:59,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 18:02:01,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85897.234
2025-03-09 18:02:01,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:02,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 18:02:04,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83945.719
2025-03-09 18:02:04,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:05,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 18:02:08,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86215.828
2025-03-09 18:02:08,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:09,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 18:02:11,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86192.102
2025-03-09 18:02:11,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:12,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 18:02:14,335 :: INFO :: evodenss.train.trainers :: [2051] -- [2.11s] TRAIN epoch 94 -- loss: tensor([84316.7344], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:02:14,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84316.734
2025-03-09 18:02:14,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:15,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 18:02:17,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84100.828
2025-03-09 18:02:17,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:18,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 18:02:20,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83676.016
2025-03-09 18:02:20,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:21,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 18:02:23,907 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84521.289
2025-03-09 18:02:23,907 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:24,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 18:02:27,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84894.258
2025-03-09 18:02:27,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:28,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 18:02:30,207 :: INFO :: evodenss.train.trainers :: [2051] -- [2.08s] TRAIN epoch 99 -- loss: tensor([85418.9609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:02:30,207 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85418.961
2025-03-09 18:02:30,207 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:32,259 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 4258.56787
2025-03-09 18:02:32,260 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 18:02:32,260 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 2, id: 2
2025-03-09 18:02:32,260 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 1000, 1000]
2025-03-09 18:02:32,260 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 18:02:32,264 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [4077.07715, 4027.01953, 4004.8313, 4258.56787]
2025-03-09 18:02:32,539 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([20298.1230], device='cuda:0')
2025-03-09 18:02:32,539 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 13: 4004.8313
2025-03-09 18:02:32,540 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3575.71436



2025-03-09 18:02:32,622 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 14
2025-03-09 18:02:32,622 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 18:02:32,632 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a layer removed from Module 0: features; Position: 12
2025-03-09 18:02:32,633 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have an extra layer at Module 0: features; Position: 6. Reused?: True
2025-03-09 18:02:32,634 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have an extra layer at Module 0: features; Position: 12. Reused?: False
2025-03-09 18:02:32,634 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 18:02:32,635 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 18:02:32,636 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 18:02:32,636 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 18:02:32,639 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 8. Reused?: True
2025-03-09 18:02:32,640 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 8. Reused?: False
2025-03-09 18:02:32,640 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 18:02:32,641 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 18:02:32,642 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 18:02:32,643 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 18:02:32,643 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 18:02:32,644 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 18:02:32,645 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 18:02:32,645 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 18:02:32,648 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 8
2025-03-09 18:02:32,649 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have an extra layer at Module 0: features; Position: 11. Reused?: False
2025-03-09 18:02:32,649 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a layer removed from Module 0: features; Position: 12
2025-03-09 18:02:32,650 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have an extra layer at Module 0: features; Position: 4. Reused?: False
2025-03-09 18:02:32,651 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 18:02:32,651 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 18:02:32,652 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 18:02:32,653 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 18:02:32,653 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 18:02:32,656 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 18:02:32,659 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 18:02:32,660 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :conv1d out_channels:120 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:6 epochs:100
2025-03-09 18:02:32,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 18:02:32,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 18:02:38,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 309001.094
2025-03-09 18:02:38,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:39,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 18:02:44,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 128816.32
2025-03-09 18:02:44,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:45,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 18:02:51,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112318.75
2025-03-09 18:02:51,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:02:52,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 18:02:59,308 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105692.938
2025-03-09 18:02:59,309 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:03:00,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 18:03:05,909 :: INFO :: evodenss.train.trainers :: [2051] -- [5.47s] TRAIN epoch 4 -- loss: tensor([100599.7812], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:03:05,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100599.781
2025-03-09 18:03:05,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:03:07,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 18:03:12,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97561.258
2025-03-09 18:03:12,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:03:13,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 18:03:19,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95331.68
2025-03-09 18:03:19,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:03:20,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 18:03:25,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93305.969
2025-03-09 18:03:25,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:03:27,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 18:03:32,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91587.906
2025-03-09 18:03:32,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:03:33,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 18:03:39,403 :: INFO :: evodenss.train.trainers :: [2051] -- [5.56s] TRAIN epoch 9 -- loss: tensor([90052.6172], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:03:39,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90052.617
2025-03-09 18:03:39,403 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:03:40,533 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 18:03:46,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89489.375
2025-03-09 18:03:46,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:03:47,154 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 18:03:52,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88007.445
2025-03-09 18:03:52,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:03:53,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 18:03:59,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87004.109
2025-03-09 18:03:59,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:04:00,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 18:04:06,140 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85689.008
2025-03-09 18:04:06,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:04:07,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 18:04:12,841 :: INFO :: evodenss.train.trainers :: [2051] -- [5.55s] TRAIN epoch 14 -- loss: tensor([85110.1016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:04:12,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85110.102
2025-03-09 18:04:12,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:04:13,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 18:04:19,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84207.742
2025-03-09 18:04:19,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:04:20,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 18:04:26,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83445.008
2025-03-09 18:04:26,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:04:27,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 18:04:33,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83601.539
2025-03-09 18:04:33,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:04:34,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 18:04:39,798 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82124.797
2025-03-09 18:04:39,798 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:04:40,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 18:04:46,502 :: INFO :: evodenss.train.trainers :: [2051] -- [5.56s] TRAIN epoch 19 -- loss: tensor([81956.5234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:04:46,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81956.523
2025-03-09 18:04:46,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:04:47,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 18:04:53,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81638.789
2025-03-09 18:04:53,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:04:54,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 18:04:59,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80790.875
2025-03-09 18:04:59,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:05:01,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 18:05:06,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79902.977
2025-03-09 18:05:06,608 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:05:07,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 18:05:13,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80460.258
2025-03-09 18:05:13,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:05:14,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 18:05:20,027 :: INFO :: evodenss.train.trainers :: [2051] -- [5.65s] TRAIN epoch 24 -- loss: tensor([78982.2109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:05:20,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78982.211
2025-03-09 18:05:20,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:05:21,141 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 18:05:26,643 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79071.438
2025-03-09 18:05:26,643 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:05:27,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 18:05:33,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78747.172
2025-03-09 18:05:33,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:05:34,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 18:05:39,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77897.766
2025-03-09 18:05:39,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:05:41,067 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 18:05:46,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78854.578
2025-03-09 18:05:46,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:05:47,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 18:05:53,374 :: INFO :: evodenss.train.trainers :: [2051] -- [5.65s] TRAIN epoch 29 -- loss: tensor([77527.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:05:53,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77527.477
2025-03-09 18:05:53,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:05:54,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 18:05:59,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76802.578
2025-03-09 18:05:59,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:06:01,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 18:06:06,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76714.93
2025-03-09 18:06:06,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:06:07,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 18:06:13,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76367.789
2025-03-09 18:06:13,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:06:14,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 18:06:20,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75614.352
2025-03-09 18:06:20,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:06:21,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 18:06:26,698 :: INFO :: evodenss.train.trainers :: [2051] -- [5.5s] TRAIN epoch 34 -- loss: tensor([75510.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:06:26,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75510.477
2025-03-09 18:06:26,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:06:27,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 18:06:33,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74904.922
2025-03-09 18:06:33,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:06:34,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 18:06:39,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74589.648
2025-03-09 18:06:39,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:06:41,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 18:06:46,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75460.266
2025-03-09 18:06:46,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:06:47,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 18:06:53,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74768.078
2025-03-09 18:06:53,311 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:06:54,436 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 18:06:59,967 :: INFO :: evodenss.train.trainers :: [2051] -- [5.53s] TRAIN epoch 39 -- loss: tensor([74327.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:06:59,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74327.5
2025-03-09 18:06:59,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:07:01,116 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 18:07:06,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73691.352
2025-03-09 18:07:06,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:07:07,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 18:07:13,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73835.398
2025-03-09 18:07:13,275 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:07:14,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 18:07:20,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73070.469
2025-03-09 18:07:20,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:07:21,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 18:07:26,688 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73452.734
2025-03-09 18:07:26,688 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:07:27,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 18:07:33,279 :: INFO :: evodenss.train.trainers :: [2051] -- [5.49s] TRAIN epoch 44 -- loss: tensor([73419.8047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:07:33,279 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73419.805
2025-03-09 18:07:33,279 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:07:34,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 18:07:39,894 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73522.133
2025-03-09 18:07:39,894 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:07:41,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 18:07:46,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72728.141
2025-03-09 18:07:46,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:07:47,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 18:07:53,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72179.414
2025-03-09 18:07:53,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:07:54,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 18:07:59,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72085.852
2025-03-09 18:07:59,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:08:01,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 18:08:06,614 :: INFO :: evodenss.train.trainers :: [2051] -- [5.54s] TRAIN epoch 49 -- loss: tensor([71631.5156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:08:06,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71631.516
2025-03-09 18:08:06,614 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:08:07,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 18:08:13,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71675.992
2025-03-09 18:08:13,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:08:14,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 18:08:19,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71778.711
2025-03-09 18:08:19,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:08:21,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 18:08:26,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70995.555
2025-03-09 18:08:26,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:08:27,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 18:08:33,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71760.672
2025-03-09 18:08:33,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:08:34,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 18:08:39,931 :: INFO :: evodenss.train.trainers :: [2051] -- [5.48s] TRAIN epoch 54 -- loss: tensor([71099.3906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:08:39,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71099.391
2025-03-09 18:08:39,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:08:41,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 18:08:46,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69718.383
2025-03-09 18:08:46,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:08:47,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 18:08:53,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70907.523
2025-03-09 18:08:53,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:08:54,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 18:09:00,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70685.484
2025-03-09 18:09:00,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:09:01,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 18:09:06,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70007.07
2025-03-09 18:09:06,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:09:07,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 18:09:13,315 :: INFO :: evodenss.train.trainers :: [2051] -- [5.5s] TRAIN epoch 59 -- loss: tensor([69692.4844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:09:13,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69692.484
2025-03-09 18:09:13,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:09:14,446 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 18:09:20,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69909.242
2025-03-09 18:09:20,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:09:21,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 18:09:26,795 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69808.656
2025-03-09 18:09:26,795 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:09:27,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 18:09:33,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69653.477
2025-03-09 18:09:33,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:09:34,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 18:09:40,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69210.172
2025-03-09 18:09:40,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:09:41,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 18:09:46,855 :: INFO :: evodenss.train.trainers :: [2051] -- [5.56s] TRAIN epoch 64 -- loss: tensor([69941.7812], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:09:46,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69941.781
2025-03-09 18:09:46,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:09:48,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 18:09:53,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68983.328
2025-03-09 18:09:53,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:09:54,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 18:10:00,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68987.375
2025-03-09 18:10:00,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:10:01,563 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 18:10:07,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68664.844
2025-03-09 18:10:07,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:10:08,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 18:10:13,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68150.859
2025-03-09 18:10:13,890 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:10:15,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 18:10:20,738 :: INFO :: evodenss.train.trainers :: [2051] -- [5.69s] TRAIN epoch 69 -- loss: tensor([68746.4844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:10:20,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68746.484
2025-03-09 18:10:20,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:10:21,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 18:10:27,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67771.891
2025-03-09 18:10:27,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:10:28,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 18:10:34,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67683.586
2025-03-09 18:10:34,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:10:35,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 18:10:40,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67858.312
2025-03-09 18:10:40,825 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:10:41,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 18:10:47,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68350.211
2025-03-09 18:10:47,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:10:48,684 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 18:10:54,391 :: INFO :: evodenss.train.trainers :: [2051] -- [5.7s] TRAIN epoch 74 -- loss: tensor([68410.1016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:10:54,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68410.102
2025-03-09 18:10:54,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:10:55,555 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 18:11:01,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67215.594
2025-03-09 18:11:01,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:11:02,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 18:11:07,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67983.797
2025-03-09 18:11:07,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:11:09,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 18:11:14,617 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67060.312
2025-03-09 18:11:14,617 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:11:15,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 18:11:21,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67094.727
2025-03-09 18:11:21,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:11:22,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 18:11:28,112 :: INFO :: evodenss.train.trainers :: [2051] -- [5.51s] TRAIN epoch 79 -- loss: tensor([66699.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:11:28,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66699.477
2025-03-09 18:11:28,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:11:29,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 18:11:34,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66509.422
2025-03-09 18:11:34,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:11:35,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 18:11:41,489 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67515.352
2025-03-09 18:11:41,489 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:11:42,630 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 18:11:48,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66989.18
2025-03-09 18:11:48,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:11:49,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 18:11:55,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66606.062
2025-03-09 18:11:55,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:11:56,191 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 18:12:01,746 :: INFO :: evodenss.train.trainers :: [2051] -- [5.55s] TRAIN epoch 84 -- loss: tensor([66910.7734], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:12:01,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66910.773
2025-03-09 18:12:01,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:12:02,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 18:12:08,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66592.109
2025-03-09 18:12:08,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:12:09,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 18:12:15,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66511.961
2025-03-09 18:12:15,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:12:16,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 18:12:21,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65803.016
2025-03-09 18:12:21,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:12:22,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 18:12:28,309 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65818.875
2025-03-09 18:12:28,309 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:12:29,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 18:12:34,959 :: INFO :: evodenss.train.trainers :: [2051] -- [5.52s] TRAIN epoch 89 -- loss: tensor([65676.7969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:12:34,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65676.797
2025-03-09 18:12:34,959 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:12:36,111 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 18:12:41,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66075.461
2025-03-09 18:12:41,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:12:42,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 18:12:48,309 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65761.852
2025-03-09 18:12:48,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:12:49,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 18:12:55,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65419.551
2025-03-09 18:12:55,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:12:56,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 18:13:01,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65809.383
2025-03-09 18:13:01,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:13:02,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 18:13:08,528 :: INFO :: evodenss.train.trainers :: [2051] -- [5.58s] TRAIN epoch 94 -- loss: tensor([65898.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:13:08,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65898.125
2025-03-09 18:13:08,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:13:09,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 18:13:15,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65302.617
2025-03-09 18:13:15,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:13:16,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 18:13:22,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65472.832
2025-03-09 18:13:22,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:13:23,238 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 18:13:28,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65262.211
2025-03-09 18:13:28,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:13:29,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 18:13:35,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65474.164
2025-03-09 18:13:35,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:13:36,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 18:13:42,291 :: INFO :: evodenss.train.trainers :: [2051] -- [5.6s] TRAIN epoch 99 -- loss: tensor([65516.2461], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:13:42,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65516.246
2025-03-09 18:13:42,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:13:44,670 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 3845.52393
2025-03-09 18:13:44,675 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 18:13:44,676 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 
layer11: :conv1d out_channels:120 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:95 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 
layer13: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:7 epochs:100
2025-03-09 18:13:44,688 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 18:13:44,688 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 18:13:50,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 987337.75
2025-03-09 18:13:50,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:13:51,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 18:13:57,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 829637.75
2025-03-09 18:13:57,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:13:58,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 18:14:03,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 807041.0
2025-03-09 18:14:03,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:14:04,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 18:14:09,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.938
2025-03-09 18:14:09,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:14:10,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 18:14:16,205 :: INFO :: evodenss.train.trainers :: [2051] -- [5.23s] TRAIN epoch 4 -- loss: tensor([804482.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:14:16,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.938
2025-03-09 18:14:16,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:14:17,303 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 18:14:22,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.938
2025-03-09 18:14:22,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:14:23,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 18:14:29,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.938
2025-03-09 18:14:29,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:14:30,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 18:14:35,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806838.062
2025-03-09 18:14:35,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:14:36,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 18:14:41,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 809435.75
2025-03-09 18:14:41,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:14:42,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 18:14:48,093 :: INFO :: evodenss.train.trainers :: [2051] -- [5.25s] TRAIN epoch 9 -- loss: tensor([805175.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:14:48,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805175.312
2025-03-09 18:14:48,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:14:49,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 18:14:54,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 809592.188
2025-03-09 18:14:54,642 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:14:55,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 18:15:01,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 808550.25
2025-03-09 18:15:01,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:15:02,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 18:15:07,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804800.688
2025-03-09 18:15:07,413 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:15:08,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 18:15:13,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805904.312
2025-03-09 18:15:13,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:15:14,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 18:15:20,336 :: INFO :: evodenss.train.trainers :: [2051] -- [5.43s] TRAIN epoch 14 -- loss: tensor([806118.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:15:20,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 806118.812
2025-03-09 18:15:20,336 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:15:21,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 18:15:26,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.938
2025-03-09 18:15:26,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:15:27,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 18:15:33,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804789.188
2025-03-09 18:15:33,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:15:34,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 18:15:39,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804612.312
2025-03-09 18:15:39,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:15:40,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 18:15:45,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804643.5
2025-03-09 18:15:45,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:15:46,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 18:15:52,246 :: INFO :: evodenss.train.trainers :: [2051] -- [5.38s] TRAIN epoch 19 -- loss: tensor([805264.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:15:52,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805264.125
2025-03-09 18:15:52,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:15:53,387 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 18:15:58,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804523.688
2025-03-09 18:15:58,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:15:59,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 18:16:05,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804700.062
2025-03-09 18:16:05,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:16:06,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 18:16:11,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804570.562
2025-03-09 18:16:11,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:16:12,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 18:16:17,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805835.438
2025-03-09 18:16:17,769 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:16:18,897 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 18:16:24,315 :: INFO :: evodenss.train.trainers :: [2051] -- [5.42s] TRAIN epoch 24 -- loss: tensor([804643.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:16:24,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804643.875
2025-03-09 18:16:24,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:16:25,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 18:16:30,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805028.875
2025-03-09 18:16:30,743 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:16:31,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 18:16:37,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804835.25
2025-03-09 18:16:37,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:16:38,283 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 18:16:43,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 805096.438
2025-03-09 18:16:43,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:16:44,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 18:16:50,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 807826.188
2025-03-09 18:16:50,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:16:51,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 18:16:56,494 :: INFO :: evodenss.train.trainers :: [2051] -- [5.32s] TRAIN epoch 29 -- loss: tensor([631576.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:16:56,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 631576.625
2025-03-09 18:16:56,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:16:57,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 18:17:02,870 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 150822.031
2025-03-09 18:17:02,871 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:17:03,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 18:17:09,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 132732.875
2025-03-09 18:17:09,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:17:10,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 18:17:15,623 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 127340.016
2025-03-09 18:17:15,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:17:16,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 18:17:22,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 122807.961
2025-03-09 18:17:22,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:17:23,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 18:17:28,451 :: INFO :: evodenss.train.trainers :: [2051] -- [5.23s] TRAIN epoch 34 -- loss: tensor([120346.7969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:17:28,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 120346.797
2025-03-09 18:17:28,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:17:29,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 18:17:34,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 119088.766
2025-03-09 18:17:34,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:17:35,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 18:17:41,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 117133.484
2025-03-09 18:17:41,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:17:42,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 18:17:47,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 116724.078
2025-03-09 18:17:47,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:17:48,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 18:17:54,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114226.812
2025-03-09 18:17:54,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:17:55,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 18:18:00,450 :: INFO :: evodenss.train.trainers :: [2051] -- [5.25s] TRAIN epoch 39 -- loss: tensor([114242.6953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:18:00,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114242.695
2025-03-09 18:18:00,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:18:01,554 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 18:18:06,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112924.086
2025-03-09 18:18:06,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:18:07,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 18:18:13,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 111550.523
2025-03-09 18:18:13,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:18:14,307 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 18:18:19,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 111877.938
2025-03-09 18:18:19,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:18:20,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 18:18:25,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110093.398
2025-03-09 18:18:25,942 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:18:27,067 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 18:18:32,278 :: INFO :: evodenss.train.trainers :: [2051] -- [5.21s] TRAIN epoch 44 -- loss: tensor([108561.4141], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:18:32,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108561.414
2025-03-09 18:18:32,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:18:33,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 18:18:38,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110270.695
2025-03-09 18:18:38,622 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:18:39,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 18:18:44,978 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108232.773
2025-03-09 18:18:44,978 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:18:46,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 18:18:51,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108216.531
2025-03-09 18:18:51,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:18:52,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 18:18:57,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106528.742
2025-03-09 18:18:57,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:18:58,897 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 18:19:04,143 :: INFO :: evodenss.train.trainers :: [2051] -- [5.24s] TRAIN epoch 49 -- loss: tensor([106749.0703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:19:04,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106749.07
2025-03-09 18:19:04,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:19:05,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 18:19:10,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107807.352
2025-03-09 18:19:10,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:19:11,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 18:19:16,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107228.547
2025-03-09 18:19:16,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:19:17,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 18:19:23,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105855.656
2025-03-09 18:19:23,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:19:24,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 18:19:29,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105438.289
2025-03-09 18:19:29,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:19:30,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 18:19:36,094 :: INFO :: evodenss.train.trainers :: [2051] -- [5.26s] TRAIN epoch 54 -- loss: tensor([105732.3984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:19:36,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105732.398
2025-03-09 18:19:36,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:19:37,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 18:19:42,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104772.625
2025-03-09 18:19:42,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:19:43,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 18:19:48,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104050.438
2025-03-09 18:19:48,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:19:50,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 18:19:55,456 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102732.016
2025-03-09 18:19:55,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:19:56,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 18:20:01,875 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103256.977
2025-03-09 18:20:01,876 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:20:03,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 18:20:08,314 :: INFO :: evodenss.train.trainers :: [2051] -- [5.3s] TRAIN epoch 59 -- loss: tensor([101895.4453], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:20:08,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101895.445
2025-03-09 18:20:08,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:20:09,436 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 18:20:14,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100832.984
2025-03-09 18:20:14,674 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:20:15,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 18:20:21,234 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102398.805
2025-03-09 18:20:21,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:20:22,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 18:20:27,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102154.75
2025-03-09 18:20:27,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:20:28,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 18:20:34,047 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101491.719
2025-03-09 18:20:34,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:20:35,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 18:20:40,467 :: INFO :: evodenss.train.trainers :: [2051] -- [5.32s] TRAIN epoch 64 -- loss: tensor([101167.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:20:40,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101167.477
2025-03-09 18:20:40,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:20:41,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 18:20:46,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100893.953
2025-03-09 18:20:46,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:20:47,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 18:20:53,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101992.031
2025-03-09 18:20:53,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:20:54,491 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 18:20:59,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101950.711
2025-03-09 18:20:59,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:21:00,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 18:21:06,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100870.094
2025-03-09 18:21:06,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:21:07,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 18:21:12,542 :: INFO :: evodenss.train.trainers :: [2051] -- [5.3s] TRAIN epoch 69 -- loss: tensor([101444.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:21:12,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 101444.812
2025-03-09 18:21:12,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:21:13,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 18:21:18,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99961.352
2025-03-09 18:21:18,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:21:20,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 18:21:25,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99659.914
2025-03-09 18:21:25,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:21:26,649 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 18:21:31,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99307.477
2025-03-09 18:21:31,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:21:33,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 18:21:38,326 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99867.555
2025-03-09 18:21:38,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:21:39,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 18:21:44,677 :: INFO :: evodenss.train.trainers :: [2051] -- [5.23s] TRAIN epoch 74 -- loss: tensor([98626.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:21:44,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98626.562
2025-03-09 18:21:44,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:21:45,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 18:21:51,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100129.93
2025-03-09 18:21:51,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:21:52,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 18:21:57,528 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98831.523
2025-03-09 18:21:57,528 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:21:58,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 18:22:03,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98798.453
2025-03-09 18:22:03,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:22:05,091 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 18:22:10,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99382.336
2025-03-09 18:22:10,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:22:11,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 18:22:16,880 :: INFO :: evodenss.train.trainers :: [2051] -- [5.35s] TRAIN epoch 79 -- loss: tensor([97990.7109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:22:16,880 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97990.711
2025-03-09 18:22:16,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:22:18,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 18:22:23,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 99070.922
2025-03-09 18:22:23,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:22:24,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 18:22:29,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98262.93
2025-03-09 18:22:29,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:22:31,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 18:22:36,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98424.242
2025-03-09 18:22:36,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:22:37,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 18:22:42,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97761.797
2025-03-09 18:22:42,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:22:43,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 18:22:49,504 :: INFO :: evodenss.train.trainers :: [2051] -- [5.51s] TRAIN epoch 84 -- loss: tensor([97443.0547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:22:49,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97443.055
2025-03-09 18:22:49,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:22:50,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 18:22:56,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96566.93
2025-03-09 18:22:56,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:22:57,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 18:23:02,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97256.773
2025-03-09 18:23:02,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:23:03,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 18:23:08,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96965.195
2025-03-09 18:23:08,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:23:10,079 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 18:23:15,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97575.711
2025-03-09 18:23:15,425 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:23:16,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 18:23:21,992 :: INFO :: evodenss.train.trainers :: [2051] -- [5.41s] TRAIN epoch 89 -- loss: tensor([96473.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:23:21,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96473.477
2025-03-09 18:23:21,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:23:23,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 18:23:28,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96545.625
2025-03-09 18:23:28,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:23:29,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 18:23:34,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97097.977
2025-03-09 18:23:34,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:23:36,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 18:23:41,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96417.734
2025-03-09 18:23:41,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:23:42,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 18:23:47,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95704.344
2025-03-09 18:23:47,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:23:48,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 18:23:54,377 :: INFO :: evodenss.train.trainers :: [2051] -- [5.41s] TRAIN epoch 94 -- loss: tensor([97221.7656], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:23:54,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97221.766
2025-03-09 18:23:54,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:23:55,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 18:24:00,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96650.32
2025-03-09 18:24:00,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:24:02,032 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 18:24:07,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95243.672
2025-03-09 18:24:07,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:24:08,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 18:24:13,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96144.758
2025-03-09 18:24:13,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:24:15,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 18:24:20,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95333.758
2025-03-09 18:24:20,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:24:22,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 18:24:27,460 :: INFO :: evodenss.train.trainers :: [2051] -- [5.37s] TRAIN epoch 99 -- loss: tensor([94690.2031], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:24:27,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94690.203
2025-03-09 18:24:27,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:24:29,923 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 4977.59814
2025-03-09 18:24:29,928 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 18:24:29,928 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 
layer10: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 
layer13: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 
layer15: :fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100
2025-03-09 18:24:29,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 18:24:29,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 18:24:33,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 341326.344
2025-03-09 18:24:33,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:24:34,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 18:24:38,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 148337.984
2025-03-09 18:24:38,292 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:24:39,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 18:24:42,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 126951.5
2025-03-09 18:24:42,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:24:44,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 18:24:47,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 113103.305
2025-03-09 18:24:47,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:24:48,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 18:24:52,298 :: INFO :: evodenss.train.trainers :: [2051] -- [3.62s] TRAIN epoch 4 -- loss: tensor([106622.4531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:24:52,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106622.453
2025-03-09 18:24:52,298 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:24:53,465 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 18:24:56,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102045.406
2025-03-09 18:24:56,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:24:58,086 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 18:25:01,570 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100141.305
2025-03-09 18:25:01,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:02,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 18:25:06,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96741.891
2025-03-09 18:25:06,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:07,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 18:25:10,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93801.648
2025-03-09 18:25:10,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:11,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 18:25:15,416 :: INFO :: evodenss.train.trainers :: [2051] -- [3.42s] TRAIN epoch 9 -- loss: tensor([91934.6641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:25:15,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91934.664
2025-03-09 18:25:15,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:16,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 18:25:20,202 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90185.594
2025-03-09 18:25:20,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:21,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 18:25:24,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88575.602
2025-03-09 18:25:24,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:25,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 18:25:29,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87367.961
2025-03-09 18:25:29,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:30,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 18:25:33,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86839.367
2025-03-09 18:25:33,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:34,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 18:25:38,068 :: INFO :: evodenss.train.trainers :: [2051] -- [3.38s] TRAIN epoch 14 -- loss: tensor([85739.1953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:25:38,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85739.195
2025-03-09 18:25:38,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:39,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 18:25:42,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85898.016
2025-03-09 18:25:42,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:43,663 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 18:25:47,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83793.273
2025-03-09 18:25:47,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:48,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 18:25:51,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83428.641
2025-03-09 18:25:51,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:52,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 18:25:56,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82317.25
2025-03-09 18:25:56,145 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:25:57,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 18:26:00,639 :: INFO :: evodenss.train.trainers :: [2051] -- [3.38s] TRAIN epoch 19 -- loss: tensor([81209.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:26:00,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81209.375
2025-03-09 18:26:00,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:01,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 18:26:05,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81427.836
2025-03-09 18:26:05,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:06,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 18:26:09,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81255.312
2025-03-09 18:26:09,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:10,741 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 18:26:14,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80575.516
2025-03-09 18:26:14,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:15,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 18:26:18,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79825.188
2025-03-09 18:26:18,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:19,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 18:26:23,304 :: INFO :: evodenss.train.trainers :: [2051] -- [3.37s] TRAIN epoch 24 -- loss: tensor([78600.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:26:23,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78600.875
2025-03-09 18:26:23,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:24,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 18:26:27,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78231.859
2025-03-09 18:26:27,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:28,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 18:26:32,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78015.789
2025-03-09 18:26:32,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:33,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 18:26:36,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76878.328
2025-03-09 18:26:36,777 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:37,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 18:26:41,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76172.352
2025-03-09 18:26:41,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:42,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 18:26:45,790 :: INFO :: evodenss.train.trainers :: [2051] -- [3.41s] TRAIN epoch 29 -- loss: tensor([76133.1406], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:26:45,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76133.141
2025-03-09 18:26:45,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:46,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 18:26:50,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76050.914
2025-03-09 18:26:50,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:51,597 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 18:26:55,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75210.25
2025-03-09 18:26:55,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:26:56,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 18:26:59,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75153.273
2025-03-09 18:26:59,504 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:00,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 18:27:04,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75275.867
2025-03-09 18:27:04,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:05,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 18:27:08,515 :: INFO :: evodenss.train.trainers :: [2051] -- [3.38s] TRAIN epoch 34 -- loss: tensor([75191.1562], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:27:08,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75191.156
2025-03-09 18:27:08,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:09,638 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 18:27:13,032 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74450.719
2025-03-09 18:27:13,032 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:14,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 18:27:17,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73948.289
2025-03-09 18:27:17,559 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:18,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 18:27:22,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73619.062
2025-03-09 18:27:22,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:23,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 18:27:26,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73752.969
2025-03-09 18:27:26,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:27,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 18:27:31,347 :: INFO :: evodenss.train.trainers :: [2051] -- [3.38s] TRAIN epoch 39 -- loss: tensor([73043.9844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:27:31,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73043.984
2025-03-09 18:27:31,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:32,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 18:27:35,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72750.375
2025-03-09 18:27:35,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:36,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 18:27:40,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71917.711
2025-03-09 18:27:40,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:41,541 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 18:27:44,910 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72712.219
2025-03-09 18:27:44,911 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:46,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 18:27:49,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71487.125
2025-03-09 18:27:49,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:50,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 18:27:54,157 :: INFO :: evodenss.train.trainers :: [2051] -- [3.38s] TRAIN epoch 44 -- loss: tensor([71800.6172], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:27:54,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71800.617
2025-03-09 18:27:54,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:55,297 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 18:27:58,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71105.812
2025-03-09 18:27:58,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:27:59,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 18:28:03,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70914.289
2025-03-09 18:28:03,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:04,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 18:28:07,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70497.844
2025-03-09 18:28:07,776 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:08,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 18:28:12,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70293.398
2025-03-09 18:28:12,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:13,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 18:28:16,888 :: INFO :: evodenss.train.trainers :: [2051] -- [3.44s] TRAIN epoch 49 -- loss: tensor([70161.1484], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:28:16,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70161.148
2025-03-09 18:28:16,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:18,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 18:28:21,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70260.914
2025-03-09 18:28:21,583 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:22,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 18:28:26,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69312.227
2025-03-09 18:28:26,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:27,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 18:28:30,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68777.008
2025-03-09 18:28:30,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:31,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 18:28:35,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69135.289
2025-03-09 18:28:35,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:36,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 18:28:39,784 :: INFO :: evodenss.train.trainers :: [2051] -- [3.44s] TRAIN epoch 54 -- loss: tensor([68781.9375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:28:39,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68781.938
2025-03-09 18:28:39,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:40,928 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 18:28:44,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68722.625
2025-03-09 18:28:44,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:45,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 18:28:48,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68255.586
2025-03-09 18:28:48,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:50,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 18:28:53,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69181.234
2025-03-09 18:28:53,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:54,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 18:28:58,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68023.875
2025-03-09 18:28:58,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:28:59,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 18:29:02,756 :: INFO :: evodenss.train.trainers :: [2051] -- [3.4s] TRAIN epoch 59 -- loss: tensor([67280.4531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:29:02,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67280.453
2025-03-09 18:29:02,756 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:03,860 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 18:29:07,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68352.508
2025-03-09 18:29:07,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:08,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 18:29:11,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67660.711
2025-03-09 18:29:11,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:12,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 18:29:16,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67838.805
2025-03-09 18:29:16,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:17,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 18:29:20,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67490.352
2025-03-09 18:29:20,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:22,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 18:29:25,407 :: INFO :: evodenss.train.trainers :: [2051] -- [3.39s] TRAIN epoch 64 -- loss: tensor([66990.3203], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:29:25,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66990.32
2025-03-09 18:29:25,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:26,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 18:29:29,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66891.219
2025-03-09 18:29:29,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:30,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 18:29:34,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67064.031
2025-03-09 18:29:34,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:35,441 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 18:29:38,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66729.328
2025-03-09 18:29:38,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:39,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 18:29:43,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66277.93
2025-03-09 18:29:43,269 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:44,379 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 18:29:47,785 :: INFO :: evodenss.train.trainers :: [2051] -- [3.4s] TRAIN epoch 69 -- loss: tensor([66681.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:29:47,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66681.953
2025-03-09 18:29:47,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:48,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 18:29:52,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66524.703
2025-03-09 18:29:52,398 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:53,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 18:29:56,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64959.023
2025-03-09 18:29:56,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:29:57,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 18:30:01,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66678.953
2025-03-09 18:30:01,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:02,466 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 18:30:05,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65149.172
2025-03-09 18:30:05,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:06,967 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 18:30:10,347 :: INFO :: evodenss.train.trainers :: [2051] -- [3.38s] TRAIN epoch 74 -- loss: tensor([65942.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:30:10,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65942.25
2025-03-09 18:30:10,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:11,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 18:30:14,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65590.891
2025-03-09 18:30:14,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:15,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 18:30:19,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65319.199
2025-03-09 18:30:19,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:20,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 18:30:23,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64988.105
2025-03-09 18:30:23,943 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:25,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 18:30:28,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65131.879
2025-03-09 18:30:28,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:29,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 18:30:32,843 :: INFO :: evodenss.train.trainers :: [2051] -- [3.36s] TRAIN epoch 79 -- loss: tensor([64599.3945], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:30:32,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64599.395
2025-03-09 18:30:32,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:33,925 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 18:30:37,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64803.859
2025-03-09 18:30:37,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:38,387 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 18:30:41,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64635.25
2025-03-09 18:30:41,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:42,845 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 18:30:46,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63405.469
2025-03-09 18:30:46,233 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:47,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 18:30:50,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64398.281
2025-03-09 18:30:50,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:52,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 18:30:55,380 :: INFO :: evodenss.train.trainers :: [2051] -- [3.37s] TRAIN epoch 84 -- loss: tensor([64244.8398], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:30:55,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64244.84
2025-03-09 18:30:55,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:30:56,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 18:30:59,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64025.871
2025-03-09 18:30:59,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:00,994 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 18:31:04,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64504.094
2025-03-09 18:31:04,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:05,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 18:31:08,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64366.07
2025-03-09 18:31:08,824 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:09,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 18:31:13,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64606.551
2025-03-09 18:31:13,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:14,385 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 18:31:17,780 :: INFO :: evodenss.train.trainers :: [2051] -- [3.39s] TRAIN epoch 89 -- loss: tensor([64632.3477], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:31:17,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64632.348
2025-03-09 18:31:17,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:18,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 18:31:22,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63669.25
2025-03-09 18:31:22,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:23,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 18:31:26,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63597.895
2025-03-09 18:31:26,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:27,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 18:31:31,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63852.211
2025-03-09 18:31:31,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:32,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 18:31:35,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63303.918
2025-03-09 18:31:35,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:36,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 18:31:40,281 :: INFO :: evodenss.train.trainers :: [2051] -- [3.37s] TRAIN epoch 94 -- loss: tensor([63131.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:31:40,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63131.812
2025-03-09 18:31:40,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:41,396 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 18:31:44,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63383.531
2025-03-09 18:31:44,802 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:45,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 18:31:49,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63323.156
2025-03-09 18:31:49,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:50,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 18:31:53,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 62909.234
2025-03-09 18:31:53,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:55,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 18:31:58,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63284.891
2025-03-09 18:31:58,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:31:59,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 18:32:02,837 :: INFO :: evodenss.train.trainers :: [2051] -- [3.37s] TRAIN epoch 99 -- loss: tensor([62601.3945], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:32:02,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 62601.395
2025-03-09 18:32:02,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:32:05,065 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 3705.05347
2025-03-09 18:32:05,070 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 18:32:05,071 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:93 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 
layer6: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 
layer7: :deconv1d out_channels:61 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 
layer10: :conv1d out_channels:105 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 
layer11: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :deconv1d out_channels:117 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.08884997001126517 alpha:0.8206719784965377 weight_decay:5.42399008810406e-05 batch_size:6 epochs:100
2025-03-09 18:32:05,081 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 18:32:05,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 18:32:11,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3846353.75
2025-03-09 18:32:11,628 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:32:12,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 18:32:18,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 4142681.0
2025-03-09 18:32:18,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:32:20,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 18:32:25,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 3156773.0
2025-03-09 18:32:25,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:32:26,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 18:32:32,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2517244.0
2025-03-09 18:32:32,836 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:32:34,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 18:32:39,820 :: INFO :: evodenss.train.trainers :: [2051] -- [5.82s] TRAIN epoch 4 -- loss: tensor([2007812.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:32:39,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2007812.0
2025-03-09 18:32:39,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:32:40,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 18:32:46,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2267079.0
2025-03-09 18:32:46,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:32:47,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 18:32:54,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1972627.0
2025-03-09 18:32:54,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:32:55,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 18:33:00,968 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2204004.5
2025-03-09 18:33:00,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:33:02,134 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 18:33:07,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1961552.375
2025-03-09 18:33:07,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:33:09,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 18:33:15,001 :: INFO :: evodenss.train.trainers :: [2051] -- [5.85s] TRAIN epoch 9 -- loss: tensor([2244110.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:33:15,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2244110.75
2025-03-09 18:33:15,001 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:33:16,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 18:33:22,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2049301.5
2025-03-09 18:33:22,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:33:23,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 18:33:29,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1798949.75
2025-03-09 18:33:29,139 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:33:30,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 18:33:36,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1874213.125
2025-03-09 18:33:36,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:33:37,276 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 18:33:43,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1957463.875
2025-03-09 18:33:43,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:33:44,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 18:33:50,164 :: INFO :: evodenss.train.trainers :: [2051] -- [5.93s] TRAIN epoch 14 -- loss: tensor([1853312.8750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:33:50,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1853312.875
2025-03-09 18:33:50,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:33:51,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 18:33:57,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1801040.625
2025-03-09 18:33:57,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:33:58,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 18:34:04,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1733944.25
2025-03-09 18:34:04,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:34:05,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 18:34:11,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1623877.625
2025-03-09 18:34:11,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:34:12,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 18:34:18,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1824933.625
2025-03-09 18:34:18,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:34:19,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 18:34:25,138 :: INFO :: evodenss.train.trainers :: [2051] -- [5.91s] TRAIN epoch 19 -- loss: tensor([1629197.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:34:25,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1629197.75
2025-03-09 18:34:25,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:34:26,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 18:34:32,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2315739.0
2025-03-09 18:34:32,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:34:33,436 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 18:34:39,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2308034.0
2025-03-09 18:34:39,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:34:40,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 18:34:46,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2021630.0
2025-03-09 18:34:46,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:34:47,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 18:34:53,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1840800.25
2025-03-09 18:34:53,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:34:54,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 18:35:00,543 :: INFO :: evodenss.train.trainers :: [2051] -- [5.84s] TRAIN epoch 24 -- loss: tensor([1874978.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:35:00,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1874978.5
2025-03-09 18:35:00,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:35:01,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 18:35:07,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1709061.125
2025-03-09 18:35:07,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:35:08,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 18:35:14,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1921045.0
2025-03-09 18:35:14,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:35:15,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 18:35:21,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1748712.875
2025-03-09 18:35:21,782 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:35:22,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 18:35:28,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1969277.625
2025-03-09 18:35:28,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:35:30,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 18:35:35,891 :: INFO :: evodenss.train.trainers :: [2051] -- [5.85s] TRAIN epoch 29 -- loss: tensor([2185715.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:35:35,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2185715.0
2025-03-09 18:35:35,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:35:37,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 18:35:42,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1830908.375
2025-03-09 18:35:42,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:35:44,067 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 18:35:50,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1704937.25
2025-03-09 18:35:50,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:35:51,212 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 18:35:57,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1652037.5
2025-03-09 18:35:57,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:35:58,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 18:36:04,042 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1707309.0
2025-03-09 18:36:04,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:36:05,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 18:36:11,023 :: INFO :: evodenss.train.trainers :: [2051] -- [5.84s] TRAIN epoch 34 -- loss: tensor([1705432.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:36:11,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1705432.25
2025-03-09 18:36:11,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:36:12,172 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 18:36:18,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1603581.5
2025-03-09 18:36:18,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:36:19,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 18:36:25,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1653193.25
2025-03-09 18:36:25,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:36:26,323 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 18:36:32,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1624407.875
2025-03-09 18:36:32,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:36:33,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 18:36:39,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1563935.25
2025-03-09 18:36:39,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:36:40,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 18:36:46,189 :: INFO :: evodenss.train.trainers :: [2051] -- [5.86s] TRAIN epoch 39 -- loss: tensor([1469282.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:36:46,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1469282.75
2025-03-09 18:36:46,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:36:47,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 18:36:53,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1649000.5
2025-03-09 18:36:53,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:36:54,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 18:37:00,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1690724.5
2025-03-09 18:37:00,372 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:37:01,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 18:37:07,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1617316.625
2025-03-09 18:37:07,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:37:08,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 18:37:14,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1555049.125
2025-03-09 18:37:14,366 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:37:15,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 18:37:21,646 :: INFO :: evodenss.train.trainers :: [2051] -- [6.09s] TRAIN epoch 44 -- loss: tensor([1701298.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:37:21,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1701298.5
2025-03-09 18:37:21,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:37:22,820 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 18:37:28,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1638143.75
2025-03-09 18:37:28,698 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:37:29,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 18:37:35,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1683435.125
2025-03-09 18:37:35,799 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:37:36,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 18:37:42,832 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1519779.5
2025-03-09 18:37:42,833 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:37:44,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 18:37:50,022 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1663048.5
2025-03-09 18:37:50,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:37:51,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 18:37:57,075 :: INFO :: evodenss.train.trainers :: [2051] -- [5.86s] TRAIN epoch 49 -- loss: tensor([1740476.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:37:57,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1740476.625
2025-03-09 18:37:57,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:37:58,244 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 18:38:04,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1625149.5
2025-03-09 18:38:04,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:38:05,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 18:38:11,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1850218.75
2025-03-09 18:38:11,204 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:38:12,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 18:38:18,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1946992.25
2025-03-09 18:38:18,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:38:19,635 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 18:38:25,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1800118.875
2025-03-09 18:38:25,574 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:38:26,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 18:38:32,683 :: INFO :: evodenss.train.trainers :: [2051] -- [5.9s] TRAIN epoch 54 -- loss: tensor([1715323.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:38:32,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1715323.0
2025-03-09 18:38:32,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:38:33,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 18:38:39,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1722377.0
2025-03-09 18:38:39,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:38:40,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 18:38:46,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1740388.0
2025-03-09 18:38:46,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:38:47,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 18:38:54,060 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1885574.75
2025-03-09 18:38:54,060 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:38:55,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 18:39:01,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1758971.0
2025-03-09 18:39:01,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:39:02,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 18:39:08,257 :: INFO :: evodenss.train.trainers :: [2051] -- [5.91s] TRAIN epoch 59 -- loss: tensor([1668117.], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:39:08,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1668117.0
2025-03-09 18:39:08,257 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:39:09,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 18:39:15,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1738656.375
2025-03-09 18:39:15,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:39:16,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 18:39:22,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1656894.25
2025-03-09 18:39:22,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:39:23,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 18:39:29,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1682727.375
2025-03-09 18:39:29,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:39:30,818 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 18:39:36,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1660648.875
2025-03-09 18:39:36,664 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:39:37,887 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 18:39:43,791 :: INFO :: evodenss.train.trainers :: [2051] -- [5.9s] TRAIN epoch 64 -- loss: tensor([1651226.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:39:43,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1651226.5
2025-03-09 18:39:43,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:39:45,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 18:39:50,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1692617.75
2025-03-09 18:39:50,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:39:52,227 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 18:39:58,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1847207.125
2025-03-09 18:39:58,049 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:39:59,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 18:40:05,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1928643.25
2025-03-09 18:40:05,167 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:40:06,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 18:40:12,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1691799.0
2025-03-09 18:40:12,249 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:40:13,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 18:40:19,482 :: INFO :: evodenss.train.trainers :: [2051] -- [6.02s] TRAIN epoch 69 -- loss: tensor([1910672.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:40:19,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1910672.125
2025-03-09 18:40:19,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:40:20,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 18:40:26,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1743832.75
2025-03-09 18:40:26,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:40:27,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 18:40:33,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2000874.5
2025-03-09 18:40:33,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:40:34,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 18:40:40,878 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1897484.625
2025-03-09 18:40:40,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:40:42,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 18:40:47,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1673026.0
2025-03-09 18:40:47,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:40:49,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 18:40:55,272 :: INFO :: evodenss.train.trainers :: [2051] -- [5.94s] TRAIN epoch 74 -- loss: tensor([1641204.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:40:55,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1641204.75
2025-03-09 18:40:55,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:40:56,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 18:41:02,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1888092.375
2025-03-09 18:41:02,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:41:03,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 18:41:09,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1662596.625
2025-03-09 18:41:09,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:41:10,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 18:41:16,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1807809.5
2025-03-09 18:41:16,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:41:17,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 18:41:23,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2108096.25
2025-03-09 18:41:23,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:41:24,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 18:41:30,560 :: INFO :: evodenss.train.trainers :: [2051] -- [5.89s] TRAIN epoch 79 -- loss: tensor([1538707.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:41:30,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1538707.125
2025-03-09 18:41:30,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:41:31,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 18:41:37,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 2337970.5
2025-03-09 18:41:37,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:41:38,784 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 18:41:44,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1732639.0
2025-03-09 18:41:44,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:41:45,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 18:41:51,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1828098.875
2025-03-09 18:41:51,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:41:53,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 18:41:58,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1678164.875
2025-03-09 18:41:58,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:42:00,026 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 18:42:05,931 :: INFO :: evodenss.train.trainers :: [2051] -- [5.9s] TRAIN epoch 84 -- loss: tensor([1530242.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:42:05,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1530242.625
2025-03-09 18:42:05,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:42:07,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 18:42:12,908 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1993005.5
2025-03-09 18:42:12,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:42:14,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 18:42:20,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1543779.375
2025-03-09 18:42:20,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:42:21,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 18:42:27,163 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1638383.375
2025-03-09 18:42:27,164 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:42:28,327 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 18:42:34,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1518421.25
2025-03-09 18:42:34,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:42:35,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 18:42:41,242 :: INFO :: evodenss.train.trainers :: [2051] -- [5.87s] TRAIN epoch 89 -- loss: tensor([1829958.7500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:42:41,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1829958.75
2025-03-09 18:42:41,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:42:42,408 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 18:42:48,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1500633.0
2025-03-09 18:42:48,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:42:49,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 18:42:55,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1590818.125
2025-03-09 18:42:55,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:42:56,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 18:43:02,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1517868.625
2025-03-09 18:43:02,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:43:03,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 18:43:09,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1696636.25
2025-03-09 18:43:09,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:43:10,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 18:43:16,680 :: INFO :: evodenss.train.trainers :: [2051] -- [5.84s] TRAIN epoch 94 -- loss: tensor([1548181.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:43:16,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1548181.25
2025-03-09 18:43:16,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:43:17,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 18:43:23,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1520016.625
2025-03-09 18:43:23,902 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:43:25,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 18:43:30,926 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1638586.375
2025-03-09 18:43:30,926 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:43:32,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 18:43:39,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1615924.625
2025-03-09 18:43:39,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:43:40,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 18:43:46,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1586271.5
2025-03-09 18:43:46,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:43:47,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 18:43:53,641 :: INFO :: evodenss.train.trainers :: [2051] -- [6.03s] TRAIN epoch 99 -- loss: tensor([1637943.1250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:43:53,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1637943.125
2025-03-09 18:43:53,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:43:56,224 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 251896.48438
2025-03-09 18:43:56,224 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 18:43:56,225 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 2, id: 2
2025-03-09 18:43:56,225 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 1000, 1000]
2025-03-09 18:43:56,225 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 18:43:56,228 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [3845.52393, 4977.59814, 3705.05347, 251896.48438]
2025-03-09 18:43:56,611 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([18986.2520], device='cuda:0')
2025-03-09 18:43:56,611 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 14: 3705.05347
2025-03-09 18:43:56,611 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3575.71436



2025-03-09 18:43:56,728 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 15
2025-03-09 18:43:56,728 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 18:43:56,740 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a layer removed from Module 0: features; Position: 13
2025-03-09 18:43:56,741 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have an extra layer at Module 0: features; Position: 7. Reused?: False
2025-03-09 18:43:56,742 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 18:43:56,743 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 18:43:56,743 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 18:43:56,744 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 18:43:56,745 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 18:43:56,746 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 18:43:56,746 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 18:43:56,747 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 14
2025-03-09 18:43:56,747 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 18:43:56,751 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 total training time extended to 2000
2025-03-09 18:43:56,754 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 total training time extended to 2000
2025-03-09 18:43:56,756 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 18:43:56,761 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 18:43:56,762 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 
layer10: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 
layer13: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 
layer15: :fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100
2025-03-09 18:43:56,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 18:43:56,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 18:44:00,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 379207.938
2025-03-09 18:44:00,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:01,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 18:44:04,907 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 178144.703
2025-03-09 18:44:04,907 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:06,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 18:44:09,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 139627.375
2025-03-09 18:44:09,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:10,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 18:44:14,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 124606.531
2025-03-09 18:44:14,113 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:15,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 18:44:18,731 :: INFO :: evodenss.train.trainers :: [2051] -- [3.46s] TRAIN epoch 4 -- loss: tensor([117479.7656], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:44:18,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 117479.766
2025-03-09 18:44:18,732 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:20,035 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 18:44:23,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 111861.305
2025-03-09 18:44:23,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:24,675 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 18:44:28,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 108349.586
2025-03-09 18:44:28,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:29,301 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 18:44:32,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106560.195
2025-03-09 18:44:32,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:33,935 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 18:44:37,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102927.383
2025-03-09 18:44:37,365 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:38,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 18:44:42,081 :: INFO :: evodenss.train.trainers :: [2051] -- [3.56s] TRAIN epoch 9 -- loss: tensor([100484.1328], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:44:42,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100484.133
2025-03-09 18:44:42,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:43,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 18:44:46,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98832.258
2025-03-09 18:44:46,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:47,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 18:44:51,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97550.57
2025-03-09 18:44:51,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:52,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 18:44:56,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95712.672
2025-03-09 18:44:56,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:44:57,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 18:45:00,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95540.023
2025-03-09 18:45:00,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:01,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 18:45:05,361 :: INFO :: evodenss.train.trainers :: [2051] -- [3.48s] TRAIN epoch 14 -- loss: tensor([93280.6094], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:45:05,361 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93280.609
2025-03-09 18:45:05,361 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:06,509 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 18:45:10,002 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92545.828
2025-03-09 18:45:10,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:11,166 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 18:45:14,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92321.016
2025-03-09 18:45:14,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:15,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 18:45:19,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91809.234
2025-03-09 18:45:19,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:20,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 18:45:24,073 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90629.656
2025-03-09 18:45:24,073 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:25,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 18:45:28,718 :: INFO :: evodenss.train.trainers :: [2051] -- [3.48s] TRAIN epoch 19 -- loss: tensor([90205.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:45:28,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90205.188
2025-03-09 18:45:28,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:29,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 18:45:33,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89477.578
2025-03-09 18:45:33,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:34,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 18:45:38,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88437.023
2025-03-09 18:45:38,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:39,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 18:45:42,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87552.211
2025-03-09 18:45:42,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:43,906 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 18:45:47,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87571.742
2025-03-09 18:45:47,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:48,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 18:45:52,300 :: INFO :: evodenss.train.trainers :: [2051] -- [3.69s] TRAIN epoch 24 -- loss: tensor([86485.2266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:45:52,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86485.227
2025-03-09 18:45:52,300 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:53,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 18:45:57,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86050.836
2025-03-09 18:45:57,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:45:58,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 18:46:01,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86726.414
2025-03-09 18:46:01,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:02,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 18:46:06,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85337.758
2025-03-09 18:46:06,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:07,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 18:46:10,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85308.07
2025-03-09 18:46:10,773 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:11,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 18:46:15,331 :: INFO :: evodenss.train.trainers :: [2051] -- [3.44s] TRAIN epoch 29 -- loss: tensor([84620.1484], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:46:15,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84620.148
2025-03-09 18:46:15,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:16,460 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 18:46:20,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83785.586
2025-03-09 18:46:20,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:21,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 18:46:24,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83614.539
2025-03-09 18:46:24,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:25,804 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 18:46:29,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83668.031
2025-03-09 18:46:29,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:30,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 18:46:33,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83636.102
2025-03-09 18:46:33,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:34,947 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 18:46:38,422 :: INFO :: evodenss.train.trainers :: [2051] -- [3.47s] TRAIN epoch 34 -- loss: tensor([83128.6328], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:46:38,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83128.633
2025-03-09 18:46:38,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:39,567 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 18:46:43,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82181.109
2025-03-09 18:46:43,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:44,171 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 18:46:47,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81822.594
2025-03-09 18:46:47,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:48,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 18:46:52,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82527.172
2025-03-09 18:46:52,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:53,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 18:46:57,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81910.18
2025-03-09 18:46:57,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:46:58,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 18:47:01,576 :: INFO :: evodenss.train.trainers :: [2051] -- [3.42s] TRAIN epoch 39 -- loss: tensor([81402.1641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:47:01,576 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81402.164
2025-03-09 18:47:01,576 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:02,693 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 18:47:06,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81473.711
2025-03-09 18:47:06,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:07,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 18:47:10,759 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81499.883
2025-03-09 18:47:10,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:11,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 18:47:15,354 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79785.461
2025-03-09 18:47:15,355 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:16,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 18:47:20,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79981.469
2025-03-09 18:47:20,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:21,213 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 18:47:24,750 :: INFO :: evodenss.train.trainers :: [2051] -- [3.53s] TRAIN epoch 44 -- loss: tensor([80873.2266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:47:24,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80873.227
2025-03-09 18:47:24,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:25,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 18:47:29,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79794.102
2025-03-09 18:47:29,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:30,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 18:47:33,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78981.883
2025-03-09 18:47:33,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:35,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 18:47:38,533 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79364.148
2025-03-09 18:47:38,533 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:39,653 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 18:47:43,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78963.125
2025-03-09 18:47:43,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:44,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 18:47:47,770 :: INFO :: evodenss.train.trainers :: [2051] -- [3.51s] TRAIN epoch 49 -- loss: tensor([79116.7031], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:47:47,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79116.703
2025-03-09 18:47:47,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:48,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 18:47:52,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78527.062
2025-03-09 18:47:52,561 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:53,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 18:47:57,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78107.008
2025-03-09 18:47:57,215 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:47:58,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 18:48:01,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81544.773
2025-03-09 18:48:01,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:02,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 18:48:06,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80803.984
2025-03-09 18:48:06,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:07,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 18:48:11,075 :: INFO :: evodenss.train.trainers :: [2051] -- [3.45s] TRAIN epoch 54 -- loss: tensor([79870.3828], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:48:11,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79870.383
2025-03-09 18:48:11,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:12,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 18:48:15,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79865.836
2025-03-09 18:48:15,719 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:16,881 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 18:48:20,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79690.219
2025-03-09 18:48:20,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:21,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 18:48:25,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79511.289
2025-03-09 18:48:25,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:26,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 18:48:29,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79823.0
2025-03-09 18:48:29,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:31,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 18:48:34,522 :: INFO :: evodenss.train.trainers :: [2051] -- [3.48s] TRAIN epoch 59 -- loss: tensor([77835.8672], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:48:34,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77835.867
2025-03-09 18:48:34,522 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:35,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 18:48:39,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78251.352
2025-03-09 18:48:39,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:40,388 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 18:48:43,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77881.297
2025-03-09 18:48:43,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:45,009 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 18:48:48,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78300.008
2025-03-09 18:48:48,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:49,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 18:48:53,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77656.805
2025-03-09 18:48:53,335 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:54,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 18:48:58,039 :: INFO :: evodenss.train.trainers :: [2051] -- [3.54s] TRAIN epoch 64 -- loss: tensor([77150.7188], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:48:58,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77150.719
2025-03-09 18:48:58,039 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:48:59,205 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 18:49:02,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77011.805
2025-03-09 18:49:02,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:03,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 18:49:07,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77886.438
2025-03-09 18:49:07,346 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:08,511 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 18:49:12,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76883.414
2025-03-09 18:49:12,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:13,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 18:49:16,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76894.555
2025-03-09 18:49:16,646 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:17,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 18:49:21,467 :: INFO :: evodenss.train.trainers :: [2051] -- [3.65s] TRAIN epoch 69 -- loss: tensor([76129.2734], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:49:21,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76129.273
2025-03-09 18:49:21,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:22,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 18:49:26,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76749.031
2025-03-09 18:49:26,031 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:27,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 18:49:30,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75596.203
2025-03-09 18:49:30,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:31,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 18:49:35,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76921.914
2025-03-09 18:49:35,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:36,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 18:49:39,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76817.227
2025-03-09 18:49:39,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:40,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 18:49:44,316 :: INFO :: evodenss.train.trainers :: [2051] -- [3.47s] TRAIN epoch 74 -- loss: tensor([75782.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:49:44,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75782.188
2025-03-09 18:49:44,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:45,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 18:49:48,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75991.828
2025-03-09 18:49:48,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:50,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 18:49:53,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75359.812
2025-03-09 18:49:53,753 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:54,907 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 18:49:58,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75257.109
2025-03-09 18:49:58,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:49:59,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 18:50:03,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75389.805
2025-03-09 18:50:03,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:50:04,161 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 18:50:07,658 :: INFO :: evodenss.train.trainers :: [2051] -- [3.5s] TRAIN epoch 79 -- loss: tensor([75441.4844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:50:07,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75441.484
2025-03-09 18:50:07,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:50:08,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 18:50:12,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74418.805
2025-03-09 18:50:12,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:50:13,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 18:50:17,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74710.359
2025-03-09 18:50:17,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:50:18,160 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 18:50:21,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74207.375
2025-03-09 18:50:21,811 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:50:22,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 18:50:26,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75204.086
2025-03-09 18:50:26,510 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:50:27,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 18:50:31,153 :: INFO :: evodenss.train.trainers :: [2051] -- [3.46s] TRAIN epoch 84 -- loss: tensor([74056.8516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:50:31,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74056.852
2025-03-09 18:50:31,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:50:32,304 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 18:50:35,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73611.781
2025-03-09 18:50:35,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:50:36,957 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 18:50:40,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74820.602
2025-03-09 18:50:40,457 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:50:41,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 18:50:45,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73411.438
2025-03-09 18:50:45,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:50:46,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 18:50:49,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73959.5
2025-03-09 18:50:49,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:50:51,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 18:50:54,648 :: INFO :: evodenss.train.trainers :: [2051] -- [3.52s] TRAIN epoch 89 -- loss: tensor([73347.5234], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:50:54,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73347.523
2025-03-09 18:50:54,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:50:55,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 18:50:59,237 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74311.805
2025-03-09 18:50:59,237 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:00,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 18:51:03,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73431.156
2025-03-09 18:51:03,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:04,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 18:51:08,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74003.469
2025-03-09 18:51:08,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:09,552 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 18:51:12,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73573.961
2025-03-09 18:51:12,988 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:14,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 18:51:17,587 :: INFO :: evodenss.train.trainers :: [2051] -- [3.48s] TRAIN epoch 94 -- loss: tensor([72350.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:51:17,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72350.477
2025-03-09 18:51:17,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:18,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 18:51:22,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73157.43
2025-03-09 18:51:22,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:23,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 18:51:26,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72566.562
2025-03-09 18:51:26,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:28,109 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 18:51:31,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72652.5
2025-03-09 18:51:31,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:32,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 18:51:36,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73281.391
2025-03-09 18:51:36,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:37,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 18:51:40,616 :: INFO :: evodenss.train.trainers :: [2051] -- [3.43s] TRAIN epoch 99 -- loss: tensor([72245.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:51:40,616 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72245.688
2025-03-09 18:51:40,616 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:42,907 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 4194.12061
2025-03-09 18:51:42,913 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 18:51:42,914 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :deconv1d out_channels:115 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :deconv1d out_channels:39 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 
layer11: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 
layer14: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 
layer15: :fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100
2025-03-09 18:51:42,925 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 18:51:42,925 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 18:51:46,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 354318.531
2025-03-09 18:51:46,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:47,508 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 18:51:50,885 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 156112.141
2025-03-09 18:51:50,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:52,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 18:51:55,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 125826.469
2025-03-09 18:51:55,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:51:56,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 18:51:59,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 115493.43
2025-03-09 18:51:59,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:00,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 18:52:03,961 :: INFO :: evodenss.train.trainers :: [2051] -- [3.23s] TRAIN epoch 4 -- loss: tensor([109454.6953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:52:03,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 109454.695
2025-03-09 18:52:03,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:05,093 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 18:52:08,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 103843.758
2025-03-09 18:52:08,317 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:09,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 18:52:12,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100975.898
2025-03-09 18:52:12,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:13,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 18:52:16,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97296.531
2025-03-09 18:52:16,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:18,041 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 18:52:21,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95763.445
2025-03-09 18:52:21,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:22,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 18:52:25,801 :: INFO :: evodenss.train.trainers :: [2051] -- [3.23s] TRAIN epoch 9 -- loss: tensor([93829.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:52:25,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93829.688
2025-03-09 18:52:25,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:26,907 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 18:52:30,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93748.43
2025-03-09 18:52:30,124 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:31,245 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 18:52:34,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90986.648
2025-03-09 18:52:34,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:35,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 18:52:38,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94108.242
2025-03-09 18:52:38,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:39,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 18:52:43,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90226.453
2025-03-09 18:52:43,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:44,279 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 18:52:47,531 :: INFO :: evodenss.train.trainers :: [2051] -- [3.25s] TRAIN epoch 14 -- loss: tensor([88741.1094], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:52:47,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88741.109
2025-03-09 18:52:47,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:48,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 18:52:52,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87480.492
2025-03-09 18:52:52,114 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:53,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 18:52:56,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87244.633
2025-03-09 18:52:56,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:52:57,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 18:53:00,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85853.492
2025-03-09 18:53:00,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:01,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 18:53:05,260 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85302.305
2025-03-09 18:53:05,261 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:06,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 18:53:09,621 :: INFO :: evodenss.train.trainers :: [2051] -- [3.23s] TRAIN epoch 19 -- loss: tensor([84748.5547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:53:09,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84748.555
2025-03-09 18:53:09,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:10,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 18:53:13,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84257.281
2025-03-09 18:53:13,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:15,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 18:53:18,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82945.297
2025-03-09 18:53:18,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:19,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 18:53:23,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83200.82
2025-03-09 18:53:23,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:24,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 18:53:27,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82467.43
2025-03-09 18:53:27,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:28,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 18:53:31,765 :: INFO :: evodenss.train.trainers :: [2051] -- [3.29s] TRAIN epoch 24 -- loss: tensor([81348.7031], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:53:31,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81348.703
2025-03-09 18:53:31,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:32,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 18:53:36,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81452.086
2025-03-09 18:53:36,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:37,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 18:53:40,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80495.688
2025-03-09 18:53:40,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:41,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 18:53:44,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80278.25
2025-03-09 18:53:44,854 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:45,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 18:53:49,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78827.156
2025-03-09 18:53:49,313 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:50,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 18:53:53,780 :: INFO :: evodenss.train.trainers :: [2051] -- [3.22s] TRAIN epoch 29 -- loss: tensor([78483.7422], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:53:53,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78483.742
2025-03-09 18:53:53,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:54,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 18:53:58,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78319.82
2025-03-09 18:53:58,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:53:59,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 18:54:02,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77214.953
2025-03-09 18:54:02,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:03,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 18:54:06,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77154.133
2025-03-09 18:54:06,948 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:08,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 18:54:11,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77162.5
2025-03-09 18:54:11,286 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:12,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 18:54:15,694 :: INFO :: evodenss.train.trainers :: [2051] -- [3.25s] TRAIN epoch 34 -- loss: tensor([76923.7969], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:54:15,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76923.797
2025-03-09 18:54:15,694 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:16,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 18:54:20,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76039.117
2025-03-09 18:54:20,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:21,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 18:54:24,528 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75739.812
2025-03-09 18:54:24,528 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:25,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 18:54:28,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76031.297
2025-03-09 18:54:28,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:30,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 18:54:33,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75801.055
2025-03-09 18:54:33,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:34,465 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 18:54:37,701 :: INFO :: evodenss.train.trainers :: [2051] -- [3.23s] TRAIN epoch 39 -- loss: tensor([75896.4375], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:54:37,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75896.438
2025-03-09 18:54:37,702 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:38,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 18:54:42,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74453.336
2025-03-09 18:54:42,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:43,191 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 18:54:46,423 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75687.789
2025-03-09 18:54:46,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:47,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 18:54:50,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74200.914
2025-03-09 18:54:50,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:52,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 18:54:55,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73979.82
2025-03-09 18:54:55,337 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:54:56,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 18:54:59,685 :: INFO :: evodenss.train.trainers :: [2051] -- [3.22s] TRAIN epoch 44 -- loss: tensor([74188.9688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:54:59,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74188.969
2025-03-09 18:54:59,685 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:00,828 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 18:55:04,043 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73494.086
2025-03-09 18:55:04,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:05,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 18:55:08,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72939.633
2025-03-09 18:55:08,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:09,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 18:55:12,791 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72686.047
2025-03-09 18:55:12,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:13,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 18:55:17,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72679.25
2025-03-09 18:55:17,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:18,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 18:55:21,710 :: INFO :: evodenss.train.trainers :: [2051] -- [3.41s] TRAIN epoch 49 -- loss: tensor([73035.5000], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:55:21,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73035.5
2025-03-09 18:55:21,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:22,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 18:55:26,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72062.547
2025-03-09 18:55:26,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:27,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 18:55:30,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72173.672
2025-03-09 18:55:30,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:31,596 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 18:55:34,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72416.812
2025-03-09 18:55:34,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:35,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 18:55:39,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71805.734
2025-03-09 18:55:39,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:40,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 18:55:43,618 :: INFO :: evodenss.train.trainers :: [2051] -- [3.24s] TRAIN epoch 54 -- loss: tensor([71427.1016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:55:43,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71427.102
2025-03-09 18:55:43,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:44,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 18:55:47,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71376.898
2025-03-09 18:55:47,937 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:49,063 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 18:55:52,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71315.086
2025-03-09 18:55:52,487 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:53,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 18:55:56,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70903.883
2025-03-09 18:55:56,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:55:57,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 18:56:01,169 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71055.961
2025-03-09 18:56:01,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:02,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 18:56:05,495 :: INFO :: evodenss.train.trainers :: [2051] -- [3.21s] TRAIN epoch 59 -- loss: tensor([70345.9297], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:56:05,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70345.93
2025-03-09 18:56:05,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:06,619 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 18:56:09,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70163.016
2025-03-09 18:56:09,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:10,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 18:56:14,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70105.242
2025-03-09 18:56:14,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:15,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 18:56:18,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69827.836
2025-03-09 18:56:18,499 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:19,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 18:56:23,106 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70188.039
2025-03-09 18:56:23,106 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:24,240 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 18:56:27,435 :: INFO :: evodenss.train.trainers :: [2051] -- [3.19s] TRAIN epoch 64 -- loss: tensor([69801.6953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:56:27,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69801.695
2025-03-09 18:56:27,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:28,553 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 18:56:31,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69317.508
2025-03-09 18:56:31,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:32,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 18:56:36,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69950.164
2025-03-09 18:56:36,306 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:37,412 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 18:56:40,627 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68857.961
2025-03-09 18:56:40,628 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:41,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 18:56:44,998 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69366.758
2025-03-09 18:56:44,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:46,098 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 18:56:49,543 :: INFO :: evodenss.train.trainers :: [2051] -- [3.44s] TRAIN epoch 69 -- loss: tensor([69125.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:56:49,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69125.25
2025-03-09 18:56:49,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:50,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 18:56:53,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68632.391
2025-03-09 18:56:53,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:55,008 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 18:56:58,294 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68548.289
2025-03-09 18:56:58,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:56:59,415 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 18:57:02,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69032.805
2025-03-09 18:57:02,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:03,793 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 18:57:07,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68281.531
2025-03-09 18:57:07,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:08,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 18:57:11,364 :: INFO :: evodenss.train.trainers :: [2051] -- [3.24s] TRAIN epoch 74 -- loss: tensor([68184.9531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:57:11,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68184.953
2025-03-09 18:57:11,364 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:12,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 18:57:15,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66898.727
2025-03-09 18:57:15,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:16,840 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 18:57:20,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68128.32
2025-03-09 18:57:20,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:21,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 18:57:24,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67384.664
2025-03-09 18:57:24,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:25,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 18:57:28,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68182.625
2025-03-09 18:57:28,966 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:30,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 18:57:33,353 :: INFO :: evodenss.train.trainers :: [2051] -- [3.24s] TRAIN epoch 79 -- loss: tensor([68603.6641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:57:33,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68603.664
2025-03-09 18:57:33,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:34,479 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 18:57:37,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66982.977
2025-03-09 18:57:37,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:38,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 18:57:42,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67380.961
2025-03-09 18:57:42,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:43,217 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 18:57:46,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67305.312
2025-03-09 18:57:46,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:47,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 18:57:50,994 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67923.797
2025-03-09 18:57:50,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:52,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 18:57:55,383 :: INFO :: evodenss.train.trainers :: [2051] -- [3.24s] TRAIN epoch 84 -- loss: tensor([66735.0156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:57:55,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66735.016
2025-03-09 18:57:55,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:57:56,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 18:57:59,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66636.305
2025-03-09 18:57:59,757 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:00,892 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 18:58:04,116 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66946.875
2025-03-09 18:58:04,116 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:05,256 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 18:58:08,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66276.922
2025-03-09 18:58:08,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:09,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 18:58:12,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66488.062
2025-03-09 18:58:12,858 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:13,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 18:58:17,220 :: INFO :: evodenss.train.trainers :: [2051] -- [3.22s] TRAIN epoch 89 -- loss: tensor([65475.6875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:58:17,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65475.688
2025-03-09 18:58:17,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:18,358 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 18:58:21,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66018.742
2025-03-09 18:58:21,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:22,913 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 18:58:26,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65438.324
2025-03-09 18:58:26,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:27,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 18:58:30,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66598.562
2025-03-09 18:58:30,515 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:31,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 18:58:34,907 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66127.234
2025-03-09 18:58:34,907 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:36,040 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 18:58:39,302 :: INFO :: evodenss.train.trainers :: [2051] -- [3.26s] TRAIN epoch 94 -- loss: tensor([66376.4297], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:58:39,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66376.43
2025-03-09 18:58:39,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:40,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 18:58:43,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66282.891
2025-03-09 18:58:43,680 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:44,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 18:58:48,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65817.891
2025-03-09 18:58:48,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:49,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 18:58:52,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65448.387
2025-03-09 18:58:52,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:53,921 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 18:58:57,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65468.438
2025-03-09 18:58:57,178 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:58:58,310 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 18:59:01,571 :: INFO :: evodenss.train.trainers :: [2051] -- [3.26s] TRAIN epoch 99 -- loss: tensor([65725.7031], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:59:01,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65725.703
2025-03-09 18:59:01,571 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:59:03,845 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 3932.854
2025-03-09 18:59:03,851 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 18:59:03,852 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 
layer10: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 
layer13: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 
layer15: :fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100
2025-03-09 18:59:03,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 18:59:05,007 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 3705.05347
2025-03-09 18:59:05,012 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 18:59:05,013 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 
layer10: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 
layer13: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 
layer15: :fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100
2025-03-09 18:59:05,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 18:59:06,176 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 3705.05347
2025-03-09 18:59:06,176 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 18:59:06,177 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 2, id: 2
2025-03-09 18:59:06,177 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [1000, 1000, 2000, 2000]
2025-03-09 18:59:06,177 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 18:59:06,177 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Individuals trained for the minimum time: [True, True, False, False]
2025-03-09 18:59:06,177 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Min train time parent: idx: 1, id: 1, max fitness detected: 3932.854
2025-03-09 18:59:06,177 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Fitnesses from min train individuals before selecting best individual: [4194.12061, 3932.854]
2025-03-09 18:59:06,177 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Individual 1 has its train extended. Current fitness 3932.854
2025-03-09 18:59:06,182 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 18:59:06,183 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :deconv1d out_channels:115 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :deconv1d out_channels:39 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 
layer8: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 
layer9: :conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 
layer11: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 
layer14: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 
layer15: :fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100
2025-03-09 18:59:06,207 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 18:59:07,357 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [4194.12061, 3932.854, 3705.05347, 3705.05347]
2025-03-09 18:59:07,609 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([18895.8125], device='cuda:0')
2025-03-09 18:59:07,609 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 15: 3705.05347
2025-03-09 18:59:07,610 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3575.71436



2025-03-09 18:59:07,699 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 16
2025-03-09 18:59:07,699 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 18:59:07,711 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a layer removed from Module 0: features; Position: 11
2025-03-09 18:59:07,712 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 18:59:07,713 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 18:59:07,714 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 18:59:07,714 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 18:59:07,715 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 18:59:07,718 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a layer removed from Module 0: features; Position: 4
2025-03-09 18:59:07,719 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 8. Reused?: False
2025-03-09 18:59:07,719 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 18:59:07,720 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 18:59:07,721 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 18:59:07,721 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 18:59:07,722 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 18:59:07,722 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 18:59:07,726 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 18:59:07,727 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 18:59:07,727 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 18:59:07,728 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 18:59:07,729 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 14
2025-03-09 18:59:07,729 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 18:59:07,732 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 18:59:07,736 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 2000 secs
2025-03-09 18:59:07,737 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 
layer10: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 
layer13: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 
layer15: :fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100
2025-03-09 18:59:07,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 18:59:07,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 18:59:11,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 472787.344
2025-03-09 18:59:11,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:59:12,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 18:59:15,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 236967.047
2025-03-09 18:59:15,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:59:16,778 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 18:59:20,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 139919.656
2025-03-09 18:59:20,363 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:59:21,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 18:59:24,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112350.695
2025-03-09 18:59:24,917 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:59:26,037 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 18:59:29,428 :: INFO :: evodenss.train.trainers :: [2051] -- [3.39s] TRAIN epoch 4 -- loss: tensor([105268.7891], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:59:29,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 105268.789
2025-03-09 18:59:29,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:59:30,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 18:59:33,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100347.133
2025-03-09 18:59:33,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:59:35,084 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 18:59:38,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96968.078
2025-03-09 18:59:38,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:59:39,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 18:59:43,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94530.148
2025-03-09 18:59:43,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:59:44,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 18:59:47,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92395.789
2025-03-09 18:59:47,625 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:59:48,762 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 18:59:52,383 :: INFO :: evodenss.train.trainers :: [2051] -- [3.62s] TRAIN epoch 9 -- loss: tensor([90076.8281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 18:59:52,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90076.828
2025-03-09 18:59:52,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:59:53,527 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 18:59:56,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88151.0
2025-03-09 18:59:56,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 18:59:58,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 19:00:01,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86782.953
2025-03-09 19:00:01,492 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:02,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 19:00:06,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85735.562
2025-03-09 19:00:06,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:07,189 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 19:00:10,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84640.812
2025-03-09 19:00:10,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:11,696 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 19:00:15,092 :: INFO :: evodenss.train.trainers :: [2051] -- [3.39s] TRAIN epoch 14 -- loss: tensor([82798.5547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:00:15,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82798.555
2025-03-09 19:00:15,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:16,230 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 19:00:19,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82553.039
2025-03-09 19:00:19,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:21,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 19:00:24,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81872.82
2025-03-09 19:00:24,427 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:25,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 19:00:28,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81583.539
2025-03-09 19:00:28,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:30,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 19:00:33,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80785.156
2025-03-09 19:00:33,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:34,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 19:00:38,029 :: INFO :: evodenss.train.trainers :: [2051] -- [3.37s] TRAIN epoch 19 -- loss: tensor([79694.5156], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:00:38,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79694.516
2025-03-09 19:00:38,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:39,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 19:00:42,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78921.758
2025-03-09 19:00:42,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:43,695 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 19:00:47,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78412.422
2025-03-09 19:00:47,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:48,203 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 19:00:51,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78066.312
2025-03-09 19:00:51,794 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:52,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 19:00:56,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77304.484
2025-03-09 19:00:56,350 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:00:57,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 19:01:00,864 :: INFO :: evodenss.train.trainers :: [2051] -- [3.37s] TRAIN epoch 24 -- loss: tensor([77146.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:01:00,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77146.312
2025-03-09 19:01:00,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:02,010 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 19:01:05,408 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76467.125
2025-03-09 19:01:05,408 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:06,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 19:01:09,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76316.094
2025-03-09 19:01:09,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:11,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 19:01:14,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76557.242
2025-03-09 19:01:14,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:15,637 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 19:01:19,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76901.898
2025-03-09 19:01:19,068 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:20,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 19:01:23,815 :: INFO :: evodenss.train.trainers :: [2051] -- [3.4s] TRAIN epoch 29 -- loss: tensor([74132.1719], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:01:23,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74132.172
2025-03-09 19:01:23,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:24,969 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 19:01:28,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75032.68
2025-03-09 19:01:28,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:29,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 19:01:32,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74872.062
2025-03-09 19:01:32,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:34,137 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 19:01:37,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73576.008
2025-03-09 19:01:37,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:38,709 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 19:01:42,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73160.18
2025-03-09 19:01:42,135 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:43,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 19:01:46,718 :: INFO :: evodenss.train.trainers :: [2051] -- [3.43s] TRAIN epoch 34 -- loss: tensor([72974.3594], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:01:46,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72974.359
2025-03-09 19:01:46,718 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:47,879 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 19:01:51,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72480.141
2025-03-09 19:01:51,480 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:52,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 19:01:56,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73049.789
2025-03-09 19:01:56,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:01:57,285 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 19:02:00,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72451.203
2025-03-09 19:02:00,710 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:01,870 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 19:02:05,279 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72726.469
2025-03-09 19:02:05,280 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:06,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 19:02:09,859 :: INFO :: evodenss.train.trainers :: [2051] -- [3.42s] TRAIN epoch 39 -- loss: tensor([71949.8516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:02:09,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71949.852
2025-03-09 19:02:09,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:11,025 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 19:02:14,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72214.57
2025-03-09 19:02:14,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:15,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 19:02:19,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70867.93
2025-03-09 19:02:19,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:20,370 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 19:02:23,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71017.773
2025-03-09 19:02:23,783 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:24,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 19:02:28,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71712.516
2025-03-09 19:02:28,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:29,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 19:02:32,952 :: INFO :: evodenss.train.trainers :: [2051] -- [3.42s] TRAIN epoch 44 -- loss: tensor([71882.8047], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:02:32,953 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71882.805
2025-03-09 19:02:32,953 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:34,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 19:02:37,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71398.758
2025-03-09 19:02:37,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:38,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 19:02:42,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70239.5
2025-03-09 19:02:42,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:43,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 19:02:46,644 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69543.062
2025-03-09 19:02:46,644 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:47,754 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 19:02:51,331 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70143.031
2025-03-09 19:02:51,332 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:52,455 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 19:02:55,865 :: INFO :: evodenss.train.trainers :: [2051] -- [3.41s] TRAIN epoch 49 -- loss: tensor([70022.9766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:02:55,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70022.977
2025-03-09 19:02:55,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:02:56,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 19:03:00,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69559.969
2025-03-09 19:03:00,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:01,466 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 19:03:04,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69054.93
2025-03-09 19:03:04,855 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:05,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 19:03:09,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69520.211
2025-03-09 19:03:09,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:10,518 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 19:03:13,957 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69566.047
2025-03-09 19:03:13,957 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:15,097 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 19:03:18,482 :: INFO :: evodenss.train.trainers :: [2051] -- [3.38s] TRAIN epoch 54 -- loss: tensor([68257.3750], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:03:18,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68257.375
2025-03-09 19:03:18,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:19,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 19:03:23,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69666.602
2025-03-09 19:03:23,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:24,442 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 19:03:27,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68787.805
2025-03-09 19:03:27,859 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:29,015 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 19:03:32,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70021.867
2025-03-09 19:03:32,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:33,617 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 19:03:37,058 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68759.68
2025-03-09 19:03:37,058 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:38,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 19:03:41,620 :: INFO :: evodenss.train.trainers :: [2051] -- [3.4s] TRAIN epoch 59 -- loss: tensor([68220.8203], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:03:41,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68220.82
2025-03-09 19:03:41,620 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:42,736 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 19:03:46,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68405.07
2025-03-09 19:03:46,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:47,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 19:03:50,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69350.219
2025-03-09 19:03:50,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:51,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 19:03:55,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68186.148
2025-03-09 19:03:55,316 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:03:56,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 19:03:59,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68046.281
2025-03-09 19:03:59,852 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:00,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 19:04:04,349 :: INFO :: evodenss.train.trainers :: [2051] -- [3.39s] TRAIN epoch 64 -- loss: tensor([67366.1016], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:04:04,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67366.102
2025-03-09 19:04:04,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:05,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 19:04:08,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67731.266
2025-03-09 19:04:08,872 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:09,983 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 19:04:13,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67767.266
2025-03-09 19:04:13,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:14,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 19:04:17,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67716.984
2025-03-09 19:04:17,861 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:18,987 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 19:04:22,535 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67886.578
2025-03-09 19:04:22,536 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:23,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 19:04:27,090 :: INFO :: evodenss.train.trainers :: [2051] -- [3.43s] TRAIN epoch 69 -- loss: tensor([67709.5938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:04:27,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67709.594
2025-03-09 19:04:27,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:28,227 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 19:04:31,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67347.812
2025-03-09 19:04:31,612 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:32,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 19:04:36,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67395.586
2025-03-09 19:04:36,133 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:37,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 19:04:40,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67213.172
2025-03-09 19:04:40,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:41,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 19:04:45,192 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67121.789
2025-03-09 19:04:45,193 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:46,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 19:04:49,954 :: INFO :: evodenss.train.trainers :: [2051] -- [3.62s] TRAIN epoch 74 -- loss: tensor([67591.3203], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:04:49,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67591.32
2025-03-09 19:04:49,955 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:51,091 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 19:04:54,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66753.516
2025-03-09 19:04:54,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:04:55,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 19:04:59,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66217.359
2025-03-09 19:04:59,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:00,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 19:05:03,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66146.445
2025-03-09 19:05:03,537 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:04,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 19:05:08,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66294.555
2025-03-09 19:05:08,048 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:09,180 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 19:05:12,562 :: INFO :: evodenss.train.trainers :: [2051] -- [3.38s] TRAIN epoch 79 -- loss: tensor([66118.5703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:05:12,562 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66118.57
2025-03-09 19:05:12,562 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:13,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 19:05:17,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66032.328
2025-03-09 19:05:17,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:18,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 19:05:21,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65353.668
2025-03-09 19:05:21,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:22,984 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 19:05:26,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65360.25
2025-03-09 19:05:26,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:27,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 19:05:30,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65470.883
2025-03-09 19:05:30,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:32,030 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 19:05:35,488 :: INFO :: evodenss.train.trainers :: [2051] -- [3.46s] TRAIN epoch 84 -- loss: tensor([66000.9688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:05:35,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66000.969
2025-03-09 19:05:35,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:36,637 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 19:05:40,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66433.25
2025-03-09 19:05:40,062 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:41,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 19:05:44,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65043.285
2025-03-09 19:05:44,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:45,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 19:05:49,129 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65822.844
2025-03-09 19:05:49,130 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:50,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 19:05:53,838 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64874.707
2025-03-09 19:05:53,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:54,975 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 19:05:58,393 :: INFO :: evodenss.train.trainers :: [2051] -- [3.42s] TRAIN epoch 89 -- loss: tensor([65013.1758], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:05:58,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65013.176
2025-03-09 19:05:58,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:05:59,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 19:06:02,944 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65422.602
2025-03-09 19:06:02,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:06:04,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 19:06:07,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65414.48
2025-03-09 19:06:07,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:06:08,572 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 19:06:11,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64944.477
2025-03-09 19:06:11,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:06:13,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 19:06:16,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64706.34
2025-03-09 19:06:16,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:06:17,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 19:06:21,146 :: INFO :: evodenss.train.trainers :: [2051] -- [3.6s] TRAIN epoch 94 -- loss: tensor([63904.1094], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:06:21,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63904.109
2025-03-09 19:06:21,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:06:22,258 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 19:06:25,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64281.617
2025-03-09 19:06:25,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:06:26,737 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 19:06:30,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 63598.91
2025-03-09 19:06:30,110 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:06:31,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 19:06:34,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64043.832
2025-03-09 19:06:34,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:06:35,742 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 19:06:39,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64468.625
2025-03-09 19:06:39,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:06:40,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 19:06:43,617 :: INFO :: evodenss.train.trainers :: [2051] -- [3.39s] TRAIN epoch 99 -- loss: tensor([64499.5977], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:06:43,617 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64499.598
2025-03-09 19:06:43,618 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:06:45,871 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 4202.90137
2025-03-09 19:06:45,876 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 19:06:45,877 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 
layer10: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :deconv1d out_channels:104 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:15 epochs:100
2025-03-09 19:06:45,888 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 19:06:45,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 19:06:50,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 1404837.125
2025-03-09 19:06:50,849 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:06:51,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 19:06:56,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:06:56,647 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:06:57,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 19:07:02,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:07:02,453 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:07:03,582 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 19:07:08,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:07:08,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:07:09,378 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 19:07:14,125 :: INFO :: evodenss.train.trainers :: [2051] -- [4.75s] TRAIN epoch 4 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:07:14,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:07:14,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:07:15,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 19:07:20,004 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:07:20,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:07:21,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 19:07:25,763 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:07:25,763 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:07:26,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 19:07:31,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:07:31,560 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:07:32,699 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 19:07:37,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:07:37,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:07:38,467 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 19:07:43,094 :: INFO :: evodenss.train.trainers :: [2051] -- [4.63s] TRAIN epoch 9 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:07:43,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:07:43,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:07:44,225 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 19:07:48,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:07:48,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:07:50,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 19:07:54,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:07:54,960 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:07:56,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 19:08:00,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:08:00,746 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:08:01,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 19:08:06,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:08:06,540 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:08:07,677 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 19:08:12,356 :: INFO :: evodenss.train.trainers :: [2051] -- [4.68s] TRAIN epoch 14 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:08:12,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:08:12,356 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:08:13,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 19:08:18,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:08:18,100 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:08:19,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 19:08:24,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:08:24,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:08:25,305 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 19:08:29,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:08:29,981 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:08:31,128 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 19:08:35,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:08:35,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:08:36,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 19:08:41,519 :: INFO :: evodenss.train.trainers :: [2051] -- [4.66s] TRAIN epoch 19 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:08:41,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:08:41,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:08:42,650 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 19:08:47,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:08:47,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:08:48,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 19:08:53,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:08:53,345 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:08:54,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 19:08:59,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:08:59,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:09:00,328 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 19:09:04,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:09:04,972 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:09:06,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 19:09:10,781 :: INFO :: evodenss.train.trainers :: [2051] -- [4.68s] TRAIN epoch 24 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:09:10,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:09:10,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:09:11,915 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 19:09:16,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:09:16,590 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:09:17,730 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 19:09:22,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:09:22,557 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:09:23,637 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 19:09:28,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:09:28,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:09:29,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 19:09:34,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:09:34,149 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:09:35,293 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 19:09:39,911 :: INFO :: evodenss.train.trainers :: [2051] -- [4.62s] TRAIN epoch 29 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:09:39,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:09:39,912 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:09:41,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 19:09:45,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:09:45,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:09:46,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 19:09:51,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:09:51,690 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:09:52,803 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 19:09:57,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:09:57,471 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:09:58,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 19:10:03,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:10:03,302 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:10:04,429 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 19:10:09,080 :: INFO :: evodenss.train.trainers :: [2051] -- [4.65s] TRAIN epoch 34 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:10:09,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:10:09,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:10:10,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 19:10:14,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:10:14,925 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:10:16,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 19:10:20,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:10:20,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:10:22,060 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 19:10:26,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:10:26,714 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:10:27,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 19:10:32,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:10:32,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:10:33,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 19:10:38,406 :: INFO :: evodenss.train.trainers :: [2051] -- [4.68s] TRAIN epoch 39 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:10:38,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:10:38,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:10:39,539 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 19:10:44,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:10:44,210 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:10:45,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 19:10:50,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:10:50,089 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:10:51,243 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 19:10:55,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:10:55,974 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:10:57,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 19:11:01,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:11:01,708 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:11:02,863 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 19:11:07,489 :: INFO :: evodenss.train.trainers :: [2051] -- [4.62s] TRAIN epoch 44 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:11:07,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:11:07,490 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:11:08,644 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 19:11:13,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:11:13,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:11:14,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 19:11:19,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:11:19,333 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:11:20,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 19:11:25,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:11:25,223 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:11:26,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 19:11:31,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:11:31,003 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:11:32,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 19:11:36,866 :: INFO :: evodenss.train.trainers :: [2051] -- [4.7s] TRAIN epoch 49 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:11:36,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:11:36,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:11:38,051 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 19:11:42,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:11:42,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:11:44,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 19:11:48,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:11:48,723 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:11:50,108 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 19:11:54,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:11:54,758 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:11:55,938 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 19:12:00,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:12:00,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:12:01,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 19:12:06,475 :: INFO :: evodenss.train.trainers :: [2051] -- [4.7s] TRAIN epoch 54 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:12:06,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:12:06,475 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:12:07,654 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 19:12:12,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:12:12,417 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:12:13,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 19:12:18,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:12:18,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:12:19,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 19:12:24,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:12:24,440 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:12:25,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 19:12:30,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:12:30,359 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:12:31,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 19:12:36,295 :: INFO :: evodenss.train.trainers :: [2051] -- [4.77s] TRAIN epoch 59 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:12:36,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:12:36,296 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:12:37,498 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 19:12:42,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:12:42,252 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:12:43,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 19:12:48,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:12:48,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:12:49,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 19:12:54,290 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:12:54,290 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:12:55,500 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 19:13:00,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:13:00,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:13:01,419 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 19:13:06,153 :: INFO :: evodenss.train.trainers :: [2051] -- [4.73s] TRAIN epoch 64 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:13:06,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:13:06,153 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:13:07,344 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 19:13:11,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:13:11,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:13:13,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 19:13:17,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:13:17,883 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:13:19,071 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 19:13:23,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:13:23,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:13:25,177 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 19:13:29,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:13:29,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:13:31,090 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 19:13:35,841 :: INFO :: evodenss.train.trainers :: [2051] -- [4.75s] TRAIN epoch 69 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:13:35,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:13:35,841 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:13:37,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 19:13:41,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:13:41,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:13:42,889 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 19:13:47,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:13:47,615 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:13:48,795 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 19:13:53,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:13:53,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:13:54,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 19:13:59,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:13:59,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:14:00,772 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 19:14:05,470 :: INFO :: evodenss.train.trainers :: [2051] -- [4.7s] TRAIN epoch 74 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:14:05,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:14:05,470 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:14:06,668 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 19:14:11,443 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:14:11,444 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:14:12,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 19:14:17,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:14:17,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:14:18,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 19:14:23,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:14:23,558 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:14:24,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 19:14:29,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:14:29,507 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:14:30,716 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 19:14:35,483 :: INFO :: evodenss.train.trainers :: [2051] -- [4.76s] TRAIN epoch 79 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:14:35,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:14:35,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:14:36,705 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 19:14:41,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:14:41,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:14:42,641 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 19:14:47,368 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:14:47,369 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:14:48,577 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 19:14:53,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:14:53,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:14:54,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 19:14:59,456 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:14:59,456 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:15:00,669 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 19:15:05,406 :: INFO :: evodenss.train.trainers :: [2051] -- [4.74s] TRAIN epoch 84 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:15:05,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:15:05,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:15:06,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 19:15:11,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:15:11,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:15:12,445 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 19:15:17,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:15:17,117 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:15:18,291 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 19:15:23,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:15:23,168 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:15:24,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 19:15:29,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:15:29,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:15:30,251 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 19:15:34,904 :: INFO :: evodenss.train.trainers :: [2051] -- [4.65s] TRAIN epoch 89 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:15:34,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:15:34,904 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:15:36,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 19:15:40,733 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:15:40,734 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:15:41,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 19:15:46,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:15:46,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:15:47,866 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 19:15:52,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:15:52,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:15:53,868 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 19:15:58,586 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:15:58,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:15:59,781 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 19:16:04,519 :: INFO :: evodenss.train.trainers :: [2051] -- [4.74s] TRAIN epoch 94 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:16:04,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:16:04,519 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:16:05,717 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 19:16:10,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:16:10,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:16:11,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 19:16:16,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:16:16,347 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:16:17,546 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 19:16:22,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:16:22,482 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:16:23,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 19:16:28,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:16:28,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:16:29,575 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 19:16:34,218 :: INFO :: evodenss.train.trainers :: [2051] -- [4.64s] TRAIN epoch 99 -- loss: tensor([804482.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:16:34,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 804482.562
2025-03-09 19:16:34,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:16:37,284 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 1 fitness: 44608.84766
2025-03-09 19:16:37,289 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 2 for 1000 secs
2025-03-09 19:16:37,291 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 
layer5: :deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 
layer8: :conv1d out_channels:84 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 
layer9: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 
layer10: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 
layer13: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 
layer15: :fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100
2025-03-09 19:16:37,303 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 19:16:37,304 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 19:16:40,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 276847.25
2025-03-09 19:16:40,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:16:42,208 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 19:16:45,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 157669.688
2025-03-09 19:16:45,452 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:16:46,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 19:16:50,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 137059.375
2025-03-09 19:16:50,052 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:16:51,232 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 19:16:54,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 129343.289
2025-03-09 19:16:54,473 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:16:55,686 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 19:16:58,897 :: INFO :: evodenss.train.trainers :: [2051] -- [3.21s] TRAIN epoch 4 -- loss: tensor([123755.0781], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:16:58,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 123755.078
2025-03-09 19:16:58,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:00,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 19:17:03,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 118570.219
2025-03-09 19:17:03,334 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:04,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 19:17:07,786 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 114466.523
2025-03-09 19:17:07,786 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:08,971 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 19:17:12,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112151.938
2025-03-09 19:17:12,211 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:13,401 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 19:17:16,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 107588.797
2025-03-09 19:17:16,640 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:17,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 19:17:21,248 :: INFO :: evodenss.train.trainers :: [2051] -- [3.43s] TRAIN epoch 9 -- loss: tensor([106529.6953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:17:21,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 106529.695
2025-03-09 19:17:21,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:22,439 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 19:17:25,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104760.539
2025-03-09 19:17:25,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:26,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 19:17:29,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102376.312
2025-03-09 19:17:29,933 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:31,055 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 19:17:34,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100521.086
2025-03-09 19:17:34,236 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:35,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 19:17:38,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100550.219
2025-03-09 19:17:38,532 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:39,651 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 19:17:42,823 :: INFO :: evodenss.train.trainers :: [2051] -- [3.17s] TRAIN epoch 14 -- loss: tensor([98025.4609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:17:42,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98025.461
2025-03-09 19:17:42,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:43,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 19:17:47,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 97933.266
2025-03-09 19:17:47,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:48,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 19:17:51,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96462.367
2025-03-09 19:17:51,607 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:52,750 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 19:17:55,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96205.562
2025-03-09 19:17:55,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:17:57,054 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 19:18:00,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95493.594
2025-03-09 19:18:00,186 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:01,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 19:18:04,448 :: INFO :: evodenss.train.trainers :: [2051] -- [3.15s] TRAIN epoch 19 -- loss: tensor([94584.3984], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:18:04,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94584.398
2025-03-09 19:18:04,449 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:05,568 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 19:18:08,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 94167.461
2025-03-09 19:18:08,707 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:09,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 19:18:13,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93231.906
2025-03-09 19:18:13,019 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:14,150 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 19:18:17,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92323.062
2025-03-09 19:18:17,324 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:18,450 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 19:18:21,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 92119.109
2025-03-09 19:18:21,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:22,952 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 19:18:26,141 :: INFO :: evodenss.train.trainers :: [2051] -- [3.19s] TRAIN epoch 24 -- loss: tensor([91081.5547], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:18:26,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91081.555
2025-03-09 19:18:26,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:27,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 19:18:30,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91050.328
2025-03-09 19:18:30,493 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:31,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 19:18:34,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89879.711
2025-03-09 19:18:34,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:35,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 19:18:39,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89485.086
2025-03-09 19:18:39,138 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:40,281 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 19:18:43,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 89692.414
2025-03-09 19:18:43,484 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:44,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 19:18:47,787 :: INFO :: evodenss.train.trainers :: [2051] -- [3.19s] TRAIN epoch 29 -- loss: tensor([88368.8125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:18:47,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88368.812
2025-03-09 19:18:47,787 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:48,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 19:18:52,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87994.203
2025-03-09 19:18:52,319 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:53,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 19:18:56,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88031.258
2025-03-09 19:18:56,626 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:18:57,768 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 19:19:00,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87384.891
2025-03-09 19:19:00,946 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:02,082 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 19:19:05,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87515.031
2025-03-09 19:19:05,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:06,421 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 19:19:09,601 :: INFO :: evodenss.train.trainers :: [2051] -- [3.18s] TRAIN epoch 34 -- loss: tensor([86829.1875], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:19:09,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86829.188
2025-03-09 19:19:09,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:10,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 19:19:13,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86211.703
2025-03-09 19:19:13,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:15,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 19:19:18,238 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86128.203
2025-03-09 19:19:18,239 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:19,638 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 19:19:22,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85774.422
2025-03-09 19:19:22,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:24,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 19:19:27,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85185.523
2025-03-09 19:19:27,214 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:28,391 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 19:19:31,609 :: INFO :: evodenss.train.trainers :: [2051] -- [3.22s] TRAIN epoch 39 -- loss: tensor([84716.9688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:19:31,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84716.969
2025-03-09 19:19:31,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:32,765 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 19:19:35,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84589.984
2025-03-09 19:19:35,970 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:37,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 19:19:40,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83977.805
2025-03-09 19:19:40,299 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:41,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 19:19:44,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84032.688
2025-03-09 19:19:44,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:45,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 19:19:48,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83991.703
2025-03-09 19:19:48,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:50,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 19:19:53,579 :: INFO :: evodenss.train.trainers :: [2051] -- [3.22s] TRAIN epoch 44 -- loss: tensor([83234.2266], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:19:53,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83234.227
2025-03-09 19:19:53,579 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:54,747 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 19:19:57,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83033.062
2025-03-09 19:19:57,932 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:19:59,059 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 19:20:02,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82919.656
2025-03-09 19:20:02,229 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:03,351 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 19:20:06,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83046.375
2025-03-09 19:20:06,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:07,604 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 19:20:10,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82694.109
2025-03-09 19:20:10,774 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:11,898 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 19:20:15,063 :: INFO :: evodenss.train.trainers :: [2051] -- [3.16s] TRAIN epoch 49 -- loss: tensor([81554.6641], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:20:15,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81554.664
2025-03-09 19:20:15,064 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:16,190 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 19:20:19,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82213.469
2025-03-09 19:20:19,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:20,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 19:20:23,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82392.469
2025-03-09 19:20:23,900 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:25,018 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 19:20:28,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81961.977
2025-03-09 19:20:28,173 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:29,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 19:20:32,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81134.469
2025-03-09 19:20:32,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:33,543 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 19:20:36,721 :: INFO :: evodenss.train.trainers :: [2051] -- [3.18s] TRAIN epoch 54 -- loss: tensor([80906.8516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:20:36,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80906.852
2025-03-09 19:20:36,721 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:37,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 19:20:41,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80712.219
2025-03-09 19:20:41,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:42,102 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 19:20:45,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81003.102
2025-03-09 19:20:45,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:46,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 19:20:49,779 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80623.156
2025-03-09 19:20:49,780 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:50,901 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 19:20:54,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80609.32
2025-03-09 19:20:54,092 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:55,226 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 19:20:58,424 :: INFO :: evodenss.train.trainers :: [2051] -- [3.2s] TRAIN epoch 59 -- loss: tensor([80813.3359], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:20:58,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80813.336
2025-03-09 19:20:58,424 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:20:59,544 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 19:21:02,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80252.422
2025-03-09 19:21:02,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:03,830 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 19:21:07,006 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79069.805
2025-03-09 19:21:07,007 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:08,116 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 19:21:11,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79805.242
2025-03-09 19:21:11,262 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:12,384 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 19:21:15,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79300.117
2025-03-09 19:21:15,531 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:16,658 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 19:21:20,027 :: INFO :: evodenss.train.trainers :: [2051] -- [3.37s] TRAIN epoch 64 -- loss: tensor([79910.9062], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:21:20,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79910.906
2025-03-09 19:21:20,027 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:21,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 19:21:24,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79253.688
2025-03-09 19:21:24,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:25,438 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 19:21:28,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78123.312
2025-03-09 19:21:28,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:29,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 19:21:32,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78866.328
2025-03-09 19:21:32,893 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:34,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 19:21:37,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78660.828
2025-03-09 19:21:37,176 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:38,295 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 19:21:41,458 :: INFO :: evodenss.train.trainers :: [2051] -- [3.16s] TRAIN epoch 69 -- loss: tensor([78067.4219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:21:41,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78067.422
2025-03-09 19:21:41,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:42,591 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 19:21:45,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78604.664
2025-03-09 19:21:45,789 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:46,922 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 19:21:50,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78005.008
2025-03-09 19:21:50,282 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:51,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 19:21:54,602 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77753.328
2025-03-09 19:21:54,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:21:55,740 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 19:21:58,929 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77483.93
2025-03-09 19:21:58,930 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:00,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 19:22:03,250 :: INFO :: evodenss.train.trainers :: [2051] -- [3.18s] TRAIN epoch 74 -- loss: tensor([77130.9688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:22:03,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77130.969
2025-03-09 19:22:03,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:04,380 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 19:22:07,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77538.5
2025-03-09 19:22:07,545 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:08,671 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 19:22:11,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78006.453
2025-03-09 19:22:11,843 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:12,964 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 19:22:16,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77112.656
2025-03-09 19:22:16,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:17,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 19:22:20,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77405.695
2025-03-09 19:22:20,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:21,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 19:22:24,922 :: INFO :: evodenss.train.trainers :: [2051] -- [3.15s] TRAIN epoch 79 -- loss: tensor([76989.6953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:22:24,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76989.695
2025-03-09 19:22:24,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:26,050 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 19:22:29,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76955.352
2025-03-09 19:22:29,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:30,399 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 19:22:33,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76921.797
2025-03-09 19:22:33,605 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:34,752 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 19:22:37,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76940.883
2025-03-09 19:22:37,939 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:39,083 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 19:22:42,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76972.594
2025-03-09 19:22:42,272 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:43,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 19:22:46,569 :: INFO :: evodenss.train.trainers :: [2051] -- [3.16s] TRAIN epoch 84 -- loss: tensor([76526.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:22:46,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76526.25
2025-03-09 19:22:46,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:47,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 19:22:51,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77456.336
2025-03-09 19:22:51,053 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:52,197 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 19:22:55,374 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76456.992
2025-03-09 19:22:55,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:22:56,523 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 19:22:59,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75815.266
2025-03-09 19:22:59,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:00,831 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 19:23:04,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75664.227
2025-03-09 19:23:04,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:05,165 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 19:23:08,360 :: INFO :: evodenss.train.trainers :: [2051] -- [3.19s] TRAIN epoch 89 -- loss: tensor([75916.1406], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:23:08,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75916.141
2025-03-09 19:23:08,360 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:09,503 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 19:23:12,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75940.422
2025-03-09 19:23:12,672 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:13,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 19:23:17,054 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76091.836
2025-03-09 19:23:17,054 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:18,212 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 19:23:21,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75303.117
2025-03-09 19:23:21,603 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:22,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 19:23:25,979 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74697.828
2025-03-09 19:23:25,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:27,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 19:23:30,315 :: INFO :: evodenss.train.trainers :: [2051] -- [3.19s] TRAIN epoch 94 -- loss: tensor([75667.4141], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:23:30,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75667.414
2025-03-09 19:23:30,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:31,483 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 19:23:34,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74592.633
2025-03-09 19:23:34,687 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:35,877 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 19:23:39,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74789.617
2025-03-09 19:23:39,061 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:40,231 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 19:23:43,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74399.969
2025-03-09 19:23:43,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:44,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 19:23:47,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75040.281
2025-03-09 19:23:47,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:48,995 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 19:23:52,433 :: INFO :: evodenss.train.trainers :: [2051] -- [3.44s] TRAIN epoch 99 -- loss: tensor([74355.9922], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:23:52,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74355.992
2025-03-09 19:23:52,434 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:23:54,822 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 2 fitness: 4298.65723
2025-03-09 19:23:54,827 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 3 for 1000 secs
2025-03-09 19:23:54,828 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 
layer10: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:25 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 
layer13: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 
layer15: :fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:8 epochs:100
2025-03-09 19:23:54,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 19:23:54,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 19:24:00,065 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 253841.516
2025-03-09 19:24:00,066 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:24:01,270 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 19:24:06,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 128356.969
2025-03-09 19:24:06,357 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:24:07,552 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 19:24:12,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 112337.453
2025-03-09 19:24:12,655 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:24:13,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 19:24:18,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 104639.578
2025-03-09 19:24:18,919 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:24:20,341 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 19:24:25,448 :: INFO :: evodenss.train.trainers :: [2051] -- [5.11s] TRAIN epoch 4 -- loss: tensor([100480.2891], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:24:25,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 100480.289
2025-03-09 19:24:25,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:24:26,632 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 19:24:31,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 96501.609
2025-03-09 19:24:31,689 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:24:32,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 19:24:37,940 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93034.938
2025-03-09 19:24:37,941 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:24:39,144 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 19:24:44,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90753.672
2025-03-09 19:24:44,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:24:45,468 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 19:24:50,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88927.898
2025-03-09 19:24:50,792 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:24:51,980 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 19:24:57,159 :: INFO :: evodenss.train.trainers :: [2051] -- [5.18s] TRAIN epoch 9 -- loss: tensor([87945.9766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:24:57,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87945.977
2025-03-09 19:24:57,159 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:24:58,395 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 19:25:03,520 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 87026.539
2025-03-09 19:25:03,521 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:25:04,701 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 19:25:09,808 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 85628.328
2025-03-09 19:25:09,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:25:10,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 19:25:16,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84955.531
2025-03-09 19:25:16,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:25:17,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 19:25:22,644 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84282.031
2025-03-09 19:25:22,644 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:25:23,788 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 19:25:28,806 :: INFO :: evodenss.train.trainers :: [2051] -- [5.02s] TRAIN epoch 14 -- loss: tensor([83178.6094], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:25:28,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83178.609
2025-03-09 19:25:28,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:25:29,923 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 19:25:34,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82533.188
2025-03-09 19:25:34,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:25:36,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 19:25:41,219 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82154.539
2025-03-09 19:25:41,220 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:25:42,377 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 19:25:47,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80432.414
2025-03-09 19:25:47,464 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:25:48,629 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 19:25:53,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80497.484
2025-03-09 19:25:53,882 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:25:55,056 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 19:26:00,085 :: INFO :: evodenss.train.trainers :: [2051] -- [5.03s] TRAIN epoch 19 -- loss: tensor([79831.6953], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:26:00,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79831.695
2025-03-09 19:26:00,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:26:01,206 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 19:26:06,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79375.453
2025-03-09 19:26:06,259 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:26:07,416 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 19:26:12,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78822.18
2025-03-09 19:26:12,448 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:26:13,631 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 19:26:18,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78800.883
2025-03-09 19:26:18,735 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:26:20,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 19:26:25,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77502.273
2025-03-09 19:26:25,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:26:26,349 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 19:26:31,367 :: INFO :: evodenss.train.trainers :: [2051] -- [5.02s] TRAIN epoch 24 -- loss: tensor([77028.4922], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:26:31,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77028.492
2025-03-09 19:26:31,367 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:26:32,526 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 19:26:37,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76541.891
2025-03-09 19:26:37,581 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:26:38,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 19:26:43,816 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75966.797
2025-03-09 19:26:43,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:26:44,989 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 19:26:50,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75688.43
2025-03-09 19:26:50,250 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:26:51,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 19:26:56,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75924.312
2025-03-09 19:26:56,472 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:26:57,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 19:27:02,744 :: INFO :: evodenss.train.trainers :: [2051] -- [5.1s] TRAIN epoch 29 -- loss: tensor([75338.9688], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:27:02,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75338.969
2025-03-09 19:27:02,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:27:03,929 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 19:27:08,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74845.078
2025-03-09 19:27:08,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:27:10,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 19:27:15,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74817.602
2025-03-09 19:27:15,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:27:16,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 19:27:21,809 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74017.664
2025-03-09 19:27:21,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:27:22,963 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 19:27:28,050 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74069.656
2025-03-09 19:27:28,050 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:27:29,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 19:27:34,362 :: INFO :: evodenss.train.trainers :: [2051] -- [5.11s] TRAIN epoch 34 -- loss: tensor([73732.5703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:27:34,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73732.57
2025-03-09 19:27:34,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:27:35,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 19:27:40,666 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73143.812
2025-03-09 19:27:40,667 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:27:41,856 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 19:27:46,953 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73666.711
2025-03-09 19:27:46,953 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:27:48,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 19:27:53,505 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72970.453
2025-03-09 19:27:53,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:27:54,722 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 19:27:59,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73131.453
2025-03-09 19:27:59,857 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:28:01,085 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 19:28:06,242 :: INFO :: evodenss.train.trainers :: [2051] -- [5.15s] TRAIN epoch 39 -- loss: tensor([73299.0078], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:28:06,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73299.008
2025-03-09 19:28:06,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:28:07,422 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 19:28:12,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73135.102
2025-03-09 19:28:12,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:28:13,682 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 19:28:18,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72830.961
2025-03-09 19:28:18,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:28:20,253 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 19:28:25,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72313.695
2025-03-09 19:28:25,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:28:26,600 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 19:28:31,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72575.703
2025-03-09 19:28:31,745 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:28:32,954 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 19:28:38,118 :: INFO :: evodenss.train.trainers :: [2051] -- [5.16s] TRAIN epoch 44 -- loss: tensor([72043.0703], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:28:38,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72043.07
2025-03-09 19:28:38,118 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:28:39,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 19:28:44,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71944.055
2025-03-09 19:28:44,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:28:45,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 19:28:51,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71981.906
2025-03-09 19:28:51,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:28:52,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 19:28:57,387 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70527.484
2025-03-09 19:28:57,387 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:28:58,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 19:29:03,633 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71155.922
2025-03-09 19:29:03,634 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:29:04,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 19:29:09,928 :: INFO :: evodenss.train.trainers :: [2051] -- [5.08s] TRAIN epoch 49 -- loss: tensor([71021.9219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:29:09,929 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71021.922
2025-03-09 19:29:09,929 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:29:11,142 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 19:29:16,264 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71044.312
2025-03-09 19:29:16,265 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:29:17,469 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 19:29:22,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70181.492
2025-03-09 19:29:22,815 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:29:24,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 19:29:29,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70789.672
2025-03-09 19:29:29,094 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:29:30,267 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 19:29:35,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69568.711
2025-03-09 19:29:35,381 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:29:36,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 19:29:41,729 :: INFO :: evodenss.train.trainers :: [2051] -- [5.13s] TRAIN epoch 54 -- loss: tensor([70581.3125], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:29:41,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70581.312
2025-03-09 19:29:41,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:29:42,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 19:29:48,087 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70019.484
2025-03-09 19:29:48,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:29:49,461 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 19:29:54,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69713.711
2025-03-09 19:29:54,621 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:29:55,844 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 19:30:00,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69809.438
2025-03-09 19:30:00,931 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:30:02,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 19:30:07,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69841.25
2025-03-09 19:30:07,209 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:30:08,420 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 19:30:13,546 :: INFO :: evodenss.train.trainers :: [2051] -- [5.12s] TRAIN epoch 59 -- loss: tensor([69760.5625], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:30:13,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69760.562
2025-03-09 19:30:13,547 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:30:14,748 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 19:30:20,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69696.953
2025-03-09 19:30:20,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:30:21,278 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 19:30:26,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69502.578
2025-03-09 19:30:26,394 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:30:27,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 19:30:32,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68936.492
2025-03-09 19:30:32,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:30:33,886 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 19:30:38,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68161.625
2025-03-09 19:30:38,997 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:30:40,151 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 19:30:45,234 :: INFO :: evodenss.train.trainers :: [2051] -- [5.08s] TRAIN epoch 64 -- loss: tensor([68561.2578], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:30:45,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68561.258
2025-03-09 19:30:45,235 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:30:46,435 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 19:30:51,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68703.258
2025-03-09 19:30:51,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:30:52,978 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 19:30:58,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68430.43
2025-03-09 19:30:58,096 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:30:59,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 19:31:04,485 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69119.609
2025-03-09 19:31:04,486 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:31:05,692 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 19:31:10,806 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67528.078
2025-03-09 19:31:10,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:31:12,020 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 19:31:17,122 :: INFO :: evodenss.train.trainers :: [2051] -- [5.1s] TRAIN epoch 69 -- loss: tensor([67898.4844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:31:17,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67898.484
2025-03-09 19:31:17,122 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:31:18,342 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 19:31:23,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67988.328
2025-03-09 19:31:23,648 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:31:24,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 19:31:30,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68066.516
2025-03-09 19:31:30,014 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:31:31,246 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 19:31:36,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67587.641
2025-03-09 19:31:36,373 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:31:37,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 19:31:42,727 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68000.039
2025-03-09 19:31:42,728 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:31:43,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 19:31:49,069 :: INFO :: evodenss.train.trainers :: [2051] -- [5.15s] TRAIN epoch 74 -- loss: tensor([67237.0859], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:31:49,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67237.086
2025-03-09 19:31:49,069 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:31:50,494 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 19:31:55,584 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67971.984
2025-03-09 19:31:55,584 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:31:56,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 19:32:01,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67153.375
2025-03-09 19:32:01,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:32:03,148 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 19:32:08,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67046.883
2025-03-09 19:32:08,268 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:32:09,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 19:32:14,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67282.117
2025-03-09 19:32:14,599 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:32:15,785 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 19:32:21,079 :: INFO :: evodenss.train.trainers :: [2051] -- [5.29s] TRAIN epoch 79 -- loss: tensor([66241.5859], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:32:21,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66241.586
2025-03-09 19:32:21,080 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:32:22,279 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 19:32:27,352 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66244.883
2025-03-09 19:32:27,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:32:28,556 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 19:32:33,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66575.398
2025-03-09 19:32:33,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:32:34,891 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 19:32:39,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65436.395
2025-03-09 19:32:39,999 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:32:41,207 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 19:32:46,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66646.805
2025-03-09 19:32:46,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:32:47,542 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 19:32:52,839 :: INFO :: evodenss.train.trainers :: [2051] -- [5.3s] TRAIN epoch 84 -- loss: tensor([65845.9844], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:32:52,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65845.984
2025-03-09 19:32:52,839 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:32:54,074 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 19:32:59,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65473.453
2025-03-09 19:32:59,170 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:33:00,375 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 19:33:05,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66103.719
2025-03-09 19:33:05,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:33:06,673 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 19:33:11,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65886.102
2025-03-09 19:33:11,801 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:33:13,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 19:33:18,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65154.996
2025-03-09 19:33:18,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:33:19,432 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 19:33:24,587 :: INFO :: evodenss.train.trainers :: [2051] -- [5.15s] TRAIN epoch 89 -- loss: tensor([65008.9766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:33:24,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65008.977
2025-03-09 19:33:24,587 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:33:25,822 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 19:33:30,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65904.086
2025-03-09 19:33:30,934 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:33:32,152 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 19:33:37,247 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64906.191
2025-03-09 19:33:37,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:33:38,458 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 19:33:43,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65160.047
2025-03-09 19:33:43,598 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:33:44,790 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 19:33:50,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65048.004
2025-03-09 19:33:50,104 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:33:51,320 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 19:33:56,423 :: INFO :: evodenss.train.trainers :: [2051] -- [5.1s] TRAIN epoch 94 -- loss: tensor([64285.0586], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:33:56,423 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64285.059
2025-03-09 19:33:56,423 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:33:57,578 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 19:34:02,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65068.379
2025-03-09 19:34:02,657 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:34:03,819 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 19:34:08,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64893.145
2025-03-09 19:34:08,800 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:34:09,956 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 19:34:14,992 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64949.828
2025-03-09 19:34:14,993 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:34:16,162 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 19:34:21,389 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64807.02
2025-03-09 19:34:21,390 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:34:22,549 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 19:34:27,595 :: INFO :: evodenss.train.trainers :: [2051] -- [5.04s] TRAIN epoch 99 -- loss: tensor([65125.6211], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:34:27,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65125.621
2025-03-09 19:34:27,595 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:34:29,921 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 3 fitness: 3856.12061
2025-03-09 19:34:29,921 :: INFO :: evodenss.evolution.engine :: [2051] -- Selecting the fittest individual
2025-03-09 19:34:29,921 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Parent: idx: 3, id: 3
2025-03-09 19:34:29,921 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- Training times: [2000, 1000, 1000, 1000]
2025-03-09 19:34:29,921 :: INFO :: evodenss.evolution.operators.selection :: [2051] -- ids: [0, 1, 2, 3]
2025-03-09 19:34:29,925 :: INFO :: evodenss.evolution.engine :: [2051] -- Fitnesses: [4202.90137, 44608.84766, 4298.65723, 3856.12061]
2025-03-09 19:34:30,238 :: INFO :: evodenss.evolution.engine :: [2051] -- Generation best test fitness: tensor([20823.6406], device='cuda:0')
2025-03-09 19:34:30,238 :: INFO :: evodenss.evolution.engine :: [2051] -- Best fitness of generation 16: 3856.12061
2025-03-09 19:34:30,238 :: INFO :: evodenss.evolution.engine :: [2051] -- Best overall fitness: 3575.71436



2025-03-09 19:34:30,382 :: INFO :: evodenss.evolution.engine :: [2051] -- Performing generation: 17
2025-03-09 19:34:30,382 :: INFO :: evodenss.evolution.engine :: [2051] -- Applying mutation operators
2025-03-09 19:34:30,394 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a layer removed from Module 0: features; Position: 5
2025-03-09 19:34:30,395 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 19:34:30,396 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 6
2025-03-09 19:34:30,397 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 8
2025-03-09 19:34:30,398 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 19:34:30,398 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 19:34:30,399 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 19:34:30,399 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 1 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 19:34:30,403 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have an extra layer at Module 0: features; Position: 10. Reused?: False
2025-03-09 19:34:30,404 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 19:34:30,405 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 19:34:30,405 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 7
2025-03-09 19:34:30,406 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 19:34:30,407 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 19:34:30,407 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 19:34:30,408 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 19:34:30,409 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 2 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 19:34:30,412 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have an extra layer at Module 0: features; Position: 15. Reused?: False
2025-03-09 19:34:30,413 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 4
2025-03-09 19:34:30,414 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 5
2025-03-09 19:34:30,414 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 9
2025-03-09 19:34:30,415 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 10
2025-03-09 19:34:30,416 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 11
2025-03-09 19:34:30,416 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 12
2025-03-09 19:34:30,417 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 13
2025-03-09 19:34:30,418 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Module 0 Position: 15
2025-03-09 19:34:30,418 :: INFO :: evodenss.evolution.operators.mutation_tracker :: [2051] -- Individual 3 is going to have a DSGE mutation on Non-topological component: learning
2025-03-09 19:34:30,421 :: INFO :: evodenss.evolution.engine :: [2051] -- mutation has been performed
2025-03-09 19:34:30,425 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 0 for 1000 secs
2025-03-09 19:34:30,426 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 
layer6: :deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 
layer7: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 
layer8: :conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 
layer9: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 
layer10: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :deconv1d out_channels:25 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 
layer12: :conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 
layer13: :conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 
layer14: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 
layer15: :fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:8 epochs:100
2025-03-09 19:34:30,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 19:34:30,437 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 19:34:35,462 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 366491.125
2025-03-09 19:34:35,462 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:34:36,610 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 1
2025-03-09 19:34:41,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 199726.656
2025-03-09 19:34:41,661 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:34:42,823 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 2
2025-03-09 19:34:47,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 159689.047
2025-03-09 19:34:47,874 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:34:49,033 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 3
2025-03-09 19:34:54,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 110911.68
2025-03-09 19:34:54,284 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:34:55,430 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 4
2025-03-09 19:35:00,447 :: INFO :: evodenss.train.trainers :: [2051] -- [5.01s] TRAIN epoch 4 -- loss: tensor([102865.5938], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:35:00,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 102865.594
2025-03-09 19:35:00,447 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:35:01,609 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 5
2025-03-09 19:35:06,644 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 98436.219
2025-03-09 19:35:06,644 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:35:07,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 6
2025-03-09 19:35:12,812 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 95718.602
2025-03-09 19:35:12,813 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:35:14,012 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 7
2025-03-09 19:35:19,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 93808.219
2025-03-09 19:35:19,036 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:35:20,431 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 8
2025-03-09 19:35:25,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 91522.859
2025-03-09 19:35:25,538 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:35:26,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 9
2025-03-09 19:35:31,723 :: INFO :: evodenss.train.trainers :: [2051] -- [5.0s] TRAIN epoch 9 -- loss: tensor([90536.7656], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:35:31,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 90536.766
2025-03-09 19:35:31,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:35:32,903 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 10
2025-03-09 19:35:37,924 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 88660.68
2025-03-09 19:35:37,925 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:35:39,103 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 11
2025-03-09 19:35:44,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86683.445
2025-03-09 19:35:44,224 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:35:45,407 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 12
2025-03-09 19:35:50,659 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 86518.625
2025-03-09 19:35:50,660 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:35:51,837 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 13
2025-03-09 19:35:56,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84878.234
2025-03-09 19:35:56,949 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:35:58,158 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 14
2025-03-09 19:36:03,273 :: INFO :: evodenss.train.trainers :: [2051] -- [5.11s] TRAIN epoch 14 -- loss: tensor([84245.2109], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:36:03,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 84245.211
2025-03-09 19:36:03,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:36:04,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 15
2025-03-09 19:36:09,488 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83303.078
2025-03-09 19:36:09,489 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:36:10,670 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 16
2025-03-09 19:36:15,724 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 82609.773
2025-03-09 19:36:15,725 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:36:16,899 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 17
2025-03-09 19:36:22,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 83376.633
2025-03-09 19:36:22,200 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:36:23,405 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 18
2025-03-09 19:36:28,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 81685.984
2025-03-09 19:36:28,496 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:36:29,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 19
2025-03-09 19:36:34,842 :: INFO :: evodenss.train.trainers :: [2051] -- [5.16s] TRAIN epoch 19 -- loss: tensor([80228.3516], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:36:34,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 80228.352
2025-03-09 19:36:34,842 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:36:36,044 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 20
2025-03-09 19:36:41,125 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 79203.25
2025-03-09 19:36:41,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:36:42,309 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 21
2025-03-09 19:36:47,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78699.438
2025-03-09 19:36:47,428 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:36:48,606 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 22
2025-03-09 19:36:54,028 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78134.883
2025-03-09 19:36:54,029 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:36:55,218 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 23
2025-03-09 19:37:00,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 78203.344
2025-03-09 19:37:00,315 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:37:01,512 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 24
2025-03-09 19:37:06,624 :: INFO :: evodenss.train.trainers :: [2051] -- [5.11s] TRAIN epoch 24 -- loss: tensor([77185.4609], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:37:06,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77185.461
2025-03-09 19:37:06,624 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:37:07,814 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 25
2025-03-09 19:37:12,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 77280.961
2025-03-09 19:37:12,920 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:37:14,115 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 26
2025-03-09 19:37:19,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76310.43
2025-03-09 19:37:19,273 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:37:20,636 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 27
2025-03-09 19:37:25,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 76252.516
2025-03-09 19:37:25,775 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:37:26,977 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 28
2025-03-09 19:37:32,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75522.992
2025-03-09 19:37:32,005 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:37:33,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 29
2025-03-09 19:37:38,277 :: INFO :: evodenss.train.trainers :: [2051] -- [5.08s] TRAIN epoch 29 -- loss: tensor([75213.5391], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:37:38,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75213.539
2025-03-09 19:37:38,277 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:37:39,478 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 30
2025-03-09 19:37:44,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75464.789
2025-03-09 19:37:44,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:37:45,770 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 31
2025-03-09 19:37:51,075 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 75552.0
2025-03-09 19:37:51,076 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:37:52,279 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 32
2025-03-09 19:37:57,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74559.086
2025-03-09 19:37:57,400 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:37:58,588 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 33
2025-03-09 19:38:03,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74083.797
2025-03-09 19:38:03,711 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:38:04,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 34
2025-03-09 19:38:10,016 :: INFO :: evodenss.train.trainers :: [2051] -- [5.11s] TRAIN epoch 34 -- loss: tensor([74029.3906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:38:10,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 74029.391
2025-03-09 19:38:10,017 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:38:11,212 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 35
2025-03-09 19:38:16,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73616.703
2025-03-09 19:38:16,329 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:38:17,524 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 36
2025-03-09 19:38:22,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73521.773
2025-03-09 19:38:22,807 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:38:24,021 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 37
2025-03-09 19:38:29,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73139.688
2025-03-09 19:38:29,112 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:38:30,318 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 38
2025-03-09 19:38:35,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73023.102
2025-03-09 19:38:35,426 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:38:36,601 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 39
2025-03-09 19:38:41,677 :: INFO :: evodenss.train.trainers :: [2051] -- [5.07s] TRAIN epoch 39 -- loss: tensor([72490.4766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:38:41,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72490.477
2025-03-09 19:38:41,678 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:38:42,884 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 40
2025-03-09 19:38:47,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 73471.602
2025-03-09 19:38:47,965 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:38:49,234 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 41
2025-03-09 19:38:54,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72945.898
2025-03-09 19:38:54,506 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:38:55,731 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 42
2025-03-09 19:39:00,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72530.492
2025-03-09 19:39:00,817 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:39:02,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 43
2025-03-09 19:39:07,126 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 72167.398
2025-03-09 19:39:07,127 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:39:08,343 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 44
2025-03-09 19:39:13,450 :: INFO :: evodenss.train.trainers :: [2051] -- [5.1s] TRAIN epoch 44 -- loss: tensor([71622.4531], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:39:13,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71622.453
2025-03-09 19:39:13,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:39:14,683 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 45
2025-03-09 19:39:20,050 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71534.125
2025-03-09 19:39:20,050 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:39:21,255 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 46
2025-03-09 19:39:26,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70985.016
2025-03-09 19:39:26,321 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:39:27,548 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 47
2025-03-09 19:39:32,628 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69896.977
2025-03-09 19:39:32,628 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:39:33,865 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 48
2025-03-09 19:39:39,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70693.938
2025-03-09 19:39:39,000 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:39:40,228 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 49
2025-03-09 19:39:45,330 :: INFO :: evodenss.train.trainers :: [2051] -- [5.1s] TRAIN epoch 49 -- loss: tensor([70143.4219], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:39:45,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 70143.422
2025-03-09 19:39:45,330 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:39:46,552 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 50
2025-03-09 19:39:51,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69858.406
2025-03-09 19:39:51,867 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:39:53,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 51
2025-03-09 19:39:58,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 71250.102
2025-03-09 19:39:58,175 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:39:59,406 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 52
2025-03-09 19:40:04,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69977.312
2025-03-09 19:40:04,534 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:40:05,760 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 53
2025-03-09 19:40:10,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69121.969
2025-03-09 19:40:10,873 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:40:12,095 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 54
2025-03-09 19:40:17,248 :: INFO :: evodenss.train.trainers :: [2051] -- [5.15s] TRAIN epoch 54 -- loss: tensor([69457.8281], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:40:17,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69457.828
2025-03-09 19:40:17,248 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:40:18,474 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 55
2025-03-09 19:40:23,846 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69207.43
2025-03-09 19:40:23,847 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:40:25,077 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 56
2025-03-09 19:40:30,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68476.234
2025-03-09 19:40:30,196 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:40:31,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 57
2025-03-09 19:40:36,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68749.578
2025-03-09 19:40:36,530 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:40:37,771 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 58
2025-03-09 19:40:42,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69167.688
2025-03-09 19:40:42,826 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:40:44,046 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 59
2025-03-09 19:40:49,183 :: INFO :: evodenss.train.trainers :: [2051] -- [5.13s] TRAIN epoch 59 -- loss: tensor([68589.9766], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:40:49,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68589.977
2025-03-09 19:40:49,183 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:40:50,613 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 60
2025-03-09 19:40:55,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68982.914
2025-03-09 19:40:55,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:40:56,916 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 61
2025-03-09 19:41:01,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68380.594
2025-03-09 19:41:01,950 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:41:03,105 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 62
2025-03-09 19:41:08,155 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67908.141
2025-03-09 19:41:08,156 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:41:09,314 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 63
2025-03-09 19:41:14,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67917.297
2025-03-09 19:41:14,383 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:41:15,550 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 64
2025-03-09 19:41:20,827 :: INFO :: evodenss.train.trainers :: [2051] -- [5.28s] TRAIN epoch 64 -- loss: tensor([69050.9062], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:41:20,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 69050.906
2025-03-09 19:41:20,827 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:41:21,991 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 65
2025-03-09 19:41:27,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67797.258
2025-03-09 19:41:27,078 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:41:28,254 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 66
2025-03-09 19:41:33,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68533.312
2025-03-09 19:41:33,322 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:41:34,463 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 67
2025-03-09 19:41:39,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 68154.859
2025-03-09 19:41:39,529 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:41:40,679 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 68
2025-03-09 19:41:45,738 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66713.625
2025-03-09 19:41:45,739 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:41:46,896 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 69
2025-03-09 19:41:52,157 :: INFO :: evodenss.train.trainers :: [2051] -- [5.26s] TRAIN epoch 69 -- loss: tensor([66816.1484], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:41:52,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66816.148
2025-03-09 19:41:52,157 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:41:53,287 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 70
2025-03-09 19:41:58,339 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67804.852
2025-03-09 19:41:58,339 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:41:59,476 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 71
2025-03-09 19:42:04,593 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66844.289
2025-03-09 19:42:04,593 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:42:05,749 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 72
2025-03-09 19:42:10,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66651.523
2025-03-09 19:42:10,805 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:42:11,962 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 73
2025-03-09 19:42:17,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66936.484
2025-03-09 19:42:17,011 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:42:18,188 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 74
2025-03-09 19:42:23,408 :: INFO :: evodenss.train.trainers :: [2051] -- [5.22s] TRAIN epoch 74 -- loss: tensor([66676.1797], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:42:23,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66676.18
2025-03-09 19:42:23,409 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:42:24,569 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 75
2025-03-09 19:42:29,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66418.883
2025-03-09 19:42:29,656 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:42:30,810 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 76
2025-03-09 19:42:35,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67176.391
2025-03-09 19:42:35,835 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:42:36,990 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 77
2025-03-09 19:42:42,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 67267.445
2025-03-09 19:42:42,016 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:42:43,146 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 78
2025-03-09 19:42:48,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65750.023
2025-03-09 19:42:48,119 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:42:49,410 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 79
2025-03-09 19:42:54,501 :: INFO :: evodenss.train.trainers :: [2051] -- [5.09s] TRAIN epoch 79 -- loss: tensor([66265.6250], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:42:54,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66265.625
2025-03-09 19:42:54,501 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:42:55,729 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 80
2025-03-09 19:43:00,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66564.492
2025-03-09 19:43:00,744 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:43:01,945 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 81
2025-03-09 19:43:06,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 66392.43
2025-03-09 19:43:06,986 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:43:08,123 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 82
2025-03-09 19:43:13,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65980.852
2025-03-09 19:43:13,143 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:43:14,288 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 83
2025-03-09 19:43:19,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65927.156
2025-03-09 19:43:19,495 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:43:20,751 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 84
2025-03-09 19:43:25,761 :: INFO :: evodenss.train.trainers :: [2051] -- [5.01s] TRAIN epoch 84 -- loss: tensor([65523.2500], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:43:25,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65523.25
2025-03-09 19:43:25,761 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:43:26,909 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 85
2025-03-09 19:43:31,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65608.703
2025-03-09 19:43:31,982 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:43:33,132 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 86
2025-03-09 19:43:38,198 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65265.992
2025-03-09 19:43:38,199 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:43:39,362 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 87
2025-03-09 19:43:44,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65462.973
2025-03-09 19:43:44,376 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:43:45,551 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 88
2025-03-09 19:43:50,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64841.504
2025-03-09 19:43:50,834 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:43:52,013 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 89
2025-03-09 19:43:57,071 :: INFO :: evodenss.train.trainers :: [2051] -- [5.06s] TRAIN epoch 89 -- loss: tensor([64870.1211], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:43:57,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64870.121
2025-03-09 19:43:57,072 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:43:58,234 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 90
2025-03-09 19:44:03,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65677.117
2025-03-09 19:44:03,274 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:44:04,451 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 91
2025-03-09 19:44:09,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64983.285
2025-03-09 19:44:09,477 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:44:10,645 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 92
2025-03-09 19:44:15,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65628.359
2025-03-09 19:44:15,755 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:44:16,918 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 93
2025-03-09 19:44:22,184 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64842.566
2025-03-09 19:44:22,185 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:44:23,353 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 94
2025-03-09 19:44:28,417 :: INFO :: evodenss.train.trainers :: [2051] -- [5.06s] TRAIN epoch 94 -- loss: tensor([65027.3906], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:44:28,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65027.391
2025-03-09 19:44:28,418 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:44:29,611 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 95
2025-03-09 19:44:34,663 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 65354.113
2025-03-09 19:44:34,663 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:44:35,821 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 96
2025-03-09 19:44:40,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64336.062
2025-03-09 19:44:40,895 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:44:42,038 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 97
2025-03-09 19:44:47,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64331.883
2025-03-09 19:44:47,088 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:44:48,242 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 98
2025-03-09 19:44:53,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64781.359
2025-03-09 19:44:53,517 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:44:54,676 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 99
2025-03-09 19:44:59,681 :: INFO :: evodenss.train.trainers :: [2051] -- [5.0s] TRAIN epoch 99 -- loss: tensor([64014.5898], device='cuda:0', grad_fn=<AddBackward0>)
2025-03-09 19:44:59,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 64014.59
2025-03-09 19:44:59,681 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
2025-03-09 19:45:02,006 :: INFO :: evodenss.evolution.engine :: [2051] -- Individual 0 fitness: 4539.55566
2025-03-09 19:45:02,011 :: INFO :: evodenss.evolution.individual :: [2051] -- -----> Starting evaluation for individual 1 for 1000 secs
2025-03-09 19:45:02,012 :: INFO :: evodenss.networks.evaluators :: [2051] -- layer-1: 
layer0: :punctual_mlp input:-1 
layer1: :punctual_mlp input:-1 
layer2: :punctual_mlp input:-1 
layer3: :punctual_mlp input:-1 
layer4: :deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 
layer5: :conv1d out_channels:105 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 
layer6: :conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 
layer7: :conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 
layer8: :conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 
layer9: :deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 
layer10: :conv1d out_channels:11 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 
layer11: :conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 
layer12: :conv1d out_channels:111 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 
layer13: :conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 
layer14: :fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:5 epochs:100
2025-03-09 19:45:02,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Initiating supervised training
2025-03-09 19:45:02,023 :: DEBUG :: evodenss.train.trainers :: [2051] -- Starting Downstream Epoch 0
2025-03-09 19:45:08,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- Loss: 329413.938
2025-03-09 19:45:08,973 :: DEBUG :: evodenss.train.trainers :: [2051] -- =============================================================
