id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	3794.05249		582726	15	-1	372.0499143600464	{'train_loss': [407441.281, 210198.438, 189037.938, 168418.922, 154485.109, 143029.719, 131676.109, 123712.289, 117022.68, 110395.102, 109048.125, 104576.891, 102709.508, 100652.164, 101048.125, 97612.062, 97014.047, 95737.32, 95159.258, 94557.211, 94405.242, 93445.398, 91178.523, 90527.812, 89183.008, 88403.953, 88713.102, 87747.203, 87415.156, 86305.977, 85827.602, 85486.633, 84920.648, 83532.117, 83507.773, 83430.25, 83067.766, 82918.852, 81983.82, 81604.516, 85500.047, 80196.047, 80161.82, 79204.805, 79456.828, 79575.789, 78659.633, 78276.297, 77196.828, 78397.195, 76573.852, 76692.453, 75421.586, 75709.484, 74974.844, 74516.812, 74602.609, 74402.164, 74849.734, 74233.703, 74246.539, 73236.367, 73456.25, 72548.445, 72846.234, 73286.438, 72217.242, 71455.547, 71383.82, 71025.828, 71824.219, 70892.031, 70819.445, 70337.539, 70471.914, 69879.789, 70808.523, 68976.031, 69707.844, 69738.547, 70451.406, 69210.82, 70082.25, 69192.984, 68987.867, 68380.797, 69288.859, 68543.891, 68432.812, 67965.555, 66774.867, 67808.836, 67553.086, 67326.797, 67416.344, 66692.453, 68529.578, 65998.602, 66909.469, 67505.594], 'val_loss': [2327.328, 3244.512, 2164.424, 1835.429, 1869.63, 1740.198, 1568.895, 1622.792, 1592.694, 1569.976, 1449.951, 1441.018, 1428.591, 1376.751, 1414.422, 1518.744, 1357.303, 1287.599, 1471.732, 1388.756, 1376.082, 1300.84, 1409.239, 1273.887, 1370.696, 1437.776, 1386.203, 1235.394, 1289.791, 1324.12, 1224.082, 1240.871, 1203.772, 1152.771, 1131.933, 1149.713, 1240.868, 1267.547, 1210.386, 1132.99, 1115.789, 1272.828, 1063.553, 1045.071, 1223.695, 1046.782, 1210.767, 1040.228, 1215.895, 1055.545, 1018.55, 1105.279, 1032.49, 1254.47, 1038.017, 1013.758, 1145.107, 1087.171, 1153.189, 986.598, 1060.502, 1061.643, 966.964, 1057.932, 1006.822, 1151.419, 1091.1, 999.847, 982.87, 1028.419, 1043.854, 954.024, 1064.939, 1003.459, 1055.559, 1067.297, 1050.632, 1075.744, 984.719, 1019.0, 980.732, 939.206, 946.014, 933.709, 1019.811, 954.529, 925.986, 938.941, 950.235, 991.684, 947.547, 927.628, 892.517, 964.004, 959.725, 924.359, 919.561, 930.887, 915.264, 893.602]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4233.76758		532390	14	-1	357.6869788169861	{'train_loss': [433636.75, 223179.781, 177967.766, 154436.219, 138757.828, 129847.609, 121573.383, 115496.906, 111087.289, 108321.516, 105842.367, 103443.984, 101973.961, 99838.141, 98476.75, 98160.289, 96615.828, 95665.867, 94183.891, 94275.172, 92525.453, 90456.812, 91985.047, 89941.938, 88707.742, 88870.93, 88416.898, 87795.469, 86735.203, 86267.188, 84994.305, 84906.672, 83997.656, 83955.484, 83419.836, 82943.188, 82955.812, 81674.969, 81750.805, 81046.57, 81490.75, 80917.875, 80802.141, 80062.109, 80830.461, 79306.234, 78210.633, 79389.461, 77894.211, 78188.586, 78398.859, 77110.0, 77555.117, 77430.352, 77541.992, 77228.18, 76859.266, 76199.227, 75708.703, 76202.891, 76474.0, 75301.961, 75803.773, 74846.672, 74987.094, 74685.75, 75246.383, 74306.93, 74566.508, 73744.562, 73144.938, 73639.773, 73325.555, 73524.812, 72798.297, 72453.18, 72862.328, 73473.391, 72815.25, 71655.211, 71431.211, 71305.422, 71096.852, 71855.773, 72182.438, 71334.836, 71213.156, 71268.766, 70886.602, 70543.773, 70717.688, 70364.914, 70269.734, 70381.125, 70033.156, 69710.266, 70894.484, 69407.898, 69559.0, 69618.305], 'val_loss': [4881.417, 2068.025, 2083.408, 1827.428, 1641.975, 1701.262, 1572.829, 1658.617, 1626.224, 1581.712, 1556.129, 1509.818, 1420.11, 1358.513, 1347.377, 1357.694, 1371.539, 1335.896, 1405.911, 1377.401, 1293.682, 1273.145, 1326.129, 1225.237, 1095.284, 1237.238, 1224.274, 1166.019, 1298.617, 1262.104, 1228.379, 1161.329, 1177.337, 1088.174, 1177.634, 1155.239, 1110.979, 1072.548, 1232.169, 1101.606, 1129.546, 1047.969, 1055.898, 1059.157, 1080.956, 1094.22, 1100.417, 1001.617, 1006.114, 1074.133, 1061.762, 1148.529, 1070.38, 1065.794, 1048.034, 1038.117, 1084.422, 1105.122, 1020.637, 1082.287, 1026.435, 1008.753, 1050.92, 1015.4, 1046.435, 1023.641, 1005.634, 999.896, 989.514, 1022.692, 1027.39, 951.045, 997.941, 992.912, 968.059, 991.983, 994.191, 945.996, 1014.225, 934.604, 995.665, 1018.044, 992.826, 973.327, 970.49, 971.374, 996.148, 963.192, 948.243, 1001.517, 974.927, 969.071, 959.365, 927.083, 975.497, 988.331, 972.532, 952.463, 938.453, 1014.626]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:84 kernel_size:5 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.15325615523249037 alpha:0.9472563778469602 weight_decay:3.033766983682965e-05 batch_size:32 epochs:100	100	1000	True	74907.34375		614382	15	-1	374.3414719104767	{'train_loss': [887237.312, 804482.438, 804482.438, 1319543.5, 804482.438, 804482.438, 804482.438, 804482.438, 1708279.375, 804482.438, 804482.438, 804482.438, 804482.438, 804466.688, 1324179.375, 804482.438, 804482.438, 804482.438, 804482.438, 1247429.25, 811087.562, 804557.938, 804482.438, 3409000.75, 815686.125, 804482.438, 1165307.25, 804482.438, 804482.438, 910146.062, 804482.438, 804482.438, 804482.438, 826912.188, 1412636.5, 804482.438, 804482.438, 804482.438, 2050709.0, 872656.188, 1637625.0, 804482.438, 804482.438, 804482.438, 1430517.75, 985223.688, 1048689.625, 804482.438, 804482.438, 1116363.5, 865119.062, 892455.812, 1000706.812, 805044.5, 951468.438, 955496.562, 1076685.25, 903096.562, 879421.375, 849515.562, 904048.562, 944736.125, 922596.812, 952520.438, 913932.25, 834224.25, 851156.75, 934307.875, 964010.688, 890826.438, 922029.0, 881789.938, 872742.125, 835388.062, 855122.062, 847988.312, 1034154.75, 883384.438, 856273.25, 878125.562, 930978.875, 863062.562, 873036.312, 921188.812, 928581.375, 877782.688, 864855.375, 884713.438, 846091.75, 845957.5, 956934.812, 944923.875, 863438.438, 942476.312, 898389.812, 942996.688, 847626.438, 827020.75, 898586.0, 878824.125], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.451, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 36826.523, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 15065.124, 11044.443, 15249.545, 11044.455, 11044.455, 11044.456, 23724.918, 11044.456, 11044.455, 11044.456, 11044.456, 11044.455, 11044.456, 11021.553, 14245.071, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 20984.129, 12580.936, 11044.456, 11044.455, 11044.456, 11044.456, 11044.456, 11044.262, 29694.176, 11044.453, 12207.463, 11044.455, 11044.456, 11044.455, 11044.456, 11044.453, 11044.455, 11525.355, 11038.752, 11044.456, 11044.456, 11044.403, 11044.456, 11044.195, 12478.64, 11044.455, 11044.441, 11044.456, 11044.456, 11044.445, 18633.588]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:33 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:33 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:88 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.21392777113391248 beta1:0.9520928483562265 beta2:0.996180483672492 weight_decay:0.0001621548771194498 batch_size:32 epochs:100	100	1000	True	44624.05078		2204795	13	-1	294.089786529541	{'train_loss': [830282.312, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804501.688, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 2014073.125, 1630881.25, 804482.438, 988490.938, 1009376.062, 1052831.75, 922801.562, 845349.312, 943182.688, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804549.312, 804857.188, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804484.812, 804482.438, 804502.812, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804595.438, 875623.062, 804482.438, 804482.438, 804482.438, 808191.312, 804482.438, 923409.562, 804525.688, 804482.438, 804482.438, 804482.438, 827446.688, 907474.062, 910992.625, 804482.438, 811740.188, 812163.812, 3410328.25, 1132731.75, 944342.438, 1419886.625, 895964.812, 899645.188, 804482.438, 856862.062, 804506.938, 824987.438, 934243.812], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 18537.48, 11044.456, 11044.456, 11044.456, 28122.252, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 934103.625, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 24268.01, 11044.456]}	100	100	True
