id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4002.26025		452251	14	-1	272.5034284591675	{'train_loss': [285469.062, 169993.906, 154926.984, 144834.562, 136458.953, 130787.516, 124687.758, 120632.664, 117284.523, 114407.195, 112171.227, 109617.641, 107715.328, 106042.508, 104984.25, 103542.664, 102676.75, 100861.695, 99683.18, 98196.664, 98020.648, 96884.938, 95788.617, 95393.617, 94715.766, 94546.523, 93202.664, 92492.219, 92373.234, 91305.852, 90111.039, 90070.062, 89951.891, 89945.352, 88767.781, 88994.508, 88165.562, 87207.297, 88563.992, 87426.914, 86931.094, 86864.594, 86550.094, 85498.773, 85288.07, 85075.234, 84918.203, 85103.195, 83978.602, 84461.391, 83706.227, 83799.391, 82790.305, 82168.898, 82878.172, 83057.617, 82962.977, 81604.719, 81900.82, 81831.867, 81234.039, 80860.258, 81606.609, 80212.969, 80826.297, 80052.875, 80304.438, 79269.914, 80385.234, 79879.461, 79808.664, 79537.031, 79900.445, 78319.852, 79907.023, 78181.875, 79216.422, 78505.539, 78873.117, 77562.297, 78902.344, 78054.211, 77795.211, 77780.781, 77697.977, 77117.469, 76850.508, 77478.812, 76899.977, 76325.953, 77083.953, 76734.672, 76470.922, 76239.305, 76647.961, 77565.398, 76260.367, 75995.922, 76505.078, 75754.398], 'val_loss': [2019.109, 1900.332, 1893.816, 1918.426, 1810.045, 1614.136, 1615.895, 1641.22, 1860.743, 1526.095, 1492.454, 1518.115, 1488.936, 1479.631, 1604.883, 1531.398, 1479.798, 1333.578, 1400.575, 1334.364, 1536.742, 1291.23, 1384.057, 1228.977, 1323.637, 1256.182, 1281.528, 1282.606, 1234.253, 1273.01, 1183.532, 1253.683, 1193.236, 1252.578, 1258.842, 1133.504, 1139.215, 1106.864, 1202.834, 1158.343, 1111.473, 1126.437, 1134.277, 1192.366, 1162.682, 1089.258, 1088.499, 1126.461, 1197.398, 1144.338, 1104.644, 1134.597, 1130.798, 1055.019, 1111.352, 1132.542, 1112.753, 1158.708, 1091.611, 1075.856, 1072.792, 1082.692, 1092.147, 1063.985, 1083.484, 1069.913, 1095.217, 1049.541, 1062.494, 1082.351, 1102.339, 1128.674, 1068.796, 1096.347, 1065.568, 1079.599, 1103.369, 1070.827, 1056.09, 1037.3, 1021.009, 1035.078, 1068.372, 1013.867, 1020.98, 1030.622, 1038.823, 1095.429, 1006.941, 1055.843, 1071.651, 1004.286, 1008.892, 1015.985, 1093.836, 1066.902, 1020.31, 1009.234, 1014.241, 1041.562]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	3818.04004		428075	14	-1	275.87739610671997	{'train_loss': [390863.938, 224557.25, 157982.938, 142425.406, 132839.812, 126447.32, 123616.344, 119047.812, 116400.422, 113703.75, 111409.742, 107686.938, 105854.703, 102874.641, 101172.695, 99822.578, 97342.93, 96461.68, 94700.648, 92756.352, 91717.484, 91125.883, 91206.648, 90160.188, 88079.984, 89188.906, 86819.18, 87515.484, 85958.406, 85984.555, 84957.422, 84716.094, 84346.203, 84074.797, 83185.062, 82966.516, 83136.594, 82510.258, 82200.5, 81171.969, 82113.266, 80628.938, 80961.828, 79863.211, 80055.672, 78747.922, 79318.523, 79718.938, 78552.172, 78270.688, 78484.125, 77946.609, 77092.023, 77461.164, 77785.039, 77085.398, 76915.961, 76158.07, 76390.586, 76557.648, 75790.984, 75644.594, 76279.023, 75058.031, 75466.719, 75614.125, 76045.578, 74761.273, 75072.484, 75285.117, 74458.453, 74758.969, 73735.125, 74553.008, 73888.188, 73682.945, 73821.188, 72883.195, 73472.719, 72862.719, 73939.633, 73724.766, 72932.688, 72544.656, 72816.555, 72737.531, 72255.586, 72576.906, 72188.219, 72561.617, 71913.484, 71721.43, 71850.008, 71857.523, 71766.242, 72077.203, 70680.961, 71731.305, 71061.062, 70125.547], 'val_loss': [4803.236, 2123.133, 1901.183, 1729.01, 1639.744, 1637.128, 1545.516, 1607.279, 1539.016, 1500.878, 1527.677, 1413.656, 1400.611, 1364.717, 1349.995, 1325.739, 1318.237, 1296.119, 1211.654, 1202.084, 1244.415, 1249.382, 1158.749, 1157.533, 1219.233, 1111.698, 1150.016, 1110.368, 1097.178, 1184.395, 1098.147, 1124.698, 1083.081, 1082.547, 1130.119, 1077.704, 1106.743, 1146.006, 1070.646, 1075.327, 1081.089, 1090.584, 1137.276, 1081.06, 1095.266, 1071.552, 1026.201, 1030.608, 1073.06, 1084.313, 987.112, 1049.356, 1054.652, 1030.768, 1019.226, 971.243, 1016.859, 1031.621, 1038.62, 1049.211, 1052.883, 1087.081, 993.501, 978.244, 1009.939, 1018.751, 975.84, 1044.28, 1012.988, 983.704, 1082.039, 978.059, 943.048, 991.761, 963.637, 985.737, 1056.046, 988.964, 1001.198, 987.347, 1008.022, 997.383, 974.435, 963.588, 941.361, 951.455, 948.078, 985.731, 945.15, 974.169, 962.891, 953.33, 998.878, 996.5, 955.775, 981.053, 915.261, 956.438, 943.326, 927.377]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:48 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4134.28027		421635	14	-1	269.6585257053375	{'train_loss': [287728.031, 158714.188, 144911.797, 139385.047, 134676.891, 130170.539, 125888.703, 123101.273, 119162.023, 116771.383, 114595.695, 112363.469, 111221.516, 109259.516, 107138.023, 105895.141, 105000.664, 104094.789, 103149.859, 101786.898, 100282.383, 99534.602, 96908.008, 97170.172, 96512.328, 95508.297, 95745.641, 94034.688, 93639.438, 92683.141, 92763.18, 92128.312, 91382.016, 90697.352, 90695.352, 90148.117, 89282.969, 89145.852, 89206.711, 87704.633, 87957.734, 87835.398, 86101.828, 86683.133, 85724.523, 85771.18, 85544.93, 84728.953, 85239.688, 84587.469, 83634.727, 84564.109, 83704.875, 83004.617, 83294.008, 82865.0, 83057.148, 82657.344, 82448.508, 81945.953, 81915.68, 81579.273, 80827.516, 80953.188, 80501.195, 81117.438, 80229.961, 79924.469, 79761.648, 79930.234, 79846.18, 79603.133, 79601.156, 79339.07, 79319.297, 78224.727, 78216.055, 78760.297, 79124.375, 78189.461, 77813.023, 77182.984, 77838.898, 77840.25, 77382.492, 76161.031, 77358.172, 77078.016, 77084.633, 77193.242, 77005.25, 76732.016, 77213.984, 76996.602, 76741.516, 76154.438, 76823.164, 75876.797, 76089.297, 75017.969], 'val_loss': [2025.718, 1795.32, 1634.426, 1685.896, 1738.258, 1603.359, 1594.687, 1625.151, 1588.918, 1598.096, 1603.013, 1611.855, 1616.073, 1617.371, 1465.738, 1451.872, 1425.974, 1461.587, 1390.705, 1358.733, 1413.581, 1397.984, 1300.279, 1333.917, 1282.208, 1261.608, 1267.161, 1272.492, 1250.732, 1295.697, 1224.648, 1267.118, 1198.628, 1222.947, 1183.541, 1171.395, 1268.454, 1170.128, 1256.075, 1223.672, 1222.462, 1244.738, 1257.859, 1170.146, 1192.873, 1170.288, 1158.696, 1164.703, 1200.555, 1161.335, 1151.327, 1131.15, 1145.001, 1144.108, 1203.706, 1100.901, 1110.949, 1200.678, 1063.592, 1062.303, 1183.08, 1099.467, 1121.285, 1062.891, 1056.249, 1116.201, 1079.392, 1074.274, 1080.952, 1089.693, 1093.574, 1053.491, 1044.857, 1044.102, 1051.559, 1035.573, 1085.711, 1074.207, 1057.028, 1109.938, 1083.745, 1025.455, 1021.281, 1078.093, 1041.753, 1048.134, 1041.432, 1042.681, 1014.964, 1044.7, 1082.965, 1085.091, 1043.939, 1068.434, 1046.897, 1023.723, 1027.27, 1053.843, 1061.465, 1024.391]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:45 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:91 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4305.87207		402436	14	-1	272.8662178516388	{'train_loss': [391265.75, 347417.0, 221400.828, 185830.125, 154742.391, 130563.141, 124012.82, 118814.281, 114992.594, 112123.672, 109598.852, 107418.562, 106729.781, 105146.945, 103416.016, 102425.211, 101237.109, 101675.086, 99900.539, 99432.445, 98136.641, 98258.93, 97327.344, 96610.758, 96055.32, 94670.617, 94653.844, 94322.852, 93608.18, 93329.305, 92871.672, 91821.086, 92798.82, 91629.172, 91251.836, 91284.711, 90718.211, 89918.539, 90124.039, 89243.625, 89182.984, 89248.242, 88940.398, 88008.453, 87838.305, 87155.789, 87625.523, 86816.734, 86194.898, 85626.188, 86095.812, 85697.711, 85288.703, 85682.281, 84934.766, 85188.773, 83996.93, 83749.844, 84189.148, 83659.164, 83244.562, 83653.164, 83252.516, 82967.352, 83606.797, 82397.984, 82107.883, 83300.555, 82700.68, 80901.297, 81763.352, 82754.461, 82304.719, 81557.141, 81248.594, 80223.32, 81221.562, 81446.578, 80932.727, 80046.352, 80504.656, 80736.711, 79579.945, 80982.273, 80349.812, 79176.609, 79154.367, 79647.656, 79805.133, 79441.367, 79310.164, 78975.984, 79229.438, 79147.734, 78601.953, 78813.867, 78037.016, 77442.727, 77776.078, 77540.078], 'val_loss': [5734.161, 3182.794, 2723.553, 2940.275, 2195.623, 1932.636, 1735.04, 1660.137, 1639.614, 1629.322, 1547.051, 1557.578, 1456.924, 1405.882, 1519.677, 1340.468, 1398.14, 1403.109, 1340.522, 1300.895, 1297.659, 1321.461, 1274.72, 1291.282, 1321.199, 1325.888, 1291.212, 1251.257, 1222.735, 1266.663, 1231.333, 1262.678, 1242.939, 1214.041, 1238.882, 1233.323, 1242.712, 1219.865, 1308.11, 1166.154, 1241.846, 1202.433, 1171.912, 1214.273, 1265.692, 1240.778, 1205.617, 1118.823, 1199.522, 1167.784, 1204.066, 1120.191, 1205.647, 1204.506, 1188.703, 1148.967, 1167.183, 1206.293, 1189.398, 1107.201, 1133.305, 1119.358, 1105.371, 1098.969, 1107.672, 1100.344, 1091.495, 1126.254, 1106.581, 1134.714, 1088.585, 1107.389, 1141.557, 1079.794, 1086.754, 1081.036, 1079.148, 1049.018, 1125.584, 1092.395, 1120.885, 1093.772, 1133.203, 1126.76, 1057.165, 1129.184, 1059.015, 1099.116, 1055.057, 1110.94, 1041.237, 1044.433, 1082.888, 1089.12, 1112.553, 1077.847, 1074.708, 1056.459, 1097.229, 1101.866]}	100	100	True
