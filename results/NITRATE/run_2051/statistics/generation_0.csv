id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4096.40234		452251	14	-1	188.94046139717102	{'train_loss': [287639.031, 168536.562, 150977.375, 141737.125, 134015.812, 126342.297, 121106.922, 118048.5, 114725.867, 110943.195, 108856.727, 106882.883, 105298.625, 104055.938, 102412.773, 101181.477, 99407.023, 99833.453, 97472.453, 97349.297, 96425.414, 95937.758, 94747.781, 93934.078, 93151.922, 92093.648, 92046.438, 91611.57, 91430.812, 90818.047, 90325.008, 89433.555, 89620.312, 88147.867, 88360.43, 87855.758, 86919.031, 86650.008, 86855.008, 86422.797, 86533.016, 85842.516, 85682.43, 86163.93, 84746.383, 84379.0, 84582.156, 84850.297, 83800.945, 83532.477, 82946.961, 83263.703, 83107.633, 82799.453, 82600.672, 81841.641, 81897.984, 81573.82, 81417.578, 81421.828, 80564.164, 81485.562, 80239.359, 80964.375, 80537.289, 80021.375, 79975.086, 80381.641, 79658.039, 79251.938, 79217.984, 79566.938, 80169.789, 78053.664, 78021.461, 79296.57, 78130.0, 77690.352, 78209.773, 77448.945, 77652.516, 76859.305, 77375.461, 77336.242, 77325.078, 76823.656, 76602.195, 76241.109, 76752.5, 76455.57, 76613.133, 75888.734, 75544.133, 75843.125, 74979.133, 74997.656, 74581.633, 74657.141, 74791.734, 74539.523], 'val_loss': [2223.416, 1693.329, 1650.285, 1572.14, 1584.245, 1609.803, 1570.109, 1649.271, 1595.197, 1530.626, 1438.843, 1474.933, 1511.834, 1440.314, 1378.966, 1370.442, 1349.848, 1348.262, 1321.004, 1283.883, 1279.086, 1306.375, 1245.831, 1208.06, 1316.031, 1208.46, 1220.543, 1207.059, 1165.202, 1201.361, 1177.664, 1157.702, 1187.757, 1136.573, 1173.348, 1149.648, 1181.445, 1108.419, 1150.301, 1191.935, 1143.476, 1161.001, 1138.09, 1098.729, 1077.052, 1150.096, 1134.084, 1081.071, 1143.692, 1155.523, 1071.057, 1117.082, 1097.631, 1078.573, 1089.962, 1094.179, 1098.915, 1102.089, 1120.394, 1149.044, 1041.186, 1142.57, 1134.725, 1090.823, 1082.408, 1115.882, 1043.635, 1025.161, 1044.195, 1081.193, 1051.026, 1078.459, 1055.379, 1078.062, 1094.903, 1046.843, 1049.243, 1050.887, 1033.703, 1013.179, 1001.92, 1056.357, 1031.256, 1059.644, 1002.733, 1068.906, 1036.318, 994.222, 1015.226, 1036.35, 1031.321, 1015.535, 1059.28, 1011.1, 996.339, 974.416, 998.173, 1020.008, 1010.073, 997.893]}	100	100	True
