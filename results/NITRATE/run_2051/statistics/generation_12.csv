id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	4151.00586		582726	15	-1	383.7352063655853	{'train_loss': [535767.062, 226549.953, 186367.141, 157811.453, 141432.156, 130046.844, 121335.398, 116856.938, 112629.484, 108316.742, 106426.18, 103660.219, 102894.906, 101059.711, 99488.023, 99060.156, 97680.594, 96748.773, 94982.914, 94640.055, 93794.648, 94248.234, 94377.172, 92924.758, 90638.18, 91361.602, 90158.742, 88974.047, 88798.758, 88208.867, 87190.602, 87500.109, 86828.656, 85434.914, 85298.227, 84612.781, 84489.461, 83608.727, 83831.188, 83960.867, 81520.438, 82465.555, 82094.633, 81633.055, 80401.359, 81474.828, 80204.523, 80047.641, 80357.938, 80010.375, 80333.781, 79175.922, 79603.766, 80047.445, 78337.477, 79111.641, 78885.508, 78125.992, 78367.75, 78735.531, 77681.578, 77775.711, 76703.539, 76221.406, 77326.523, 76565.016, 76414.305, 76215.57, 76303.461, 75226.938, 75732.023, 76609.398, 74609.445, 74831.5, 74935.719, 75146.75, 74598.391, 75004.469, 74658.883, 73999.023, 75266.422, 74789.117, 73078.703, 73671.523, 74096.234, 73182.891, 74389.328, 73443.344, 73628.828, 73710.984, 73707.156, 72742.531, 72917.852, 73064.391, 73031.234, 72485.344, 71515.938, 71932.805, 71580.266, 70949.078], 'val_loss': [3443.392, 2946.997, 2007.128, 1752.373, 1643.312, 1556.005, 1470.508, 1436.599, 1375.209, 1406.944, 1360.634, 1351.01, 1310.273, 1296.194, 1290.438, 1282.363, 1286.202, 1284.085, 1343.985, 1237.039, 1264.001, 1295.242, 1339.617, 1254.474, 1337.531, 1205.174, 1199.13, 1271.051, 1239.628, 1250.008, 1176.931, 1211.662, 1179.333, 1194.214, 1235.3, 1173.971, 1126.249, 1198.819, 1161.01, 1211.647, 1117.089, 1180.559, 1178.185, 1216.186, 1125.168, 1159.988, 1130.897, 1163.825, 1188.832, 1165.631, 1108.727, 1117.853, 1151.869, 1174.472, 1136.49, 1128.457, 1108.068, 1087.216, 1119.151, 1133.498, 1143.736, 1052.409, 1077.967, 1094.381, 1111.33, 1067.789, 1061.0, 1064.076, 1111.485, 1074.06, 1126.332, 1077.881, 1112.687, 1118.444, 1130.373, 1068.285, 1110.791, 1060.392, 1179.38, 1071.573, 1073.857, 1111.096, 1079.628, 1104.63, 1070.67, 1085.275, 1049.168, 1047.36, 1077.304, 1077.257, 1156.659, 1088.885, 1052.16, 1088.624, 1059.454, 1038.153, 1016.041, 1013.357, 1029.191, 1053.413]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.17498681609496702 beta1:0.8438161378069726 beta2:0.9055656386308496 weight_decay:9.30774889297765e-05 batch_size:32 epochs:100	100	1000	True	45176.64844		545216	14	-1	369.08841490745544	{'train_loss': [811840.438, 804482.438, 1080172.375, 814706.812, 808697.188, 917229.188, 822242.812, 857293.25, 836963.75, 817118.188, 812486.625, 839640.688, 818216.875, 811849.875, 810470.625, 819109.938, 816966.125, 811694.562, 810178.75, 814967.875, 830148.562, 817073.25, 811756.625, 815817.562, 827412.312, 819341.125, 813692.812, 813768.125, 813096.938, 830860.312, 815745.688, 809695.938, 813183.938, 817903.0, 816199.188, 811583.625, 811338.375, 815409.125, 827547.625, 812646.188, 810494.5, 813695.438, 820195.938, 823651.375, 813787.688, 810330.25, 821580.938, 817965.375, 819177.125, 811673.562, 816610.625, 815959.062, 819240.062, 811915.625, 814231.25, 817360.5, 817993.625, 820258.062, 811632.75, 814724.062, 821911.188, 819984.375, 811434.625, 809226.875, 822304.312, 814738.375, 822193.875, 814297.5, 814745.312, 817705.188, 822284.312, 814767.438, 814819.75, 820288.0, 823121.625, 819769.312, 813732.562, 815057.312, 820831.438, 818577.75, 818059.812, 814230.0, 816975.375, 817949.75, 816480.938, 815373.938, 817115.5, 815412.062, 817016.0, 816197.875, 822129.25, 818625.375, 819543.062, 815231.688, 819196.562, 823151.625, 815170.438, 816720.0, 814715.188, 819205.125], 'val_loss': [11044.456, 11044.456, 11027.637, 11044.456, 11810.52, 11044.456, 11044.456, 11043.303, 11038.085, 11044.456, 11303.08, 11513.393, 11018.397, 11044.456, 11044.281, 11980.859, 11044.442, 11044.44, 11044.455, 11044.456, 11069.984, 11643.824, 11090.671, 11044.424, 11138.735, 11042.652, 11044.456, 11044.456, 11826.055, 11044.385, 11044.453, 11775.877, 11639.811, 11253.062, 11041.58, 11044.456, 11062.025, 11035.914, 11044.456, 11044.451, 11044.456, 11044.456, 11370.037, 11193.021, 11044.455, 11457.23, 11226.936, 11453.971, 11044.456, 11044.456, 11043.833, 11208.425, 11428.176, 11044.456, 11044.456, 11479.854, 11021.651, 11044.456, 11294.463, 11044.2, 11044.377, 11058.828, 11041.864, 11167.969, 11044.328, 11001.371, 11044.456, 11043.82, 11011.635, 11688.104, 11256.731, 11433.925, 11044.438, 11588.02, 13613.426, 11040.678, 11044.456, 11189.82, 11681.938, 11044.456, 11044.355, 11044.452, 11044.048, 11044.456, 11384.459, 13202.933, 11044.455, 11341.535, 11044.456, 11620.009, 11044.456, 11044.456, 11044.445, 11201.399, 11041.283, 11048.491, 11029.213, 11044.456, 11044.456, 11195.078]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:81 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:42 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:deconv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adadelta batch_size:32 epochs:100	100	1000	True	44550.28906		16986538	17	-1	436.4637305736542	{'train_loss': [919371.312, 804482.438, 804908.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:40 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4319.64062		441201	14	-1	316.00654697418213	{'train_loss': [429008.812, 384534.375, 247340.266, 177701.5, 136230.453, 124229.961, 118562.406, 113257.844, 109186.094, 106653.43, 104984.867, 102991.555, 101545.555, 100269.945, 98468.484, 97286.859, 96629.125, 94506.836, 94551.406, 92363.281, 92010.719, 90585.609, 90651.289, 89356.109, 88476.5, 88152.414, 87272.672, 85842.867, 86916.438, 85491.328, 85595.828, 85011.203, 84210.078, 83770.828, 84062.805, 83132.188, 83420.375, 82436.406, 82453.188, 81644.633, 81976.0, 80506.258, 81497.367, 80340.383, 80527.891, 80553.203, 79606.461, 80089.633, 80181.016, 78734.227, 79744.742, 78588.312, 79012.023, 78204.469, 78358.812, 78668.75, 77208.891, 77531.328, 77329.109, 76465.867, 76476.383, 76822.828, 76679.648, 75475.961, 76599.297, 75755.164, 75813.945, 75995.172, 76049.742, 74925.758, 75946.352, 75426.852, 74891.5, 74503.383, 73996.812, 73171.07, 74541.742, 73232.422, 75084.375, 75016.102, 73220.07, 73202.523, 73880.594, 72990.555, 74312.008, 73837.062, 72533.055, 73140.828, 72849.398, 72369.828, 72167.5, 72485.297, 71736.703, 73523.172, 72332.531, 72113.195, 72294.258, 71433.375, 71784.945, 71892.953], 'val_loss': [5003.408, 5199.089, 2641.564, 2503.35, 1938.582, 1879.064, 1681.251, 1547.512, 1658.743, 1482.785, 1658.728, 1477.195, 1499.159, 1489.549, 1490.709, 1389.694, 1442.807, 1371.114, 1387.063, 1295.591, 1373.396, 1295.689, 1423.26, 1320.668, 1284.441, 1276.86, 1315.177, 1409.554, 1276.568, 1315.449, 1294.321, 1300.202, 1351.995, 1346.046, 1298.823, 1311.018, 1342.112, 1495.234, 1346.781, 1302.667, 1418.088, 1292.584, 1399.331, 1316.313, 1208.957, 1220.081, 1277.049, 1244.06, 1171.206, 1197.249, 1235.413, 1151.335, 1159.442, 1189.362, 1180.212, 1239.797, 1233.799, 1163.136, 1257.238, 1269.902, 1203.525, 1157.15, 1232.402, 1211.745, 1131.614, 1194.184, 1173.594, 1109.26, 1136.835, 1177.007, 1151.726, 1103.133, 1177.147, 1146.574, 1131.981, 1107.122, 1133.873, 1163.973, 1134.683, 1123.681, 1132.014, 1145.935, 1101.935, 1094.565, 1089.508, 1137.81, 1105.0, 1127.571, 1146.484, 1166.952, 1134.128, 1093.687, 1051.297, 1065.755, 1076.538, 1092.198, 1060.927, 1103.3, 1102.752, 1091.084]}	100	100	True
