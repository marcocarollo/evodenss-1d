id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100	100	2000	True	4202.90137		606861	16	-1	458.1325876712799	{'train_loss': [472787.344, 236967.047, 139919.656, 112350.695, 105268.789, 100347.133, 96968.078, 94530.148, 92395.789, 90076.828, 88151.0, 86782.953, 85735.562, 84640.812, 82798.555, 82553.039, 81872.82, 81583.539, 80785.156, 79694.516, 78921.758, 78412.422, 78066.312, 77304.484, 77146.312, 76467.125, 76316.094, 76557.242, 76901.898, 74132.172, 75032.68, 74872.062, 73576.008, 73160.18, 72974.359, 72480.141, 73049.789, 72451.203, 72726.469, 71949.852, 72214.57, 70867.93, 71017.773, 71712.516, 71882.805, 71398.758, 70239.5, 69543.062, 70143.031, 70022.977, 69559.969, 69054.93, 69520.211, 69566.047, 68257.375, 69666.602, 68787.805, 70021.867, 68759.68, 68220.82, 68405.07, 69350.219, 68186.148, 68046.281, 67366.102, 67731.266, 67767.266, 67716.984, 67886.578, 67709.594, 67347.812, 67395.586, 67213.172, 67121.789, 67591.32, 66753.516, 66217.359, 66146.445, 66294.555, 66118.57, 66032.328, 65353.668, 65360.25, 65470.883, 66000.969, 66433.25, 65043.285, 65822.844, 64874.707, 65013.176, 65422.602, 65414.48, 64944.477, 64706.34, 63904.109, 64281.617, 63598.91, 64043.832, 64468.625, 64499.598], 'val_loss': [1608.024, 1142.085, 757.311, 655.177, 625.333, 583.717, 597.055, 560.442, 555.295, 508.399, 535.513, 501.861, 502.383, 491.559, 496.077, 473.566, 496.821, 513.606, 466.241, 476.121, 487.233, 451.871, 452.089, 463.592, 454.041, 446.129, 444.14, 451.819, 454.035, 461.136, 452.379, 454.308, 456.342, 448.976, 433.032, 447.963, 442.066, 458.216, 437.486, 473.487, 449.31, 447.929, 440.027, 434.072, 433.028, 433.898, 436.567, 408.896, 430.777, 433.674, 425.034, 422.762, 413.918, 428.227, 517.546, 424.234, 432.7, 454.988, 424.593, 425.036, 425.825, 419.305, 420.092, 403.946, 439.969, 435.669, 410.886, 406.373, 408.178, 413.085, 436.501, 439.337, 414.282, 412.897, 429.23, 419.902, 418.866, 411.141, 397.103, 434.978, 438.301, 404.572, 393.809, 448.381, 424.285, 395.54, 413.116, 394.534, 404.982, 409.243, 393.472, 453.034, 431.848, 396.978, 436.861, 394.187, 398.213, 389.275, 389.787, 440.385]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:104 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:15 epochs:100	100	1000	True	44608.84766		34577958	15	-1	591.4057912826538	{'train_loss': [1404837.125, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562, 804482.562], 'val_loss': [4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647, 4908.647]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:84 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100	100	1000	True	4298.65723		505996	16	-1	437.53001832962036	{'train_loss': [276847.25, 157669.688, 137059.375, 129343.289, 123755.078, 118570.219, 114466.523, 112151.938, 107588.797, 106529.695, 104760.539, 102376.312, 100521.086, 100550.219, 98025.461, 97933.266, 96462.367, 96205.562, 95493.594, 94584.398, 94167.461, 93231.906, 92323.062, 92119.109, 91081.555, 91050.328, 89879.711, 89485.086, 89692.414, 88368.812, 87994.203, 88031.258, 87384.891, 87515.031, 86829.188, 86211.703, 86128.203, 85774.422, 85185.523, 84716.969, 84589.984, 83977.805, 84032.688, 83991.703, 83234.227, 83033.062, 82919.656, 83046.375, 82694.109, 81554.664, 82213.469, 82392.469, 81961.977, 81134.469, 80906.852, 80712.219, 81003.102, 80623.156, 80609.32, 80813.336, 80252.422, 79069.805, 79805.242, 79300.117, 79910.906, 79253.688, 78123.312, 78866.328, 78660.828, 78067.422, 78604.664, 78005.008, 77753.328, 77483.93, 77130.969, 77538.5, 78006.453, 77112.656, 77405.695, 76989.695, 76955.352, 76921.797, 76940.883, 76972.594, 76526.25, 77456.336, 76456.992, 75815.266, 75664.227, 75916.141, 75940.422, 76091.836, 75303.117, 74697.828, 75667.414, 74592.633, 74789.617, 74399.969, 75040.281, 74355.992], 'val_loss': [1263.37, 872.283, 779.759, 845.144, 877.377, 722.081, 729.628, 699.023, 632.89, 685.146, 632.868, 579.473, 594.6, 591.95, 562.916, 582.365, 559.581, 575.341, 582.709, 614.836, 565.806, 559.584, 551.664, 522.283, 529.286, 559.702, 528.96, 529.841, 546.23, 518.438, 582.566, 567.452, 543.609, 547.576, 540.675, 523.417, 514.085, 549.622, 518.97, 498.582, 518.248, 521.734, 489.723, 511.961, 489.131, 511.689, 491.216, 495.013, 500.485, 496.453, 509.115, 494.684, 508.627, 486.337, 487.189, 469.898, 468.275, 476.442, 477.734, 485.336, 474.366, 477.849, 466.85, 499.289, 477.483, 498.635, 489.99, 481.546, 465.41, 463.474, 486.011, 456.586, 476.75, 496.207, 468.761, 462.474, 456.1, 460.177, 486.068, 485.938, 448.982, 448.938, 472.996, 467.928, 471.736, 458.542, 459.01, 465.742, 481.632, 447.526, 446.261, 468.356, 459.038, 470.887, 460.449, 471.054, 456.79, 473.04, 460.711, 461.927]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:25 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:8 epochs:100	100	1000	True	3856.12061		589861	16	-1	635.0910363197327	{'train_loss': [253841.516, 128356.969, 112337.453, 104639.578, 100480.289, 96501.609, 93034.938, 90753.672, 88927.898, 87945.977, 87026.539, 85628.328, 84955.531, 84282.031, 83178.609, 82533.188, 82154.539, 80432.414, 80497.484, 79831.695, 79375.453, 78822.18, 78800.883, 77502.273, 77028.492, 76541.891, 75966.797, 75688.43, 75924.312, 75338.969, 74845.078, 74817.602, 74017.664, 74069.656, 73732.57, 73143.812, 73666.711, 72970.453, 73131.453, 73299.008, 73135.102, 72830.961, 72313.695, 72575.703, 72043.07, 71944.055, 71981.906, 70527.484, 71155.922, 71021.922, 71044.312, 70181.492, 70789.672, 69568.711, 70581.312, 70019.484, 69713.711, 69809.438, 69841.25, 69760.562, 69696.953, 69502.578, 68936.492, 68161.625, 68561.258, 68703.258, 68430.43, 69119.609, 67528.078, 67898.484, 67988.328, 68066.516, 67587.641, 68000.039, 67237.086, 67971.984, 67153.375, 67046.883, 67282.117, 66241.586, 66244.883, 66575.398, 65436.395, 66646.805, 65845.984, 65473.453, 66103.719, 65886.102, 65154.996, 65008.977, 65904.086, 64906.191, 65160.047, 65048.004, 64285.059, 65068.379, 64893.145, 64949.828, 64807.02, 65125.621], 'val_loss': [495.637, 392.256, 365.221, 363.991, 336.651, 336.629, 326.131, 314.474, 319.815, 299.35, 297.638, 280.532, 292.045, 286.774, 284.947, 284.953, 294.845, 276.562, 281.395, 280.306, 286.158, 298.414, 281.574, 289.72, 265.235, 271.269, 275.066, 272.379, 257.851, 257.597, 287.745, 253.332, 265.622, 258.985, 266.918, 260.999, 273.894, 283.112, 268.764, 260.796, 280.139, 255.764, 254.164, 252.354, 264.948, 269.675, 255.713, 261.422, 250.276, 274.147, 253.12, 234.638, 247.109, 269.716, 237.677, 282.763, 236.64, 264.283, 237.538, 249.75, 231.178, 245.9, 269.257, 259.722, 270.498, 266.894, 232.199, 251.593, 277.874, 255.907, 259.453, 259.662, 250.093, 263.874, 268.879, 283.238, 257.202, 258.329, 234.965, 237.187, 255.925, 248.164, 247.089, 280.671, 244.279, 221.141, 246.941, 271.325, 226.713, 275.725, 254.032, 244.392, 261.671, 238.224, 237.495, 245.248, 224.599, 258.674, 259.146, 248.881]}	100	100	True
