id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	3986.04321		696089	14	-1	320.4357259273529	{'train_loss': [653905.312, 345124.094, 256841.922, 216752.938, 189313.688, 144258.297, 129197.039, 119385.375, 114304.82, 111157.898, 106769.586, 104421.523, 102125.562, 100477.914, 98183.984, 96423.516, 95206.391, 94055.578, 94161.359, 91208.797, 90244.18, 89454.391, 89258.492, 88423.211, 87679.883, 87056.391, 85662.289, 85929.289, 85303.492, 86122.859, 84655.508, 84241.797, 82982.219, 82840.172, 82714.828, 81726.305, 81508.875, 81187.75, 80755.781, 80872.461, 79701.766, 79629.383, 79758.938, 78529.828, 80007.359, 77855.234, 79173.328, 77183.812, 78123.469, 76810.609, 76580.047, 77310.023, 77007.211, 76634.25, 77061.102, 75546.68, 77344.953, 76354.867, 76230.406, 75432.32, 76060.461, 75117.109, 73674.883, 74815.867, 75535.289, 75331.602, 74538.789, 72953.336, 74334.352, 73298.688, 73110.273, 73894.922, 72811.023, 73658.93, 73029.383, 72317.398, 72041.32, 72754.516, 73391.102, 72093.281, 72317.289, 72033.266, 71292.805, 71740.359, 73230.938, 72829.031, 73739.719, 71775.789, 71764.953, 73152.797, 72689.227, 72088.422, 71214.742, 69975.734, 70029.133, 71687.547, 70135.812, 71365.539, 70543.414, 69417.742], 'val_loss': [4868.854, 3954.454, 3346.056, 3025.104, 2403.672, 2234.713, 2041.309, 1810.266, 1802.066, 1657.759, 1734.845, 1556.203, 1613.896, 1508.256, 1479.731, 1496.057, 1460.492, 1405.412, 1379.923, 1413.925, 1369.069, 1386.634, 1325.039, 1194.668, 1345.652, 1169.062, 1310.766, 1211.078, 1225.383, 1285.82, 1219.505, 1386.179, 1236.893, 1231.001, 1184.166, 1208.815, 1117.883, 1184.715, 1252.108, 1122.798, 1190.843, 1211.361, 1118.708, 1160.636, 1182.509, 1199.728, 1141.829, 1155.577, 1255.905, 1134.428, 1145.787, 1221.515, 1085.369, 1145.322, 1141.781, 1179.976, 1099.645, 1188.064, 1113.092, 1109.718, 1017.58, 1040.483, 1036.306, 1126.637, 1093.057, 1028.703, 1019.263, 1099.918, 996.603, 1051.388, 1098.23, 1045.811, 1041.512, 1156.249, 1039.041, 984.241, 1038.317, 1119.013, 1090.847, 1031.443, 1022.076, 1146.558, 998.76, 1087.922, 1163.709, 971.343, 1069.094, 1016.095, 1100.9, 1097.792, 1068.394, 1137.135, 1014.321, 1013.205, 1008.394, 1088.895, 988.944, 1083.812, 994.455, 993.951]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	3675.68726		582726	15	-1	368.0395874977112	{'train_loss': [487407.406, 206545.938, 161781.453, 143412.141, 129347.984, 120017.852, 112579.906, 109149.891, 105489.109, 102658.844, 101242.398, 97822.328, 96474.219, 94198.297, 93210.102, 91307.094, 90203.695, 89491.43, 88304.805, 87029.602, 86266.742, 84760.867, 84919.883, 83353.797, 83016.336, 82685.156, 81661.25, 80675.562, 79871.195, 80358.836, 79559.203, 78409.305, 78654.469, 77275.531, 77216.109, 76691.719, 76513.828, 76087.398, 75214.305, 75263.961, 74322.43, 74169.023, 74288.898, 74077.242, 73523.898, 73276.172, 72609.875, 72849.648, 72593.805, 72604.352, 72370.695, 70695.172, 71478.594, 71325.805, 71000.68, 70106.047, 69832.734, 69137.43, 69834.234, 69719.586, 69040.914, 69345.711, 69309.594, 68827.883, 68583.055, 67926.688, 67180.719, 67899.828, 69244.359, 67529.57, 67940.586, 67366.055, 67738.211, 67142.328, 67060.734, 65892.234, 66433.594, 65970.695, 66544.758, 66252.367, 65157.066, 65933.562, 66029.961, 65517.09, 65505.656, 66082.938, 64517.039, 65548.492, 64890.242, 65848.586, 64145.219, 64957.754, 63954.672, 63975.066, 65590.656, 63762.035, 62667.82, 65182.121, 64533.449, 63528.449], 'val_loss': [3413.724, 2110.667, 1685.25, 1601.166, 1513.01, 1546.074, 1464.694, 1439.032, 1418.713, 1397.265, 1319.587, 1263.854, 1340.636, 1195.348, 1184.811, 1215.508, 1251.015, 1137.826, 1223.757, 1142.336, 1116.971, 1108.51, 1060.609, 1066.95, 1104.751, 1060.679, 1069.693, 1058.231, 1044.941, 1060.383, 995.63, 1030.101, 1055.474, 1013.218, 1035.706, 1040.235, 1032.236, 990.715, 1032.502, 988.865, 958.435, 1042.759, 1030.371, 1007.482, 990.477, 1018.792, 964.376, 946.966, 952.876, 950.93, 963.76, 950.804, 958.844, 974.401, 955.427, 955.301, 947.528, 970.257, 965.645, 928.425, 924.921, 953.739, 939.487, 969.206, 970.288, 1033.089, 989.911, 944.026, 926.018, 957.015, 939.525, 1009.672, 926.751, 913.797, 941.346, 891.065, 909.344, 946.63, 951.018, 914.707, 947.714, 906.829, 943.116, 923.321, 892.491, 933.474, 923.605, 986.155, 891.051, 937.557, 974.339, 941.79, 945.978, 929.565, 914.997, 995.687, 937.801, 946.815, 1000.578, 920.438]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:81 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.18217518027729646 alpha:0.9528526802138942 weight_decay:0.0009608786234735189 batch_size:32 epochs:100	100	1000	True	44535.45703		649999	15	-1	321.800594329834	{'train_loss': [1484723.375, 804482.438, 826108.062, 5540940.5, 804482.438, 804482.438, 804482.438, 5197190.5, 865546.438, 804769.562, 804482.438, 804482.438, 2080590.5, 804482.438, 804482.438, 804482.438, 2928079.5, 966455.438, 804482.438, 804482.438, 804482.438, 6692867.0, 809353.688, 806279.25, 5382647.5, 804482.438, 804482.438, 804482.438, 5196698.0, 804486.688, 804482.438, 805576.438, 1380964.125, 1060414.25, 1026350.375, 5108249.5, 804482.438, 987462.188, 804482.938, 3785223.5, 805756.188, 804482.438, 807135.125, 804694.062, 3926275.75, 827969.562, 829210.812, 804482.438, 5160698.0, 804482.438, 804482.438, 2473418.25, 804490.812, 804482.438, 818468.625, 3552641.25, 831808.688, 804482.438, 804482.438, 4998513.5, 804482.438, 804482.438, 804482.438, 1251312.75, 804516.062, 1744345.5, 804482.438, 804482.438, 4633257.5, 986738.312, 804482.438, 929941.438, 804482.438, 3573057.75, 804482.438, 804482.438, 3644247.25, 804482.438, 804482.438, 804482.438, 4406545.5, 804482.438, 804482.438, 804482.438, 5916527.0, 804482.438, 804482.438, 804482.438, 3336176.25, 804482.438, 804482.438, 804482.438, 813384.438, 1217271.875, 804482.438, 804482.438, 804482.438, 1574616.0, 805864.938, 804482.438], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 34283.766, 11044.455, 11042.708, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11124.099, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.455, 11044.279, 11044.456, 11044.456, 11044.456, 2162499.75, 11044.456, 11044.456, 11044.456, 11044.456, 15560275.0, 11044.456, 11044.456, 11044.455, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11560.822, 15655.494, 11044.456, 11044.456, 11044.456, 11041.73, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 67775.289, 11044.456, 11044.456, 11044.456]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	4256.84766		423115	12	-1	294.49685049057007	{'train_loss': [361900.875, 163739.891, 144241.625, 133271.031, 129699.555, 122691.891, 118789.203, 116031.984, 113379.609, 110498.852, 108167.82, 105658.258, 103542.32, 102894.383, 100953.492, 99414.891, 98295.68, 97050.891, 96431.703, 95335.312, 94214.383, 93658.164, 92870.5, 92670.945, 92387.336, 91565.539, 90024.375, 89276.133, 89189.32, 88713.922, 87433.969, 87851.078, 86991.32, 87185.672, 86036.156, 85453.148, 86412.781, 85062.914, 84942.617, 85143.484, 84175.711, 83340.602, 83791.531, 83519.781, 82758.367, 82496.367, 82765.938, 82084.664, 81872.977, 81511.789, 81900.938, 80470.945, 81029.602, 79768.672, 80663.75, 79681.633, 80142.688, 79985.234, 78473.305, 79804.727, 78926.211, 78482.797, 78480.188, 78494.25, 77660.57, 77658.719, 77789.023, 78045.484, 77464.445, 77962.023, 77557.977, 77879.734, 76425.977, 76830.875, 77332.344, 76624.695, 76065.773, 76685.469, 75333.422, 76364.953, 75703.352, 75804.469, 75404.375, 74421.664, 75506.008, 75371.734, 75199.93, 74733.633, 74223.953, 73514.797, 74940.297, 74537.43, 73650.883, 74768.703, 74259.664, 73421.242, 73623.695, 72976.969, 73061.281, 73904.18], 'val_loss': [2274.43, 1959.732, 1861.129, 1865.482, 2046.86, 1743.49, 1682.107, 1776.164, 1750.378, 1764.567, 1620.773, 1567.962, 1598.971, 1620.22, 1613.763, 1546.584, 1486.668, 1498.614, 1398.594, 1479.834, 1376.427, 1379.154, 1407.094, 1390.484, 1322.231, 1404.767, 1312.578, 1252.392, 1308.221, 1239.101, 1281.482, 1264.294, 1295.934, 1248.905, 1283.186, 1272.066, 1194.92, 1250.923, 1299.261, 1196.875, 1203.298, 1202.917, 1175.007, 1223.929, 1167.024, 1230.607, 1215.215, 1216.552, 1176.416, 1191.865, 1167.055, 1240.299, 1197.217, 1163.043, 1242.079, 1156.493, 1124.7, 1164.615, 1110.386, 1194.015, 1129.97, 1177.314, 1132.252, 1181.017, 1096.927, 1185.225, 1146.116, 1169.158, 1132.283, 1081.704, 1087.664, 1134.597, 1107.912, 1051.195, 1119.357, 1101.268, 1128.259, 1102.85, 1070.763, 1067.802, 1066.11, 1089.067, 1057.995, 1052.432, 1122.299, 1071.613, 1088.603, 1064.567, 1114.958, 1006.113, 1108.285, 1065.914, 1030.907, 1054.729, 1071.932, 1046.797, 1077.641, 1008.938, 1044.577, 1053.049]}	100	100	True
