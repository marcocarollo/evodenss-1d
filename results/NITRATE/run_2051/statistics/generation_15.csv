id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100	100	1000	True	4194.12061		606861	16	-1	466.1443259716034	{'train_loss': [379207.938, 178144.703, 139627.375, 124606.531, 117479.766, 111861.305, 108349.586, 106560.195, 102927.383, 100484.133, 98832.258, 97550.57, 95712.672, 95540.023, 93280.609, 92545.828, 92321.016, 91809.234, 90629.656, 90205.188, 89477.578, 88437.023, 87552.211, 87571.742, 86485.227, 86050.836, 86726.414, 85337.758, 85308.07, 84620.148, 83785.586, 83614.539, 83668.031, 83636.102, 83128.633, 82181.109, 81822.594, 82527.172, 81910.18, 81402.164, 81473.711, 81499.883, 79785.461, 79981.469, 80873.227, 79794.102, 78981.883, 79364.148, 78963.125, 79116.703, 78527.062, 78107.008, 81544.773, 80803.984, 79870.383, 79865.836, 79690.219, 79511.289, 79823.0, 77835.867, 78251.352, 77881.297, 78300.008, 77656.805, 77150.719, 77011.805, 77886.438, 76883.414, 76894.555, 76129.273, 76749.031, 75596.203, 76921.914, 76817.227, 75782.188, 75991.828, 75359.812, 75257.109, 75389.805, 75441.484, 74418.805, 74710.359, 74207.375, 75204.086, 74056.852, 73611.781, 74820.602, 73411.438, 73959.5, 73347.523, 74311.805, 73431.156, 74003.469, 73573.961, 72350.477, 73157.43, 72566.562, 72652.5, 73281.391, 72245.688], 'val_loss': [1558.107, 824.28, 737.842, 752.846, 637.293, 635.649, 601.68, 651.48, 633.331, 635.052, 600.224, 581.546, 606.489, 545.511, 576.695, 533.68, 573.331, 570.869, 544.357, 566.625, 562.614, 549.416, 602.022, 536.297, 523.059, 550.328, 573.232, 507.175, 482.509, 463.726, 482.265, 492.14, 530.038, 468.194, 496.223, 472.196, 475.488, 485.618, 500.881, 462.745, 466.958, 451.77, 490.759, 468.217, 442.427, 473.52, 442.574, 468.906, 474.296, 461.695, 442.374, 448.958, 517.097, 470.539, 484.368, 493.54, 486.127, 510.826, 483.895, 475.312, 480.839, 495.065, 475.408, 489.487, 438.333, 489.972, 462.404, 470.836, 457.358, 540.673, 449.23, 484.396, 447.278, 456.254, 517.43, 452.566, 519.682, 460.878, 475.184, 495.975, 463.472, 436.557, 457.034, 479.017, 481.315, 499.746, 462.279, 469.666, 479.225, 494.722, 454.305, 537.177, 475.195, 485.682, 485.854, 429.258, 481.719, 456.02, 462.571, 437.454]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:115 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:39 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100	100	2000	True	3932.854		535730	16	-1	442.1003272533417	{'train_loss': [354318.531, 156112.141, 125826.469, 115493.43, 109454.695, 103843.758, 100975.898, 97296.531, 95763.445, 93829.688, 93748.43, 90986.648, 94108.242, 90226.453, 88741.109, 87480.492, 87244.633, 85853.492, 85302.305, 84748.555, 84257.281, 82945.297, 83200.82, 82467.43, 81348.703, 81452.086, 80495.688, 80278.25, 78827.156, 78483.742, 78319.82, 77214.953, 77154.133, 77162.5, 76923.797, 76039.117, 75739.812, 76031.297, 75801.055, 75896.438, 74453.336, 75687.789, 74200.914, 73979.82, 74188.969, 73494.086, 72939.633, 72686.047, 72679.25, 73035.5, 72062.547, 72173.672, 72416.812, 71805.734, 71427.102, 71376.898, 71315.086, 70903.883, 71055.961, 70345.93, 70163.016, 70105.242, 69827.836, 70188.039, 69801.695, 69317.508, 69950.164, 68857.961, 69366.758, 69125.25, 68632.391, 68548.289, 69032.805, 68281.531, 68184.953, 66898.727, 68128.32, 67384.664, 68182.625, 68603.664, 66982.977, 67380.961, 67305.312, 67923.797, 66735.016, 66636.305, 66946.875, 66276.922, 66488.062, 65475.688, 66018.742, 65438.324, 66598.562, 66127.234, 66376.43, 66282.891, 65817.891, 65448.387, 65468.438, 65725.703], 'val_loss': [975.449, 736.008, 690.918, 648.485, 609.221, 597.894, 627.547, 577.957, 578.674, 562.175, 547.337, 555.186, 517.412, 522.219, 500.402, 517.582, 500.072, 491.606, 467.492, 513.705, 481.688, 471.664, 494.811, 483.087, 449.287, 478.224, 474.841, 455.51, 457.009, 449.752, 447.239, 448.38, 444.826, 453.234, 459.469, 444.741, 447.219, 455.291, 438.532, 446.681, 442.07, 436.761, 433.757, 433.875, 432.488, 426.735, 421.808, 438.729, 428.273, 432.566, 432.199, 431.742, 426.744, 444.586, 426.089, 423.858, 415.338, 432.2, 424.858, 417.758, 420.989, 423.96, 450.326, 420.972, 421.489, 431.523, 418.914, 412.625, 414.946, 416.588, 441.6, 418.658, 413.172, 404.732, 430.137, 412.881, 416.307, 411.048, 423.402, 406.583, 405.326, 408.483, 440.801, 433.095, 401.593, 412.389, 402.644, 399.907, 430.744, 437.538, 398.005, 421.7, 422.889, 427.174, 434.389, 417.404, 462.974, 400.769, 439.851, 422.874]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100	100	2000	True	3705.05347		606861	16	-1	456.29018902778625	{'train_loss': [341326.344, 148337.984, 126951.5, 113103.305, 106622.453, 102045.406, 100141.305, 96741.891, 93801.648, 91934.664, 90185.594, 88575.602, 87367.961, 86839.367, 85739.195, 85898.016, 83793.273, 83428.641, 82317.25, 81209.375, 81427.836, 81255.312, 80575.516, 79825.188, 78600.875, 78231.859, 78015.789, 76878.328, 76172.352, 76133.141, 76050.914, 75210.25, 75153.273, 75275.867, 75191.156, 74450.719, 73948.289, 73619.062, 73752.969, 73043.984, 72750.375, 71917.711, 72712.219, 71487.125, 71800.617, 71105.812, 70914.289, 70497.844, 70293.398, 70161.148, 70260.914, 69312.227, 68777.008, 69135.289, 68781.938, 68722.625, 68255.586, 69181.234, 68023.875, 67280.453, 68352.508, 67660.711, 67838.805, 67490.352, 66990.32, 66891.219, 67064.031, 66729.328, 66277.93, 66681.953, 66524.703, 64959.023, 66678.953, 65149.172, 65942.25, 65590.891, 65319.199, 64988.105, 65131.879, 64599.395, 64803.859, 64635.25, 63405.469, 64398.281, 64244.84, 64025.871, 64504.094, 64366.07, 64606.551, 64632.348, 63669.25, 63597.895, 63852.211, 63303.918, 63131.812, 63383.531, 63323.156, 62909.234, 63284.891, 62601.395], 'val_loss': [852.69, 739.146, 653.697, 624.793, 604.742, 595.33, 577.72, 543.759, 547.015, 522.609, 512.053, 493.326, 539.535, 486.324, 479.663, 470.554, 471.602, 484.39, 481.45, 473.137, 467.465, 469.51, 463.681, 466.387, 462.585, 467.049, 447.442, 449.274, 460.809, 440.311, 443.16, 433.267, 465.464, 430.586, 429.846, 425.677, 472.084, 427.842, 425.144, 416.896, 420.886, 409.926, 433.272, 418.976, 459.468, 440.838, 419.634, 459.547, 434.651, 411.606, 407.902, 403.335, 412.303, 419.44, 416.026, 402.694, 430.71, 401.555, 395.337, 407.666, 399.125, 393.001, 395.997, 386.516, 389.341, 408.332, 431.039, 382.529, 448.192, 409.885, 406.958, 414.328, 389.157, 417.052, 385.54, 406.529, 400.731, 390.278, 399.996, 432.776, 409.979, 391.45, 400.635, 401.336, 426.14, 415.532, 414.874, 425.355, 393.868, 406.934, 387.389, 399.976, 399.065, 383.836, 390.508, 418.706, 382.99, 389.595, 390.758, 383.676]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100	100	2000	True	3705.05347		606861	16	-1	456.29815196990967	{'train_loss': [341326.344, 148337.984, 126951.5, 113103.305, 106622.453, 102045.406, 100141.305, 96741.891, 93801.648, 91934.664, 90185.594, 88575.602, 87367.961, 86839.367, 85739.195, 85898.016, 83793.273, 83428.641, 82317.25, 81209.375, 81427.836, 81255.312, 80575.516, 79825.188, 78600.875, 78231.859, 78015.789, 76878.328, 76172.352, 76133.141, 76050.914, 75210.25, 75153.273, 75275.867, 75191.156, 74450.719, 73948.289, 73619.062, 73752.969, 73043.984, 72750.375, 71917.711, 72712.219, 71487.125, 71800.617, 71105.812, 70914.289, 70497.844, 70293.398, 70161.148, 70260.914, 69312.227, 68777.008, 69135.289, 68781.938, 68722.625, 68255.586, 69181.234, 68023.875, 67280.453, 68352.508, 67660.711, 67838.805, 67490.352, 66990.32, 66891.219, 67064.031, 66729.328, 66277.93, 66681.953, 66524.703, 64959.023, 66678.953, 65149.172, 65942.25, 65590.891, 65319.199, 64988.105, 65131.879, 64599.395, 64803.859, 64635.25, 63405.469, 64398.281, 64244.84, 64025.871, 64504.094, 64366.07, 64606.551, 64632.348, 63669.25, 63597.895, 63852.211, 63303.918, 63131.812, 63383.531, 63323.156, 62909.234, 63284.891, 62601.395], 'val_loss': [852.69, 739.146, 653.697, 624.793, 604.742, 595.33, 577.72, 543.759, 547.015, 522.609, 512.053, 493.326, 539.535, 486.324, 479.663, 470.554, 471.602, 484.39, 481.45, 473.137, 467.465, 469.51, 463.681, 466.387, 462.585, 467.049, 447.442, 449.274, 460.809, 440.311, 443.16, 433.267, 465.464, 430.586, 429.846, 425.677, 472.084, 427.842, 425.144, 416.896, 420.886, 409.926, 433.272, 418.976, 459.468, 440.838, 419.634, 459.547, 434.651, 411.606, 407.902, 403.335, 412.303, 419.44, 416.026, 402.694, 430.71, 401.555, 395.337, 407.666, 399.125, 393.001, 395.997, 386.516, 389.341, 408.332, 431.039, 382.529, 448.192, 409.885, 406.958, 414.328, 389.157, 417.052, 385.54, 406.529, 400.731, 390.278, 399.996, 432.776, 409.979, 391.45, 400.635, 401.336, 426.14, 415.532, 414.874, 425.355, 393.868, 406.934, 387.389, 399.976, 399.065, 383.836, 390.508, 418.706, 382.99, 389.595, 390.758, 383.676]}	0	100	True
