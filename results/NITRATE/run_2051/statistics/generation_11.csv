id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	3575.71436		582726	15	-1	374.1408529281616	{'train_loss': [524067.375, 189744.906, 156704.828, 129719.266, 120239.461, 109623.164, 108693.32, 101985.133, 101700.906, 97313.07, 96283.82, 93871.242, 93408.266, 89566.102, 89301.789, 90434.133, 87633.562, 86493.43, 85419.32, 83979.523, 84048.234, 82762.492, 82158.281, 81773.227, 80685.906, 78870.062, 78418.422, 77961.445, 78648.508, 77610.195, 77157.078, 75839.773, 76605.391, 76364.906, 75322.938, 74323.945, 75318.391, 73742.281, 72814.336, 72571.977, 72453.258, 73556.93, 71776.484, 72254.391, 72259.906, 71020.617, 70858.234, 70615.328, 70563.531, 69600.078, 69727.523, 70981.711, 69572.219, 68798.625, 68701.445, 69014.375, 68216.031, 67714.297, 68007.266, 67433.766, 67110.859, 67057.711, 65952.469, 66512.305, 66660.773, 65993.156, 66641.125, 66064.992, 65981.102, 66221.43, 66053.883, 65830.703, 65300.582, 65292.559, 64680.098, 64643.441, 65219.691, 63864.551, 64679.648, 64474.746, 64118.188, 64725.0, 63466.621, 64622.738, 63593.508, 63915.977, 62579.879, 63712.668, 62882.16, 62691.047, 63866.871, 63133.555, 62728.082, 62668.227, 62220.391, 61628.371, 62053.707, 63077.055, 62839.758, 63036.469], 'val_loss': [3093.941, 2769.186, 1677.252, 1449.658, 1695.625, 1426.555, 1489.97, 1265.506, 1483.213, 1207.508, 1182.595, 1148.428, 1261.214, 1130.786, 1093.626, 1123.309, 1106.603, 1051.501, 1095.691, 1054.362, 1053.385, 1070.328, 1095.707, 1022.186, 995.967, 1039.825, 1143.766, 974.111, 966.755, 994.834, 979.843, 1110.418, 1009.102, 961.623, 1022.277, 1059.045, 1060.088, 977.649, 936.408, 933.017, 943.715, 958.113, 938.199, 994.172, 960.345, 919.504, 924.391, 935.078, 961.638, 973.75, 934.848, 953.198, 928.54, 935.104, 913.51, 963.106, 899.807, 906.895, 903.058, 948.314, 904.628, 920.293, 916.885, 920.516, 925.407, 875.331, 906.876, 925.063, 915.44, 941.281, 894.796, 914.089, 898.426, 878.434, 921.699, 899.203, 880.96, 857.384, 898.491, 879.579, 866.394, 890.17, 918.271, 941.41, 866.982, 877.861, 874.828, 897.577, 863.272, 877.737, 928.986, 925.045, 893.444, 879.901, 876.043, 860.921, 859.016, 871.9, 909.039, 842.56]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:35 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4118.32861		724884	14	-1	343.6813259124756	{'train_loss': [825558.688, 333790.375, 251052.641, 212514.375, 181927.406, 167914.75, 153608.062, 140526.266, 135035.781, 128950.672, 121499.141, 116228.961, 113588.086, 110219.547, 108541.305, 105886.547, 103741.266, 102762.312, 101064.133, 100343.648, 98205.664, 98127.445, 97748.172, 96670.141, 96640.398, 95892.055, 93752.68, 94018.633, 94160.328, 92302.547, 91428.453, 91907.695, 92358.641, 90087.562, 90238.023, 90944.328, 89655.383, 89210.586, 88807.211, 89128.031, 88592.281, 87169.188, 88235.359, 86992.406, 86679.797, 85987.945, 84765.117, 85326.438, 84536.562, 83768.484, 83317.016, 82948.219, 82962.836, 82592.695, 82017.078, 82227.844, 81275.414, 81305.898, 79856.547, 79780.383, 80420.422, 78019.273, 78645.828, 79246.844, 76961.812, 77359.734, 77354.539, 77316.375, 76720.703, 77005.094, 75789.328, 75809.656, 74868.023, 75786.359, 75648.312, 74221.547, 74871.984, 74110.68, 74625.414, 73851.922, 74243.875, 73002.844, 72358.477, 73341.281, 72619.742, 72626.891, 73636.477, 72516.891, 72389.219, 73114.391, 72495.383, 71367.914, 71390.922, 71184.438, 70860.445, 71882.664, 71823.336, 70503.172, 72277.109, 68965.25], 'val_loss': [4195.702, 2802.973, 2278.882, 2135.775, 1884.12, 2077.54, 1930.806, 1892.209, 1625.455, 1662.138, 1540.429, 1476.094, 1437.807, 1441.877, 1465.33, 1424.754, 1380.454, 1557.83, 1603.809, 1531.527, 1325.387, 1487.58, 1439.859, 1461.113, 1385.318, 1281.264, 1351.524, 1348.016, 1318.948, 1336.414, 1358.806, 1324.696, 1359.246, 1361.533, 1300.09, 1354.936, 1205.505, 1323.323, 1254.655, 1270.212, 1274.338, 1221.826, 1275.308, 1200.052, 1215.524, 1194.523, 1243.136, 1223.639, 1157.933, 1175.811, 1150.878, 1196.501, 1077.036, 1199.043, 1226.115, 1119.854, 1085.039, 1073.36, 1072.47, 1115.993, 1006.807, 1115.335, 1210.38, 1034.95, 1033.243, 1117.055, 997.095, 1026.861, 1037.038, 1066.289, 1015.076, 1047.688, 991.985, 1055.604, 998.378, 1032.628, 993.623, 990.43, 1006.17, 961.639, 970.048, 966.311, 984.852, 988.901, 967.139, 1071.676, 980.674, 974.583, 974.1, 970.41, 952.041, 950.034, 1015.171, 968.788, 946.347, 993.321, 972.243, 1004.998, 981.917, 989.894]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	3898.19165		582726	15	-1	372.61500453948975	{'train_loss': [445558.281, 222234.406, 170624.484, 146490.016, 132334.297, 119415.125, 111457.188, 106064.695, 103022.391, 98984.781, 96862.953, 94961.0, 93735.391, 91932.672, 90390.039, 90157.445, 88459.789, 87689.539, 87715.695, 86111.094, 84808.469, 85818.43, 84555.156, 83101.438, 83016.422, 81528.492, 81738.141, 80582.258, 81087.727, 80016.195, 80087.633, 78816.281, 79137.781, 78024.938, 77645.562, 77118.055, 76731.375, 77197.109, 76420.016, 75551.719, 75451.594, 74852.859, 74591.055, 73791.508, 74326.398, 73676.25, 73255.203, 72504.523, 73482.992, 72459.359, 72049.68, 71568.141, 71730.039, 71290.594, 71753.508, 71438.68, 70390.945, 71368.703, 69782.859, 70723.672, 70236.5, 70466.016, 70290.914, 69863.766, 69262.938, 69168.031, 68284.75, 68962.109, 68051.07, 68510.82, 68678.539, 66929.547, 67945.086, 67658.5, 67636.68, 66829.086, 67872.641, 67205.219, 66934.336, 67081.789, 67091.227, 67183.156, 66521.492, 66435.617, 65974.391, 66049.953, 65422.82, 65172.57, 64800.793, 66134.484, 66127.828, 65307.305, 66574.812, 65747.469, 65382.816, 65520.805, 65064.809, 64814.5, 64802.484, 64271.754], 'val_loss': [3099.391, 2330.56, 1925.016, 1638.601, 1605.699, 1487.456, 1393.354, 1339.05, 1345.753, 1306.919, 1265.307, 1235.023, 1202.084, 1203.187, 1229.504, 1192.43, 1195.357, 1200.873, 1197.221, 1191.836, 1131.048, 1164.036, 1112.504, 1088.418, 1098.359, 1135.74, 1132.473, 1127.233, 1068.21, 1062.422, 1062.621, 1066.424, 1117.961, 1089.62, 1050.452, 1063.262, 1040.923, 1004.828, 1029.458, 1039.486, 1019.66, 989.233, 1034.545, 1085.649, 1063.744, 973.414, 978.537, 1021.708, 1042.549, 979.344, 991.824, 1052.461, 1010.294, 1047.814, 959.811, 1032.462, 1018.581, 976.757, 1084.183, 1008.156, 990.632, 948.37, 973.067, 969.042, 971.203, 1013.109, 934.818, 977.268, 1006.28, 967.573, 973.936, 968.045, 951.14, 943.329, 956.125, 951.845, 947.413, 973.213, 967.397, 945.616, 972.63, 980.663, 987.11, 927.46, 925.691, 940.834, 912.669, 939.341, 966.179, 943.769, 928.223, 958.46, 972.713, 975.731, 975.811, 981.214, 913.182, 944.202, 926.115, 979.25]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:73 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:111 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:111 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.15625491466532437 beta1:0.8611698301838286 beta2:0.8442834887728108 weight_decay:0.0001057198629418226 batch_size:32 epochs:100	100	1000	True	23986.4082		442264	16	-1	317.7181520462036	{'train_loss': [786721.688, 620984.625, 313801.25, 276816.156, 264546.219, 282174.844, 267706.812, 275609.469, 260675.188, 250084.531, 251038.969, 241237.406, 241758.047, 250453.094, 242521.891, 213433.719, 227673.297, 214845.141, 234922.797, 228960.875, 229496.156, 249916.125, 240162.484, 214735.688, 210550.641, 225743.984, 215770.859, 220909.797, 220177.125, 223460.453, 223868.797, 227207.156, 229753.844, 207898.344, 232965.328, 217818.125, 290342.938, 278895.062, 247895.094, 258899.906, 247375.547, 258817.969, 233783.828, 242877.094, 238800.281, 221810.328, 221330.625, 236964.859, 222889.062, 219881.234, 225404.516, 221528.062, 251518.062, 237538.391, 251574.281, 262380.219, 235489.797, 218541.391, 262888.844, 215291.312, 216629.969, 259535.094, 212862.016, 213975.531, 199608.5, 222176.328, 223172.312, 213519.453, 229977.281, 210589.219, 198380.438, 213372.5, 236141.016, 222039.531, 237589.016, 248586.828, 230112.5, 217322.375, 218446.953, 210847.703, 210812.844, 200985.062, 195717.844, 244268.047, 211753.953, 207589.297, 213432.656, 215238.922, 205315.125, 211055.578, 204254.547, 222170.703, 213709.156, 215853.781, 197970.141, 216594.094, 242157.578, 212317.312, 195842.953, 228240.656], 'val_loss': [11226.259, 6941.215, 4614.513, 3118.66, 3405.361, 4853.859, 4269.028, 5358.292, 3551.237, 3575.539, 3249.97, 2928.763, 4593.72, 3439.777, 2853.033, 3478.219, 2879.941, 4072.2, 3012.561, 2609.0, 3794.056, 5249.684, 4180.188, 2833.84, 2765.393, 3126.639, 3504.386, 3709.339, 2793.315, 3411.799, 3238.23, 7303.855, 3546.924, 2577.981, 2629.217, 2764.88, 4812.021, 5021.689, 4786.241, 4189.982, 6402.324, 4477.609, 2599.635, 2838.469, 4254.216, 3131.589, 4397.934, 3608.989, 3121.238, 3800.895, 2377.13, 4571.641, 4256.326, 4899.725, 2825.181, 3586.934, 7503.278, 2798.114, 3656.333, 5268.789, 3331.499, 3681.747, 5582.039, 2606.093, 3534.009, 4552.419, 3109.709, 2371.842, 5517.194, 2634.957, 3601.453, 6726.089, 3974.962, 3879.027, 2405.899, 2670.562, 3758.153, 5450.311, 2650.134, 3559.6, 3763.907, 2416.997, 3365.926, 3117.944, 3478.603, 3633.867, 3899.462, 4908.455, 2454.103, 4132.919, 7570.002, 5773.848, 4362.225, 3672.312, 4418.743, 3256.474, 4208.534, 4210.236, 2994.097, 5811.085]}	100	100	True
