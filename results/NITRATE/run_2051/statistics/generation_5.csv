id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	2000	True	4048.47729		428075	14	-1	279.9090337753296	{'train_loss': [251562.828, 168955.75, 151061.656, 140939.656, 134076.078, 129721.305, 123849.039, 120166.078, 117648.508, 114960.922, 113045.602, 110670.562, 108196.258, 107766.508, 105775.836, 103610.148, 102021.25, 100698.812, 100570.672, 99827.469, 97340.828, 96769.336, 96121.984, 95368.766, 93110.773, 93233.914, 92832.328, 91827.594, 91508.18, 91518.531, 90456.492, 88868.664, 88733.703, 88861.062, 87980.508, 87658.641, 86416.539, 86771.125, 86128.953, 86513.516, 85395.562, 85413.641, 84800.32, 84728.523, 84339.57, 84576.148, 83318.328, 83566.297, 83484.008, 83498.906, 82556.836, 82987.445, 82827.406, 82498.32, 82534.789, 81311.773, 81111.055, 80896.57, 80331.898, 80196.805, 80549.203, 80450.156, 79943.82, 79128.898, 80006.047, 79314.344, 79343.578, 78832.047, 78690.914, 78454.164, 79187.805, 78161.297, 78012.164, 78442.859, 76993.75, 76642.211, 77895.125, 77007.742, 77678.977, 76726.828, 76431.031, 76246.039, 76517.617, 75585.344, 75942.539, 75625.352, 76025.359, 75751.695, 75445.023, 75213.664, 74754.414, 75567.484, 74768.023, 74222.648, 74557.422, 73958.336, 73690.969, 74245.141, 73593.086, 74160.664], 'val_loss': [2397.851, 1703.302, 1693.595, 1699.039, 1693.07, 1591.905, 1763.778, 1647.99, 1864.41, 1756.867, 1743.839, 1754.081, 1759.078, 1753.624, 1515.428, 1525.639, 1574.157, 1512.399, 1424.967, 1248.403, 1522.733, 1381.976, 1266.842, 1520.351, 1439.126, 1369.262, 1481.703, 1163.469, 1372.449, 1275.916, 1214.899, 1320.38, 1253.313, 1222.003, 1168.501, 1174.956, 1188.842, 1215.011, 1198.264, 1120.798, 1157.321, 1189.151, 1160.805, 1137.372, 1200.289, 1201.536, 1141.917, 1040.729, 1050.653, 1068.958, 1041.456, 1093.356, 1060.434, 1043.397, 1047.526, 1059.608, 1179.281, 1045.894, 1033.239, 1047.926, 1052.309, 1084.782, 1026.196, 1082.764, 1086.65, 1027.52, 1038.886, 1051.939, 1118.48, 1048.051, 1037.013, 1054.763, 1024.6, 1032.371, 1004.264, 1059.568, 1040.264, 1062.517, 992.191, 973.593, 978.338, 1047.783, 990.882, 1051.813, 1037.254, 1026.62, 1057.003, 984.778, 970.286, 971.484, 996.943, 976.487, 948.361, 957.147, 1004.111, 1012.725, 986.947, 1005.495, 958.213, 982.949]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	3000	True	3818.04004		428075	14	-1	277.7764148712158	{'train_loss': [390863.938, 224557.25, 157982.938, 142425.406, 132839.812, 126447.32, 123616.344, 119047.812, 116400.422, 113703.75, 111409.742, 107686.938, 105854.703, 102874.641, 101172.695, 99822.578, 97342.93, 96461.68, 94700.648, 92756.352, 91717.484, 91125.883, 91206.648, 90160.188, 88079.984, 89188.906, 86819.18, 87515.484, 85958.406, 85984.555, 84957.422, 84716.094, 84346.203, 84074.797, 83185.062, 82966.516, 83136.594, 82510.258, 82200.5, 81171.969, 82113.266, 80628.938, 80961.828, 79863.211, 80055.672, 78747.922, 79318.523, 79718.938, 78552.172, 78270.688, 78484.125, 77946.609, 77092.023, 77461.164, 77785.039, 77085.398, 76915.961, 76158.07, 76390.586, 76557.648, 75790.984, 75644.594, 76279.023, 75058.031, 75466.719, 75614.125, 76045.578, 74761.273, 75072.484, 75285.117, 74458.453, 74758.969, 73735.125, 74553.008, 73888.188, 73682.945, 73821.188, 72883.195, 73472.719, 72862.719, 73939.633, 73724.766, 72932.688, 72544.656, 72816.555, 72737.531, 72255.586, 72576.906, 72188.219, 72561.617, 71913.484, 71721.43, 71850.008, 71857.523, 71766.242, 72077.203, 70680.961, 71731.305, 71061.062, 70125.547], 'val_loss': [4803.236, 2123.133, 1901.183, 1729.01, 1639.744, 1637.128, 1545.516, 1607.279, 1539.016, 1500.878, 1527.677, 1413.656, 1400.611, 1364.717, 1349.995, 1325.739, 1318.237, 1296.119, 1211.654, 1202.084, 1244.415, 1249.382, 1158.749, 1157.533, 1219.233, 1111.698, 1150.016, 1110.368, 1097.178, 1184.395, 1098.147, 1124.698, 1083.081, 1082.547, 1130.119, 1077.704, 1106.743, 1146.006, 1070.646, 1075.327, 1081.089, 1090.584, 1137.276, 1081.06, 1095.266, 1071.552, 1026.201, 1030.608, 1073.06, 1084.313, 987.112, 1049.356, 1054.652, 1030.768, 1019.226, 971.243, 1016.859, 1031.621, 1038.62, 1049.211, 1052.883, 1087.081, 993.501, 978.244, 1009.939, 1018.751, 975.84, 1044.28, 1012.988, 983.704, 1082.039, 978.059, 943.048, 991.761, 963.637, 985.737, 1056.046, 988.964, 1001.198, 987.347, 1008.022, 997.383, 974.435, 963.588, 941.361, 951.455, 948.078, 985.731, 945.15, 974.169, 962.891, 953.33, 998.878, 996.5, 955.775, 981.053, 915.261, 956.438, 943.326, 927.377]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	3758.75586		696089	14	-1	309.3529932498932	{'train_loss': [693125.625, 343091.25, 253088.188, 194008.562, 144761.297, 131332.547, 121636.664, 115783.766, 110531.203, 106918.219, 103750.922, 101093.07, 99336.391, 96796.406, 95476.969, 93507.203, 92316.797, 91831.688, 89859.008, 89897.211, 88683.438, 86757.773, 86764.836, 86794.195, 85732.805, 84569.344, 84655.906, 83971.516, 83339.125, 82266.891, 82398.164, 81781.18, 80887.672, 80341.094, 81224.969, 80160.375, 79224.273, 80116.32, 79978.875, 78058.539, 77600.062, 78689.094, 77047.562, 77690.297, 78181.352, 76989.148, 76838.344, 75999.82, 76317.484, 76272.352, 76175.68, 74508.688, 76717.055, 75739.852, 73238.328, 75406.617, 75447.352, 73362.945, 73467.453, 74101.742, 74336.641, 73396.188, 74154.141, 72276.594, 73853.25, 73268.469, 72352.188, 73141.586, 71581.859, 71978.242, 71865.0, 72210.43, 72468.242, 71808.344, 71085.023, 71225.32, 71316.305, 72484.094, 70609.57, 70983.562, 71100.125, 69968.305, 70945.773, 70068.109, 70457.68, 70132.414, 71142.977, 69027.82, 68782.547, 70250.914, 69549.039, 70761.719, 69964.75, 68378.32, 69077.375, 69600.352, 68633.875, 68109.906, 68946.992, 69539.727], 'val_loss': [5508.294, 3696.591, 3524.575, 2393.41, 2110.606, 1881.732, 1828.496, 1804.07, 1584.562, 1682.053, 1606.541, 1553.203, 1504.552, 1412.529, 1335.762, 1289.109, 1313.655, 1271.302, 1295.78, 1265.369, 1231.477, 1204.144, 1235.611, 1181.392, 1189.719, 1158.444, 1187.428, 1144.594, 1095.59, 1123.963, 1121.552, 1119.037, 1105.734, 1116.559, 1170.809, 1065.716, 1072.231, 1035.519, 1073.068, 1033.23, 1057.573, 1110.53, 1046.976, 1088.266, 1084.954, 1014.145, 1069.141, 1039.882, 1031.38, 1014.338, 1046.322, 983.445, 1006.429, 1022.137, 1020.052, 1001.859, 971.697, 1058.389, 986.258, 1011.841, 1015.286, 994.104, 1008.516, 992.289, 1001.718, 993.564, 964.258, 983.928, 980.273, 933.446, 960.545, 952.572, 1000.133, 987.42, 952.613, 988.018, 983.814, 955.333, 955.498, 1042.35, 966.42, 1000.479, 966.953, 942.372, 946.137, 965.773, 956.589, 924.935, 938.798, 972.257, 945.102, 919.626, 934.373, 938.261, 921.018, 896.633, 893.084, 908.565, 911.676, 948.404]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	3927.66016		370651	12	-1	269.7634365558624	{'train_loss': [307409.812, 169062.891, 154457.938, 146206.906, 138060.453, 131427.938, 126332.617, 123014.445, 118863.414, 116457.164, 114710.156, 111882.172, 109087.312, 107571.336, 106861.352, 104775.188, 103540.711, 102210.117, 101414.445, 100325.875, 99961.328, 99041.078, 98409.281, 97572.734, 97562.484, 97257.367, 96824.07, 95519.688, 94355.555, 93663.562, 93622.203, 93262.0, 92660.602, 92250.828, 91550.148, 90389.117, 90271.469, 89994.75, 89687.445, 89177.859, 88483.688, 89064.328, 87472.367, 87530.734, 87192.688, 86559.609, 87112.789, 85608.039, 86221.336, 85455.531, 84594.477, 85364.656, 83918.719, 83373.062, 83298.297, 83353.492, 82916.008, 82691.508, 82451.219, 81604.367, 81925.078, 81040.297, 80762.828, 80812.781, 81172.242, 80940.32, 79907.094, 80272.258, 80325.25, 80129.336, 79419.406, 79077.773, 78842.938, 78716.703, 80011.875, 78653.273, 78103.719, 78187.922, 77985.75, 77578.422, 78086.367, 79034.211, 78003.648, 76285.297, 76822.562, 76667.172, 77559.75, 75914.156, 76700.578, 77525.172, 75373.914, 76481.641, 75455.188, 75386.75, 76019.508, 76650.453, 75729.281, 75343.07, 75079.18, 75378.086], 'val_loss': [2082.586, 1674.714, 1782.941, 1818.853, 1701.678, 1672.288, 1622.331, 1596.597, 1802.899, 1627.278, 1521.8, 1477.527, 1502.606, 1508.535, 1459.246, 1491.653, 1365.477, 1375.471, 1365.995, 1356.211, 1402.39, 1344.119, 1412.852, 1439.286, 1323.125, 1253.893, 1261.924, 1333.502, 1335.082, 1284.607, 1308.008, 1240.396, 1229.745, 1237.833, 1309.673, 1242.489, 1237.181, 1259.232, 1274.444, 1238.115, 1233.957, 1174.127, 1237.267, 1138.698, 1193.656, 1114.169, 1218.051, 1183.471, 1205.413, 1190.099, 1206.241, 1212.435, 1271.539, 1115.628, 1133.22, 1218.135, 1163.181, 1135.854, 1158.591, 1118.138, 1135.164, 1116.899, 1123.137, 1150.683, 1079.478, 1093.765, 1093.661, 1109.165, 1086.668, 1141.259, 1216.199, 1138.035, 1146.61, 1108.58, 1146.899, 1110.664, 1096.353, 1081.032, 1111.27, 1051.093, 1073.57, 1106.824, 1078.712, 1027.924, 1064.803, 1049.903, 1066.117, 1099.528, 1057.57, 1028.86, 1047.096, 1121.837, 1039.816, 1017.975, 1047.431, 1077.017, 1041.17, 1108.186, 1076.415, 1011.1]}	100	100	True
