id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	3973.1875		452251	14	-1	279.91840624809265	{'train_loss': [393084.719, 270742.281, 216484.203, 199114.875, 190343.141, 172106.531, 136736.625, 125587.406, 120836.555, 116763.875, 112966.883, 110432.484, 106398.508, 106558.32, 103658.422, 102147.414, 100275.68, 99229.273, 98512.102, 96885.336, 95551.609, 94728.211, 94892.414, 93744.766, 92983.281, 91889.188, 92277.5, 91673.102, 90826.922, 90614.398, 88417.094, 88712.266, 88716.922, 87921.039, 88075.719, 87135.469, 86452.18, 85749.875, 85261.234, 84930.086, 84793.516, 84892.242, 84041.617, 83228.703, 82221.977, 82963.273, 82881.781, 81774.93, 82223.914, 81736.625, 81493.797, 81863.562, 81408.672, 79959.695, 80660.039, 80579.375, 80355.789, 79659.281, 79668.352, 79563.148, 79042.297, 79283.305, 78632.711, 78328.961, 78536.117, 77809.391, 77741.203, 78106.547, 77194.531, 78258.805, 77081.312, 78099.867, 76910.586, 77460.516, 77666.477, 76454.719, 76576.102, 76677.156, 76162.781, 76404.586, 76084.062, 75835.055, 76033.836, 75439.602, 76040.711, 75206.672, 75301.453, 74725.992, 75461.305, 75557.562, 74400.125, 74234.055, 74738.133, 75083.398, 74858.648, 73990.469, 74288.133, 73740.336, 74303.188, 75125.0], 'val_loss': [5472.091, 3099.051, 2878.931, 3008.784, 2556.496, 2587.598, 1848.432, 1719.262, 1752.401, 1710.79, 1663.023, 1617.603, 1601.151, 1520.474, 1575.211, 1508.154, 1437.946, 1299.43, 1397.688, 1285.416, 1250.948, 1351.426, 1292.115, 1223.4, 1250.747, 1227.946, 1231.449, 1168.094, 1208.182, 1144.837, 1145.726, 1191.288, 1183.124, 1131.286, 1123.152, 1226.684, 1146.616, 1146.222, 1118.879, 1159.557, 1120.801, 1107.592, 1076.801, 1134.758, 1079.032, 1049.486, 1077.514, 1126.467, 1076.256, 1086.93, 1098.851, 1093.314, 1052.221, 1060.937, 1077.108, 1018.458, 1039.222, 1064.603, 1061.473, 1036.694, 1042.951, 1071.188, 1030.751, 1049.133, 1070.407, 1036.829, 1045.519, 1037.078, 1018.702, 1029.55, 973.282, 1025.203, 1020.352, 992.821, 1047.615, 1007.484, 1005.779, 1037.744, 1005.196, 1078.696, 1010.336, 1041.03, 1024.974, 1031.1, 976.226, 1044.081, 988.089, 996.379, 1054.596, 982.704, 973.85, 992.21, 978.396, 979.381, 1051.516, 1018.831, 998.032, 1002.421, 947.39, 984.625]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:33 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:119 kernel_size:8 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:79 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	4447.9624		483840	15	-1	315.6519317626953	{'train_loss': [267036.5, 165373.594, 153554.328, 143843.25, 139458.375, 134928.453, 128588.242, 126184.57, 120512.984, 118211.094, 114861.375, 113527.742, 110684.664, 108648.492, 106797.297, 106060.094, 104238.758, 102742.695, 101742.742, 100845.875, 99773.898, 99492.477, 98223.383, 98256.984, 96148.352, 96225.68, 95666.805, 93965.695, 93467.461, 92092.078, 91928.508, 91370.25, 90419.109, 92223.156, 89978.0, 89943.0, 89582.047, 90691.734, 90238.508, 87880.312, 87244.82, 87483.617, 87791.133, 87135.117, 86171.203, 85365.609, 85330.648, 84531.398, 84624.391, 83618.312, 84299.641, 82843.188, 83954.609, 84344.266, 83141.922, 83228.898, 82616.445, 82347.055, 82661.367, 82161.32, 81837.406, 81825.703, 81751.992, 81079.094, 80344.883, 82449.258, 81541.961, 79510.883, 79947.523, 80421.82, 80284.734, 79116.914, 80094.047, 80007.648, 79496.75, 79884.383, 79620.102, 79394.711, 78478.68, 78977.344, 79209.18, 78696.125, 78495.766, 78580.234, 78241.594, 78390.789, 77612.57, 78663.188, 77481.492, 77681.398, 77432.938, 77186.359, 77743.859, 77495.25, 77350.391, 77254.086, 76944.266, 76050.164, 76880.906, 77433.469], 'val_loss': [1947.301, 1683.637, 1676.748, 1679.387, 1565.687, 1706.192, 1644.657, 1600.668, 1662.688, 1549.143, 1568.714, 1606.882, 1506.184, 1479.261, 1524.607, 1426.47, 1418.863, 1516.888, 1516.75, 1421.477, 1332.958, 1364.036, 1348.806, 1350.165, 1370.309, 1318.32, 1344.533, 1297.12, 1296.584, 1393.171, 1324.85, 1294.032, 1265.378, 1291.717, 1237.251, 1266.034, 1249.256, 1205.871, 1227.687, 1190.719, 1145.182, 1281.547, 1209.416, 1172.681, 1098.075, 1161.071, 1139.663, 1181.323, 1175.856, 1092.702, 1154.645, 1171.091, 1173.777, 1109.687, 1168.942, 1159.73, 1092.332, 1184.65, 1180.691, 1099.56, 1082.279, 1205.208, 1146.513, 1119.878, 1103.272, 1069.433, 1056.959, 1124.01, 1165.097, 1171.227, 1141.406, 1099.603, 1089.486, 1105.931, 1120.224, 1141.439, 1112.767, 1182.635, 1117.869, 1200.752, 1136.557, 1061.834, 1094.846, 1137.769, 1091.267, 1048.88, 1100.537, 1130.682, 1062.912, 1018.853, 1084.979, 1046.544, 1035.459, 1060.595, 1107.282, 1127.112, 1090.572, 1058.491, 1105.675, 1093.82]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	44536.77344		3066331	15	-1	272.4509735107422	{'train_loss': [920818.438, 804794.812, 815130.688, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	2000	True	4112.39697		452251	14	-1	268.3578817844391	{'train_loss': [393296.0, 302335.062, 215387.547, 192545.438, 172955.969, 142480.812, 127072.391, 118982.234, 115750.664, 110730.797, 108592.398, 105672.586, 103304.367, 101510.453, 100513.148, 98552.406, 96996.547, 96976.062, 95476.828, 94791.578, 93367.492, 92978.57, 92437.555, 91827.297, 90515.945, 89318.281, 89209.094, 89617.875, 87341.914, 87778.172, 87569.82, 87660.945, 86263.133, 86425.773, 86058.117, 84665.555, 85912.031, 84589.344, 84455.469, 83797.344, 83398.047, 84462.508, 83559.258, 83303.82, 83236.883, 82569.812, 81232.219, 82871.688, 82479.117, 81498.727, 82121.336, 81719.008, 81092.625, 81654.945, 80699.039, 80501.789, 81334.008, 79542.195, 79760.508, 79311.844, 79396.562, 79841.266, 79710.562, 79641.023, 78542.359, 78893.156, 79115.469, 78233.5, 78123.008, 78187.523, 78159.883, 77797.555, 78077.594, 76706.562, 77551.625, 77609.75, 77518.711, 77477.344, 77725.156, 76763.219, 76541.031, 76574.07, 76838.664, 75721.562, 76234.266, 76450.266, 75971.609, 76090.805, 75939.328, 75283.555, 74378.883, 75034.539, 75110.414, 75685.195, 75103.711, 74563.273, 74614.867, 74439.914, 74641.586, 74054.945], 'val_loss': [5261.463, 3044.576, 2798.413, 2690.253, 2654.315, 2276.821, 2146.921, 2020.159, 1874.601, 1713.919, 1755.663, 1659.563, 1530.79, 1616.251, 1447.979, 1486.07, 1468.163, 1353.817, 1322.387, 1339.783, 1293.496, 1339.613, 1286.103, 1218.533, 1225.481, 1282.887, 1212.001, 1229.362, 1156.752, 1170.249, 1138.546, 1167.456, 1132.722, 1151.541, 1169.613, 1152.968, 1165.87, 1145.091, 1206.333, 1131.665, 1140.419, 1099.254, 1126.46, 1106.818, 1138.897, 1114.888, 1112.179, 1114.852, 1115.94, 1123.382, 1110.829, 1094.736, 1136.862, 1065.158, 1079.248, 1153.04, 1090.612, 1105.167, 1061.504, 1079.413, 1065.887, 1117.145, 1069.488, 1095.199, 1073.639, 1095.663, 1042.443, 1066.338, 1066.566, 1029.875, 1087.894, 1061.506, 1053.734, 1038.688, 1029.147, 1041.453, 1068.317, 1029.132, 1050.335, 1043.176, 1061.257, 1038.942, 1010.664, 1018.141, 1028.49, 979.856, 1003.504, 1029.879, 1000.665, 1002.34, 1035.892, 997.777, 1064.119, 1039.424, 1042.001, 1013.133, 995.21, 985.064, 1065.15, 1002.504]}	0	100	True
