id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4168.11719		696089	14	-1	315.25251626968384	{'train_loss': [765254.938, 320873.5, 210512.984, 175085.016, 152402.078, 140366.797, 131075.406, 124104.258, 117578.523, 114158.539, 110267.352, 107840.5, 105854.641, 101849.914, 101503.852, 98732.891, 98546.547, 97309.18, 95703.938, 95377.836, 94497.203, 92884.758, 92792.742, 91393.336, 91096.922, 89955.734, 89673.18, 88541.344, 88569.898, 87617.688, 87085.891, 86823.016, 87052.648, 86684.047, 86898.258, 85457.008, 85825.977, 84999.062, 84840.758, 83501.82, 83743.969, 83559.438, 83678.172, 82315.008, 83164.469, 83029.938, 81652.578, 82369.844, 81546.984, 82255.508, 81392.562, 81752.727, 81525.984, 81547.836, 80450.828, 81035.414, 80407.125, 80545.664, 79766.461, 79731.398, 79596.555, 80200.727, 79896.602, 80044.812, 79567.25, 79803.281, 78358.625, 78988.219, 78678.508, 78287.516, 78243.805, 77502.727, 77895.211, 77771.555, 78696.062, 77443.547, 78523.852, 77555.836, 77177.633, 78037.312, 77097.109, 77792.461, 76349.883, 77263.781, 76611.477, 76234.914, 76946.453, 75589.297, 76688.516, 76189.922, 75712.711, 75134.609, 75386.164, 75087.359, 75381.016, 75568.68, 74903.953, 75879.625, 74538.297, 75156.961], 'val_loss': [6307.938, 2811.097, 2853.456, 2721.906, 2146.313, 1795.726, 1843.358, 1695.851, 1720.295, 1588.395, 1557.532, 1507.966, 1542.095, 1647.491, 1457.363, 1518.477, 1446.272, 1681.946, 1573.217, 1671.961, 1420.046, 1318.031, 1379.759, 1356.753, 1387.759, 1344.535, 1368.301, 1376.459, 1472.543, 1407.118, 1415.904, 1231.161, 1336.24, 1240.429, 1353.323, 1291.195, 1206.048, 1219.833, 1291.269, 1246.669, 1239.967, 1404.657, 1256.823, 1354.47, 1243.521, 1224.413, 1229.257, 1191.588, 1306.512, 1253.131, 1251.349, 1281.044, 1189.522, 1276.394, 1169.891, 1182.689, 1245.627, 1329.819, 1118.539, 1168.454, 1157.442, 1300.361, 1174.676, 1133.423, 1176.388, 1178.645, 1277.503, 1174.51, 1330.525, 1093.685, 1088.844, 1212.568, 1188.956, 1148.194, 1187.151, 1127.06, 1208.717, 1155.841, 1115.695, 1144.708, 1091.373, 1103.052, 1151.518, 1158.096, 1044.86, 1147.364, 1079.841, 1133.629, 1080.008, 1143.587, 1059.894, 1141.666, 1075.588, 1084.752, 1096.998, 1102.638, 1207.789, 1156.811, 1123.861, 1068.402]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:12 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:91 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:77 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.10134321967434042 beta1:0.8757456768906996 beta2:0.9185131468757451 weight_decay:7.996517212786006e-05 batch_size:32 epochs:100	100	1000	True	44567.78906		24616732	13	-1	337.03886365890503	{'train_loss': [916670.812, 804482.438, 804482.438, 4803381.5, 804812.062, 818853.562, 931315.875, 3501521.5, 833478.062, 842501.938, 1134832.25, 2864791.75, 804482.438, 1058648.125, 1187249.5, 2371651.75, 1567333.75, 1267671.625, 871830.688, 1635234.875, 1412862.875, 1520228.75, 804746.312, 970007.25, 2320586.25, 1564767.25, 1445744.5, 924401.25, 1707206.0, 1813917.75, 1158733.375, 816327.188, 1172340.0, 1928776.875, 1229229.5, 944945.938, 1079400.75, 1714544.25, 1061495.125, 1098806.5, 876180.75, 1112748.25, 1156922.0, 1061566.875, 835341.75, 1285087.875, 1393284.625, 1243783.0, 900691.688, 1055973.375, 1544676.125, 1134703.75, 988094.625, 906096.25, 1152307.25, 1578522.625, 990796.875, 1003346.625, 1065011.625, 1757635.0, 1628336.125, 842196.438, 1019801.125, 1167408.625, 1539838.875, 1560187.25, 966606.0, 1031118.0, 1512713.0, 1172376.25, 1004721.938, 1054288.5, 1107232.625, 1283976.375, 1415350.5, 988802.438, 1180059.75, 1343890.25, 1248552.5, 1259201.25, 1014053.875, 1252152.625, 1397855.875, 1091194.5, 968647.688, 1037391.312, 1333785.75, 1217124.5, 1209654.75, 931346.938, 1226004.0, 1314655.5, 1052353.875, 1093931.0, 1057024.25, 1179607.875, 1097080.0, 1159134.0, 906876.375, 1041044.938], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 14766.377, 11044.456, 11044.456, 56689.43, 143203.703, 11044.456, 46906.375, 1610550.75, 2613795.0, 82284.375, 1776553.125, 466360.562, 11044.456, 556019.625, 11042.421, 300093.062, 649062.25, 1709156.5, 1435737.25, 58194.766, 2904578.25, 270214.219, 320665.719, 711326.125, 215355.938, 780395.5, 27489.402, 142620.781, 127451.141, 288383.75, 121056.188, 31692.941, 151360.516, 11044.456, 19471.219, 1880149.125, 11044.456, 11044.456, 11044.456, 11044.456, 401658.188, 640699.312, 39512.336, 230629.656, 503066.531, 11036.017, 344089.625, 12149.017, 356965.625, 11044.395, 23939.266, 11044.456, 21690.646, 682708.25, 2309103.0, 119490.516, 11044.456, 135109.906, 267354.25, 46348.531, 111605.203, 429877.188, 123982.836, 895608.625, 666839.125, 916985.625, 11044.456, 137319.375, 10869.109, 11044.456, 163378.141, 975520.812, 840059.25, 393498.844, 250273.875, 120260.156, 216010.828, 147527.531, 364413.219, 243023.594, 24214.928, 61622.969, 192774.781, 11044.456, 2004092.75, 545318.5, 11044.456, 11044.456, 11044.456, 11044.456]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:52 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.24135323220063165 alpha:0.8661623565077308 weight_decay:5.1621231932745314e-05 batch_size:32 epochs:100	100	1000	True	47737.57031		695817	14	-1	313.8370695114136	{'train_loss': [1102239.25, 1291857.75, 830711.938, 4160031.5, 805935.312, 2333978.5, 804482.438, 2467569.75, 804482.438, 1152271.125, 1392017.75, 1971294.625, 970427.812, 804482.438, 1369050.25, 804482.438, 2586036.75, 804482.438, 2276230.75, 2748880.0, 804482.438, 810129.375, 1847972.75, 2524805.25, 3025667.75, 860648.188, 804482.438, 4673301.0, 1368765.625, 871837.812, 2380018.25, 804482.438, 2263582.75, 1275946.5, 804482.438, 2583008.75, 1997196.125, 823590.25, 2214056.5, 938890.938, 2340382.5, 1056190.5, 846585.438, 1182955.375, 1651020.0, 960783.375, 1262077.625, 1066704.625, 1362200.125, 1106733.875, 1043999.375, 1322851.375, 1228229.5, 1173103.375, 1231617.625, 1206747.625, 1239222.875, 1214131.375, 1082976.75, 1430369.875, 1022701.062, 1258057.625, 1350301.375, 1306006.875, 1010820.312, 1292384.375, 1345117.75, 1231184.0, 1302530.25, 1335556.75, 1227055.125, 1070598.75, 1217709.75, 1556927.125, 1144413.875, 1094077.25, 1547603.125, 1259160.0, 1002196.938, 1355243.875, 1383133.375, 1117799.125, 1445263.25, 1236412.625, 1063738.875, 1241618.375, 1311614.125, 1192441.125, 1094483.625, 1325608.5, 1196562.875, 1091476.75, 1414214.5, 1093803.25, 1075311.0, 1277317.125, 1480764.125, 1318987.625, 1043109.312, 1204451.625], 'val_loss': [11044.456, 11044.456, 13236.791, 11044.442, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 985150.312, 11044.456, 11044.456, 11044.456, 11044.455, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 495243.375, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 13179.722, 11044.456, 187971.469, 1279634.5, 11044.456, 11044.456, 11044.456, 11044.456, 14616.381, 11989.929, 11044.456, 15417.664, 11044.456, 11044.456, 11044.456, 11025.939, 11044.456, 13449.359, 11044.456, 11940.767, 14602.83, 11044.456, 16621.928, 11856.573, 11044.456, 11044.456, 11044.455, 14803.4, 17217.584, 12199.145, 11044.456, 11605.795, 11044.456, 11044.455, 11044.456, 11044.456, 11044.456, 15491.299, 11044.456, 18825.539, 11044.456, 33990.414, 19273.781, 11044.455, 11044.456, 11044.453, 11508.308, 11044.451, 11044.456, 11039.135, 11044.456, 13844.954, 11043.258, 14096.56, 37411.312, 18890.887, 11044.456, 11044.382, 11044.456, 17551.951, 11044.456, 11044.456, 11044.456, 11033.475, 11041.605, 11044.456, 11044.456, 11844.362]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:16 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	44538.54297		4892139	13	-1	297.9684193134308	{'train_loss': [1284107.625, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456]}	100	100	True
