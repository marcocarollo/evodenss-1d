id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	3683.25952		582726	15	-1	359.5439262390137	{'train_loss': [491947.125, 214926.312, 168618.719, 147184.281, 132856.562, 122673.922, 116227.391, 111045.883, 107550.305, 103709.539, 100086.602, 98432.523, 96395.203, 93963.914, 92208.352, 91940.367, 89095.438, 88174.969, 87817.164, 85664.461, 85937.25, 84391.391, 84707.266, 83070.156, 83589.438, 81443.367, 81452.195, 81076.945, 80140.477, 80553.414, 78217.055, 79060.32, 77928.75, 78285.227, 77315.695, 77375.172, 76325.898, 75932.062, 76470.375, 76154.641, 75284.648, 74482.055, 74297.25, 73222.32, 73336.25, 72957.406, 72635.453, 71947.234, 72129.609, 70773.094, 70901.797, 70343.18, 70370.328, 70206.68, 69305.93, 70274.508, 68928.297, 69030.039, 70577.656, 68358.047, 68339.383, 68129.508, 68454.523, 67709.07, 68559.602, 68217.047, 67348.258, 67003.094, 67333.844, 67076.586, 67497.367, 67632.578, 66643.531, 65767.711, 66297.422, 66867.266, 66274.406, 65606.992, 66250.086, 65055.355, 66776.672, 65210.215, 64875.238, 65800.562, 64876.68, 64425.199, 65172.422, 64725.0, 64640.164, 64173.574, 64792.988, 64007.113, 64520.004, 63422.746, 63940.805, 64903.117, 63116.559, 64066.832, 63135.504, 63078.207], 'val_loss': [3329.328, 2539.926, 2039.946, 1842.944, 1575.718, 1480.258, 1445.528, 1391.59, 1344.854, 1283.797, 1353.49, 1328.86, 1228.664, 1213.159, 1260.353, 1165.729, 1103.291, 1174.544, 1165.682, 1137.715, 1142.298, 1102.488, 1152.438, 1122.225, 1153.556, 1105.606, 1165.136, 1069.463, 1131.085, 1088.929, 1118.831, 1000.328, 1024.936, 1369.767, 1046.544, 986.021, 996.111, 1034.469, 970.674, 1039.714, 977.547, 1022.612, 1011.784, 1022.761, 1024.963, 1016.691, 1008.786, 1046.568, 968.194, 999.871, 1003.818, 944.097, 959.504, 967.225, 965.647, 1030.867, 961.33, 1005.458, 982.173, 926.168, 961.724, 933.955, 929.03, 990.518, 916.511, 1007.764, 929.726, 911.913, 1011.24, 937.104, 916.076, 992.438, 914.871, 941.096, 914.855, 910.356, 904.774, 920.377, 903.486, 930.075, 934.21, 886.443, 940.159, 918.047, 907.913, 905.767, 923.315, 880.094, 904.37, 917.794, 918.037, 859.513, 931.078, 881.458, 879.103, 912.231, 896.17, 932.517, 949.192, 904.15]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:51 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:32 epochs:100	100	1000	True	4103.36133		628847	16	-1	369.14390444755554	{'train_loss': [562683.188, 358686.156, 194502.312, 154489.578, 135826.844, 123528.977, 119269.664, 114173.242, 110959.406, 108281.852, 106093.672, 104601.859, 102185.992, 101270.922, 100088.516, 99570.539, 97959.789, 96287.938, 95429.516, 95172.734, 93077.922, 92713.945, 91440.664, 91411.672, 89660.062, 89040.195, 89192.898, 87992.852, 86788.953, 86911.859, 85022.773, 86120.414, 85436.188, 85653.953, 83207.562, 82853.383, 82055.688, 82092.266, 81347.398, 82705.141, 81363.969, 80716.742, 79163.148, 80977.516, 79401.938, 78969.391, 78960.203, 79716.461, 78431.18, 78619.102, 77237.891, 76909.812, 77699.539, 77221.273, 76268.711, 76382.461, 76213.984, 75023.125, 76553.68, 75837.148, 76092.789, 76130.805, 75589.883, 76252.695, 76207.445, 75717.07, 74905.539, 74576.289, 74165.672, 74015.164, 73903.234, 74643.57, 72574.641, 73374.891, 72533.711, 72984.656, 72626.609, 72471.469, 72127.859, 72355.32, 72198.438, 71138.578, 71711.469, 72502.789, 71353.211, 71093.609, 72321.836, 71606.133, 70828.695, 71059.656, 71634.547, 70940.133, 70535.766, 71730.438, 70168.273, 69619.188, 70258.812, 71346.477, 70732.289, 69841.312], 'val_loss': [6902.551, 3824.439, 2335.713, 2217.036, 1717.446, 1603.392, 1511.682, 1463.487, 1459.721, 1446.114, 1446.492, 1423.343, 1371.828, 1374.087, 1407.099, 1378.862, 1337.907, 1334.715, 1319.888, 1286.112, 1303.793, 1295.164, 1284.441, 1267.738, 1275.57, 1245.705, 1266.117, 1296.022, 1300.636, 1247.823, 1215.257, 1280.759, 1223.41, 1228.759, 1187.189, 1233.57, 1315.805, 1298.442, 1228.01, 1138.784, 1189.55, 1237.372, 1257.34, 1135.579, 1103.213, 1101.535, 1148.207, 1133.89, 1125.91, 1155.797, 1151.491, 1121.736, 1097.057, 1121.534, 1046.746, 1083.938, 1059.574, 1062.822, 1078.114, 1045.972, 1076.714, 1052.244, 1099.227, 1132.949, 1114.797, 1073.969, 1059.71, 1042.725, 1084.544, 1076.183, 1074.672, 1010.561, 1015.899, 1025.693, 1050.683, 1071.992, 1043.932, 1070.813, 1055.363, 1033.668, 1052.547, 1045.928, 1037.711, 992.195, 995.717, 1037.683, 1102.533, 1022.837, 1083.586, 1013.075, 1008.588, 1045.591, 1047.873, 1017.198, 1026.199, 990.345, 1015.801, 1109.154, 1037.791, 1000.001]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	3974.46729		468974	14	-1	322.9912931919098	{'train_loss': [347604.125, 171842.109, 144309.141, 129782.016, 122469.711, 116777.125, 113613.133, 110119.328, 105656.898, 103617.219, 101877.484, 99991.781, 98730.156, 96959.641, 96806.258, 95349.461, 94249.539, 93087.469, 92546.82, 91760.094, 91087.547, 90016.414, 90498.625, 87973.891, 88474.945, 87614.914, 87396.531, 86184.547, 85112.242, 85722.602, 84610.219, 84133.031, 83389.156, 83973.992, 81913.336, 81522.695, 80466.461, 81459.68, 79703.594, 80349.531, 79281.188, 78275.023, 78088.484, 78175.172, 76872.57, 76983.062, 75681.227, 75671.633, 75870.312, 74846.531, 74587.938, 74647.492, 73696.641, 73500.109, 73859.594, 73502.102, 72291.969, 72462.336, 73332.891, 71532.047, 71777.25, 72422.516, 71894.25, 71271.234, 71193.734, 70577.805, 70957.898, 71058.461, 71204.641, 71140.078, 70958.312, 71185.117, 68838.133, 70428.828, 69247.805, 69870.797, 69186.672, 69470.641, 68115.055, 69932.25, 68559.023, 68982.586, 68468.078, 68081.859, 68380.562, 68886.758, 68016.062, 68328.406, 67120.109, 68000.047, 67714.859, 67758.266, 67237.57, 67811.281, 66955.914, 66474.641, 67023.281, 66477.188, 66350.398, 66591.766], 'val_loss': [2494.946, 2371.9, 2485.75, 1520.011, 1468.165, 1475.602, 1434.125, 1371.145, 1372.648, 1323.698, 1359.157, 1307.266, 1263.273, 1281.233, 1244.961, 1248.962, 1216.809, 1214.377, 1196.145, 1209.162, 1170.024, 1165.553, 1166.341, 1185.884, 1192.778, 1171.579, 1142.58, 1144.172, 1151.698, 1148.456, 1119.713, 1125.571, 1150.587, 1156.205, 1142.82, 1125.143, 1131.359, 1147.309, 1165.227, 1109.184, 1135.897, 1120.69, 1100.51, 1074.048, 1143.219, 1173.4, 1107.164, 1178.774, 1097.51, 1100.861, 1061.896, 1068.436, 1121.24, 1127.466, 1043.25, 1077.662, 1036.46, 1077.127, 1069.03, 1067.157, 1079.198, 1033.641, 1036.762, 1047.35, 1016.664, 1120.058, 1004.211, 1074.694, 973.751, 993.666, 1042.705, 1017.775, 1036.005, 1005.265, 1075.471, 1049.333, 986.054, 1052.391, 1011.466, 965.726, 986.03, 994.058, 1030.312, 1050.622, 974.087, 972.338, 979.176, 988.089, 1079.089, 1004.458, 999.762, 985.571, 1004.428, 991.397, 1066.709, 974.448, 972.706, 1001.29, 983.592, 965.601]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:100 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:100 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:23 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.17086469976195787 alpha:0.9852932025065296 weight_decay:0.00038315889612095384 batch_size:32 epochs:100	100	1000	True	44534.8125		451542	15	-1	298.6663315296173	{'train_loss': [897797.938, 804482.438, 804458.188, 804482.438, 804482.438, 804523.938, 804482.438, 854251.938, 804482.438, 804482.438, 807686.812, 807620.688, 862719.5, 804837.188, 804482.438, 804482.438, 1194882.0, 806129.125, 806370.5, 806681.688, 804918.375, 1046976.562, 807302.5, 805666.062, 866449.875, 804480.625, 804479.375, 832666.0, 811507.75, 805648.375, 838442.938, 808204.5, 804699.312, 859867.438, 808776.438, 809040.938, 809064.688, 878457.75, 805359.375, 808666.562, 819942.312, 805446.938, 807338.188, 809243.688, 947542.688, 804715.75, 806411.5, 814829.938, 805235.312, 851484.062, 805207.188, 805087.375, 823500.312, 812730.188, 804567.812, 806759.188, 804458.438, 843772.062, 806013.312, 810230.688, 807343.812, 953516.125, 804901.25, 805122.875, 804606.875, 805586.875, 844797.75, 805884.812, 845537.25, 806891.0, 806786.188, 810860.125, 835490.75, 805010.375, 906298.562, 804690.875, 804515.875, 804779.125, 805488.5, 804470.438, 817447.562, 804932.75, 804482.438, 825299.75, 806104.5, 1026685.375, 787626.25, 770986.938, 722169.25, 798629.75, 698916.812, 772232.125, 813450.75, 819799.438, 826800.562, 804849.0, 882153.062, 805758.562, 804590.0, 806556.125], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.455, 11044.455, 11044.455, 11044.455, 11044.455, 11044.455, 11043.785, 11044.441, 11044.436, 11039.154, 11044.439, 14424.117, 11035.782, 11041.916, 11044.456, 11044.455, 11044.456, 11044.532, 11044.451, 11044.418, 11044.456, 11043.98, 11044.455, 11044.455, 11044.456, 11044.456, 11044.456, 11044.403, 11044.386, 11044.327, 11044.456, 11044.455, 11044.455, 10985.454, 11044.373, 11044.445, 11044.414, 11044.38, 11027.186, 11044.42, 11043.843, 11041.095, 11044.456, 11044.456, 11044.456, 11044.456, 11044.455, 11044.456, 11044.456, 11044.456, 11044.455, 11044.452, 11044.455, 11044.455, 11044.455, 10754.842, 11044.426, 11044.414, 11044.456, 11044.456, 11044.456, 11044.396, 11044.456, 11044.455, 11044.383, 11044.277, 11044.456, 11044.455, 11044.443, 11044.449, 11044.456, 11044.456, 11044.455, 11044.456, 11044.455, 10988.176, 10911.887, 10066.951, 9495.137, 10697.04, 9359.348, 11044.404, 11044.363, 11044.17, 11044.42, 11044.434, 11044.455, 11044.456, 11044.456, 11044.456]}	100	100	True
