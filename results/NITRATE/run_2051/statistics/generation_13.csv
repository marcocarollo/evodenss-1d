id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	4077.07715		582726	15	-1	383.7468509674072	{'train_loss': [476484.0, 229088.0, 181090.531, 161527.641, 141731.125, 130650.625, 123735.898, 118323.078, 113909.062, 109690.703, 109134.695, 107201.508, 103535.836, 102861.031, 102982.742, 99586.539, 98015.273, 98152.438, 96636.055, 94630.641, 95259.031, 93039.734, 92267.602, 93003.07, 90758.156, 90044.648, 90608.25, 89363.305, 88206.359, 88078.695, 87408.312, 86713.023, 86588.148, 85942.727, 84844.859, 84351.969, 84323.258, 84338.68, 83207.008, 82895.93, 82449.117, 82439.711, 82477.328, 81812.5, 81565.141, 80633.086, 80345.672, 81332.617, 79548.023, 79919.953, 79090.008, 78901.602, 78565.258, 78390.305, 77714.227, 76954.078, 77262.609, 77936.547, 76686.789, 77330.297, 76443.562, 76171.719, 75461.391, 75273.891, 76165.812, 75255.133, 74784.672, 74241.188, 74356.383, 75313.875, 73877.406, 73745.359, 73422.969, 74358.422, 72788.953, 72656.656, 72980.125, 73031.359, 72611.945, 71766.469, 72126.266, 71761.875, 71613.109, 71581.891, 71156.508, 70921.773, 71676.82, 71270.609, 71868.672, 70554.719, 71884.742, 69917.156, 69662.883, 71142.203, 71675.93, 70346.789, 71171.953, 68893.789, 69971.297, 69841.469], 'val_loss': [3129.621, 2176.793, 2185.129, 1748.37, 1674.522, 1572.763, 1585.882, 1549.142, 1473.709, 1393.568, 1494.066, 1330.813, 1385.394, 1339.297, 1373.646, 1341.948, 1365.081, 1310.453, 1223.929, 1243.04, 1227.801, 1267.378, 1300.794, 1224.328, 1287.821, 1191.005, 1215.485, 1249.062, 1236.721, 1267.129, 1240.429, 1135.331, 1284.172, 1286.961, 1152.783, 1217.623, 1108.209, 1171.172, 1142.247, 1182.662, 1269.942, 1099.748, 1187.124, 1197.322, 1140.518, 1230.7, 1238.554, 1110.685, 1092.848, 1162.35, 1107.047, 1153.123, 1208.426, 1200.132, 1088.937, 1035.934, 1209.403, 1228.012, 1126.606, 1240.78, 1196.358, 1194.586, 1069.658, 1167.477, 1022.064, 1080.66, 1258.872, 1084.698, 1167.149, 1002.376, 1035.803, 1094.77, 1139.471, 1036.946, 1084.372, 1004.691, 1103.433, 1139.324, 1205.407, 1135.139, 1131.269, 1121.733, 1124.719, 1062.952, 1064.605, 1074.719, 1042.67, 1067.315, 1036.008, 1022.203, 1066.644, 952.933, 1000.987, 1012.413, 1007.703, 1004.693, 1061.63, 1049.355, 1000.625, 1023.595]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:112 kernel_size:10 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4027.01953		698609	14	-1	381.2786810398102	{'train_loss': [555818.438, 244927.203, 206573.844, 186306.094, 173300.953, 160686.188, 148282.734, 141400.953, 132206.969, 130846.672, 123210.898, 120539.727, 116024.094, 114526.078, 112082.398, 109274.273, 108184.656, 107380.945, 105147.57, 103516.68, 102645.672, 100950.773, 100344.688, 98971.383, 98132.375, 96839.133, 95432.594, 94689.945, 94263.859, 93068.641, 92598.945, 92439.289, 91159.453, 90157.875, 91518.359, 89490.5, 87913.359, 87236.391, 87361.984, 87297.875, 85910.656, 85993.391, 85263.492, 85820.844, 84700.625, 85173.875, 85109.883, 84672.086, 84035.891, 83243.227, 83893.391, 81924.805, 82484.031, 82714.227, 81715.844, 80972.078, 80507.625, 80972.883, 80553.492, 79306.57, 80041.617, 79856.352, 80333.148, 79071.492, 78943.93, 80052.469, 77494.406, 79359.602, 77756.805, 78482.297, 76827.617, 78327.656, 76743.945, 77506.453, 77364.922, 77194.602, 76980.672, 76880.32, 75257.227, 77464.898, 77528.781, 74975.414, 75760.781, 76493.508, 75633.312, 76170.078, 75375.977, 76040.461, 74252.477, 75651.477, 75187.008, 75335.445, 75395.562, 75791.125, 74643.312, 74169.836, 73937.844, 74201.328, 73918.414, 74808.523], 'val_loss': [5786.442, 2679.673, 1926.811, 2001.037, 2209.255, 2235.135, 1981.184, 1920.771, 1773.562, 1630.532, 1541.044, 1584.549, 1500.078, 1520.666, 1556.434, 1514.015, 1459.604, 1408.352, 1366.675, 1414.453, 1367.872, 1384.663, 1347.549, 1371.615, 1328.185, 1328.236, 1318.376, 1258.71, 1270.99, 1312.588, 1215.358, 1244.115, 1190.761, 1178.734, 1179.556, 1184.651, 1129.168, 1202.483, 1206.005, 1115.764, 1164.853, 1125.077, 1135.292, 1134.056, 1121.301, 1212.245, 1091.066, 1116.961, 1156.009, 1093.749, 1205.904, 1086.637, 1073.881, 1045.014, 1067.36, 1075.283, 1096.886, 1046.273, 1032.385, 1017.978, 1033.045, 1048.463, 1026.907, 1014.824, 1028.722, 1011.393, 1053.971, 996.942, 1009.672, 1000.62, 1001.552, 995.078, 1003.171, 1034.321, 977.54, 1021.919, 962.048, 973.93, 1037.471, 1003.666, 1038.311, 984.213, 962.444, 1005.927, 1026.797, 1008.473, 1020.78, 995.761, 959.713, 988.085, 972.471, 992.509, 981.121, 963.405, 1000.922, 978.604, 954.095, 962.276, 1037.356, 950.746]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:120 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:6 epochs:100	100	1000	True	4004.8313		503058	14	-1	669.2962901592255	{'train_loss': [226889.688, 131475.812, 116954.25, 109837.531, 105737.195, 102380.562, 98315.234, 95839.938, 95284.898, 92454.891, 91798.305, 90674.836, 89223.219, 88360.734, 87583.578, 85774.594, 85401.914, 84959.531, 84097.805, 83592.367, 82278.125, 81623.922, 81279.75, 80886.0, 80126.875, 80546.156, 80107.242, 79221.281, 78649.016, 78532.188, 77813.453, 77617.719, 77436.562, 76810.852, 76553.367, 76711.422, 76038.195, 76051.852, 75424.305, 75405.844, 74129.617, 74414.75, 74750.453, 74172.102, 74618.695, 74376.5, 73407.609, 73392.43, 73515.328, 72875.797, 73691.516, 73121.5, 73284.0, 72856.531, 73004.195, 72128.203, 71865.172, 72134.258, 72360.133, 71451.805, 71806.0, 71423.188, 71233.016, 71069.375, 71064.758, 70586.172, 70945.133, 70832.617, 70419.102, 69718.617, 70803.875, 69807.523, 70092.109, 70069.156, 69804.938, 69456.766, 69293.555, 69051.531, 68665.188, 68774.68, 68193.82, 68869.641, 67980.852, 68731.047, 68800.656, 68918.812, 68695.516, 68072.742, 68431.211, 68421.445, 67508.828, 67190.32, 67130.016, 67569.062, 68045.188, 67983.844, 67434.5, 67858.852, 67420.453, 67907.125], 'val_loss': [342.677, 284.56, 286.167, 315.068, 273.619, 301.7, 259.468, 283.472, 258.442, 259.74, 268.056, 235.079, 249.99, 235.719, 239.284, 267.031, 236.872, 230.418, 249.119, 228.292, 231.091, 227.543, 222.085, 221.41, 212.842, 205.489, 215.61, 200.341, 215.811, 222.755, 204.613, 201.275, 196.364, 215.022, 192.832, 192.815, 206.424, 196.792, 197.895, 194.335, 199.27, 205.201, 194.012, 194.314, 196.486, 191.238, 189.925, 188.19, 185.565, 187.309, 193.985, 184.878, 189.053, 186.868, 199.877, 195.954, 192.231, 188.917, 205.777, 191.127, 192.857, 201.49, 185.889, 181.282, 193.415, 185.421, 186.922, 188.284, 190.833, 181.992, 182.598, 202.925, 193.029, 181.195, 183.652, 180.971, 186.963, 187.988, 183.601, 181.991, 182.405, 187.789, 195.834, 180.796, 185.26, 194.243, 178.285, 185.112, 186.769, 180.578, 173.996, 184.201, 173.294, 185.73, 178.028, 169.554, 187.374, 174.473, 188.772, 178.431]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:109 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:109 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:37 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	4258.56787		1181691	15	-1	316.47178649902344	{'train_loss': [908653.625, 810485.25, 923091.875, 813466.875, 826561.25, 836892.5, 813306.062, 633188.438, 248115.859, 152227.0, 136177.891, 127691.328, 121898.633, 117703.445, 112577.07, 110537.727, 107874.102, 105958.656, 104681.391, 103545.758, 103627.898, 101332.445, 100571.719, 101394.008, 101301.617, 98615.867, 96660.609, 97859.789, 97854.148, 96814.797, 96993.406, 96144.906, 96709.211, 96051.938, 96415.227, 94593.531, 94286.164, 94334.234, 93991.18, 93916.969, 93401.211, 93118.805, 93629.758, 91448.203, 92120.859, 92334.5, 92242.641, 91963.0, 92304.586, 89978.555, 90440.102, 90492.297, 89846.398, 90473.469, 90752.969, 90482.164, 89590.617, 90843.055, 88661.133, 89818.344, 88943.289, 89489.039, 88250.758, 89322.398, 89421.266, 88523.336, 88247.258, 88246.125, 86824.734, 89165.586, 87410.492, 87977.953, 86406.812, 87101.766, 86936.422, 87379.328, 87156.0, 87443.609, 86393.367, 86627.828, 85757.836, 84762.711, 86293.695, 85424.172, 86509.766, 86860.375, 84284.266, 85653.453, 85232.812, 84008.641, 85897.234, 83945.719, 86215.828, 86192.102, 84316.734, 84100.828, 83676.016, 84521.289, 84894.258, 85418.961], 'val_loss': [11044.354, 11043.884, 11019.976, 10951.102, 11228.352, 10920.866, 10380.268, 4402.996, 2464.395, 1735.782, 1545.349, 1472.984, 1427.371, 1361.253, 1323.951, 1329.559, 1403.168, 1295.357, 1300.391, 1276.081, 1363.859, 1342.209, 1225.97, 1265.955, 1225.84, 1264.228, 1172.262, 1196.215, 1201.081, 1211.818, 1269.556, 1331.335, 1226.444, 1249.851, 1130.08, 1146.553, 1212.16, 1161.679, 1117.376, 1105.044, 1187.717, 1215.582, 1214.783, 1116.885, 1195.132, 1226.029, 1096.769, 1137.049, 1223.071, 1176.165, 1110.267, 1165.117, 1080.246, 1059.267, 1189.193, 1101.592, 1166.287, 1189.478, 1099.773, 1103.085, 1121.583, 1155.906, 1118.428, 1109.65, 1138.827, 1175.873, 1136.843, 1137.007, 1134.845, 1122.359, 1177.014, 1066.649, 1057.971, 1180.905, 1119.437, 1079.088, 1172.411, 1061.089, 1037.252, 1021.886, 1075.086, 1083.999, 1041.079, 1165.144, 1006.115, 1092.633, 1035.312, 1031.662, 1010.657, 1038.04, 1021.189, 1210.755, 1122.054, 1151.902, 995.228, 1052.5, 989.815, 1051.874, 1115.161, 1026.332]}	100	100	True
