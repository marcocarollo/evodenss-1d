id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:120 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:6 epochs:100	100	1000	True	3845.52393		503058	14	-1	672.0091464519501	{'train_loss': [309001.094, 128816.32, 112318.75, 105692.938, 100599.781, 97561.258, 95331.68, 93305.969, 91587.906, 90052.617, 89489.375, 88007.445, 87004.109, 85689.008, 85110.102, 84207.742, 83445.008, 83601.539, 82124.797, 81956.523, 81638.789, 80790.875, 79902.977, 80460.258, 78982.211, 79071.438, 78747.172, 77897.766, 78854.578, 77527.477, 76802.578, 76714.93, 76367.789, 75614.352, 75510.477, 74904.922, 74589.648, 75460.266, 74768.078, 74327.5, 73691.352, 73835.398, 73070.469, 73452.734, 73419.805, 73522.133, 72728.141, 72179.414, 72085.852, 71631.516, 71675.992, 71778.711, 70995.555, 71760.672, 71099.391, 69718.383, 70907.523, 70685.484, 70007.07, 69692.484, 69909.242, 69808.656, 69653.477, 69210.172, 69941.781, 68983.328, 68987.375, 68664.844, 68150.859, 68746.484, 67771.891, 67683.586, 67858.312, 68350.211, 68410.102, 67215.594, 67983.797, 67060.312, 67094.727, 66699.477, 66509.422, 67515.352, 66989.18, 66606.062, 66910.773, 66592.109, 66511.961, 65803.016, 65818.875, 65676.797, 66075.461, 65761.852, 65419.551, 65809.383, 65898.125, 65302.617, 65472.832, 65262.211, 65474.164, 65516.246], 'val_loss': [353.394, 303.811, 284.236, 285.392, 300.617, 244.281, 246.03, 241.744, 232.612, 221.502, 238.57, 214.232, 235.455, 241.312, 220.834, 220.382, 223.756, 209.958, 220.159, 214.72, 216.951, 196.675, 211.332, 201.622, 211.24, 204.558, 202.535, 199.85, 206.854, 194.261, 191.241, 193.303, 190.802, 188.075, 207.731, 185.598, 189.961, 185.651, 198.315, 193.816, 190.774, 188.025, 189.838, 182.366, 183.627, 197.433, 182.643, 177.789, 196.317, 185.229, 182.096, 195.279, 209.101, 182.045, 187.996, 174.585, 188.113, 183.9, 186.628, 175.26, 182.779, 180.447, 175.435, 177.804, 175.376, 179.478, 185.776, 182.673, 172.815, 173.254, 176.539, 184.685, 183.84, 193.168, 175.591, 177.262, 186.083, 177.702, 178.135, 177.392, 176.537, 178.683, 178.714, 180.508, 170.981, 169.962, 173.083, 187.214, 176.334, 172.1, 176.172, 180.315, 177.348, 177.683, 181.53, 173.859, 178.709, 173.85, 174.82, 171.602]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:120 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:95 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:7 epochs:100	100	1000	True	4977.59814		2799459	15	-1	645.2451548576355	{'train_loss': [987337.75, 829637.75, 807041.0, 804482.938, 804482.938, 804482.938, 804482.938, 806838.062, 809435.75, 805175.312, 809592.188, 808550.25, 804800.688, 805904.312, 806118.812, 804482.938, 804789.188, 804612.312, 804643.5, 805264.125, 804523.688, 804700.062, 804570.562, 805835.438, 804643.875, 805028.875, 804835.25, 805096.438, 807826.188, 631576.625, 150822.031, 132732.875, 127340.016, 122807.961, 120346.797, 119088.766, 117133.484, 116724.078, 114226.812, 114242.695, 112924.086, 111550.523, 111877.938, 110093.398, 108561.414, 110270.695, 108232.773, 108216.531, 106528.742, 106749.07, 107807.352, 107228.547, 105855.656, 105438.289, 105732.398, 104772.625, 104050.438, 102732.016, 103256.977, 101895.445, 100832.984, 102398.805, 102154.75, 101491.719, 101167.477, 100893.953, 101992.031, 101950.711, 100870.094, 101444.812, 99961.352, 99659.914, 99307.477, 99867.555, 98626.562, 100129.93, 98831.523, 98798.453, 99382.336, 97990.711, 99070.922, 98262.93, 98424.242, 97761.797, 97443.055, 96566.93, 97256.773, 96965.195, 97575.711, 96473.477, 96545.625, 97097.977, 96417.734, 95704.344, 97221.766, 96650.32, 95243.672, 96144.758, 95333.758, 94690.203], 'val_loss': [2454.323, 2454.307, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.323, 2454.293, 627.109, 438.625, 378.82, 378.264, 358.825, 337.735, 323.299, 313.981, 331.087, 376.496, 335.476, 330.869, 296.203, 301.262, 364.807, 350.515, 340.842, 326.503, 292.74, 310.306, 353.757, 342.202, 321.455, 277.332, 268.115, 290.93, 287.473, 308.463, 278.514, 315.159, 268.813, 307.229, 284.957, 289.299, 276.571, 269.405, 287.617, 302.327, 281.271, 361.001, 303.326, 274.395, 268.465, 289.622, 279.625, 310.266, 269.283, 278.837, 261.366, 259.028, 257.245, 288.952, 277.242, 274.233, 256.043, 260.86, 321.492, 267.985, 262.787, 275.548, 255.134, 337.966, 265.927, 276.622, 266.392, 250.967, 267.925, 295.111, 258.068, 291.847, 257.357]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:33 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:25 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:122 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:15 epochs:100	100	1000	True	3705.05347		606861	16	-1	455.1352550983429	{'train_loss': [341326.344, 148337.984, 126951.5, 113103.305, 106622.453, 102045.406, 100141.305, 96741.891, 93801.648, 91934.664, 90185.594, 88575.602, 87367.961, 86839.367, 85739.195, 85898.016, 83793.273, 83428.641, 82317.25, 81209.375, 81427.836, 81255.312, 80575.516, 79825.188, 78600.875, 78231.859, 78015.789, 76878.328, 76172.352, 76133.141, 76050.914, 75210.25, 75153.273, 75275.867, 75191.156, 74450.719, 73948.289, 73619.062, 73752.969, 73043.984, 72750.375, 71917.711, 72712.219, 71487.125, 71800.617, 71105.812, 70914.289, 70497.844, 70293.398, 70161.148, 70260.914, 69312.227, 68777.008, 69135.289, 68781.938, 68722.625, 68255.586, 69181.234, 68023.875, 67280.453, 68352.508, 67660.711, 67838.805, 67490.352, 66990.32, 66891.219, 67064.031, 66729.328, 66277.93, 66681.953, 66524.703, 64959.023, 66678.953, 65149.172, 65942.25, 65590.891, 65319.199, 64988.105, 65131.879, 64599.395, 64803.859, 64635.25, 63405.469, 64398.281, 64244.84, 64025.871, 64504.094, 64366.07, 64606.551, 64632.348, 63669.25, 63597.895, 63852.211, 63303.918, 63131.812, 63383.531, 63323.156, 62909.234, 63284.891, 62601.395], 'val_loss': [852.69, 739.146, 653.697, 624.793, 604.742, 595.33, 577.72, 543.759, 547.015, 522.609, 512.053, 493.326, 539.535, 486.324, 479.663, 470.554, 471.602, 484.39, 481.45, 473.137, 467.465, 469.51, 463.681, 466.387, 462.585, 467.049, 447.442, 449.274, 460.809, 440.311, 443.16, 433.267, 465.464, 430.586, 429.846, 425.677, 472.084, 427.842, 425.144, 416.896, 420.886, 409.926, 433.272, 418.976, 459.468, 440.838, 419.634, 459.547, 434.651, 411.606, 407.902, 403.335, 412.303, 419.44, 416.026, 402.694, 430.71, 401.555, 395.337, 407.666, 399.125, 393.001, 395.997, 386.516, 389.341, 408.332, 431.039, 382.529, 448.192, 409.885, 406.958, 414.328, 389.157, 417.052, 385.54, 406.529, 400.731, 390.278, 399.996, 432.776, 409.979, 391.45, 400.635, 401.336, 426.14, 415.532, 414.874, 425.355, 393.868, 406.934, 387.389, 399.976, 399.065, 383.836, 390.508, 418.706, 382.99, 389.595, 390.758, 383.676]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:93 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:61 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:61 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:105 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:111 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:117 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.08884997001126517 alpha:0.8206719784965377 weight_decay:5.42399008810406e-05 batch_size:6 epochs:100	100	1000	True	251896.48438		10265179	14	-1	711.1516034603119	{'train_loss': [3846353.75, 4142681.0, 3156773.0, 2517244.0, 2007812.0, 2267079.0, 1972627.0, 2204004.5, 1961552.375, 2244110.75, 2049301.5, 1798949.75, 1874213.125, 1957463.875, 1853312.875, 1801040.625, 1733944.25, 1623877.625, 1824933.625, 1629197.75, 2315739.0, 2308034.0, 2021630.0, 1840800.25, 1874978.5, 1709061.125, 1921045.0, 1748712.875, 1969277.625, 2185715.0, 1830908.375, 1704937.25, 1652037.5, 1707309.0, 1705432.25, 1603581.5, 1653193.25, 1624407.875, 1563935.25, 1469282.75, 1649000.5, 1690724.5, 1617316.625, 1555049.125, 1701298.5, 1638143.75, 1683435.125, 1519779.5, 1663048.5, 1740476.625, 1625149.5, 1850218.75, 1946992.25, 1800118.875, 1715323.0, 1722377.0, 1740388.0, 1885574.75, 1758971.0, 1668117.0, 1738656.375, 1656894.25, 1682727.375, 1660648.875, 1651226.5, 1692617.75, 1847207.125, 1928643.25, 1691799.0, 1910672.125, 1743832.75, 2000874.5, 1897484.625, 1673026.0, 1641204.75, 1888092.375, 1662596.625, 1807809.5, 2108096.25, 1538707.125, 2337970.5, 1732639.0, 1828098.875, 1678164.875, 1530242.625, 1993005.5, 1543779.375, 1638383.375, 1518421.25, 1829958.75, 1500633.0, 1590818.125, 1517868.625, 1696636.25, 1548181.25, 1520016.625, 1638586.375, 1615924.625, 1586271.5, 1637943.125], 'val_loss': [2103.706, 2103.706, 2103.706, 2103.706, 2103.706, 2103.706, 7389.835, 13611.299, 7142.849, 2136.62, 2439.71, 2103.706, 2103.706, 2103.706, 2732.164, 2103.706, 3078.393, 8598.345, 2211.746, 6679.958, 15778.847, 8326.243, 5697.247, 2103.706, 2138.937, 7711.197, 4712.005, 11000.635, 4092.299, 2392.085, 6203.934, 6486.732, 2183.982, 8165.713, 3139.23, 90176.148, 2997.38, 19014.57, 6633.621, 4473.032, 11616.283, 7183.979, 6914.348, 16540.41, 13047.809, 46893.395, 2816.911, 9196.122, 6936.581, 34550.016, 81336.023, 4596.48, 35161.711, 2103.706, 2103.706, 4707.064, 2103.691, 2103.706, 93370.945, 5246.541, 19075.244, 2103.706, 2338.065, 18756.014, 2121.675, 2103.706, 7930.5, 17181.334, 6539.4, 20082.662, 90087.953, 2211.979, 10147.039, 6313.29, 2189.459, 2644.34, 2103.703, 17514.504, 5911.161, 11757.136, 2186.213, 2323.135, 97427.891, 3019.505, 3154.067, 2456.702, 2101.54, 122028.594, 3295.586, 14985.054, 6593.062, 12755.435, 11872.648, 64156.066, 190651.828, 5325.579, 16423.367, 19509.672, 26169.135, 11973.994]}	100	100	True
