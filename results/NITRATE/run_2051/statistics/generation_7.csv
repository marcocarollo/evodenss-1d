id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4082.02271		696089	14	-1	309.9648585319519	{'train_loss': [692320.312, 326408.625, 238977.688, 205425.453, 150738.016, 132594.297, 121095.734, 115033.031, 109937.398, 106089.531, 103391.891, 101687.805, 99162.391, 98755.133, 96511.336, 94907.328, 94525.969, 93101.984, 91792.797, 90917.07, 90917.352, 89744.617, 88260.898, 88943.094, 87453.977, 86940.93, 87016.75, 86279.07, 85471.633, 85111.133, 84403.391, 84138.32, 83603.891, 84002.961, 82650.531, 83292.594, 83571.008, 82345.406, 81395.812, 81111.742, 80211.023, 80752.719, 80368.039, 80795.156, 79931.391, 78982.586, 80520.781, 79296.391, 79616.422, 78686.398, 78967.453, 78361.781, 77848.617, 77368.109, 77232.867, 77132.258, 77342.109, 77299.227, 77117.336, 77436.523, 76698.594, 76038.242, 76635.68, 77195.812, 76999.312, 73927.023, 76174.844, 75230.797, 74898.758, 74963.477, 75712.781, 76208.766, 74035.977, 74393.781, 74708.781, 73688.57, 74274.672, 74729.211, 74639.984, 73090.039, 73166.922, 74229.984, 73809.531, 71999.008, 72540.828, 74663.188, 72545.484, 71987.133, 72824.578, 74365.562, 71719.211, 70840.25, 71201.219, 72997.078, 71207.617, 71092.078, 71888.781, 71701.523, 71988.312, 70923.68], 'val_loss': [8123.529, 3805.658, 3701.608, 2906.653, 2672.973, 1987.175, 1835.347, 1717.808, 1671.156, 1712.144, 1558.959, 1590.686, 1568.814, 1557.02, 1550.058, 1558.542, 1417.632, 1363.746, 1404.343, 1431.178, 1352.755, 1429.113, 1333.787, 1411.743, 1284.078, 1391.328, 1268.678, 1375.294, 1229.47, 1365.76, 1255.485, 1498.151, 1240.511, 1285.543, 1332.149, 1134.332, 1209.892, 1222.966, 1197.085, 1128.276, 1138.759, 1103.042, 1205.906, 1160.949, 1162.476, 1096.149, 1185.392, 1197.474, 1087.157, 1064.268, 1091.573, 1169.167, 1115.055, 1207.605, 1048.762, 1109.1, 1049.746, 1042.046, 1147.497, 1049.095, 1072.551, 1049.378, 1184.538, 1117.415, 1110.902, 1133.096, 1057.867, 1078.995, 1141.059, 1072.368, 1051.229, 1138.793, 998.282, 1076.21, 1052.747, 1016.802, 1045.995, 1130.732, 1191.756, 1045.16, 1031.287, 994.04, 1012.881, 1049.049, 1067.369, 1086.065, 1077.537, 1047.047, 1013.717, 1055.444, 1058.458, 1032.055, 979.124, 1157.406, 1022.156, 1077.158, 1109.592, 1040.383, 1035.979, 1076.681]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:92 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	4183.57715		1710538	15	-1	424.04197335243225	{'train_loss': [935192.312, 804482.438, 829170.625, 847364.688, 808524.812, 839887.625, 831578.938, 824242.062, 855400.125, 590912.938, 283004.188, 180693.938, 149598.406, 131257.359, 122397.234, 118376.461, 114379.18, 109503.602, 108906.203, 105820.109, 103442.07, 102533.094, 99841.055, 97240.156, 98869.688, 95885.781, 94984.883, 93718.805, 92526.742, 91654.789, 94620.023, 90589.602, 89853.633, 88137.547, 88084.398, 89934.625, 85975.07, 87279.055, 86280.961, 86869.664, 85709.234, 88785.812, 85565.297, 85331.18, 85460.055, 84907.008, 82730.828, 83842.586, 83664.688, 82656.57, 81823.477, 82263.531, 82372.484, 82797.82, 82989.906, 82471.336, 81801.102, 81190.766, 82392.219, 82017.32, 80909.727, 78835.398, 80947.766, 80084.617, 82201.953, 78249.82, 80359.945, 77534.859, 80638.469, 78651.336, 79316.328, 79464.945, 76995.359, 79008.055, 79454.289, 78494.305, 77689.172, 79439.867, 76867.234, 77806.383, 76922.43, 77365.477, 77701.484, 75290.586, 76885.664, 76493.141, 75914.547, 75820.266, 76001.32, 76499.305, 75782.695, 75821.32, 77567.43, 77760.609, 76255.891, 75633.5, 76378.062, 72639.328, 74320.008, 73642.695], 'val_loss': [11044.456, 11044.456, 11044.456, 11013.516, 11028.302, 11043.811, 11044.275, 11043.796, 10746.294, 4647.223, 2809.19, 2008.099, 1705.229, 1597.32, 1692.624, 1558.742, 1575.942, 1520.527, 1533.586, 1261.156, 1609.113, 1363.455, 1575.363, 1467.648, 1347.944, 1395.394, 1287.127, 1471.516, 1285.49, 1629.374, 1272.969, 1359.643, 1189.124, 1183.857, 1259.899, 1329.633, 1299.753, 1300.064, 1360.184, 1268.2, 1249.582, 1158.426, 1372.412, 1155.352, 1261.127, 1171.71, 1287.446, 1347.839, 1085.64, 1155.558, 1120.663, 1106.36, 1081.676, 1171.124, 1172.021, 1257.981, 1135.956, 1242.419, 1283.915, 1008.346, 1155.585, 1069.283, 1055.46, 1148.817, 1019.252, 1155.369, 1017.766, 1110.859, 1055.19, 1072.639, 1058.468, 1259.437, 1138.317, 1416.051, 1102.042, 1013.459, 1028.166, 1384.445, 1019.059, 1233.056, 989.627, 980.633, 1075.342, 1076.934, 1001.943, 1245.175, 1001.689, 984.379, 1036.258, 1054.404, 1043.079, 1136.278, 1136.384, 1067.854, 1148.094, 951.028, 1044.904, 1103.366, 1028.508, 999.554]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:48 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:15 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:95 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.13608389601433019 alpha:0.9633416312369782 weight_decay:1.9178584481764503e-05 batch_size:32 epochs:100	100	1000	True	44537.41797		647178	14	-1	367.14771699905396	{'train_loss': [963840.938, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 4757229.5, 804482.438, 804482.438, 804482.438, 7595153.0, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 3661812.5, 804482.438, 813041.75, 2947863.75, 804482.438, 804482.438, 804482.438, 804482.438, 1963062.625, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 2946813.25, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 3811725.75, 836448.438, 804482.438, 804482.438, 804482.438, 804482.438, 3276342.25, 804482.438, 804482.438, 804482.438, 829656.062, 804482.438, 804482.438, 804482.438, 2159583.5, 810289.688, 804482.438, 804482.438, 804482.438, 2783587.0, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 3180827.75, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804485.812, 934121.688, 804482.438, 804482.438, 995490.438, 857475.688, 835489.938, 1895943.25, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 899181.562, 1916854.75, 808914.938, 804482.438, 816750.188, 804482.438, 894929.25, 804482.438, 804482.438, 804482.438, 1270300.5, 892266.562, 2600977.5], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.454, 53079.051, 126245.672, 11044.456, 11044.456, 11044.455, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 26807.727, 126326.102, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.275, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 10191903.0, 11044.456, 11044.456, 11044.456, 11044.456, 65780.938, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11830.004, 11044.456, 983099.188, 11044.456, 11044.456, 11044.456, 11044.456, 31872.953, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:54 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:10 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:57 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:57 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	44673.72266		144131094	15	-1	595.9380283355713	{'train_loss': [1066973.625, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 805475.625, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456]}	100	100	True
