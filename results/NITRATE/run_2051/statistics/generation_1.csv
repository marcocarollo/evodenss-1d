id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4112.39697		452251	14	-1	267.47738337516785	{'train_loss': [393296.0, 302335.062, 215387.547, 192545.438, 172955.969, 142480.812, 127072.391, 118982.234, 115750.664, 110730.797, 108592.398, 105672.586, 103304.367, 101510.453, 100513.148, 98552.406, 96996.547, 96976.062, 95476.828, 94791.578, 93367.492, 92978.57, 92437.555, 91827.297, 90515.945, 89318.281, 89209.094, 89617.875, 87341.914, 87778.172, 87569.82, 87660.945, 86263.133, 86425.773, 86058.117, 84665.555, 85912.031, 84589.344, 84455.469, 83797.344, 83398.047, 84462.508, 83559.258, 83303.82, 83236.883, 82569.812, 81232.219, 82871.688, 82479.117, 81498.727, 82121.336, 81719.008, 81092.625, 81654.945, 80699.039, 80501.789, 81334.008, 79542.195, 79760.508, 79311.844, 79396.562, 79841.266, 79710.562, 79641.023, 78542.359, 78893.156, 79115.469, 78233.5, 78123.008, 78187.523, 78159.883, 77797.555, 78077.594, 76706.562, 77551.625, 77609.75, 77518.711, 77477.344, 77725.156, 76763.219, 76541.031, 76574.07, 76838.664, 75721.562, 76234.266, 76450.266, 75971.609, 76090.805, 75939.328, 75283.555, 74378.883, 75034.539, 75110.414, 75685.195, 75103.711, 74563.273, 74614.867, 74439.914, 74641.586, 74054.945], 'val_loss': [5261.463, 3044.576, 2798.413, 2690.253, 2654.315, 2276.821, 2146.921, 2020.159, 1874.601, 1713.919, 1755.663, 1659.563, 1530.79, 1616.251, 1447.979, 1486.07, 1468.163, 1353.817, 1322.387, 1339.783, 1293.496, 1339.613, 1286.103, 1218.533, 1225.481, 1282.887, 1212.001, 1229.362, 1156.752, 1170.249, 1138.546, 1167.456, 1132.722, 1151.541, 1169.613, 1152.968, 1165.87, 1145.091, 1206.333, 1131.665, 1140.419, 1099.254, 1126.46, 1106.818, 1138.897, 1114.888, 1112.179, 1114.852, 1115.94, 1123.382, 1110.829, 1094.736, 1136.862, 1065.158, 1079.248, 1153.04, 1090.612, 1105.167, 1061.504, 1079.413, 1065.887, 1117.145, 1069.488, 1095.199, 1073.639, 1095.663, 1042.443, 1066.338, 1066.566, 1029.875, 1087.894, 1061.506, 1053.734, 1038.688, 1029.147, 1041.453, 1068.317, 1029.132, 1050.335, 1043.176, 1061.257, 1038.942, 1010.664, 1018.141, 1028.49, 979.856, 1003.504, 1029.879, 1000.665, 1002.34, 1035.892, 997.777, 1064.119, 1039.424, 1042.001, 1013.133, 995.21, 985.064, 1065.15, 1002.504]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:conv1d out_channels:66 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	4634.74902		1706643	15	-1	273.84131383895874	{'train_loss': [844698.375, 823764.562, 832997.312, 823987.5, 810763.875, 811168.875, 860282.312, 822192.812, 828456.812, 657801.562, 315552.219, 172199.766, 146199.969, 136313.359, 129388.656, 123964.062, 121628.398, 118373.656, 115554.617, 113691.5, 112865.523, 110146.125, 108085.773, 108433.984, 108790.625, 106069.531, 105520.516, 103312.227, 104206.922, 103383.5, 102283.766, 103258.195, 100276.789, 102314.609, 99707.875, 99680.078, 100534.383, 100125.391, 98356.32, 97952.219, 98151.492, 97936.016, 96869.016, 96821.336, 96425.867, 96341.492, 97017.453, 95479.406, 95543.156, 95614.586, 94609.266, 92755.039, 94481.75, 96174.516, 94038.18, 94110.57, 93294.398, 93487.945, 92468.086, 93687.602, 92619.984, 92356.125, 93881.25, 92109.711, 93335.039, 92256.258, 91795.023, 91663.023, 90759.32, 90372.367, 91208.031, 90015.781, 90410.242, 92026.266, 91234.867, 89847.648, 91773.82, 90679.297, 89555.188, 90244.938, 89813.125, 89623.234, 90723.68, 89293.344, 89212.688, 89411.578, 88279.641, 88520.742, 89577.0, 88283.586, 88340.18, 87585.664, 90269.789, 87938.641, 88671.008, 87327.555, 88181.062, 88096.188, 86508.195, 86226.383], 'val_loss': [11044.299, 11044.446, 11044.373, 11044.45, 11044.427, 12009.887, 11044.396, 11030.338, 10657.504, 5091.885, 2733.009, 1836.325, 1647.765, 1677.979, 1530.671, 1490.757, 1520.157, 1529.279, 1499.05, 1415.166, 1573.537, 1415.507, 1450.798, 1299.559, 1302.487, 1716.989, 1531.151, 1407.171, 1340.607, 1528.476, 1496.77, 1597.824, 1533.888, 1399.967, 1288.662, 1353.458, 1273.712, 1373.236, 1328.429, 1365.891, 1261.691, 1239.458, 1144.666, 1286.451, 1286.569, 1367.043, 1152.257, 1144.532, 1285.684, 1251.193, 1178.349, 1374.118, 1368.248, 1190.872, 1328.894, 1251.826, 1311.573, 1439.279, 1279.534, 1218.523, 1286.746, 1252.833, 1381.823, 1090.158, 1319.52, 1084.452, 1289.478, 1177.983, 1141.553, 1117.431, 1206.561, 1193.795, 1174.746, 1281.077, 1232.455, 1308.383, 1095.245, 1070.076, 1247.079, 1233.578, 1236.516, 1095.273, 1140.379, 1308.11, 1250.131, 1205.493, 1258.631, 1276.106, 1214.801, 1150.091, 1158.648, 1368.763, 1094.947, 1230.199, 1313.936, 1152.906, 1123.345, 1195.058, 1109.786, 1122.993]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:rmsprop lr:0.13436482909236644 alpha:0.9021447769998236 weight_decay:2.3365448853196182e-05 batch_size:32 epochs:100	100	1000	True	1180140.25		2931618	13	-1	262.1241457462311	{'train_loss': [953235.938, 804482.438, 885611.875, 6743451.5, 804482.438, 807219.062, 1336419.625, 804482.438, 804482.438, 804482.438, 4073124.75, 804482.438, 965412.375, 2201861.25, 804482.438, 3693987.5, 1076695.375, 836091.938, 1769810.5, 1019358.312, 809618.312, 1057908.375, 858399.125, 831588.062, 855648.188, 1110336.875, 913153.25, 830682.25, 828979.812, 855445.375, 930664.875, 832123.5, 829247.625, 890129.562, 1000943.25, 822262.25, 840252.688, 926956.938, 957314.25, 840950.688, 843315.0, 873626.062, 945768.062, 898875.438, 810551.688, 881423.812, 928694.875, 879603.875, 826769.562, 869918.688, 931714.125, 883518.062, 842798.438, 884205.125, 909513.875, 882485.188, 895744.25, 875349.938, 920515.5, 881308.875, 847499.562, 855523.125, 957173.5, 874851.75, 854695.0, 865606.0, 875823.812, 923921.188, 902861.188, 834574.812, 879080.812, 855439.688, 963099.812, 881990.25, 907273.0, 850730.25, 941227.375, 865313.375, 866963.938, 922017.75, 893448.5, 881192.438, 884207.562, 851257.062, 903507.625, 853824.938, 880631.5, 871474.75, 884732.25, 882148.062, 876251.25, 875561.188, 941066.438, 887452.062, 885531.875, 835716.062, 899154.125, 937679.312, 903062.812, 857252.438], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.41, 11044.456, 11044.456, 319953.812, 321633.375, 11044.456, 11044.456, 21561.076, 11044.456, 18697.092, 62677.168, 11044.456, 11044.456, 60904.387, 198215.469, 11044.456, 208500.781, 23968.613, 583257.312, 49634.289, 11044.456, 594190.688, 423701.875, 500149.0, 14328.578, 492650.062, 701890.5, 622356.25, 506969.438, 427456.938, 483593.312, 804862.188, 484764.312, 491499.312, 984013.812, 697025.875, 417843.875, 298387.188, 548523.75, 629460.5, 443838.844, 414906.188, 495793.312, 437667.625, 528677.375, 354618.094, 695231.0, 926338.375, 779948.375, 392336.062, 716090.625, 665449.438, 670807.75, 356419.625, 386106.0, 591596.25, 511435.156, 429484.906, 316512.625, 156013.828, 378556.625, 446801.094, 527785.5, 417896.156, 571773.875, 517964.469, 650673.125, 227337.281, 322922.688, 194224.156, 482664.031, 259567.328, 215447.297, 80532.422, 84108.336, 207674.875, 331473.5, 313533.312, 286569.469, 280109.5, 372616.375, 211884.328, 281124.312, 248043.156, 107374.484, 251164.5, 480871.219, 294948.75]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:36 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:27 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:78 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.18593489360610754 beta1:0.8871517421296558 beta2:0.8879306712944008 weight_decay:5.3379628344897027e-05 batch_size:32 epochs:100	100	1000	True	22923.03711		425830	14	-1	304.36998081207275	{'train_loss': [445383.062, 418361.062, 400917.281, 383636.281, 394546.688, 419884.375, 394067.188, 398292.781, 403127.906, 415985.906, 397595.031, 388248.125, 387628.312, 420195.938, 431576.062, 395090.312, 388054.75, 384259.281, 386769.031, 392032.438, 393781.812, 393471.812, 394968.656, 416236.312, 398979.281, 399905.344, 410292.594, 412639.062, 405205.969, 403695.75, 386166.969, 386115.375, 391610.219, 387674.531, 393149.125, 412993.938, 442074.812, 381623.156, 400273.688, 397632.844, 437266.469, 385805.656, 385732.25, 393911.625, 398381.312, 405846.406, 403367.938, 410685.25, 387640.5, 392839.094, 393420.562, 391508.469, 407107.25, 407347.562, 395988.625, 401214.656, 417338.125, 420084.594, 419290.406, 412378.156, 391125.312, 397715.781, 407076.688, 394433.562, 427063.719, 393369.531, 410283.812, 409459.969, 390601.375, 407769.125, 390594.594, 403823.219, 408586.656, 397991.469, 414609.312, 403936.281, 408173.531, 400005.938, 408764.5, 397073.531, 411815.844, 394094.75, 395627.594, 396234.562, 403471.312, 391213.969, 394498.125, 402854.531, 401370.062, 412370.875, 409117.125, 416074.906, 408615.688, 414292.25, 399701.469, 380944.688, 419662.031, 412611.219, 408110.875, 392105.844], 'val_loss': [6862.29, 11313.65, 6203.853, 5097.414, 5561.829, 4961.003, 5453.564, 6588.078, 6823.042, 5797.635, 6580.554, 8079.307, 6296.163, 9440.437, 6295.775, 6479.31, 5800.007, 5000.473, 4762.475, 4664.264, 5679.665, 4720.383, 9933.861, 5926.512, 5004.676, 4826.895, 8205.188, 4813.134, 6081.585, 4826.698, 5823.549, 5418.071, 6207.295, 6045.065, 6752.877, 8565.061, 4939.919, 7126.476, 7576.514, 8900.184, 5135.331, 6139.328, 4653.545, 7080.458, 6250.521, 4890.588, 6067.51, 5267.238, 7437.7, 4706.127, 6117.303, 4757.361, 4967.538, 4667.247, 5083.505, 6755.759, 5117.92, 8618.174, 5368.078, 5161.302, 5742.744, 4829.784, 5512.07, 4914.687, 4766.396, 4990.394, 4654.394, 4826.072, 4882.058, 4962.577, 5797.52, 4698.467, 4682.133, 4922.768, 5778.201, 4725.336, 4827.86, 4842.705, 4996.298, 5659.09, 6779.307, 4718.892, 4704.258, 5026.79, 4774.003, 6720.382, 4857.104, 4760.147, 4888.955, 5468.793, 6613.639, 4914.581, 6428.26, 5207.747, 4744.478, 5790.612, 4677.614, 7448.991, 4776.743, 5167.333]}	100	100	True
