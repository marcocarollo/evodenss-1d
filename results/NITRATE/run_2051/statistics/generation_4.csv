id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	2000	True	4447.35645		428075	14	-1	283.31384682655334	{'train_loss': [297206.906, 162818.266, 146582.047, 136636.078, 130067.75, 123504.438, 118451.891, 114362.836, 111502.547, 110309.711, 107713.438, 106135.211, 104571.039, 103812.578, 102436.523, 100272.531, 99341.195, 97829.422, 96753.836, 95652.633, 94026.047, 93058.758, 93148.688, 92119.141, 91746.211, 91196.438, 90028.094, 90328.516, 88990.086, 88754.008, 88787.008, 88170.102, 87318.328, 86952.547, 86621.75, 85820.992, 85667.344, 85948.32, 85102.695, 85775.703, 83782.078, 84473.031, 83587.023, 83982.031, 83126.68, 82927.422, 82897.414, 81268.727, 81370.32, 82343.766, 81048.891, 81620.078, 80607.805, 80572.828, 80079.727, 79987.547, 80190.664, 79955.5, 80060.922, 79917.781, 79802.445, 79536.477, 79087.383, 78442.242, 78250.234, 78884.227, 78138.031, 77980.68, 77770.586, 78499.305, 77925.375, 77219.375, 77950.047, 77610.719, 77043.719, 77733.984, 75942.922, 76631.57, 76118.672, 76458.008, 76291.0, 76050.562, 75524.758, 76545.484, 75353.516, 75808.656, 75184.109, 75388.445, 74486.133, 75057.078, 74307.688, 75697.648, 73345.227, 75425.945, 73627.234, 74120.477, 75148.125, 74162.453, 74010.117, 74827.531], 'val_loss': [2314.796, 2234.62, 1889.903, 1708.413, 1739.343, 1696.505, 1681.728, 1690.974, 1531.647, 1542.653, 1485.814, 1478.175, 1441.646, 1425.922, 1454.41, 1505.774, 1488.347, 1645.897, 1472.763, 1409.985, 1480.998, 1313.725, 1477.986, 1465.864, 1446.053, 1381.931, 1378.03, 1302.574, 1477.914, 1507.113, 1359.364, 1395.033, 1340.448, 1247.547, 1174.104, 1378.357, 1263.799, 1402.297, 1336.609, 1249.089, 1238.989, 1281.783, 1385.311, 1140.875, 1161.81, 1293.687, 1192.81, 1277.245, 1264.545, 1246.858, 1193.598, 1108.829, 1138.128, 1143.85, 1118.151, 1223.49, 1266.706, 1146.151, 1176.603, 1130.531, 1170.317, 1051.108, 1082.381, 1135.78, 1162.919, 1181.362, 1084.794, 1156.073, 1134.777, 1138.147, 1123.199, 1144.946, 1084.858, 1112.006, 1142.869, 1079.188, 1073.541, 1168.949, 1122.746, 1176.037, 1240.495, 1021.446, 1134.592, 1131.759, 1066.121, 1079.72, 1147.315, 1075.597, 1153.738, 1074.363, 1149.227, 997.602, 1136.791, 1094.909, 1063.237, 1118.477, 1068.421, 1022.006, 991.032, 1079.419]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	2000	True	3818.04004		428075	14	-1	276.85290145874023	{'train_loss': [390863.938, 224557.25, 157982.938, 142425.406, 132839.812, 126447.32, 123616.344, 119047.812, 116400.422, 113703.75, 111409.742, 107686.938, 105854.703, 102874.641, 101172.695, 99822.578, 97342.93, 96461.68, 94700.648, 92756.352, 91717.484, 91125.883, 91206.648, 90160.188, 88079.984, 89188.906, 86819.18, 87515.484, 85958.406, 85984.555, 84957.422, 84716.094, 84346.203, 84074.797, 83185.062, 82966.516, 83136.594, 82510.258, 82200.5, 81171.969, 82113.266, 80628.938, 80961.828, 79863.211, 80055.672, 78747.922, 79318.523, 79718.938, 78552.172, 78270.688, 78484.125, 77946.609, 77092.023, 77461.164, 77785.039, 77085.398, 76915.961, 76158.07, 76390.586, 76557.648, 75790.984, 75644.594, 76279.023, 75058.031, 75466.719, 75614.125, 76045.578, 74761.273, 75072.484, 75285.117, 74458.453, 74758.969, 73735.125, 74553.008, 73888.188, 73682.945, 73821.188, 72883.195, 73472.719, 72862.719, 73939.633, 73724.766, 72932.688, 72544.656, 72816.555, 72737.531, 72255.586, 72576.906, 72188.219, 72561.617, 71913.484, 71721.43, 71850.008, 71857.523, 71766.242, 72077.203, 70680.961, 71731.305, 71061.062, 70125.547], 'val_loss': [4803.236, 2123.133, 1901.183, 1729.01, 1639.744, 1637.128, 1545.516, 1607.279, 1539.016, 1500.878, 1527.677, 1413.656, 1400.611, 1364.717, 1349.995, 1325.739, 1318.237, 1296.119, 1211.654, 1202.084, 1244.415, 1249.382, 1158.749, 1157.533, 1219.233, 1111.698, 1150.016, 1110.368, 1097.178, 1184.395, 1098.147, 1124.698, 1083.081, 1082.547, 1130.119, 1077.704, 1106.743, 1146.006, 1070.646, 1075.327, 1081.089, 1090.584, 1137.276, 1081.06, 1095.266, 1071.552, 1026.201, 1030.608, 1073.06, 1084.313, 987.112, 1049.356, 1054.652, 1030.768, 1019.226, 971.243, 1016.859, 1031.621, 1038.62, 1049.211, 1052.883, 1087.081, 993.501, 978.244, 1009.939, 1018.751, 975.84, 1044.28, 1012.988, 983.704, 1082.039, 978.059, 943.048, 991.761, 963.637, 985.737, 1056.046, 988.964, 1001.198, 987.347, 1008.022, 997.383, 974.435, 963.588, 941.361, 951.455, 948.078, 985.731, 945.15, 974.169, 962.891, 953.33, 998.878, 996.5, 955.775, 981.053, 915.261, 956.438, 943.326, 927.377]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:17 kernel_size:4 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:19 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:42 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.13213050094522136 alpha:0.8019244656848048 weight_decay:1.540340583675447e-05 batch_size:32 epochs:100	100	1000	True	44541.28125		2001548	15	-1	278.99788069725037	{'train_loss': [855427.688, 1994605.625, 804482.438, 1573907.625, 814862.812, 2331071.25, 1336351.25, 830899.688, 2191783.75, 1001067.062, 1368138.0, 1161149.875, 1530884.125, 1875682.125, 907056.375, 1398393.25, 1005721.562, 996920.0, 1035277.75, 990437.75, 999115.5, 946756.0, 901149.312, 873315.75, 946113.688, 938920.75, 893707.312, 973020.0, 918432.562, 909363.438, 914807.312, 920544.312, 899362.938, 879549.375, 946950.938, 916966.625, 901984.438, 925534.188, 913782.125, 924224.312, 909782.875, 922375.875, 935022.688, 920968.375, 928021.875, 931163.25, 925019.375, 898642.5, 947518.438, 930811.25, 949370.5, 925653.312, 870708.0, 972956.5, 879565.5, 859956.062, 961301.25, 912503.625, 1016232.25, 877059.688, 975130.312, 912180.25, 942694.438, 915839.75, 941872.312, 936540.5, 884061.688, 947372.812, 957046.125, 947582.812, 920042.562, 947396.125, 961380.0, 919894.625, 922299.688, 934904.5, 983378.25, 863574.625, 1055392.875, 882806.625, 927990.75, 962403.125, 1024889.438, 927232.562, 945876.312, 966513.125, 922844.438, 930942.875, 909872.438, 939301.0, 956252.688, 914271.375, 1001627.062, 892240.5, 928961.25, 923370.812, 907480.438, 947716.438, 938357.25, 972275.188], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11248.607, 11044.456, 11040.588, 11044.445, 11044.456, 17385.705, 14761.213, 11035.44, 11044.456, 11044.456, 91602.266, 16659.52, 33115.836, 11042.662, 11073.103, 13385.31, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 15266.253, 11314.813, 11044.456, 11044.456, 11044.456, 14931.844, 17806.406, 11044.456, 11044.456, 20741.982, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11352.424, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 12382.283, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 12493.91, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 27170.396, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 14209.009, 11044.456]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:rmsprop lr:0.10330427658126845 alpha:0.9188290896159456 weight_decay:0.00046552104501895747 batch_size:32 epochs:100	100	1000	True	26888.01367		463979	13	-1	271.08306789398193	{'train_loss': [1065155.125, 808067.875, 792652.375, 925906.5, 836631.688, 810423.188, 809550.812, 848682.438, 796615.125, 912388.375, 758872.625, 834385.312, 786024.5, 808391.938, 748985.625, 918084.25, 851585.375, 858789.938, 731445.312, 795353.875, 757837.375, 920272.75, 860253.688, 688112.062, 900834.625, 877744.062, 890449.0, 892822.062, 867784.812, 888966.875, 831281.75, 1042858.0, 826897.938, 864421.5, 872841.875, 876068.562, 965683.562, 818970.25, 857425.375, 899585.812, 832590.125, 841311.438, 822945.312, 869886.062, 857979.062, 843529.938, 830259.062, 891506.438, 863271.688, 859171.5, 825057.0, 820563.188, 866389.125, 839187.812, 857607.125, 932403.188, 864249.688, 850448.688, 971849.562, 815523.125, 871117.0, 898305.312, 888446.25, 839111.312, 861401.812, 861219.75, 841428.688, 853044.062, 884729.938, 833410.5, 844764.062, 920720.812, 837598.25, 807520.375, 845504.625, 881497.188, 863915.5, 817893.688, 864726.5, 875798.688, 824847.25, 818926.188, 873015.688, 814397.938, 836803.562, 855749.625, 829900.375, 726709.062, 813522.125, 802046.25, 800497.938, 800074.438, 824461.75, 700816.5, 788133.688, 752042.312, 658274.875, 838969.062, 849883.125, 651858.0], 'val_loss': [7477.002, 11642.589, 8116.329, 8133.887, 8292.538, 6861.247, 10232.145, 8336.5, 10448.271, 10354.146, 7486.159, 9013.502, 7685.143, 9350.897, 8137.851, 10150.604, 11030.578, 9550.803, 6725.983, 6583.981, 10868.852, 10174.116, 6998.772, 11007.677, 11044.417, 11062.522, 11043.814, 10974.613, 11044.456, 11039.74, 11044.378, 11692.281, 8206.323, 8464.963, 9829.826, 8571.08, 9920.881, 10846.631, 10862.699, 9336.869, 9316.815, 9739.562, 10068.539, 9703.851, 9643.121, 10918.645, 10020.239, 9697.436, 10740.316, 9421.912, 10038.554, 9751.021, 9255.988, 10915.314, 9071.024, 10888.194, 10787.631, 10923.868, 10525.0, 10986.52, 10943.57, 10594.0, 10519.652, 9933.692, 10725.301, 10094.904, 10392.886, 11132.754, 10650.656, 10267.398, 10557.438, 9350.924, 11962.78, 9132.74, 12145.885, 10411.194, 8992.417, 8771.956, 10471.567, 10657.438, 10950.465, 10769.045, 11233.472, 8922.64, 11049.156, 8781.877, 8583.122, 8135.974, 8785.014, 8629.021, 7839.208, 10549.98, 9613.277, 11055.525, 7765.264, 7325.587, 6550.871, 10252.602, 8305.517, 6197.394]}	100	100	True
