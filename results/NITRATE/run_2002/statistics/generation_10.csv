id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:35 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:0.0008426385295062967 batch_size:49 epochs:100	100	1000	True	1037.31873		516642	18	-1	157.2708294391632	{'train_loss': [3357.658, 2499.675, 2302.244, 2187.277, 2140.753, 2103.88, 2077.095, 2054.515, 2035.055, 1980.478, 1897.412, 1836.395, 1771.493, 1725.702, 1706.287, 1609.347, 1572.651, 1548.763, 1505.995, 1470.546, 1444.663, 1415.571, 1392.023, 1362.155, 1358.549, 1361.498, 1322.855, 1308.424, 1305.448, 1285.877, 1293.086, 1262.865, 1267.119, 1259.184, 1248.33, 1250.016, 1246.965, 1251.368, 1247.072, 1233.471, 1247.222, 1240.467, 1224.422, 1225.523, 1218.086, 1217.649, 1209.56, 1201.406, 1209.444, 1195.964, 1195.479, 1196.679, 1193.478, 1187.808, 1192.24, 1194.91, 1190.671, 1190.48, 1187.272, 1178.669, 1184.257, 1182.777, 1184.761, 1184.23, 1173.565, 1164.49, 1176.03, 1183.86, 1169.468, 1166.138, 1170.152, 1172.624, 1168.221, 1172.851, 1154.983, 1157.599, 1153.357, 1159.54, 1155.985, 1162.205, 1162.712, 1154.228, 1153.25, 1152.818, 1149.754, 1144.373, 1137.388, 1140.735, 1138.836, 1135.614, 1136.777, 1140.58, 1135.153, 1148.091, 1140.621, 1142.292, 1136.264, 1135.593, 1147.834, 1155.927], 'val_loss': [2551.685, 2183.172, 1928.239, 1854.491, 1894.126, 1825.039, 1816.965, 1775.629, 1827.496, 1687.343, 1639.699, 1546.682, 1572.174, 1492.177, 1455.954, 1383.036, 1391.714, 1302.6, 1264.711, 1256.545, 1244.935, 1209.415, 1199.843, 1141.261, 1218.351, 1168.868, 1131.092, 1109.019, 1118.579, 1092.041, 1113.8, 1087.108, 1072.552, 1060.446, 1087.208, 1062.475, 1047.038, 1072.463, 1046.462, 1050.137, 1052.333, 1049.658, 1069.611, 1018.06, 1025.979, 1016.705, 1022.238, 1022.846, 1050.166, 1000.057, 1028.771, 1053.422, 1034.09, 1039.132, 1009.659, 1011.812, 1050.047, 1049.944, 1052.383, 1021.696, 1013.015, 996.138, 998.867, 998.56, 1009.628, 992.189, 1018.153, 1000.955, 991.524, 1028.166, 1009.314, 1013.549, 995.491, 989.788, 1004.197, 995.441, 995.007, 1002.631, 1008.085, 1016.308, 988.496, 1005.731, 976.217, 968.653, 967.914, 981.768, 959.695, 974.427, 971.804, 961.81, 990.461, 988.84, 974.319, 990.304, 984.865, 981.318, 972.413, 972.834, 977.326, 1022.208]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:35 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:0.0008426385295062967 batch_size:49 epochs:100	100	2000	True	1026.96265		516642	18	-1	168.60562348365784	{'train_loss': [3385.737, 2581.956, 2379.372, 2247.536, 2139.687, 2125.221, 2083.061, 2079.284, 2034.386, 2035.324, 1972.101, 1947.334, 1913.566, 1883.78, 1796.166, 1593.562, 1478.24, 1445.962, 1411.998, 1390.402, 1360.742, 1346.045, 1332.608, 1313.935, 1316.809, 1304.036, 1303.099, 1280.599, 1280.003, 1268.099, 1271.818, 1262.185, 1258.854, 1260.15, 1254.646, 1254.445, 1240.631, 1232.966, 1233.432, 1226.37, 1230.13, 1227.837, 1225.37, 1217.428, 1220.88, 1225.862, 1221.82, 1204.64, 1206.25, 1201.525, 1194.475, 1193.912, 1190.112, 1194.989, 1193.816, 1187.733, 1188.416, 1177.98, 1180.075, 1188.87, 1175.08, 1180.042, 1179.893, 1170.654, 1173.563, 1174.002, 1175.661, 1178.952, 1161.628, 1166.844, 1171.109, 1160.251, 1164.487, 1159.357, 1160.081, 1163.846, 1157.131, 1156.999, 1174.96, 1166.71, 1167.237, 1164.091, 1160.486, 1157.591, 1154.338, 1149.205, 1154.02, 1152.834, 1164.426, 1157.166, 1152.266, 1151.627, 1157.31, 1154.442, 1144.799, 1138.609, 1144.373, 1140.61, 1146.489, 1145.792], 'val_loss': [2733.372, 2217.996, 2034.165, 1917.643, 1899.63, 1886.627, 1895.776, 1850.457, 1828.378, 1754.902, 1735.166, 1796.373, 1777.139, 1667.335, 1515.984, 1298.024, 1254.369, 1220.671, 1166.228, 1163.881, 1124.933, 1131.212, 1104.374, 1119.916, 1093.98, 1137.42, 1078.28, 1084.047, 1062.881, 1082.99, 1068.909, 1058.162, 1055.284, 1046.956, 1054.549, 1041.049, 1038.815, 1040.241, 1030.34, 1028.899, 1048.274, 1032.696, 1007.373, 1008.564, 1019.807, 1022.835, 1001.676, 1005.3, 1020.527, 1006.469, 1016.911, 1002.849, 1001.286, 1001.343, 993.612, 997.698, 995.056, 993.978, 986.366, 985.125, 978.727, 992.803, 979.607, 986.223, 994.482, 1000.326, 981.965, 983.981, 977.449, 983.961, 990.457, 972.463, 972.079, 977.267, 975.075, 973.566, 969.72, 970.265, 987.142, 973.604, 974.778, 997.499, 985.374, 973.505, 975.022, 967.38, 965.273, 971.762, 980.082, 970.531, 965.387, 984.755, 972.228, 974.314, 969.151, 959.796, 966.409, 962.009, 965.571, 972.244]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:35 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:126 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:3.381674346430075e-05 batch_size:49 epochs:100	100	1000	True	995.34753		641438	20	-1	211.72960424423218	{'train_loss': [4472.298, 2769.615, 2589.633, 2409.773, 2246.049, 2156.52, 2044.565, 1850.721, 1663.99, 1550.843, 1484.719, 1458.654, 1428.285, 1408.936, 1391.823, 1357.249, 1337.618, 1326.603, 1312.799, 1304.602, 1295.672, 1283.113, 1278.161, 1255.133, 1266.97, 1257.01, 1237.15, 1230.462, 1231.439, 1244.578, 1228.717, 1222.29, 1209.892, 1209.566, 1209.14, 1210.643, 1206.751, 1190.816, 1190.514, 1192.599, 1189.035, 1179.941, 1171.621, 1177.155, 1183.07, 1169.929, 1167.302, 1180.265, 1172.684, 1167.944, 1164.245, 1158.155, 1168.002, 1168.306, 1169.119, 1150.268, 1149.694, 1147.484, 1139.123, 1146.294, 1146.505, 1139.193, 1139.492, 1149.413, 1143.037, 1141.653, 1137.344, 1139.574, 1137.027, 1141.76, 1137.347, 1130.657, 1132.796, 1136.599, 1124.452, 1125.619, 1118.758, 1126.538, 1123.45, 1130.95, 1127.704, 1135.303, 1126.218, 1120.029, 1118.343, 1118.315, 1121.286, 1116.374, 1122.202, 1118.98, 1116.444, 1115.456, 1117.575, 1114.581, 1114.707, 1111.419, 1112.115, 1105.08, 1117.12, 1111.16], 'val_loss': [3039.915, 2339.765, 2166.608, 2034.216, 1870.5, 1802.631, 1703.54, 1584.352, 1456.611, 1396.869, 1309.948, 1260.523, 1259.358, 1179.781, 1180.283, 1135.53, 1151.707, 1159.654, 1158.766, 1106.151, 1079.121, 1067.314, 1098.293, 1058.794, 1097.178, 1075.603, 1039.077, 1075.189, 1042.445, 1079.212, 1048.943, 1031.835, 1005.388, 1011.357, 1042.234, 1038.271, 1029.221, 1006.816, 1011.653, 1067.636, 1015.76, 997.436, 996.412, 1016.578, 999.87, 996.166, 1005.512, 1015.229, 1021.477, 1033.629, 1010.32, 1007.424, 1016.422, 1022.68, 985.388, 984.429, 988.947, 986.738, 987.056, 999.073, 984.695, 980.604, 990.455, 996.394, 990.405, 995.21, 973.334, 980.875, 987.055, 990.351, 988.327, 983.454, 990.109, 978.918, 974.227, 974.477, 982.51, 977.468, 979.127, 980.568, 990.023, 998.319, 989.819, 973.953, 989.934, 986.548, 990.538, 987.921, 983.31, 983.343, 984.482, 1002.637, 976.487, 979.119, 979.54, 974.751, 985.235, 967.164, 963.312, 971.782]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:34 kernel_size:9 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:44 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:35 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:41 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:deconv1d out_channels:88 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.002460715196435833 beta1:0.9925057428239182 beta2:0.9711402723837911 weight_decay:0.0008426385295062967 batch_size:49 epochs:100	100	1000	True	1011.29608		510965	19	-1	162.53010773658752	{'train_loss': [3780.074, 2885.023, 2693.955, 2516.569, 2429.139, 2337.08, 2246.453, 2166.502, 2109.457, 2066.272, 2036.813, 2011.906, 1988.142, 1959.638, 1928.99, 1920.445, 1867.691, 1811.771, 1825.11, 1806.092, 1705.871, 1689.677, 1723.784, 1645.338, 1619.375, 1589.549, 1535.206, 1481.974, 1480.113, 1471.061, 1436.306, 1417.405, 1408.589, 1391.38, 1369.834, 1363.32, 1352.033, 1360.785, 1340.476, 1336.111, 1325.482, 1313.102, 1294.875, 1288.031, 1277.976, 1273.433, 1263.378, 1253.787, 1245.693, 1238.73, 1231.594, 1224.771, 1216.189, 1212.493, 1215.202, 1211.544, 1209.305, 1210.617, 1205.103, 1202.579, 1204.973, 1195.707, 1193.39, 1189.859, 1190.186, 1189.844, 1183.411, 1182.769, 1176.28, 1178.632, 1169.579, 1173.083, 1180.352, 1193.726, 1193.841, 1192.226, 1182.207, 1170.9, 1168.797, 1162.909, 1166.635, 1163.32, 1158.101, 1156.456, 1147.015, 1155.385, 1156.642, 1151.879, 1143.624, 1144.797, 1145.236, 1146.042, 1150.9, 1158.205, 1155.138, 1150.256, 1144.33, 1143.028, 1140.273, 1137.129], 'val_loss': [3838.435, 2906.247, 2665.353, 2487.072, 2385.645, 2252.0, 2043.627, 1936.468, 1914.259, 1877.315, 1820.245, 1794.293, 1769.326, 1735.217, 1693.767, 1673.769, 1637.471, 1575.708, 1529.971, 1548.427, 1472.808, 1434.214, 1507.747, 1385.916, 1366.926, 1336.358, 1327.544, 1281.969, 1277.152, 1278.838, 1232.217, 1235.423, 1214.886, 1187.658, 1176.162, 1173.776, 1169.606, 1181.451, 1191.561, 1160.986, 1139.218, 1146.117, 1116.483, 1101.118, 1096.698, 1095.768, 1095.576, 1075.049, 1080.653, 1058.362, 1058.79, 1048.696, 1048.508, 1043.486, 1040.707, 1043.923, 1036.033, 1033.156, 1034.193, 1020.123, 1053.048, 1020.619, 1028.07, 1017.943, 1025.427, 1014.685, 1017.328, 1013.552, 1003.458, 1011.974, 1008.747, 1003.619, 1024.017, 1013.013, 1021.825, 1034.17, 1014.181, 1005.692, 992.581, 993.185, 995.506, 1000.814, 1001.766, 1001.424, 993.189, 990.952, 989.13, 987.136, 985.052, 976.936, 986.849, 985.415, 983.054, 999.107, 983.896, 989.178, 983.203, 981.597, 970.289, 980.849]}	100	100	True
