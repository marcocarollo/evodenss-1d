id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	1526.87598		403446	16	-1	108.16105604171753	{'train_loss': [6126.637, 4278.659, 4106.323, 3878.707, 3749.087, 3638.066, 3515.264, 3459.773, 3415.695, 3383.13, 3352.403, 3327.774, 3311.374, 3266.59, 3251.567, 3242.572, 3239.217, 3228.811, 3208.824, 3210.59, 3197.954, 3184.899, 3129.809, 3093.661, 3060.958, 2989.346, 2944.669, 2788.177, 2650.814, 2575.633, 2438.853, 2382.155, 2357.479, 2303.156, 2262.019, 2253.082, 2218.038, 2196.135, 2170.965, 2161.668, 2128.894, 2144.876, 2128.352, 2116.542, 2090.273, 2067.281, 2084.21, 2057.544, 2060.711, 2060.985, 2044.666, 2043.95, 2061.498, 2036.323, 2020.208, 2025.938, 2023.443, 2007.455, 2010.677, 1999.346, 1995.052, 1993.158, 1982.36, 1974.744, 1983.67, 1991.156, 1978.067, 1975.498, 1980.051, 1959.383, 1958.577, 1949.68, 1958.165, 1959.096, 1972.794, 1974.497, 1982.193, 1938.556, 1943.036, 1956.433, 1928.391, 1944.744, 1946.773, 1948.793, 1942.958, 1926.379, 1936.32, 1931.476, 1933.537, 1912.426, 1904.866, 1917.02, 1915.655, 1914.416, 1904.205, 1897.756, 1913.017, 1903.265, 1906.135, 1900.034], 'val_loss': [5255.21, 3663.815, 3312.426, 3209.767, 3043.381, 2987.211, 2997.323, 2805.364, 2782.158, 2748.663, 2796.064, 2743.909, 2745.015, 2672.726, 2652.973, 2710.458, 2641.651, 2628.769, 2631.813, 2618.196, 2596.164, 2613.136, 2683.89, 2563.01, 2472.517, 2638.728, 2435.053, 2514.593, 2285.705, 2077.061, 1932.503, 1967.575, 2013.247, 1947.44, 1846.277, 1874.154, 1870.382, 1819.012, 1793.855, 1802.79, 1725.294, 1838.155, 1768.693, 1777.952, 1662.206, 1647.846, 1665.909, 1645.552, 1726.252, 1707.727, 1691.505, 1662.803, 1747.648, 1615.26, 1658.725, 1695.946, 1661.748, 1604.203, 1613.969, 1615.452, 1640.562, 1631.354, 1605.855, 1646.596, 1592.986, 1644.432, 1583.301, 1588.833, 1575.698, 1591.462, 1550.772, 1563.281, 1572.791, 1561.171, 1573.635, 1592.736, 1569.065, 1534.711, 1541.661, 1534.927, 1536.093, 1534.546, 1566.304, 1548.047, 1568.2, 1556.015, 1545.734, 1535.299, 1524.085, 1525.372, 1526.914, 1514.199, 1514.559, 1495.674, 1516.063, 1522.521, 1504.792, 1515.648, 1539.749, 1511.065]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:0.0008426385295062967 batch_size:83 epochs:100	100	1000	True	1509.86938		603506	17	-1	170.50882768630981	{'train_loss': [9197.685, 5135.752, 4567.564, 4185.239, 3928.236, 3833.911, 3673.529, 3626.653, 3579.464, 3557.94, 3479.034, 3444.105, 3392.311, 3341.295, 3267.921, 3227.108, 3012.183, 2697.387, 2522.839, 2425.622, 2359.867, 2349.885, 2304.095, 2272.907, 2231.846, 2246.685, 2208.669, 2182.406, 2189.1, 2146.964, 2107.924, 2095.298, 2107.752, 2096.311, 2086.202, 2073.955, 2060.046, 2049.088, 2038.958, 2036.688, 2046.008, 2022.498, 2022.614, 2009.531, 2005.276, 1991.451, 2003.352, 2003.61, 2006.461, 1993.754, 1977.613, 1979.798, 1975.097, 1982.333, 1980.676, 1968.564, 1960.38, 1965.01, 1960.182, 1944.908, 1941.987, 1940.078, 1929.969, 1938.101, 1945.938, 1930.127, 1937.883, 1946.133, 1939.473, 1926.624, 1924.141, 1915.548, 1917.389, 1909.708, 1921.582, 1912.727, 1903.255, 1920.774, 1900.103, 1910.534, 1907.704, 1901.863, 1899.966, 1897.684, 1885.675, 1880.54, 1894.453, 1929.25, 1919.258, 1894.546, 1874.359, 1884.602, 1890.278, 1880.282, 1875.809, 1869.865, 1861.753, 1871.458, 1860.654, 1854.178], 'val_loss': [8364.455, 4929.421, 3728.062, 3434.27, 3493.152, 3155.416, 3197.224, 3102.607, 3037.092, 2941.62, 2870.679, 2829.462, 2820.845, 2723.554, 2663.029, 2689.721, 2306.448, 2055.237, 2107.295, 1990.197, 1964.096, 1847.857, 1802.675, 1751.598, 2140.814, 1747.609, 1762.865, 1799.359, 1837.86, 1757.685, 1631.134, 1655.347, 1613.262, 1690.344, 1678.22, 1640.553, 1640.566, 1590.278, 1660.625, 1581.797, 1604.071, 1673.724, 1575.817, 1642.588, 1622.569, 1593.621, 1736.349, 1628.169, 1545.151, 1574.957, 1580.022, 1660.564, 1559.099, 1599.625, 1517.902, 1533.581, 1554.97, 1572.391, 1520.299, 1511.113, 1586.674, 1517.799, 1604.236, 1512.197, 1506.564, 1491.669, 1522.242, 1558.917, 1557.233, 1477.663, 1499.348, 1534.099, 1498.444, 1525.722, 1512.191, 1512.204, 1488.629, 1510.409, 1537.431, 1524.343, 1540.88, 1449.46, 1509.233, 1492.589, 1485.52, 1471.444, 1499.479, 1506.801, 1498.991, 1503.207, 1497.938, 1501.788, 1458.389, 1445.869, 1475.593, 1472.262, 1481.175, 1461.423, 1459.531, 1462.837]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:116 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:88 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:58 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:2.758601596101594e-05 batch_size:83 epochs:100	100	1000	True	1545.87817		801066	17	-1	186.48019456863403	{'train_loss': [9699.772, 5543.642, 4766.464, 4374.181, 4136.991, 4001.23, 3892.905, 3844.235, 3707.673, 3662.777, 3615.166, 3580.478, 3450.733, 3387.337, 3295.467, 3265.789, 3117.933, 2981.568, 2931.028, 2768.651, 2692.387, 2602.597, 2530.214, 2415.904, 2386.17, 2379.621, 2337.828, 2318.571, 2309.893, 2273.12, 2271.867, 2266.21, 2237.251, 2186.048, 2163.95, 2148.097, 2139.486, 2125.877, 2135.961, 2121.287, 2106.825, 2074.947, 2047.793, 2040.527, 2038.342, 2039.019, 2010.262, 2010.102, 1994.625, 1982.806, 1986.683, 1987.902, 1978.932, 1957.699, 1978.732, 1980.368, 1953.041, 1943.112, 1949.308, 1932.889, 1920.406, 1920.679, 1926.094, 1927.005, 1905.737, 1912.906, 1911.395, 1902.917, 1897.918, 1878.935, 1899.623, 1870.113, 1858.887, 1857.943, 1868.782, 1859.055, 1875.082, 1880.671, 1892.407, 1888.537, 1857.553, 1868.338, 1886.147, 1857.087, 1869.225, 1863.459, 1850.682, 1857.037, 1855.349, 1851.675, 1843.318, 1861.608, 1855.981, 1841.842, 1832.433, 1847.109, 1842.778, 1832.928, 1830.616, 1842.676], 'val_loss': [8391.38, 4843.201, 4113.438, 3673.555, 3497.749, 3397.37, 3315.878, 3191.526, 3123.796, 3181.02, 3109.458, 3087.421, 3070.405, 2809.137, 2795.344, 2665.476, 2428.869, 2444.239, 2415.999, 2452.077, 2303.211, 2005.184, 1977.42, 1909.064, 1942.786, 1861.902, 1859.687, 1915.889, 1995.999, 1908.796, 2037.868, 2034.475, 1946.99, 1819.516, 1765.372, 1753.02, 1717.805, 1755.224, 1771.176, 1736.704, 1678.976, 1687.441, 1658.988, 1686.07, 1669.396, 1589.886, 1647.054, 1617.303, 1591.55, 1640.711, 1671.9, 1701.668, 1619.595, 1628.683, 1611.363, 1650.431, 1529.486, 1581.394, 1618.992, 1562.533, 1513.757, 1526.61, 1568.821, 1481.099, 1554.026, 1501.698, 1480.767, 1537.932, 1483.921, 1516.871, 1498.625, 1455.259, 1472.919, 1449.202, 1459.998, 1452.412, 1513.604, 1513.312, 1622.018, 1461.83, 1510.767, 1509.352, 1469.388, 1490.112, 1549.172, 1488.451, 1490.296, 1514.93, 1518.676, 1477.866, 1441.932, 1520.229, 1482.499, 1467.883, 1457.334, 1497.545, 1472.577, 1457.504, 1472.741, 1506.976]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	2000	True	1519.01367		403446	16	-1	143.34861779212952	{'train_loss': [6081.194, 4234.111, 4069.646, 3816.634, 3690.167, 3544.13, 3453.018, 3440.507, 3396.448, 3338.198, 3329.936, 3308.844, 3287.192, 3237.097, 3197.627, 3030.782, 2877.684, 2755.868, 2695.6, 2589.318, 2515.15, 2489.789, 2433.335, 2439.551, 2382.68, 2343.49, 2327.918, 2299.654, 2309.272, 2280.725, 2285.433, 2257.806, 2247.268, 2242.574, 2206.827, 2211.752, 2221.602, 2200.023, 2159.174, 2183.458, 2145.703, 2151.233, 2125.967, 2114.267, 2140.492, 2142.689, 2119.037, 2101.984, 2093.083, 2091.659, 2092.546, 2088.669, 2077.781, 2064.264, 2058.532, 2085.837, 2075.567, 2058.805, 2081.05, 2069.079, 2060.081, 2050.397, 2037.461, 2074.186, 2045.754, 2034.287, 2022.655, 2019.735, 2036.64, 2011.91, 2017.338, 2019.066, 2013.246, 1998.392, 1990.573, 2018.407, 2012.319, 1995.049, 1993.319, 1991.512, 1991.914, 1994.269, 1980.662, 1984.804, 1988.543, 1996.455, 2006.768, 1981.152, 1988.022, 1983.095, 1995.479, 1974.632, 1977.975, 1959.152, 1995.02, 1970.49, 1954.198, 1965.235, 1963.628, 1959.892], 'val_loss': [5427.319, 3962.896, 3509.731, 3203.464, 3062.318, 2869.053, 2855.372, 2928.789, 2800.543, 2820.854, 2725.946, 2802.594, 2688.904, 2801.277, 2598.167, 2376.836, 2243.914, 2152.106, 2262.289, 2078.527, 2092.035, 2125.988, 1989.386, 2076.665, 1965.719, 1897.17, 1909.183, 1782.936, 1920.683, 1757.734, 1799.271, 1832.351, 1757.344, 1787.014, 1711.13, 1763.007, 1838.837, 1717.329, 1699.83, 1712.006, 1712.587, 1736.922, 1696.593, 1720.787, 1660.808, 1745.218, 1660.818, 1664.62, 1618.771, 1652.563, 1639.632, 1678.549, 1683.588, 1587.921, 1593.39, 1710.627, 1665.824, 1610.833, 1698.664, 1581.349, 1644.286, 1576.26, 1642.256, 1680.518, 1662.746, 1584.647, 1602.759, 1588.458, 1656.076, 1535.567, 1594.285, 1637.934, 1593.179, 1550.616, 1561.993, 1615.195, 1586.76, 1602.346, 1574.83, 1529.429, 1614.534, 1545.336, 1534.087, 1548.982, 1561.077, 1572.878, 1513.924, 1537.292, 1540.533, 1592.695, 1543.113, 1569.134, 1527.471, 1559.411, 1545.182, 1527.796, 1520.142, 1546.408, 1521.951, 1499.71]}	0	100	True
