id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.08835175881075448 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:9.685360722189728e-05 batch_size:83 epochs:100	100	1000	True	2744.12256		452251	14	-1	104.37216448783875	{'train_loss': [23866.822, 6056.614, 4280.876, 4124.176, 4025.652, 4126.079, 3988.829, 3952.175, 3946.793, 3950.899, 3942.176, 3940.645, 3943.791, 3896.552, 3792.734, 3694.789, 3611.521, 3547.617, 3487.461, 3473.68, 3414.756, 3374.803, 3381.873, 3379.331, 3401.573, 3407.18, 3435.914, 3453.836, 3373.218, 3397.064, 3410.192, 3426.803, 3402.161, 3420.763, 3444.424, 3360.438, 3612.546, 4429.895, 3901.319, 3383.917, 3428.462, 3435.928, 3460.112, 3601.19, 3357.094, 3466.316, 3358.229, 3350.427, 3363.754, 3319.301, 3288.459, 3268.265, 3276.894, 3217.224, 3305.971, 3347.334, 3254.39, 3394.371, 3388.072, 3194.461, 3145.434, 3144.834, 3403.077, 3249.119, 3673.613, 3420.526, 3485.026, 3219.048, 3160.907, 3204.606, 3211.372, 3299.364, 3112.723, 3196.671, 3101.744, 3378.094, 3627.333, 3220.398, 3192.015, 3243.312, 3203.347, 3315.813, 3130.559, 3266.218, 3295.537, 3298.812, 3285.892, 3275.203, 3181.313, 3333.256, 3116.826, 3114.722, 3162.271, 3398.937, 3222.409, 3093.281, 3200.813, 3037.798, 3190.063, 3429.72], 'val_loss': [7936.533, 3460.438, 3432.258, 3200.399, 3670.692, 3259.235, 3141.018, 3136.293, 3134.139, 3127.711, 3125.863, 3129.33, 3110.856, 3080.034, 3030.755, 2982.141, 2921.207, 2833.3, 2810.19, 2764.742, 2752.131, 2681.861, 2710.431, 2714.582, 2770.273, 2734.768, 2740.54, 2801.917, 2680.776, 2812.842, 2729.898, 2758.337, 2857.276, 2738.325, 2669.195, 3077.594, 4760.319, 3419.833, 2818.789, 2836.505, 2757.375, 2906.345, 2987.917, 2756.018, 2805.623, 2799.733, 2799.152, 2695.161, 2911.748, 2716.665, 2605.93, 2793.983, 2604.955, 2566.077, 2787.878, 2684.925, 2870.673, 2645.146, 2797.079, 2549.26, 2542.23, 2907.202, 2550.813, 2802.192, 3144.933, 3233.727, 2668.908, 2675.45, 2813.93, 2522.502, 2869.623, 2591.276, 2643.471, 2412.466, 2731.835, 2834.129, 2604.891, 2665.75, 2566.978, 2751.604, 2749.866, 2515.338, 2663.519, 2889.816, 2805.53, 2399.917, 2602.03, 2548.646, 2911.878, 2561.21, 2453.172, 2593.453, 2802.549, 3179.32, 2521.245, 2422.899, 2543.378, 2450.451, 3141.556, 2804.759]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:86 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.08835175881075448 beta1:0.9460386875373169 beta2:0.9060025915420744 weight_decay:9.685360722189728e-05 batch_size:83 epochs:100	100	1000	True	3526.20728		457869	14	-1	104.51843285560608	{'train_loss': [14786.439, 4817.387, 4281.203, 4080.979, 4131.134, 4050.203, 4057.103, 4058.46, 4111.259, 4003.371, 3978.487, 3998.977, 4057.805, 4073.784, 4009.041, 4111.185, 4067.543, 3991.773, 4125.117, 4163.088, 4023.982, 4105.935, 4321.254, 4336.148, 4280.408, 4090.046, 4220.694, 4153.96, 4057.423, 4206.786, 4179.249, 3999.602, 3989.521, 4070.002, 3998.779, 3979.513, 4089.604, 4057.887, 4052.962, 4002.049, 3982.149, 4364.613, 4673.393, 4800.589, 4978.973, 4461.253, 4120.767, 4034.221, 4032.54, 4024.659, 4072.383, 4052.746, 4121.751, 4120.996, 4009.68, 4045.081, 3977.016, 3973.446, 4037.574, 4018.171, 4009.38, 4074.771, 4087.27, 3993.175, 4020.15, 4015.43, 4012.673, 4032.845, 3994.69, 4020.107, 4039.183, 4017.189, 4041.35, 4001.154, 4003.707, 4058.678, 4088.133, 4060.299, 4093.921, 4296.798, 4045.009, 4096.69, 4016.596, 3997.365, 3995.33, 4140.777, 4376.163, 4225.344, 4552.405, 5074.662, 4343.365, 4228.122, 4006.611, 4047.088, 4069.19, 4026.042, 4101.214, 4132.396, 4057.342, 4137.146], 'val_loss': [4412.757, 3873.352, 3454.944, 3728.573, 3425.196, 3144.388, 3473.469, 3740.187, 3226.558, 3180.784, 3167.69, 3318.369, 3185.884, 3128.862, 3277.491, 3387.159, 3155.029, 3678.36, 3824.652, 3206.641, 3589.065, 3322.967, 4055.997, 3881.039, 3802.196, 3427.506, 3497.637, 3194.618, 3158.04, 4296.097, 3136.032, 3147.608, 3657.667, 3234.427, 3147.839, 3269.244, 3399.178, 3325.715, 3141.808, 3209.298, 3197.966, 3920.559, 5135.648, 3528.66, 3592.737, 3320.393, 3250.434, 3316.871, 3274.929, 3350.871, 3325.583, 3653.443, 3598.474, 3152.083, 3370.918, 3134.562, 3257.248, 3238.771, 3122.064, 3147.599, 3385.247, 3540.894, 3166.946, 3202.696, 3281.567, 3204.77, 3293.435, 3215.358, 3147.21, 3287.446, 3198.319, 3357.493, 3154.326, 3185.9, 3166.054, 3178.427, 3380.837, 3297.895, 4367.123, 3232.39, 3560.111, 3154.893, 3202.782, 3142.165, 3147.061, 4450.396, 3612.393, 4823.674, 6423.202, 4263.948, 3906.514, 3135.5, 3153.533, 3188.455, 3242.689, 3148.962, 3835.89, 3295.606, 3394.112, 3629.335]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:47 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.08835175881075448 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	2431.89282		424039	15	-1	107.91827154159546	{'train_loss': [12178.155, 4659.268, 4243.297, 4026.674, 3965.137, 3922.805, 3793.852, 3691.636, 3664.798, 3619.069, 3583.741, 3557.571, 3496.601, 3452.13, 3407.935, 3403.068, 3398.777, 3383.21, 3405.394, 3346.054, 3385.896, 3415.588, 3403.386, 3342.994, 3289.665, 3289.0, 3270.109, 3263.215, 3326.655, 3257.567, 3237.789, 3215.722, 3218.252, 3227.304, 3195.057, 3181.232, 3187.37, 3243.24, 3251.475, 3209.044, 3213.908, 3184.666, 3215.224, 3178.014, 3120.224, 3137.299, 3170.02, 3134.541, 3093.969, 3187.711, 3332.632, 3244.414, 3136.468, 3160.068, 3105.756, 3111.924, 3109.319, 3170.292, 3143.356, 3083.501, 3103.19, 3073.336, 3055.716, 3154.555, 3075.719, 3136.7, 3097.002, 3001.581, 3187.187, 3327.468, 3172.674, 3171.298, 3047.191, 3180.637, 3362.452, 3345.052, 3152.951, 3091.644, 3303.976, 3182.266, 3189.277, 3157.22, 3144.374, 3127.555, 3037.324, 3016.583, 3057.974, 3086.238, 3021.384, 3091.736, 3036.977, 3095.714, 3009.149, 3078.472, 3078.26, 3094.158, 3027.275, 3136.533, 3100.018, 3093.102], 'val_loss': [3907.484, 4219.357, 3218.187, 3257.54, 3497.392, 3176.25, 3064.319, 3014.925, 3031.287, 3026.852, 3046.224, 2872.71, 2828.528, 2843.244, 2766.426, 2813.696, 2737.388, 2835.391, 2739.972, 2796.516, 2879.653, 2865.183, 2789.59, 2755.485, 2721.513, 2782.833, 2680.435, 2735.123, 2781.926, 2769.008, 2796.682, 2696.226, 2708.714, 2748.355, 2640.778, 2661.051, 2836.156, 2730.027, 2706.052, 2706.864, 2646.566, 2636.73, 2762.677, 2655.252, 2537.468, 2527.577, 2656.136, 2530.798, 2559.434, 2769.86, 2866.924, 2511.458, 2551.623, 2585.068, 2547.313, 2505.437, 2584.905, 2530.698, 2535.147, 2460.685, 2468.254, 2453.354, 2500.075, 2544.612, 2520.979, 2535.859, 2497.604, 2472.478, 2883.327, 2678.19, 2517.524, 2580.186, 2552.174, 2580.292, 2855.799, 2567.632, 2453.037, 2628.48, 2464.057, 2557.481, 2790.569, 2398.065, 2512.289, 2376.319, 2383.066, 2501.414, 2431.49, 2666.647, 2490.348, 2519.982, 2528.391, 2540.611, 2475.389, 2491.854, 2346.644, 2413.461, 2424.702, 2533.886, 2401.891, 2472.442]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.08835175881075448 beta1:0.8252005124911841 beta2:0.9975553842663847 weight_decay:9.685360722189728e-05 batch_size:83 epochs:100	100	1000	True	23511.73633		501787	15	-1	107.47974562644958	{'train_loss': [19762.734, 4335.54, 4020.587, 3977.935, 3977.121, 3946.166, 3757.661, 3648.987, 3588.677, 3497.37, 3437.221, 3492.581, 3446.178, 3420.611, 3423.371, 3426.43, 3412.903, 3392.886, 3375.615, 3408.489, 3387.765, 3426.022, 3383.81, 3514.854, 4523.981, 3601.215, 3468.501, 3499.571, 3367.028, 3366.423, 3375.034, 3295.542, 3331.092, 3442.368, 4765.737, 9456.363, 3820.306, 3393.679, 3270.344, 3296.702, 3335.826, 3279.938, 3246.102, 3267.13, 3249.553, 3267.506, 3242.37, 3223.83, 3257.032, 3787.857, 3361.327, 3591.93, 9546.802, 37649.699, 30273.594, 30220.908, 30220.902, 30220.891, 30282.994, 30220.828, 30237.346, 30220.92, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.916, 30220.918, 30220.916, 30220.916, 30220.916, 30220.912, 30220.91, 30226.264, 30220.92, 30220.92, 30220.92, 30220.92, 30220.92, 30220.92, 30220.92, 30220.92, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.912, 30222.627, 30253.555, 30220.84, 30234.906, 30220.92, 30220.92], 'val_loss': [4270.95, 3336.194, 3155.143, 3156.728, 3159.35, 3091.127, 3067.141, 2914.339, 2881.068, 2894.879, 3164.128, 2779.969, 2949.586, 2810.446, 2823.122, 2956.074, 2830.541, 2933.43, 3038.891, 2970.83, 2793.792, 2818.725, 3013.155, 2930.922, 3303.569, 2880.637, 3157.533, 2828.452, 2822.971, 2752.998, 2918.64, 2709.242, 3034.995, 3079.57, 13717.621, 4593.151, 2868.282, 2743.292, 2925.915, 2706.205, 2794.542, 2915.854, 2601.597, 2575.378, 2626.352, 2648.219, 2558.542, 2515.165, 3286.079, 2699.001, 2636.545, 2993.021, 134187.156, 23469.027, 23495.078, 23495.074, 23495.068, 23495.043, 23495.068, 23494.525, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.08, 23495.08, 23495.08, 23495.078, 23495.07, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.08, 23495.078, 24155.902, 23495.068, 23493.697, 23495.082, 23495.082, 23495.082]}	100	100	True
