id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.0009673374036965584 batch_size:49 epochs:100	100	2000	True	1078.89282		748280	21	-1	181.18098545074463	{'train_loss': [5262.933, 2956.906, 2651.94, 2468.534, 2332.819, 2253.577, 2191.963, 2156.94, 2111.944, 2064.19, 2028.311, 1987.676, 2009.735, 1985.07, 1964.882, 1953.444, 1962.833, 1930.696, 1905.167, 1918.241, 1894.81, 1891.408, 1871.167, 1853.603, 1776.452, 1679.688, 1665.274, 1591.072, 1565.196, 1524.342, 1494.778, 1468.714, 1444.839, 1436.12, 1419.987, 1413.091, 1392.258, 1388.499, 1377.828, 1356.533, 1354.155, 1336.826, 1338.494, 1328.757, 1330.877, 1319.469, 1306.498, 1301.807, 1287.043, 1273.779, 1259.725, 1279.875, 1286.038, 1291.408, 1265.223, 1247.468, 1240.551, 1229.306, 1241.804, 1227.057, 1231.285, 1224.527, 1224.518, 1221.136, 1213.884, 1215.265, 1209.575, 1210.221, 1215.6, 1202.176, 1204.581, 1215.594, 1216.095, 1210.445, 1208.034, 1205.811, 1221.994, 1216.387, 1205.496, 1197.989, 1191.433, 1200.032, 1192.623, 1208.258, 1187.562, 1179.332, 1179.751, 1199.455, 1184.055, 1172.312, 1176.426, 1184.957, 1179.635, 1178.782, 1174.7, 1168.543, 1172.123, 1174.963, 1161.54, 1192.73], 'val_loss': [5437.443, 3079.986, 2432.038, 2062.01, 2022.67, 1943.5, 1921.243, 1865.573, 1806.368, 1818.999, 1778.104, 1753.261, 1802.927, 1742.405, 1737.507, 1746.941, 1739.77, 1700.685, 1662.625, 1680.454, 1698.075, 1634.949, 1640.527, 1610.45, 1631.634, 1438.1, 1474.229, 1500.202, 1361.237, 1376.72, 1317.75, 1353.262, 1259.912, 1236.691, 1223.121, 1212.478, 1198.063, 1229.783, 1242.333, 1165.805, 1221.294, 1172.64, 1148.063, 1193.692, 1142.963, 1113.089, 1138.884, 1082.968, 1072.839, 1068.03, 1066.495, 1086.646, 1101.365, 1141.295, 1059.12, 1046.061, 1044.922, 1047.16, 1033.48, 1059.479, 1067.385, 1030.457, 1028.27, 1030.084, 1005.573, 1016.125, 1036.908, 1025.041, 1017.778, 1019.23, 1045.365, 1031.639, 1040.732, 1050.958, 1029.064, 1057.992, 1027.032, 1023.552, 1020.47, 1003.882, 1022.241, 1012.791, 996.057, 1028.176, 995.703, 1000.307, 1006.967, 1011.79, 1002.194, 998.573, 996.89, 993.025, 1013.983, 1003.651, 1003.308, 989.978, 992.588, 985.315, 988.761, 1035.686]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:97 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:114 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:30 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.000436441315793302 batch_size:49 epochs:100	100	1000	True	1040.56714		1798512	22	-1	309.1590521335602	{'train_loss': [15486.486, 13604.872, 12907.206, 12656.733, 12503.146, 12355.146, 11133.277, 6661.025, 2868.844, 2238.19, 2039.803, 1905.082, 1664.014, 1520.276, 1449.53, 1437.402, 1388.489, 1359.908, 1348.337, 1331.122, 1334.858, 1315.566, 1316.924, 1293.243, 1282.378, 1281.651, 1272.495, 1274.683, 1297.495, 1274.303, 1255.967, 1251.329, 1265.786, 1248.206, 1231.566, 1234.886, 1218.409, 1224.888, 1227.075, 1232.147, 1231.657, 1206.904, 1190.151, 1205.972, 1200.095, 1188.018, 1188.094, 1180.813, 1172.083, 1177.783, 1179.683, 1170.683, 1157.326, 1165.407, 1158.918, 1149.36, 1147.391, 1147.557, 1140.262, 1134.737, 1127.222, 1123.929, 1131.357, 1132.199, 1124.343, 1126.816, 1128.591, 1125.189, 1140.87, 1150.492, 1125.775, 1119.156, 1125.204, 1120.922, 1117.7, 1109.26, 1106.882, 1131.931, 1143.975, 1147.676, 1121.616, 1103.849, 1104.543, 1101.06, 1120.731, 1113.133, 1100.334, 1095.818, 1092.382, 1095.632, 1084.338, 1090.461, 1081.755, 1079.664, 1091.35, 1095.593, 1115.417, 1122.943, 1116.757, 1083.589], 'val_loss': [13241.676, 11822.693, 10937.923, 10664.066, 10592.072, 10424.246, 8004.854, 3099.847, 2105.404, 1865.159, 1833.471, 1591.97, 1376.3, 1301.368, 1313.222, 1249.945, 1219.65, 1308.311, 1254.702, 1127.61, 1214.238, 1109.893, 1106.958, 1199.731, 1100.024, 1106.112, 1123.819, 1088.701, 1132.366, 1069.37, 1076.832, 1037.211, 1111.529, 1083.632, 1085.755, 1049.261, 1031.275, 1035.914, 1094.295, 1081.647, 1056.628, 1050.737, 1019.164, 1001.396, 999.639, 1000.063, 1051.849, 986.434, 1024.314, 1055.746, 989.82, 1025.001, 995.918, 977.624, 975.119, 972.359, 985.306, 978.243, 962.156, 961.25, 968.324, 949.841, 960.89, 958.955, 967.141, 995.565, 951.172, 957.457, 984.458, 955.757, 941.195, 953.503, 954.927, 954.591, 937.052, 943.79, 978.65, 971.668, 1032.261, 963.382, 953.081, 935.931, 942.453, 997.267, 989.035, 947.142, 991.946, 962.346, 953.65, 951.086, 949.474, 962.065, 959.926, 958.342, 943.473, 953.423, 982.608, 1058.009, 941.391, 950.103]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:87 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:81 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:26 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:75 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:deconv1d out_channels:26 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adadelta lr:0.002460715196435833 batch_size:49 epochs:100	100	1000	True	1454.35547		816317	22	-1	166.04416966438293	{'train_loss': [8762.093, 8018.801, 6723.708, 5307.984, 4277.623, 3605.343, 3223.734, 2995.308, 2845.039, 2747.527, 2681.359, 2631.728, 2596.864, 2572.896, 2555.55, 2543.045, 2528.983, 2518.927, 2511.799, 2505.734, 2500.06, 2497.699, 2491.031, 2488.627, 2483.729, 2480.216, 2480.047, 2478.069, 2475.342, 2472.379, 2469.06, 2469.432, 2463.705, 2464.573, 2460.144, 2458.364, 2458.007, 2457.644, 2450.202, 2446.236, 2436.382, 2416.575, 2374.309, 2328.607, 2290.297, 2265.01, 2241.393, 2226.988, 2211.581, 2194.093, 2184.792, 2169.52, 2165.801, 2156.808, 2145.004, 2133.606, 2123.896, 2121.716, 2110.521, 2103.65, 2093.522, 2085.106, 2080.727, 2075.254, 2065.559, 2061.716, 2059.764, 2053.81, 2041.354, 2032.313, 2031.836, 2024.348, 2015.976, 2006.616, 1998.744, 1990.156, 1990.251, 1985.947, 1972.874, 1957.767, 1956.709, 1950.267, 1936.173, 1935.235, 1921.61, 1917.816, 1911.669, 1901.307, 1889.509, 1879.319, 1875.279, 1866.878, 1852.245, 1863.901, 1849.376, 1833.055, 1825.58, 1820.94, 1817.808, 1812.955], 'val_loss': [5005.351, 4712.001, 6149.024, 5968.947, 4277.128, 3359.234, 2952.06, 2741.394, 2593.6, 2494.252, 2413.491, 2354.337, 2312.06, 2282.142, 2260.041, 2237.141, 2224.236, 2207.514, 2197.202, 2193.463, 2178.548, 2172.357, 2164.348, 2160.941, 2158.986, 2149.678, 2146.359, 2144.257, 2141.287, 2135.706, 2135.343, 2135.012, 2132.211, 2131.723, 2127.374, 2123.896, 2123.876, 2123.251, 2120.907, 2111.952, 2105.568, 2084.821, 2055.803, 2027.977, 2019.473, 1993.791, 1974.467, 1962.851, 1948.963, 1939.229, 1927.138, 1916.724, 1907.24, 1889.776, 1886.105, 1874.658, 1855.893, 1857.638, 1842.703, 1839.152, 1830.494, 1821.934, 1815.235, 1807.336, 1797.181, 1793.259, 1779.895, 1769.556, 1764.932, 1755.471, 1752.479, 1732.77, 1731.265, 1722.099, 1708.666, 1701.246, 1684.76, 1680.932, 1675.395, 1658.304, 1659.466, 1627.783, 1623.834, 1615.405, 1604.555, 1598.665, 1575.156, 1579.468, 1564.946, 1557.612, 1542.991, 1526.69, 1533.2, 1529.481, 1507.219, 1492.88, 1504.878, 1491.469, 1478.965, 1483.0]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:119 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:deconv1d out_channels:116 kernel_size:4 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.0009673374036965584 batch_size:80 epochs:100	100	1000	True	1523.5072		1769116	20	-1	273.9021580219269	{'train_loss': [16007.312, 10690.66, 9153.315, 8651.154, 7747.012, 6621.003, 6131.049, 5576.874, 5224.647, 5040.268, 4540.16, 4037.72, 3704.537, 3460.907, 3334.97, 3229.423, 3172.046, 3105.767, 3097.349, 3064.204, 3044.904, 3017.874, 2833.647, 2554.278, 2438.307, 2373.083, 2296.626, 2227.259, 2210.655, 2178.228, 2129.658, 2102.326, 2114.603, 2084.34, 2070.576, 2059.758, 2073.847, 2029.945, 2003.982, 1985.24, 1986.712, 1989.567, 2014.466, 1985.28, 2004.438, 1979.672, 1955.851, 1965.588, 1965.463, 1953.547, 1945.317, 1940.513, 1918.821, 1915.546, 1907.24, 1901.819, 1908.756, 1905.958, 1906.172, 1912.113, 1897.559, 1890.674, 1880.326, 1884.392, 1880.34, 1874.38, 1875.569, 1881.798, 1905.647, 1875.446, 1925.663, 1889.266, 1872.371, 1878.026, 1868.123, 1884.959, 1876.418, 1902.347, 1894.029, 1871.814, 1866.956, 1854.711, 1852.497, 1850.741, 1833.835, 1829.527, 1833.817, 1826.363, 1843.908, 1821.281, 1843.293, 1844.177, 1828.27, 1822.2, 1850.446, 1833.497, 1846.509, 1862.77, 1820.661, 1813.594], 'val_loss': [27206.555, 14860.744, 11043.184, 8006.25, 6590.831, 6873.809, 5539.522, 4998.309, 4898.584, 4142.017, 3713.847, 3424.804, 3040.029, 2927.994, 2807.703, 2701.514, 2693.316, 2625.88, 2719.188, 2615.027, 2629.945, 2533.568, 2221.401, 2029.601, 2122.328, 2177.665, 2066.409, 1849.392, 1822.109, 1791.062, 1715.779, 1700.363, 1700.894, 1789.609, 1647.229, 1653.339, 1740.277, 1639.067, 1653.113, 1614.719, 1629.955, 1679.535, 1676.255, 1675.269, 1820.616, 1861.888, 1544.686, 1584.147, 1593.367, 1892.557, 1580.653, 1565.328, 1612.106, 1566.083, 1509.851, 1532.401, 1528.47, 1561.526, 1694.156, 1523.742, 1511.634, 1493.804, 1521.899, 1516.68, 1520.158, 1473.161, 1494.37, 1563.586, 1523.166, 1634.172, 1668.444, 1530.924, 1496.402, 1509.254, 1492.042, 1565.726, 1516.981, 1596.133, 1536.486, 1577.474, 1521.649, 1497.611, 1465.687, 1493.498, 1447.146, 1486.409, 1461.634, 1473.42, 1457.272, 1463.148, 1498.461, 1453.826, 1457.642, 1496.586, 1532.617, 1495.173, 1564.125, 1472.953, 1524.816, 1457.547]}	100	100	True
