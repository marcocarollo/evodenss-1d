id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:47 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.08835175881075448 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	2315.06323		424039	15	-1	110.29104232788086	{'train_loss': [11690.261, 4516.408, 4098.016, 4030.501, 4050.12, 3896.719, 3759.044, 3672.897, 3598.688, 3516.907, 3452.351, 3434.424, 3433.109, 3455.305, 3400.766, 3362.418, 3322.718, 3316.396, 3329.226, 3325.485, 3301.842, 3302.819, 3298.624, 3286.558, 3298.427, 3330.652, 3338.787, 3354.513, 3325.625, 3291.837, 3348.386, 3281.024, 3329.133, 3311.989, 3286.14, 3295.373, 3315.185, 3306.153, 3270.351, 3241.407, 3206.59, 3190.038, 3201.278, 3291.623, 3207.99, 3196.577, 3212.98, 3302.447, 3281.164, 3204.037, 3226.839, 3286.53, 3306.557, 3282.715, 3194.079, 3238.665, 3290.153, 3350.68, 3326.404, 3372.755, 3242.856, 3230.8, 3179.479, 3154.51, 3144.521, 3286.244, 3378.826, 3541.36, 3396.662, 3283.718, 3317.502, 3341.561, 3201.673, 3181.785, 3218.89, 3209.995, 3148.887, 3219.813, 3254.91, 3286.729, 3279.389, 3287.744, 3176.564, 3183.122, 3120.917, 3109.9, 3086.698, 3088.221, 3377.538, 3330.799, 3161.248, 3053.704, 3107.43, 3176.372, 3120.159, 3027.29, 2980.659, 3087.247, 3187.649, 3016.684], 'val_loss': [4040.25, 3306.177, 3170.753, 3452.959, 3144.087, 3055.641, 2984.759, 2967.665, 2871.611, 2768.704, 2738.035, 2787.284, 2810.668, 2723.012, 2708.462, 2654.482, 2666.164, 2674.194, 2633.781, 2770.758, 2704.899, 2711.041, 2776.5, 2709.451, 2752.482, 2766.172, 2677.604, 2791.09, 2707.47, 3079.312, 2616.618, 2640.027, 2705.212, 2675.196, 2613.327, 2658.357, 2672.975, 2783.561, 2680.224, 2818.218, 2641.836, 2691.152, 2713.149, 2672.644, 2607.272, 2593.695, 2655.531, 2615.385, 2616.2, 2588.804, 2747.435, 2611.187, 2780.824, 2588.47, 2592.76, 2681.99, 2793.527, 2681.651, 2911.981, 2877.248, 2718.101, 2686.681, 2711.85, 2505.538, 2526.719, 2874.546, 2778.715, 2901.76, 2649.556, 2735.17, 3165.46, 2888.231, 2655.376, 2741.068, 2785.193, 2540.562, 2714.171, 2717.572, 2821.464, 2948.653, 2598.958, 2533.277, 2537.681, 2621.047, 2529.626, 2584.764, 2639.785, 2647.221, 3280.373, 2696.887, 2477.221, 2819.969, 2587.576, 2355.402, 2414.334, 2381.362, 2335.978, 2432.905, 2489.543, 2306.703]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:40 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:47 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.03516776157654191 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	2146.88501		335683	15	-1	108.80141425132751	{'train_loss': [10601.343, 4658.525, 4142.253, 4013.262, 3990.732, 3975.879, 3971.557, 3972.388, 3951.92, 3877.813, 3738.666, 3697.009, 3695.236, 3687.322, 3562.625, 3506.148, 3359.245, 3327.604, 3239.2, 3222.397, 3182.833, 3175.223, 3153.655, 3166.67, 3148.364, 3147.836, 3156.266, 3146.12, 3118.644, 3115.276, 3131.197, 3148.588, 3113.923, 3106.553, 3099.883, 3077.338, 3058.597, 3061.886, 3081.039, 3085.256, 3054.854, 3036.14, 3070.316, 3101.067, 3052.797, 2993.661, 3027.195, 3029.032, 3008.414, 3021.576, 2989.844, 3000.585, 2993.119, 3042.462, 3046.416, 3106.732, 3133.282, 3062.645, 2997.892, 2975.776, 3052.074, 3040.227, 3082.547, 3002.375, 2964.507, 2993.255, 3003.68, 2959.585, 2999.957, 3014.955, 2932.776, 2939.159, 2962.675, 2974.12, 2951.591, 2910.692, 2938.439, 2898.059, 2912.056, 2866.338, 2929.749, 2878.732, 2898.732, 2920.584, 2957.046, 2948.292, 3017.051, 2958.852, 2908.816, 2857.004, 2882.802, 2906.247, 2922.996, 2836.598, 2893.255, 2904.419, 3009.558, 2879.832, 2798.1, 2802.812], 'val_loss': [4554.316, 3751.315, 3172.721, 3143.661, 3150.443, 3139.129, 3146.572, 3161.519, 3091.445, 3014.141, 2952.149, 2967.129, 2961.688, 2976.582, 2942.605, 2795.343, 2631.315, 2788.536, 2639.281, 2637.252, 2591.731, 2649.754, 2602.616, 2654.66, 2637.875, 2572.428, 2757.306, 2670.713, 2659.946, 2702.418, 2683.734, 2610.529, 2586.577, 2494.298, 2662.948, 2516.991, 2600.206, 2581.036, 2523.639, 2554.934, 2465.351, 2510.093, 2474.322, 2576.007, 2496.604, 2441.613, 2593.773, 2508.759, 2410.135, 2382.361, 2403.656, 2357.067, 2443.318, 2389.969, 2478.949, 2584.182, 2513.615, 2429.523, 2345.511, 2447.405, 2325.724, 2480.693, 2384.152, 2357.693, 2296.644, 2442.799, 2323.651, 2407.959, 2308.655, 2279.871, 2336.677, 2249.632, 2341.787, 2292.73, 2259.917, 2408.101, 2225.361, 2325.814, 2347.539, 2288.623, 2297.421, 2202.543, 2564.48, 2281.853, 2303.846, 2428.64, 2332.721, 2326.349, 2187.189, 2179.461, 2295.281, 2276.043, 2195.188, 2202.298, 2282.829, 2374.697, 2276.352, 2170.16, 2161.909, 2148.488]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:47 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:60 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:48 kernel_size:5 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.08835175881075448 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:8.8270535466201e-05 batch_size:83 epochs:100	100	1000	True	2621.0354		397807	16	-1	111.42756462097168	{'train_loss': [14808.315, 4947.094, 4075.121, 3985.603, 3963.511, 3954.873, 3949.729, 3946.935, 3911.475, 3792.292, 3696.358, 3620.734, 3555.929, 3459.043, 3421.388, 3484.222, 3413.624, 3494.155, 3437.712, 3489.161, 3374.811, 3581.117, 3406.935, 3387.683, 3369.087, 3435.475, 3483.117, 3365.479, 3324.341, 3321.1, 3349.664, 3337.997, 3285.515, 3317.709, 3268.752, 3303.041, 3273.787, 3242.158, 3296.716, 3254.813, 3286.875, 3274.805, 3237.652, 3239.867, 3259.391, 3172.165, 3270.412, 3318.504, 3401.863, 3307.71, 3293.799, 3304.815, 3226.217, 3289.65, 3183.894, 3301.174, 3446.214, 3384.695, 3234.626, 3142.995, 3236.897, 3323.851, 3253.236, 3278.832, 3346.873, 3859.479, 3465.098, 3362.072, 3295.266, 3250.443, 3228.364, 3173.647, 3185.202, 3606.555, 3716.127, 3348.571, 3317.45, 3165.064, 3299.621, 3260.603, 3219.965, 3173.654, 3159.526, 3197.873, 3178.4, 3173.371, 3525.648, 3253.992, 3194.506, 3140.169, 3071.674, 3159.746, 3139.494, 3344.978, 3809.698, 3960.298, 3837.136, 3539.758, 3340.69, 3187.462], 'val_loss': [6182.468, 3358.059, 3168.183, 3139.012, 3134.291, 3131.749, 3130.741, 3128.2, 3062.718, 3002.32, 3033.537, 3012.272, 2941.215, 2772.311, 2759.657, 2882.433, 2767.356, 2995.085, 2888.172, 2917.933, 2984.903, 2854.821, 2825.646, 2887.715, 2777.957, 2777.679, 2925.186, 2767.426, 2721.037, 2674.283, 2796.691, 2944.817, 2762.796, 2691.895, 2684.999, 2732.226, 2794.052, 2760.34, 2752.904, 2621.491, 2644.414, 2772.702, 2758.498, 2597.436, 2708.336, 2683.949, 2674.925, 2822.438, 2797.051, 2903.065, 2747.443, 2636.223, 2774.246, 2723.313, 2644.629, 2752.537, 3329.086, 3096.448, 2603.915, 2714.098, 2790.848, 2787.733, 2796.006, 3024.886, 3247.993, 2834.669, 2850.165, 2669.792, 2598.478, 2656.115, 2612.723, 2640.474, 3633.061, 3529.623, 2790.5, 3058.685, 2619.05, 3090.869, 2712.521, 2698.534, 2529.288, 2647.284, 2607.577, 2555.518, 2535.493, 3715.416, 2731.436, 2782.068, 2531.688, 2534.396, 2614.375, 2536.229, 2674.568, 3262.633, 3836.076, 2941.031, 3688.45, 2704.482, 2653.61, 2688.309]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:78 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	1612.91382		449410	15	-1	138.3007287979126	{'train_loss': [6231.421, 4309.862, 4050.476, 3822.294, 3753.779, 3661.71, 3532.488, 3459.356, 3417.425, 3371.597, 3332.613, 3328.06, 3300.35, 3259.529, 3234.381, 3213.766, 3162.719, 3164.653, 3068.925, 2999.551, 2879.795, 2783.023, 2667.715, 2590.42, 2551.171, 2465.303, 2419.835, 2346.777, 2300.776, 2322.033, 2279.173, 2240.32, 2249.516, 2314.516, 2251.49, 2182.883, 2168.722, 2179.675, 2179.438, 2172.999, 2156.206, 2146.292, 2138.914, 2125.159, 2128.001, 2121.492, 2119.492, 2105.364, 2128.575, 2094.636, 2099.003, 2089.483, 2084.115, 2100.224, 2087.989, 2073.29, 2096.045, 2071.795, 2073.232, 2069.049, 2049.666, 2046.239, 2077.684, 2053.076, 2045.687, 2036.48, 2042.985, 2030.976, 2025.719, 2028.674, 2023.483, 2018.772, 2026.406, 2033.714, 2059.844, 2032.623, 2036.961, 2040.521, 2022.693, 2029.191, 2004.413, 2001.488, 2024.984, 2023.295, 2033.953, 2049.88, 2049.028, 2055.588, 2044.898, 1994.89, 2011.18, 1999.714, 1981.588, 1992.111, 1982.027, 1978.446, 1973.433, 1984.289, 1996.163, 1999.638], 'val_loss': [5003.39, 3517.912, 3253.178, 3139.076, 3054.037, 2896.092, 2866.351, 2790.661, 2762.307, 2790.691, 2717.323, 2731.624, 2653.278, 2654.534, 2712.632, 2642.998, 2517.105, 2625.493, 2494.125, 2413.204, 2337.191, 2181.492, 2082.522, 2258.288, 2047.63, 2038.061, 1921.48, 1854.631, 1926.134, 1914.783, 1782.611, 1752.616, 1814.105, 2054.58, 1856.703, 1710.312, 1699.178, 1790.533, 1927.22, 1736.459, 1730.831, 1801.795, 1749.698, 1708.951, 1783.177, 1693.054, 1745.533, 1688.065, 1823.118, 1698.031, 1670.234, 1714.057, 1815.532, 1701.879, 1715.892, 1743.75, 1707.266, 1751.672, 1696.71, 1718.117, 1651.949, 1655.742, 1738.356, 1742.517, 1641.255, 1685.066, 1678.984, 1675.141, 1657.759, 1635.317, 1675.422, 1679.332, 1633.058, 1874.414, 1661.465, 1662.22, 1692.497, 1665.865, 1647.522, 1644.818, 1650.395, 1601.887, 1618.704, 1683.947, 1633.692, 1641.389, 1647.682, 1647.089, 1591.374, 1590.304, 1629.236, 1605.856, 1575.876, 1624.105, 1616.449, 1596.431, 1580.59, 1609.208, 1628.032, 1626.435]}	100	100	True
