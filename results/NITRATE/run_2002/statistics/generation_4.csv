id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:78 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	1544.98486		449410	15	-1	144.50732421875	{'train_loss': [6023.218, 4292.218, 4141.119, 4078.811, 3898.792, 3727.914, 3594.586, 3494.852, 3450.424, 3400.07, 3355.918, 3325.213, 3297.307, 3268.396, 3251.622, 3240.175, 3222.634, 3206.03, 3205.655, 3174.227, 3166.937, 3161.984, 3129.862, 3106.29, 3099.012, 3075.617, 3024.097, 2950.461, 2834.347, 2683.324, 2565.783, 2460.418, 2424.482, 2357.745, 2325.658, 2293.906, 2274.74, 2253.421, 2221.131, 2204.687, 2188.24, 2163.844, 2153.477, 2161.955, 2139.393, 2125.124, 2118.239, 2084.681, 2112.722, 2102.729, 2115.957, 2077.5, 2094.884, 2064.648, 2059.52, 2049.521, 2046.72, 2076.305, 2058.513, 2043.212, 2035.009, 2023.72, 2012.511, 2009.31, 2013.576, 2024.639, 2019.417, 2017.941, 1982.822, 2004.472, 2002.518, 2002.169, 1992.632, 2002.553, 1984.808, 1993.026, 1987.685, 1990.876, 1988.82, 1958.281, 1975.81, 1978.314, 1979.679, 1984.933, 1992.708, 1967.825, 1955.057, 1967.168, 1949.278, 1972.784, 1950.76, 1959.576, 1957.74, 1935.497, 1950.24, 1944.025, 1936.666, 1938.832, 1972.654, 1956.181], 'val_loss': [4846.366, 3725.246, 3380.296, 3244.428, 3134.61, 3009.152, 2873.557, 2831.525, 2783.519, 2769.145, 2761.64, 2683.7, 2672.541, 2643.945, 2654.756, 2608.415, 2617.443, 2607.852, 2622.614, 2626.881, 2600.471, 2629.251, 2542.84, 2513.799, 2473.778, 2486.794, 2469.226, 2307.118, 2266.389, 2109.168, 2059.999, 1956.641, 1840.648, 1868.59, 1914.802, 1879.072, 1912.301, 1893.241, 1798.149, 1812.487, 1832.987, 1736.807, 1731.581, 1761.835, 1781.753, 1693.446, 1669.086, 1656.98, 1764.894, 1666.467, 1789.893, 1697.422, 1690.774, 1635.637, 1678.609, 1644.214, 1679.891, 1687.775, 1645.256, 1643.123, 1591.349, 1605.141, 1565.481, 1608.995, 1612.118, 1617.346, 1616.123, 1591.86, 1586.968, 1602.767, 1585.856, 1580.067, 1571.853, 1551.097, 1555.661, 1536.374, 1560.276, 1527.414, 1572.155, 1550.701, 1557.35, 1562.775, 1523.334, 1557.707, 1527.864, 1532.945, 1580.249, 1518.786, 1537.069, 1531.763, 1559.304, 1533.129, 1558.345, 1522.5, 1534.802, 1516.534, 1514.537, 1548.129, 1529.257, 1503.474]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:73 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:78 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:deconv1d out_channels:125 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:deconv1d out_channels:55 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.08291046221979216 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	23624.375		5061858	16	-1	133.37530827522278	{'train_loss': [42371.367, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918], 'val_loss': [23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:82 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:78 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.002460715196435833 beta1:0.863165869186438 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	1577.53516		451084	15	-1	143.21110439300537	{'train_loss': [5648.35, 4088.477, 3797.668, 3594.246, 3446.389, 3218.91, 2835.561, 2674.647, 2623.867, 2488.733, 2465.732, 2432.04, 2364.057, 2355.295, 2324.592, 2312.943, 2285.5, 2277.84, 2271.552, 2252.184, 2225.863, 2223.637, 2221.218, 2210.789, 2183.092, 2180.535, 2180.039, 2159.275, 2143.656, 2137.481, 2139.665, 2126.262, 2127.84, 2106.848, 2103.321, 2105.133, 2097.864, 2096.638, 2072.166, 2064.074, 2077.813, 2061.336, 2072.814, 2058.623, 2065.177, 2054.618, 2043.723, 2036.849, 2032.247, 2015.543, 2030.739, 2020.964, 2008.427, 2009.853, 1997.533, 1997.76, 1994.661, 1978.923, 1982.681, 1982.865, 1985.679, 1998.448, 1988.175, 1975.649, 1967.315, 1979.085, 1970.667, 1970.581, 1972.996, 1981.056, 1977.699, 1971.414, 1971.995, 1967.511, 1957.896, 1962.965, 1952.791, 1941.392, 1935.368, 1932.31, 1926.578, 1927.703, 1924.539, 1926.92, 1953.099, 1938.83, 1937.897, 1916.646, 1910.88, 1935.29, 1918.972, 1915.502, 1920.643, 1914.731, 1910.52, 1900.263, 1911.947, 1899.351, 1905.545, 1901.223], 'val_loss': [4199.696, 3478.965, 3169.464, 2914.651, 2719.629, 3224.262, 2334.803, 2894.059, 2230.09, 2256.44, 2175.664, 1893.138, 1897.517, 1906.343, 1907.834, 1805.331, 1859.847, 1806.201, 1824.126, 1802.591, 1778.231, 1777.705, 1760.498, 1755.739, 1759.358, 1730.697, 1703.316, 1693.146, 1689.742, 1734.607, 1737.339, 1684.439, 1703.24, 1658.337, 1695.248, 1722.192, 1653.117, 1651.52, 1613.828, 1651.827, 1650.156, 1682.259, 1613.835, 1687.083, 1640.794, 1602.128, 1624.639, 1602.535, 1586.208, 1608.196, 1606.941, 1598.414, 1569.312, 1587.941, 1585.786, 1558.699, 1568.972, 1548.376, 1585.177, 1586.865, 1542.283, 1571.586, 1521.127, 1539.071, 1571.172, 1560.417, 1566.659, 1618.898, 1633.522, 1561.506, 1580.556, 1567.609, 1579.347, 1538.338, 1538.39, 1607.027, 1534.081, 1538.87, 1514.718, 1524.51, 1504.709, 1604.654, 1564.437, 1545.583, 1539.727, 1572.959, 1555.053, 1505.24, 1542.806, 1533.903, 1537.882, 1562.344, 1506.566, 1571.691, 1527.931, 1541.353, 1559.198, 1486.804, 1553.815, 1519.339]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:86 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:78 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:deconv1d out_channels:99 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8098957281009863 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	1573.81311		485026	17	-1	155.4010443687439	{'train_loss': [6165.013, 4324.595, 3988.502, 3866.441, 3790.212, 3610.627, 3562.386, 3486.543, 3432.08, 3396.885, 3357.218, 3305.61, 3251.798, 3171.845, 3080.661, 2974.345, 2895.992, 2740.243, 2651.841, 2592.259, 2575.671, 2529.598, 2493.169, 2451.108, 2417.933, 2384.723, 2348.281, 2323.927, 2316.151, 2285.649, 2286.151, 2276.556, 2268.64, 2268.752, 2251.144, 2226.207, 2242.961, 2229.892, 2208.737, 2225.902, 2217.787, 2216.692, 2210.586, 2199.271, 2181.366, 2188.753, 2152.993, 2164.672, 2146.179, 2144.055, 2129.096, 2135.094, 2122.244, 2115.924, 2127.581, 2118.475, 2099.095, 2096.379, 2100.322, 2119.19, 2100.148, 2089.02, 2094.367, 2086.299, 2094.085, 2074.345, 2065.114, 2062.653, 2044.858, 2048.464, 2045.043, 2060.908, 2049.324, 2042.769, 2034.116, 2040.113, 2036.368, 2041.219, 2033.974, 2042.95, 2027.237, 2031.628, 2030.044, 2017.382, 2016.815, 2030.73, 2017.251, 2004.346, 2005.929, 2000.363, 2000.592, 2004.882, 2007.754, 1997.198, 1999.414, 1997.401, 1985.092, 1980.126, 1984.287, 1987.4], 'val_loss': [9008.553, 5751.938, 4190.8, 3449.882, 3170.793, 2985.384, 2879.188, 2801.806, 2749.566, 2728.422, 2680.729, 2671.862, 2619.747, 2608.435, 2543.667, 2430.201, 2340.534, 2168.406, 2096.727, 2164.495, 2165.321, 2001.04, 1979.702, 1928.969, 1905.475, 1907.118, 1804.308, 1851.923, 1813.802, 1808.57, 1786.875, 1812.462, 1741.981, 1778.206, 1770.859, 1730.083, 1744.568, 1717.849, 1709.636, 1725.719, 1748.481, 1785.934, 1712.856, 1700.762, 1707.448, 1670.146, 1734.399, 1686.168, 1651.663, 1693.583, 1662.889, 1658.936, 1637.018, 1672.31, 1680.376, 1682.532, 1680.888, 1644.631, 1683.934, 1661.894, 1635.173, 1636.85, 1649.825, 1638.115, 1607.511, 1591.398, 1598.015, 1600.8, 1577.267, 1614.558, 1605.33, 1573.565, 1574.68, 1630.247, 1627.793, 1607.799, 1585.837, 1572.698, 1577.06, 1581.121, 1580.403, 1610.463, 1597.104, 1543.267, 1578.106, 1581.314, 1558.992, 1550.25, 1555.165, 1613.778, 1556.62, 1547.645, 1527.339, 1548.39, 1538.108, 1543.024, 1548.46, 1541.25, 1544.693, 1543.8]}	100	100	True
