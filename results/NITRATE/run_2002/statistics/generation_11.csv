id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:35 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:126 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:3.381674346430075e-05 batch_size:49 epochs:100	100	1000	True	981.43201		641438	20	-1	182.94167923927307	{'train_loss': [4057.273, 2761.981, 2614.186, 2436.066, 2279.595, 2190.11, 2142.622, 2086.992, 1916.834, 1694.205, 1542.205, 1489.492, 1425.943, 1417.261, 1385.836, 1366.454, 1358.013, 1337.73, 1316.641, 1306.28, 1296.741, 1278.026, 1267.601, 1256.61, 1257.541, 1248.149, 1243.177, 1239.413, 1232.446, 1222.62, 1216.146, 1212.418, 1213.17, 1200.856, 1200.599, 1209.252, 1207.106, 1205.954, 1189.285, 1203.279, 1189.526, 1182.937, 1186.432, 1178.71, 1178.216, 1171.982, 1173.292, 1160.845, 1168.669, 1177.212, 1162.941, 1158.419, 1166.479, 1164.921, 1153.646, 1150.335, 1164.524, 1152.319, 1146.823, 1146.345, 1154.238, 1157.317, 1158.822, 1149.97, 1135.112, 1134.183, 1130.794, 1146.651, 1140.922, 1138.328, 1143.329, 1127.668, 1128.732, 1132.215, 1130.515, 1125.499, 1125.829, 1125.878, 1123.966, 1122.754, 1120.551, 1116.57, 1120.095, 1123.816, 1116.153, 1114.49, 1108.553, 1108.404, 1108.013, 1112.58, 1112.913, 1106.047, 1106.174, 1105.035, 1106.921, 1110.391, 1116.285, 1109.236, 1099.914, 1098.143], 'val_loss': [2715.146, 2264.158, 2171.102, 2035.508, 1883.035, 1914.019, 1871.492, 1769.566, 1676.546, 1416.958, 1312.95, 1248.78, 1238.544, 1228.049, 1201.554, 1186.872, 1154.247, 1126.789, 1117.992, 1089.793, 1071.989, 1074.477, 1058.21, 1064.267, 1057.227, 1064.715, 1062.421, 1028.183, 1012.082, 1019.208, 1023.757, 1001.643, 1031.32, 1006.714, 1015.274, 1015.282, 1030.648, 1028.292, 992.334, 999.731, 1009.861, 1019.356, 989.937, 986.133, 1002.014, 987.573, 981.837, 978.576, 985.196, 1025.11, 983.398, 1016.024, 994.148, 963.792, 977.519, 967.561, 1000.535, 968.605, 962.2, 966.51, 970.375, 966.41, 984.602, 974.557, 956.519, 965.277, 961.512, 962.687, 952.38, 974.611, 960.863, 971.984, 964.614, 972.262, 948.74, 948.958, 944.528, 970.143, 968.986, 969.879, 946.45, 958.183, 953.57, 951.863, 973.328, 939.01, 951.176, 938.041, 958.276, 935.27, 950.71, 943.092, 929.201, 941.819, 940.804, 961.327, 948.689, 944.444, 935.286, 935.34]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:35 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:126 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adam lr:0.002460715196435833 beta1:0.8746106036814422 beta2:0.9711402723837911 weight_decay:3.381674346430075e-05 batch_size:49 epochs:100	100	1000	True	1000.06097		560838	20	-1	143.58393144607544	{'train_loss': [3525.595, 2522.837, 2319.513, 2241.421, 2148.239, 2118.035, 2103.865, 2054.065, 2036.407, 1767.365, 1614.17, 1510.634, 1467.352, 1458.673, 1429.324, 1411.219, 1390.606, 1377.112, 1365.059, 1352.865, 1348.058, 1330.406, 1325.317, 1317.003, 1317.109, 1296.75, 1290.769, 1286.719, 1286.75, 1278.756, 1294.095, 1269.088, 1260.846, 1260.981, 1271.963, 1252.189, 1246.548, 1245.146, 1246.132, 1246.557, 1253.333, 1235.609, 1237.19, 1231.758, 1229.561, 1225.838, 1223.172, 1216.266, 1215.127, 1208.191, 1218.707, 1211.285, 1212.085, 1206.613, 1198.218, 1200.747, 1199.685, 1202.539, 1208.703, 1187.682, 1189.398, 1186.023, 1188.51, 1189.278, 1195.922, 1187.782, 1183.826, 1184.235, 1190.762, 1182.769, 1181.505, 1175.519, 1174.666, 1171.875, 1179.982, 1170.713, 1185.816, 1170.575, 1169.845, 1168.156, 1166.606, 1172.658, 1162.863, 1164.233, 1163.13, 1158.787, 1167.705, 1165.049, 1159.03, 1161.434, 1162.786, 1160.125, 1161.678, 1155.806, 1155.753, 1151.116, 1158.836, 1148.853, 1148.596, 1147.619], 'val_loss': [2364.849, 2135.303, 2026.775, 1957.699, 1861.139, 1918.122, 1816.989, 1774.659, 1717.507, 1494.026, 1425.462, 1292.832, 1293.054, 1254.747, 1209.231, 1279.859, 1193.592, 1231.672, 1190.449, 1166.35, 1162.869, 1208.166, 1109.499, 1088.501, 1083.966, 1096.325, 1109.443, 1108.734, 1104.623, 1084.578, 1091.979, 1071.847, 1082.927, 1099.318, 1061.175, 1059.562, 1084.289, 1048.891, 1035.133, 1041.183, 1061.168, 1051.969, 1033.022, 1030.325, 1041.816, 1022.71, 1028.673, 1015.445, 1030.535, 1018.892, 1017.807, 1023.897, 1030.881, 1004.733, 1001.077, 1003.319, 1005.271, 1016.739, 989.706, 1002.283, 998.242, 992.087, 1005.555, 999.283, 1001.827, 998.694, 994.035, 1009.037, 990.918, 986.702, 992.867, 981.002, 1010.578, 992.74, 997.133, 990.627, 990.448, 986.237, 972.03, 976.232, 984.747, 985.125, 981.32, 967.777, 974.974, 976.919, 980.586, 978.925, 969.572, 969.396, 992.394, 969.596, 968.704, 972.762, 968.848, 956.61, 972.829, 966.508, 972.504, 970.728]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:47 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:48 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:65 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:126 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:126 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:17 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.07459953153672465 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:3.381674346430075e-05 batch_size:49 epochs:100	100	1000	True	2225.55249		712460	22	-1	200.3323905467987	{'train_loss': [21397.928, 18399.857, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18412.705, 18407.867, 18140.605, 7564.454, 3634.278, 2592.997, 2534.735, 2529.297, 2468.647, 2574.224, 2550.422, 2490.075, 2488.075, 2716.287, 2588.417, 2509.627, 2540.294, 2497.476, 2506.494, 2830.354, 2544.938, 2569.738, 2553.551, 2485.157, 2479.503, 2620.372, 2539.007, 2606.292, 2746.327, 2476.274, 2499.001, 2550.084, 2611.526, 2593.79, 2570.873, 2516.52, 2620.846, 2568.634, 2651.409, 2511.322, 2487.225, 2647.083, 2639.537, 2546.776, 2667.676, 2581.217, 2473.985, 2522.258, 2562.118, 2558.367, 2661.138, 2498.907, 2717.446, 2555.387, 3387.902, 2562.728, 2531.741, 2589.539, 2567.273, 2599.028, 2496.816, 2483.806, 2584.405, 2599.123, 2499.756, 2611.675, 2598.485, 2555.785, 2584.413, 2489.085, 2855.76, 2504.695, 2461.133, 2536.072, 2681.878, 2575.178, 2890.379, 2712.69, 2492.404, 2554.847, 2511.581, 2602.608, 2878.277, 2490.187, 2487.028, 2646.918], 'val_loss': [28826.047, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 6520.312, 3382.192, 2551.64, 2150.367, 2152.178, 2099.398, 2121.663, 2140.788, 2127.895, 2198.996, 2110.569, 2473.949, 2110.33, 2107.119, 2352.448, 2151.915, 2139.775, 2200.978, 2287.855, 2282.568, 2120.816, 2095.739, 2127.907, 2636.161, 2152.914, 2829.59, 2096.175, 2202.239, 2098.693, 2166.88, 2107.36, 2105.863, 2092.865, 2173.844, 2169.918, 2143.342, 2144.64, 2281.158, 2115.347, 2274.381, 2085.854, 2162.733, 2255.82, 2120.816, 2125.982, 2306.621, 2131.557, 2421.948, 2096.256, 2199.354, 2195.81, 2250.842, 2566.723, 2108.598, 2112.623, 2303.208, 2197.203, 2375.337, 2125.951, 2096.696, 2307.559, 2103.391, 2096.026, 2114.743, 2158.82, 2101.644, 2189.535, 2131.673, 2189.799, 2098.151, 2114.508, 2103.928, 2122.959, 2170.79, 2371.918, 2128.697, 2095.121, 2098.264, 2193.451, 2499.697, 2200.914, 2178.056, 2103.966, 2253.474]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:37 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:35 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9530163233982891 weight_decay:3.381674346430075e-05 batch_size:49 epochs:100	100	1000	True	1047.63708		686634	21	-1	192.8242483139038	{'train_loss': [3644.261, 2677.777, 2533.712, 2375.571, 2245.959, 2179.048, 2112.124, 2002.212, 1780.484, 1596.743, 1520.094, 1460.785, 1427.853, 1398.745, 1377.077, 1366.772, 1357.117, 1332.965, 1332.975, 1328.426, 1316.116, 1316.996, 1287.079, 1287.993, 1273.66, 1268.177, 1263.065, 1238.817, 1242.89, 1245.753, 1269.0, 1256.068, 1249.568, 1262.069, 1232.509, 1229.81, 1212.813, 1226.879, 1206.857, 1206.838, 1195.278, 1184.402, 1182.285, 1207.657, 1200.918, 1210.559, 1204.285, 1197.347, 1197.421, 1175.752, 1176.254, 1169.866, 1165.268, 1171.115, 1167.489, 1165.784, 1166.76, 1156.735, 1152.535, 1157.438, 1147.663, 1142.308, 1151.515, 1153.549, 1149.572, 1157.258, 1151.033, 1151.645, 1140.595, 1154.375, 1154.739, 1152.208, 1150.891, 1153.202, 1170.171, 1144.626, 1154.845, 1157.081, 1141.555, 1144.4, 1151.66, 1142.295, 1147.282, 1140.544, 1136.633, 1134.655, 1139.124, 1131.478, 1127.618, 1138.454, 1136.077, 1132.508, 1133.979, 1132.278, 1128.352, 1123.489, 1130.974, 1130.806, 1138.021, 1127.144], 'val_loss': [2792.439, 2328.323, 2192.602, 2051.07, 1977.427, 1873.682, 1856.224, 1674.111, 1490.211, 1359.303, 1291.696, 1278.603, 1263.31, 1202.805, 1194.734, 1189.683, 1183.361, 1144.413, 1129.467, 1119.75, 1157.96, 1130.822, 1104.864, 1086.788, 1073.124, 1077.854, 1050.94, 1035.405, 1053.539, 1052.533, 1052.914, 1046.108, 1075.017, 1058.735, 1028.77, 1033.887, 1043.56, 1027.81, 1044.587, 1016.363, 1000.752, 1028.3, 1040.394, 1006.079, 1019.828, 1071.036, 1041.664, 1061.897, 1058.089, 1009.94, 1023.965, 996.6, 1005.3, 989.212, 998.113, 995.897, 982.351, 995.179, 976.0, 978.444, 976.943, 985.342, 974.451, 970.309, 976.32, 1019.048, 991.449, 978.696, 983.697, 989.625, 996.735, 989.931, 992.421, 979.938, 1015.037, 986.555, 1001.731, 1028.775, 980.563, 1001.408, 1006.968, 998.543, 1001.894, 1020.602, 1028.043, 987.162, 1010.001, 969.57, 982.621, 997.271, 999.256, 975.625, 977.971, 983.155, 971.076, 968.369, 984.854, 1025.442, 1003.571, 1011.623]}	100	100	True
