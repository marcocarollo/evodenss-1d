id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:67 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:6.030364562254975e-05 batch_size:49 epochs:100	100	1000	True	1015.48309		457804	21	-1	184.08352208137512	{'train_loss': [3488.375, 2525.055, 2317.648, 2197.504, 2129.379, 2103.291, 2081.399, 2059.896, 2024.845, 1990.855, 1825.517, 1662.219, 1588.261, 1522.885, 1476.099, 1439.141, 1422.425, 1405.202, 1380.328, 1375.332, 1367.56, 1367.578, 1360.01, 1340.187, 1322.732, 1326.071, 1319.845, 1316.345, 1308.788, 1295.46, 1286.701, 1280.01, 1295.532, 1292.778, 1278.602, 1287.18, 1291.076, 1270.508, 1274.567, 1262.562, 1259.423, 1263.41, 1261.837, 1261.62, 1252.942, 1246.062, 1245.822, 1231.605, 1241.296, 1238.349, 1229.366, 1224.278, 1240.643, 1213.301, 1213.829, 1216.625, 1208.195, 1211.854, 1215.689, 1215.049, 1211.852, 1226.863, 1224.418, 1213.112, 1212.922, 1201.779, 1201.085, 1202.357, 1211.537, 1194.517, 1187.571, 1190.154, 1187.811, 1192.222, 1191.353, 1189.099, 1190.887, 1182.969, 1182.737, 1190.952, 1188.444, 1187.402, 1184.92, 1175.498, 1181.771, 1178.582, 1170.677, 1171.519, 1170.428, 1169.305, 1165.661, 1174.974, 1171.868, 1167.113, 1159.854, 1165.733, 1152.396, 1151.047, 1155.787, 1165.319], 'val_loss': [3653.29, 2464.186, 2073.56, 1930.597, 1870.73, 1860.489, 1848.81, 1841.093, 1789.296, 1747.838, 1519.903, 1427.128, 1395.287, 1327.986, 1291.469, 1218.572, 1201.17, 1194.115, 1173.478, 1172.264, 1157.795, 1156.926, 1154.248, 1137.576, 1137.492, 1138.585, 1122.468, 1117.763, 1109.681, 1117.491, 1098.87, 1104.612, 1092.024, 1090.169, 1085.969, 1136.329, 1138.712, 1122.214, 1098.137, 1098.294, 1097.218, 1146.215, 1082.911, 1091.481, 1083.754, 1077.253, 1070.689, 1067.684, 1119.552, 1090.664, 1057.147, 1138.244, 1098.81, 1064.993, 1045.781, 1061.052, 1056.126, 1077.84, 1101.478, 1049.696, 1070.387, 1121.859, 1071.403, 1074.046, 1104.504, 1059.149, 1053.118, 1075.802, 1041.208, 1027.107, 1014.703, 1028.554, 1046.014, 1020.495, 1076.876, 1038.594, 1036.656, 1059.754, 1095.076, 1042.037, 1035.66, 1048.538, 1028.486, 1045.469, 1017.89, 1010.129, 1015.321, 1052.324, 1036.155, 1006.9, 1020.301, 1026.114, 1011.177, 1007.77, 1014.005, 1017.712, 1024.908, 1003.039, 1005.682, 1013.295]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:34 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:68 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:15 layer:conv1d out_channels:67 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:6.030364562254975e-05 batch_size:56 epochs:100	100	1000	True	2131.55469		2971439	19	-1	177.10484147071838	{'train_loss': [20417.992, 20627.074, 20638.674, 20638.674, 20638.674, 20638.674, 20639.082, 20638.787, 20639.234, 20612.658, 20298.057, 19072.221, 19451.275, 19559.74, 19624.373, 19572.145, 19476.004, 19059.148, 18081.447, 16343.007, 15144.303, 14109.799, 13385.352, 12335.293, 11174.072, 10061.813, 8380.075, 6823.907, 5610.82, 4343.961, 3655.211, 3179.526, 2938.772, 2818.86, 2774.158, 2759.6, 2756.319, 2751.477, 2755.146, 2741.297, 2741.096, 2732.661, 2733.366, 2727.006, 2727.564, 2718.467, 2715.408, 2715.376, 2717.316, 2716.421, 2714.904, 2712.719, 2714.669, 2711.291, 2711.381, 2715.477, 2709.643, 2711.29, 2712.759, 2707.052, 2711.909, 2718.917, 2714.244, 2710.369, 2712.345, 2716.813, 2712.075, 2710.75, 2714.397, 2717.061, 2712.491, 2714.045, 2714.687, 2710.677, 2710.378, 2712.791, 2710.31, 2709.244, 2713.537, 2709.417, 2711.306, 2711.991, 2717.8, 2713.623, 2710.778, 2712.96, 2712.403, 2707.985, 2713.39, 2717.04, 2710.352, 2708.71, 2717.628, 2708.014, 2712.333, 2711.33, 2710.976, 2709.607, 2711.876, 2710.553], 'val_loss': [15592.639, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15639.373, 15611.684, 14718.146, 14746.999, 14868.17, 14915.729, 14714.376, 14687.441, 14695.99, 14206.32, 12975.657, 11703.766, 11297.506, 10037.879, 9544.418, 8659.226, 7430.597, 6713.652, 4939.023, 4203.375, 2998.903, 2609.63, 2345.312, 2216.06, 2138.293, 2130.035, 2126.637, 2134.874, 2154.731, 2133.174, 2130.877, 2140.985, 2139.146, 2106.994, 2126.717, 2124.715, 2112.655, 2113.789, 2107.242, 2108.099, 2113.667, 2113.586, 2115.421, 2111.535, 2107.45, 2096.41, 2113.728, 2094.146, 2107.559, 2109.288, 2109.504, 2099.967, 2107.028, 2106.129, 2103.624, 2118.547, 2101.879, 2104.987, 2107.311, 2124.616, 2106.15, 2103.882, 2110.623, 2109.047, 2107.698, 2107.392, 2111.838, 2101.732, 2101.07, 2107.201, 2103.557, 2108.43, 2104.786, 2116.377, 2097.207, 2114.258, 2104.835, 2108.103, 2117.506, 2121.731, 2117.288, 2096.279, 2102.914, 2123.926, 2096.674, 2111.052, 2106.943, 2102.576, 2102.913, 2106.962, 2105.062, 2103.239]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:84 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:41 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:108 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:41 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:67 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:6.030364562254975e-05 batch_size:82 epochs:100	100	1000	True	1733.92993		490805	21	-1	159.20570373535156	{'train_loss': [6454.952, 4312.732, 4073.868, 3821.814, 3737.44, 3572.063, 3466.086, 3415.434, 3384.631, 3347.044, 3324.293, 3328.37, 3298.552, 3261.738, 3250.25, 3246.856, 3231.516, 3220.682, 3219.554, 3204.49, 3207.795, 3182.216, 3197.373, 3169.961, 3147.961, 3142.802, 3120.385, 3107.853, 3107.341, 3106.646, 3085.364, 3079.636, 3110.333, 3134.838, 3091.209, 3114.351, 3093.546, 3040.213, 3030.106, 3012.598, 3004.065, 3051.789, 3081.74, 3032.896, 3033.051, 3029.837, 3079.829, 3057.643, 3072.889, 3031.051, 2960.434, 2981.503, 2989.113, 2959.1, 2949.907, 2942.045, 2946.36, 2994.174, 2972.277, 2929.077, 2939.799, 2918.205, 2889.035, 2883.326, 2914.476, 2885.905, 2825.406, 2889.964, 2829.027, 2816.313, 2807.63, 2858.465, 2842.474, 2766.406, 2770.193, 2716.583, 2683.464, 2634.909, 2605.071, 2587.888, 2552.201, 2547.981, 2522.061, 2527.744, 2557.656, 2495.335, 2498.215, 2460.0, 2409.555, 2385.95, 2380.912, 2367.965, 2357.188, 2354.484, 2333.064, 2300.996, 2292.223, 2286.94, 2304.983, 2280.582], 'val_loss': [7210.498, 4877.354, 4110.857, 3454.371, 3124.296, 3033.292, 2975.448, 2897.562, 2826.639, 2780.352, 2755.777, 2720.486, 2787.839, 2676.055, 2671.044, 2700.391, 2698.624, 2648.476, 2677.254, 2671.983, 2620.43, 2641.313, 2628.468, 2566.61, 2606.1, 2576.171, 2616.715, 2541.807, 2532.695, 2531.372, 2499.74, 2459.111, 2518.33, 2520.301, 2461.275, 2555.595, 2483.278, 2432.682, 2382.589, 2440.887, 2367.158, 2523.508, 2461.464, 2382.033, 2421.967, 2441.738, 2610.986, 2495.359, 2460.604, 2444.972, 2393.077, 2355.127, 2369.728, 2404.225, 2317.64, 2375.377, 2303.995, 2457.599, 2337.648, 2335.757, 2308.564, 2252.408, 2261.75, 2265.195, 2283.246, 2233.479, 2246.703, 2286.304, 2199.781, 2166.227, 2197.271, 2194.654, 2204.529, 2132.278, 2203.982, 2095.325, 2106.314, 2105.031, 2027.562, 1958.27, 2003.363, 2034.295, 1985.123, 2000.05, 1968.909, 1942.35, 1940.263, 1893.499, 1885.9, 1876.392, 1897.731, 1844.8, 1871.365, 1816.101, 1797.163, 1781.879, 1793.529, 1765.666, 1760.852, 1768.198]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:118 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:91 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:43 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:63 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:117 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:17 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:18 layer:conv1d out_channels:67 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:19 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:20 layer:fc act:selu out_features:200 bias:True input:21 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:9.847092685367441e-05 batch_size:49 epochs:100	100	1000	True	1083.64673		530384	23	-1	182.00306129455566	{'train_loss': [4005.981, 2527.247, 2340.371, 2232.491, 2124.223, 2073.812, 2026.209, 2015.237, 2004.886, 2004.09, 1985.118, 1958.369, 1979.409, 1955.634, 1954.733, 1947.968, 1950.835, 1935.498, 1933.671, 1933.694, 1929.526, 1914.893, 1921.401, 1914.641, 1933.299, 1915.34, 1929.263, 1902.49, 1891.379, 1894.917, 1902.675, 1871.776, 1882.133, 1870.134, 1885.891, 1832.257, 1809.859, 1758.573, 1735.038, 1631.688, 1613.803, 1556.339, 1526.695, 1506.603, 1511.819, 1492.398, 1486.478, 1482.73, 1474.648, 1447.593, 1437.455, 1447.078, 1415.381, 1424.678, 1415.742, 1427.442, 1412.417, 1384.736, 1395.411, 1378.808, 1378.884, 1370.269, 1372.268, 1371.097, 1360.812, 1364.655, 1354.745, 1348.599, 1338.953, 1337.372, 1349.44, 1335.647, 1327.419, 1332.333, 1322.184, 1326.221, 1324.703, 1309.304, 1320.913, 1310.897, 1306.384, 1292.508, 1299.304, 1303.942, 1286.901, 1295.12, 1283.269, 1291.164, 1292.097, 1283.698, 1272.209, 1268.252, 1273.839, 1275.671, 1277.885, 1266.534, 1263.009, 1265.216, 1269.137, 1260.917], 'val_loss': [3548.761, 2635.453, 2110.859, 2051.45, 1876.232, 1834.675, 1827.158, 1781.025, 1819.871, 1792.746, 1777.429, 1758.719, 1768.284, 1735.095, 1758.741, 1774.741, 1749.533, 1709.817, 1721.303, 1731.959, 1738.638, 1718.505, 1721.519, 1700.186, 1737.777, 1718.536, 1714.976, 1724.371, 1662.688, 1698.637, 1705.42, 1655.663, 1729.307, 1608.345, 1634.84, 1565.849, 1531.838, 1536.539, 1388.402, 1417.464, 1389.158, 1330.824, 1294.229, 1298.493, 1318.329, 1264.941, 1276.607, 1373.386, 1293.94, 1277.387, 1291.012, 1259.169, 1244.287, 1318.045, 1224.557, 1226.66, 1207.309, 1205.365, 1222.616, 1178.79, 1202.265, 1184.967, 1173.94, 1164.145, 1160.214, 1187.645, 1177.245, 1154.226, 1155.237, 1149.399, 1151.982, 1137.597, 1180.291, 1168.432, 1127.547, 1129.996, 1113.855, 1117.484, 1154.699, 1123.017, 1117.471, 1093.994, 1146.098, 1112.152, 1144.822, 1136.069, 1095.206, 1111.404, 1098.221, 1123.45, 1072.055, 1081.973, 1079.064, 1125.37, 1117.326, 1082.287, 1074.167, 1081.38, 1090.24, 1055.586]}	100	100	True
