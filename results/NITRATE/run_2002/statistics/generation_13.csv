id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:6.030364562254975e-05 batch_size:49 epochs:100	100	2000	True	993.93262		512391	21	-1	193.57903361320496	{'train_loss': [3651.659, 2567.284, 2371.089, 2250.883, 2181.275, 2131.602, 2115.262, 2083.298, 2077.541, 2068.545, 2050.411, 2039.297, 2017.914, 1994.479, 1936.906, 1868.539, 1766.112, 1650.62, 1554.211, 1466.326, 1444.695, 1416.149, 1374.553, 1357.827, 1335.919, 1332.794, 1299.961, 1295.793, 1279.14, 1278.977, 1273.526, 1261.641, 1245.451, 1255.383, 1245.887, 1244.232, 1234.148, 1230.189, 1244.472, 1231.49, 1215.672, 1204.076, 1205.708, 1198.188, 1196.262, 1196.107, 1194.345, 1196.453, 1188.68, 1187.023, 1183.535, 1180.208, 1182.068, 1169.274, 1172.813, 1178.156, 1176.727, 1167.402, 1166.148, 1163.907, 1158.223, 1164.739, 1156.091, 1158.765, 1154.7, 1162.385, 1148.137, 1152.468, 1150.159, 1146.112, 1151.84, 1142.821, 1142.534, 1144.002, 1142.234, 1136.712, 1134.9, 1136.833, 1131.013, 1131.671, 1133.952, 1135.775, 1139.281, 1132.141, 1127.459, 1126.228, 1125.035, 1127.887, 1130.616, 1127.829, 1129.398, 1123.524, 1123.911, 1119.109, 1114.924, 1117.397, 1114.368, 1111.353, 1119.938, 1114.714], 'val_loss': [2634.279, 2188.769, 2054.292, 1930.656, 1932.01, 1904.088, 1903.887, 1877.925, 1887.28, 1870.771, 1855.454, 1787.224, 1802.446, 1790.553, 1674.219, 1593.734, 1479.718, 1381.926, 1288.533, 1221.351, 1164.153, 1164.289, 1121.876, 1108.065, 1090.656, 1075.62, 1067.109, 1052.537, 1057.844, 1057.034, 1022.251, 1039.984, 1022.654, 1037.348, 1005.199, 1016.269, 1021.815, 1005.84, 1026.967, 1007.026, 996.242, 992.954, 984.988, 994.204, 999.791, 981.044, 982.866, 991.0, 995.016, 982.181, 982.479, 990.121, 978.36, 970.599, 971.652, 984.255, 973.777, 976.5, 964.43, 970.093, 978.424, 968.169, 969.729, 962.191, 950.314, 969.673, 962.113, 982.86, 958.324, 963.094, 949.372, 969.92, 949.091, 949.011, 958.894, 965.963, 947.565, 947.75, 943.317, 957.861, 955.481, 953.161, 953.448, 940.174, 944.307, 944.082, 940.861, 932.998, 962.862, 959.44, 944.073, 948.956, 947.655, 933.474, 944.752, 947.015, 932.803, 940.614, 940.202, 939.973]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:44 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:43 kernel_size:4 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:17 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:7.42547637676291e-05 batch_size:49 epochs:100	100	1000	True	1010.33466		534166	22	-1	199.3681182861328	{'train_loss': [3299.36, 2536.882, 2339.846, 2110.867, 1869.885, 1633.166, 1568.1, 1535.235, 1492.096, 1462.569, 1448.663, 1438.394, 1414.203, 1393.438, 1385.011, 1369.881, 1353.526, 1340.095, 1335.903, 1321.838, 1318.828, 1309.256, 1306.25, 1291.324, 1276.726, 1280.005, 1269.978, 1271.723, 1263.594, 1265.449, 1255.533, 1260.739, 1263.022, 1258.642, 1238.877, 1236.76, 1227.148, 1242.319, 1237.864, 1224.88, 1219.238, 1221.371, 1213.531, 1210.569, 1214.947, 1208.486, 1213.626, 1209.0, 1203.082, 1198.599, 1198.054, 1186.316, 1196.911, 1199.689, 1186.514, 1190.268, 1206.696, 1198.312, 1187.265, 1190.603, 1190.458, 1192.97, 1182.668, 1184.354, 1176.189, 1173.648, 1177.716, 1170.693, 1167.276, 1163.851, 1171.13, 1162.886, 1168.564, 1162.912, 1166.619, 1166.165, 1160.392, 1156.363, 1151.982, 1156.616, 1154.444, 1155.132, 1151.896, 1157.312, 1153.964, 1160.172, 1146.548, 1156.166, 1160.226, 1151.176, 1148.243, 1147.318, 1139.565, 1138.154, 1141.569, 1136.311, 1141.009, 1133.96, 1140.324, 1138.795], 'val_loss': [3019.246, 2209.049, 2011.39, 1632.826, 1823.435, 1407.697, 1403.772, 1322.298, 1337.417, 1283.578, 1280.059, 1218.495, 1256.298, 1211.731, 1183.583, 1166.438, 1172.699, 1163.428, 1142.971, 1116.86, 1120.225, 1107.491, 1103.864, 1082.879, 1081.042, 1092.389, 1100.363, 1073.943, 1064.922, 1067.63, 1056.627, 1086.855, 1045.659, 1057.531, 1053.371, 1030.491, 1055.413, 1064.625, 1037.451, 1054.234, 1031.986, 1023.664, 1031.904, 1030.564, 1029.509, 1016.638, 1008.442, 1041.713, 1014.336, 1028.641, 1017.741, 988.704, 1037.336, 1013.713, 1005.309, 1004.091, 1012.512, 999.085, 1015.361, 1009.152, 999.61, 1022.757, 1014.089, 999.987, 1011.544, 981.958, 988.071, 985.569, 1001.744, 984.992, 982.194, 988.674, 975.949, 975.687, 976.499, 981.485, 980.46, 975.77, 973.306, 976.441, 966.657, 969.658, 970.968, 967.125, 968.437, 974.328, 970.756, 968.736, 988.472, 964.638, 962.174, 958.58, 955.549, 942.089, 971.951, 963.54, 953.539, 965.24, 958.452, 953.648]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:6.030364562254975e-05 batch_size:49 epochs:100	100	2000	True	981.44861		512391	21	-1	188.9737582206726	{'train_loss': [3359.946, 2505.287, 2371.866, 2266.402, 2168.048, 2148.606, 2118.088, 2090.273, 2054.849, 2063.749, 2003.666, 2013.716, 1990.69, 1949.335, 1948.973, 1938.218, 1829.433, 1651.446, 1523.739, 1468.308, 1419.562, 1388.556, 1362.742, 1365.665, 1350.962, 1346.956, 1314.034, 1298.026, 1290.199, 1290.171, 1284.368, 1268.141, 1274.32, 1260.017, 1245.464, 1250.911, 1240.285, 1244.906, 1248.929, 1247.218, 1233.383, 1224.238, 1225.608, 1220.498, 1221.469, 1206.073, 1206.716, 1205.78, 1193.847, 1193.034, 1198.957, 1205.8, 1182.29, 1179.879, 1180.831, 1189.69, 1187.009, 1206.107, 1200.006, 1191.399, 1177.14, 1179.806, 1184.718, 1185.104, 1182.425, 1170.041, 1164.749, 1167.829, 1166.26, 1165.247, 1154.501, 1165.794, 1174.684, 1173.35, 1175.968, 1157.998, 1149.676, 1148.946, 1150.111, 1147.624, 1148.476, 1151.641, 1151.279, 1148.341, 1156.916, 1143.643, 1138.391, 1143.078, 1142.997, 1141.039, 1145.551, 1133.563, 1132.014, 1134.207, 1138.966, 1144.303, 1129.023, 1122.659, 1136.255, 1137.018], 'val_loss': [2692.565, 2248.877, 2087.304, 1949.685, 1937.003, 1928.351, 1860.171, 1841.602, 1873.462, 1839.931, 1772.561, 1762.803, 1749.355, 1746.248, 1717.369, 1675.714, 1505.741, 1362.782, 1287.664, 1251.262, 1218.965, 1188.143, 1174.507, 1144.003, 1143.489, 1138.095, 1101.795, 1119.183, 1094.289, 1101.589, 1078.724, 1101.741, 1073.693, 1065.862, 1067.106, 1103.309, 1054.49, 1060.075, 1048.387, 1049.164, 1035.095, 1030.169, 1044.161, 1010.206, 1018.183, 1020.166, 1036.817, 1023.189, 1019.256, 1008.753, 1022.055, 1005.967, 991.747, 994.71, 1012.228, 1011.475, 998.276, 1011.18, 1015.993, 1010.423, 986.495, 980.904, 1002.295, 989.165, 998.453, 977.572, 975.942, 987.103, 983.959, 981.763, 974.526, 999.302, 983.329, 972.004, 1012.164, 973.924, 972.594, 970.357, 973.685, 977.843, 964.009, 972.169, 981.006, 982.678, 983.071, 960.077, 964.305, 970.607, 959.883, 998.01, 961.696, 955.822, 967.49, 961.528, 971.324, 970.854, 945.15, 955.215, 973.669, 936.906]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:57 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:55 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:120 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:17 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:6.030364562254975e-05 batch_size:84 epochs:100	100	1000	True	1572.27332		572259	22	-1	154.50246906280518	{'train_loss': [6906.598, 4571.292, 4332.735, 4043.005, 3907.818, 3808.833, 3669.554, 3605.856, 3533.14, 3483.01, 3445.167, 3405.632, 3382.652, 3397.662, 3392.589, 3343.547, 3337.721, 3341.064, 3294.099, 3287.154, 3242.017, 3131.162, 3001.061, 2906.67, 2810.592, 2725.555, 2632.891, 2570.21, 2523.188, 2451.328, 2436.397, 2406.602, 2392.313, 2389.66, 2358.216, 2345.736, 2310.942, 2292.218, 2303.922, 2290.508, 2275.94, 2278.066, 2262.298, 2241.563, 2244.806, 2213.497, 2226.871, 2236.631, 2223.493, 2181.828, 2213.339, 2186.559, 2178.563, 2177.633, 2152.906, 2181.732, 2171.619, 2156.379, 2148.354, 2143.899, 2138.176, 2113.099, 2122.438, 2118.276, 2117.374, 2142.835, 2105.362, 2096.626, 2097.492, 2095.166, 2083.603, 2089.019, 2090.383, 2089.286, 2086.697, 2090.791, 2072.264, 2091.667, 2077.531, 2077.656, 2068.853, 2039.688, 2042.746, 2065.125, 2081.284, 2057.653, 2074.721, 2057.023, 2034.138, 2037.5, 2047.843, 2051.975, 2038.215, 2034.131, 2027.068, 2039.385, 2019.659, 2031.12, 2028.793, 2023.291], 'val_loss': [4919.033, 3782.257, 3308.685, 3166.91, 3171.991, 3064.067, 2879.443, 2891.72, 2818.679, 2744.375, 2780.216, 2690.203, 2711.268, 2852.306, 2740.875, 2696.308, 2642.278, 2617.862, 2600.439, 2594.461, 2559.93, 2314.464, 2165.61, 2263.846, 2071.473, 2112.572, 2001.231, 1943.325, 2060.135, 1844.104, 1907.249, 1846.11, 1851.007, 1824.057, 1795.707, 1886.249, 1739.423, 1752.111, 1751.243, 1832.484, 1745.341, 1743.806, 1741.791, 1734.028, 1725.599, 1688.918, 1696.339, 1809.004, 1668.647, 1688.151, 1678.196, 1595.224, 1665.192, 1686.907, 1606.989, 1637.732, 1622.322, 1618.687, 1588.481, 1669.873, 1599.703, 1588.247, 1580.241, 1579.727, 1550.927, 1599.272, 1578.064, 1582.323, 1607.145, 1552.103, 1521.977, 1584.94, 1564.609, 1621.999, 1550.759, 1527.963, 1539.301, 1585.722, 1531.682, 1490.414, 1514.188, 1476.77, 1514.758, 1552.176, 1499.102, 1518.402, 1593.748, 1480.874, 1516.51, 1498.128, 1526.136, 1528.852, 1469.467, 1489.879, 1513.314, 1501.316, 1496.458, 1531.884, 1461.309, 1529.294]}	100	100	True
