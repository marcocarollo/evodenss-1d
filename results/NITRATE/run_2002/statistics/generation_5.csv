id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:78 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	1551.40576		449410	15	-1	144.8338212966919	{'train_loss': [6183.682, 4238.577, 4046.855, 3829.751, 3693.24, 3554.032, 3475.022, 3425.856, 3398.733, 3373.986, 3335.514, 3316.261, 3310.397, 3292.257, 3274.534, 3268.864, 3263.513, 3252.544, 3231.161, 3208.487, 3188.232, 3192.959, 3183.749, 3181.494, 3157.382, 3150.39, 3050.897, 2890.181, 2743.523, 2655.171, 2590.743, 2491.904, 2495.784, 2394.74, 2372.757, 2334.766, 2283.974, 2268.248, 2223.27, 2210.113, 2215.61, 2180.915, 2182.837, 2178.292, 2166.097, 2156.75, 2148.396, 2133.448, 2115.747, 2120.084, 2099.126, 2116.125, 2101.781, 2100.389, 2088.292, 2065.897, 2070.969, 2056.561, 2039.882, 2048.089, 2060.083, 2059.548, 2041.785, 2041.273, 2035.802, 2022.243, 2016.655, 2022.31, 2018.728, 2040.792, 2008.291, 2006.102, 2009.039, 2009.203, 1995.096, 1996.552, 2014.464, 2038.802, 2042.62, 1992.899, 2003.539, 1986.671, 1967.401, 1963.136, 1980.372, 1993.743, 1985.21, 1994.733, 1959.358, 1964.194, 1977.81, 1991.11, 1959.556, 1953.58, 1942.906, 1940.711, 1941.173, 1929.675, 1926.974, 1938.427], 'val_loss': [4769.027, 3625.685, 3363.159, 3184.345, 2929.725, 2805.648, 2766.378, 2734.675, 2755.801, 2738.965, 2670.24, 2682.03, 2674.942, 2665.105, 2625.099, 2641.025, 2711.589, 2665.726, 2664.176, 2636.441, 2658.228, 2638.638, 2658.166, 2553.991, 2553.748, 2534.444, 2357.127, 2244.641, 2123.039, 2253.006, 2056.694, 1962.975, 2241.12, 1917.393, 2075.359, 1843.136, 1922.711, 1947.789, 1829.904, 1824.687, 1748.336, 1762.27, 1818.4, 1818.039, 1770.522, 1829.939, 1707.882, 1738.41, 1771.104, 1701.975, 1698.717, 1694.24, 1690.656, 1679.488, 1684.657, 1655.846, 1664.224, 1596.193, 1600.565, 1658.209, 1683.37, 1662.567, 1593.01, 1648.704, 1591.23, 1577.917, 1583.866, 1626.299, 1574.006, 1582.385, 1563.153, 1628.737, 1609.714, 1597.533, 1589.38, 1598.93, 1612.829, 1615.098, 1569.066, 1569.036, 1533.805, 1545.904, 1533.308, 1551.705, 1580.369, 1593.514, 1564.121, 1520.209, 1549.93, 1571.534, 1529.015, 1553.04, 1571.136, 1513.182, 1533.029, 1555.721, 1508.147, 1529.96, 1559.117, 1538.238]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:78 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:63 kernel_size:6 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.008770895123166991 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	1690.46069		473323	16	-1	147.99531054496765	{'train_loss': [8246.435, 4609.181, 4123.286, 3827.692, 3736.131, 3638.857, 3534.667, 3410.796, 3397.328, 3377.349, 3322.943, 3290.084, 3245.33, 3200.508, 3190.094, 3166.928, 3157.501, 3166.87, 3173.373, 3130.202, 3130.052, 3111.636, 3109.468, 3112.614, 3074.078, 3087.842, 3078.01, 3085.072, 3053.653, 3043.67, 3111.628, 3084.931, 3073.363, 3043.182, 3065.609, 3058.879, 3085.003, 3058.355, 3053.863, 3075.814, 3123.765, 3079.299, 3026.974, 3044.836, 2974.844, 2979.61, 3064.333, 3053.379, 3001.236, 2958.808, 2929.885, 2933.113, 2980.708, 2980.2, 2912.457, 2937.007, 2920.658, 2922.151, 2857.926, 2857.685, 2929.784, 2943.469, 2928.544, 2853.194, 2896.421, 2864.915, 2920.146, 2859.034, 2890.147, 2841.686, 2840.665, 2812.017, 2839.065, 2819.549, 2834.818, 2791.662, 2783.712, 2810.229, 2841.336, 2870.1, 2806.625, 2780.564, 2810.166, 2695.131, 2615.897, 2527.874, 2547.897, 2464.236, 2429.383, 2346.979, 2330.217, 2290.658, 2260.675, 2235.076, 2210.549, 2197.423, 2219.817, 2205.326, 2158.85, 2155.031], 'val_loss': [6478.354, 3708.028, 3229.749, 3269.913, 3038.168, 2991.031, 2831.109, 2897.324, 2780.939, 2753.507, 2731.639, 2629.801, 2619.518, 2608.22, 2604.562, 2683.646, 2590.964, 2641.777, 2557.965, 2527.729, 2569.808, 2655.499, 2567.341, 2589.415, 2497.44, 2662.099, 2535.966, 2596.619, 2601.085, 2477.222, 2600.555, 2535.9, 2531.099, 2496.999, 2551.928, 2609.662, 2563.684, 2502.309, 2613.312, 2618.888, 2546.495, 2481.876, 2472.53, 2477.683, 2414.087, 2418.05, 2557.99, 2507.841, 2368.116, 2349.843, 2315.99, 2326.739, 2334.822, 2457.409, 2264.208, 2377.116, 2414.158, 2282.889, 2245.364, 2246.842, 2343.73, 2570.569, 2278.872, 2336.38, 2360.466, 2313.627, 2435.625, 2323.116, 2477.239, 2218.201, 2292.104, 2233.987, 2274.255, 2154.294, 2323.607, 2296.28, 2163.498, 2307.08, 2384.025, 2203.938, 2162.622, 2333.823, 2139.716, 2062.163, 2156.012, 2016.965, 2118.166, 1996.38, 2022.323, 1966.609, 1884.208, 1791.521, 1763.332, 1704.115, 1716.442, 1766.037, 1768.67, 1713.722, 1729.162, 1673.434]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:78 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	2000	True	1544.98486		449410	15	-1	144.9527714252472	{'train_loss': [6023.218, 4292.218, 4141.119, 4078.811, 3898.792, 3727.914, 3594.586, 3494.852, 3450.424, 3400.07, 3355.918, 3325.213, 3297.307, 3268.396, 3251.622, 3240.175, 3222.634, 3206.03, 3205.655, 3174.227, 3166.937, 3161.984, 3129.862, 3106.29, 3099.012, 3075.617, 3024.097, 2950.461, 2834.347, 2683.324, 2565.783, 2460.418, 2424.482, 2357.745, 2325.658, 2293.906, 2274.74, 2253.421, 2221.131, 2204.687, 2188.24, 2163.844, 2153.477, 2161.955, 2139.393, 2125.124, 2118.239, 2084.681, 2112.722, 2102.729, 2115.957, 2077.5, 2094.884, 2064.648, 2059.52, 2049.521, 2046.72, 2076.305, 2058.513, 2043.212, 2035.009, 2023.72, 2012.511, 2009.31, 2013.576, 2024.639, 2019.417, 2017.941, 1982.822, 2004.472, 2002.518, 2002.169, 1992.632, 2002.553, 1984.808, 1993.026, 1987.685, 1990.876, 1988.82, 1958.281, 1975.81, 1978.314, 1979.679, 1984.933, 1992.708, 1967.825, 1955.057, 1967.168, 1949.278, 1972.784, 1950.76, 1959.576, 1957.74, 1935.497, 1950.24, 1944.025, 1936.666, 1938.832, 1972.654, 1956.181], 'val_loss': [4846.366, 3725.246, 3380.296, 3244.428, 3134.61, 3009.152, 2873.557, 2831.525, 2783.519, 2769.145, 2761.64, 2683.7, 2672.541, 2643.945, 2654.756, 2608.415, 2617.443, 2607.852, 2622.614, 2626.881, 2600.471, 2629.251, 2542.84, 2513.799, 2473.778, 2486.794, 2469.226, 2307.118, 2266.389, 2109.168, 2059.999, 1956.641, 1840.648, 1868.59, 1914.802, 1879.072, 1912.301, 1893.241, 1798.149, 1812.487, 1832.987, 1736.807, 1731.581, 1761.835, 1781.753, 1693.446, 1669.086, 1656.98, 1764.894, 1666.467, 1789.893, 1697.422, 1690.774, 1635.637, 1678.609, 1644.214, 1679.891, 1687.775, 1645.256, 1643.123, 1591.349, 1605.141, 1565.481, 1608.995, 1612.118, 1617.346, 1616.123, 1591.86, 1586.968, 1602.767, 1585.856, 1580.067, 1571.853, 1551.097, 1555.661, 1536.374, 1560.276, 1527.414, 1572.155, 1550.701, 1557.35, 1562.775, 1523.334, 1557.707, 1527.864, 1532.945, 1580.249, 1518.786, 1537.069, 1531.763, 1559.304, 1533.129, 1558.345, 1522.5, 1534.802, 1516.534, 1514.537, 1548.129, 1529.257, 1503.474]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	1532.56372		403446	16	-1	141.59875202178955	{'train_loss': [5907.53, 4250.927, 4027.599, 3803.044, 3639.768, 3517.39, 3421.891, 3376.347, 3327.404, 3305.841, 3280.823, 3255.874, 3233.642, 3233.505, 3172.655, 3108.752, 3068.843, 2977.491, 2964.551, 2909.982, 2844.722, 2866.764, 2742.042, 2671.673, 2579.213, 2578.59, 2509.418, 2513.4, 2452.047, 2394.599, 2362.715, 2322.853, 2299.937, 2283.984, 2280.039, 2283.228, 2264.395, 2243.517, 2203.235, 2200.848, 2195.798, 2178.061, 2185.65, 2151.815, 2122.386, 2128.653, 2119.183, 2114.172, 2110.975, 2139.01, 2116.421, 2111.084, 2119.271, 2128.276, 2078.413, 2089.301, 2058.444, 2047.113, 2053.417, 2046.454, 2044.692, 2048.012, 2040.746, 2017.574, 2013.773, 1994.81, 2016.416, 2006.233, 1990.604, 1992.441, 1992.005, 1995.771, 1990.691, 1985.403, 1979.777, 1970.936, 1985.669, 1968.803, 1957.246, 1967.15, 1955.464, 1965.396, 1950.441, 1942.995, 1958.03, 1952.929, 1942.981, 1950.181, 1948.067, 1931.545, 1935.516, 1946.265, 1943.291, 1926.993, 1929.294, 1925.476, 1928.272, 1938.984, 1930.359, 1920.196], 'val_loss': [5198.579, 3695.481, 3304.63, 3079.342, 2863.883, 2847.571, 2816.435, 2810.926, 2810.217, 2757.939, 2737.204, 2773.328, 2621.835, 2596.006, 2505.554, 2416.982, 2468.63, 2453.047, 2462.713, 2273.765, 2514.762, 2145.743, 2373.99, 2280.371, 2169.585, 2107.882, 2032.031, 2023.959, 1963.969, 2003.506, 1870.664, 1902.479, 1886.904, 1803.482, 1851.553, 1901.95, 1799.98, 1860.809, 1780.572, 1728.77, 1761.038, 1793.608, 1717.436, 1706.673, 1668.276, 1681.414, 1747.397, 1657.485, 1690.514, 1732.164, 1668.003, 1763.786, 1734.539, 1830.664, 1755.898, 1646.591, 1603.176, 1680.192, 1673.223, 1625.441, 1620.798, 1637.107, 1650.146, 1652.86, 1656.987, 1644.263, 1681.173, 1662.676, 1591.656, 1640.125, 1579.438, 1611.912, 1545.547, 1636.072, 1572.861, 1546.269, 1563.438, 1601.235, 1577.911, 1566.15, 1590.995, 1575.104, 1573.136, 1583.704, 1526.14, 1575.184, 1496.433, 1595.726, 1569.281, 1514.716, 1530.997, 1598.727, 1544.042, 1536.489, 1518.224, 1520.339, 1580.664, 1514.978, 1537.982, 1516.39]}	100	100	True
