id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.0009673374036965584 batch_size:49 epochs:100	100	1000	True	997.64172		748280	21	-1	175.6550703048706	{'train_loss': [5591.378, 3315.432, 2849.208, 2605.069, 2389.513, 2250.949, 2058.998, 1821.993, 1678.229, 1565.897, 1502.445, 1458.479, 1410.176, 1383.994, 1360.915, 1340.979, 1331.77, 1320.657, 1310.967, 1298.384, 1291.205, 1284.934, 1276.294, 1281.048, 1272.914, 1273.037, 1265.503, 1271.438, 1257.751, 1257.809, 1249.418, 1268.823, 1238.361, 1224.656, 1225.509, 1226.711, 1229.087, 1226.568, 1221.374, 1218.615, 1212.968, 1205.997, 1210.813, 1203.113, 1210.231, 1204.235, 1206.419, 1204.538, 1203.246, 1196.405, 1195.231, 1196.863, 1197.946, 1196.866, 1189.076, 1180.67, 1187.862, 1197.201, 1178.958, 1170.329, 1171.328, 1165.001, 1174.337, 1175.181, 1169.333, 1176.347, 1169.135, 1166.416, 1158.124, 1156.832, 1149.863, 1171.743, 1156.701, 1146.817, 1142.682, 1140.305, 1147.835, 1190.262, 1159.135, 1147.149, 1146.219, 1147.11, 1141.423, 1140.339, 1143.928, 1139.414, 1131.689, 1128.566, 1133.976, 1158.219, 1149.357, 1135.754, 1144.762, 1133.93, 1133.143, 1124.631, 1127.844, 1131.385, 1129.705, 1135.793], 'val_loss': [3910.851, 2702.958, 2377.321, 2203.504, 1996.513, 1872.36, 1776.986, 1635.17, 1458.202, 1326.007, 1301.556, 1262.764, 1217.366, 1180.65, 1153.773, 1154.299, 1139.066, 1126.175, 1126.823, 1138.533, 1105.683, 1102.42, 1100.916, 1099.625, 1087.75, 1092.624, 1080.07, 1092.2, 1073.703, 1075.057, 1084.526, 1083.172, 1036.854, 1041.277, 1052.693, 1035.412, 1045.869, 1038.896, 1028.376, 1031.813, 1032.657, 1028.367, 1018.337, 1019.786, 1021.281, 1056.188, 1029.668, 1031.799, 1005.721, 1010.727, 1035.018, 1023.109, 1031.285, 1040.015, 1043.818, 1038.925, 1061.385, 1029.659, 1027.054, 1045.14, 1008.625, 1000.636, 1017.032, 1044.963, 1029.097, 1006.818, 1018.352, 1002.425, 1018.252, 992.015, 1015.782, 974.977, 970.532, 972.544, 983.165, 978.597, 1010.206, 1004.07, 973.553, 1021.077, 985.085, 1011.892, 1008.583, 1000.88, 1000.285, 972.666, 997.121, 968.564, 986.529, 1006.42, 977.814, 1005.22, 976.322, 974.482, 972.075, 964.024, 965.15, 965.359, 975.803, 966.42]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:90 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:11 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adadelta lr:0.002460715196435833 batch_size:49 epochs:100	100	1000	True	1587.177		533043	20	-1	148.92701888084412	{'train_loss': [8789.912, 7987.406, 6086.624, 4106.302, 3364.769, 3074.646, 2901.655, 2786.279, 2702.678, 2645.251, 2603.937, 2569.937, 2546.41, 2529.252, 2513.126, 2501.357, 2492.9, 2485.005, 2481.822, 2473.103, 2467.453, 2466.13, 2463.305, 2460.364, 2457.029, 2454.266, 2451.247, 2450.309, 2448.505, 2446.578, 2444.578, 2439.744, 2439.52, 2438.084, 2436.408, 2434.032, 2432.161, 2426.042, 2423.369, 2414.62, 2404.674, 2392.214, 2376.546, 2357.606, 2338.037, 2323.268, 2303.733, 2289.789, 2275.172, 2263.961, 2257.514, 2248.613, 2236.815, 2228.084, 2222.433, 2216.392, 2210.511, 2200.068, 2192.386, 2188.996, 2181.058, 2175.01, 2169.644, 2160.385, 2158.068, 2149.322, 2144.331, 2147.5, 2140.299, 2130.969, 2127.344, 2118.797, 2116.794, 2107.573, 2101.984, 2102.241, 2094.84, 2090.066, 2084.693, 2071.333, 2070.267, 2065.281, 2058.33, 2052.379, 2050.746, 2047.113, 2041.444, 2037.43, 2027.039, 2020.017, 2014.181, 2008.058, 2006.834, 2002.853, 1990.516, 1987.532, 1978.725, 1975.84, 1971.233, 1957.618], 'val_loss': [5365.448, 5664.746, 5388.568, 4123.462, 3458.694, 3098.909, 2866.95, 2713.892, 2590.229, 2507.893, 2441.771, 2375.333, 2340.505, 2304.367, 2273.464, 2250.152, 2234.852, 2220.646, 2205.086, 2193.89, 2182.096, 2173.011, 2167.596, 2162.912, 2152.764, 2150.965, 2145.702, 2144.335, 2136.917, 2136.232, 2131.344, 2131.373, 2125.858, 2122.0, 2120.513, 2116.728, 2113.711, 2106.751, 2104.206, 2094.62, 2087.3, 2076.334, 2068.275, 2050.573, 2046.962, 2031.547, 2017.378, 2013.558, 2005.221, 1996.102, 1987.604, 1979.835, 1975.677, 1973.544, 1959.693, 1954.273, 1949.827, 1946.435, 1937.898, 1935.384, 1930.592, 1917.756, 1916.31, 1910.857, 1904.366, 1900.499, 1896.604, 1888.8, 1881.356, 1870.966, 1872.491, 1863.512, 1855.299, 1843.624, 1843.317, 1838.672, 1829.426, 1824.433, 1821.45, 1813.543, 1804.564, 1792.557, 1793.828, 1780.898, 1774.887, 1767.399, 1765.844, 1751.167, 1749.301, 1742.823, 1734.355, 1728.249, 1714.396, 1709.059, 1704.965, 1702.629, 1689.433, 1688.833, 1680.87, 1682.053]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:30 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:93 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:30 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.000734787101105575 batch_size:49 epochs:100	100	1000	True	1000.69214		656444	22	-1	158.42401432991028	{'train_loss': [3642.884, 2664.014, 2437.757, 2327.054, 2200.226, 2118.128, 2014.266, 1897.313, 1743.637, 1616.028, 1540.774, 1486.032, 1438.039, 1429.929, 1426.61, 1390.69, 1357.815, 1358.229, 1333.289, 1322.97, 1328.095, 1306.82, 1301.339, 1298.136, 1305.808, 1286.382, 1271.705, 1270.151, 1267.4, 1272.35, 1246.589, 1263.445, 1251.956, 1243.982, 1235.732, 1228.135, 1234.06, 1221.354, 1236.749, 1233.365, 1244.28, 1253.331, 1247.128, 1234.514, 1237.468, 1234.262, 1235.978, 1215.072, 1215.057, 1215.381, 1205.948, 1213.989, 1214.862, 1218.106, 1215.721, 1224.053, 1211.443, 1211.611, 1216.383, 1209.972, 1197.856, 1202.746, 1187.171, 1191.017, 1191.293, 1185.458, 1187.47, 1182.815, 1183.291, 1177.027, 1171.182, 1184.085, 1171.688, 1170.94, 1177.811, 1176.36, 1178.751, 1178.239, 1172.951, 1171.454, 1172.532, 1175.807, 1184.908, 1170.413, 1171.9, 1163.761, 1155.311, 1167.073, 1176.094, 1163.397, 1163.812, 1166.113, 1162.716, 1154.813, 1157.473, 1155.571, 1148.142, 1155.847, 1162.995, 1153.943], 'val_loss': [3659.109, 2748.522, 2231.203, 1983.018, 1866.091, 1823.856, 1735.936, 1585.544, 1478.186, 1360.583, 1376.028, 1275.891, 1251.389, 1226.845, 1232.68, 1165.406, 1161.431, 1149.823, 1125.645, 1126.883, 1129.791, 1123.483, 1087.686, 1136.619, 1093.723, 1070.763, 1075.598, 1080.226, 1057.321, 1070.403, 1085.117, 1056.05, 1060.82, 1047.87, 1054.87, 1044.148, 1045.312, 1036.494, 1036.604, 1073.978, 1103.069, 1139.429, 1055.97, 1042.808, 1062.11, 1049.254, 1092.602, 1020.515, 1044.481, 1032.24, 1025.532, 1028.304, 1052.07, 1021.829, 1040.731, 1055.448, 1020.128, 1049.928, 1023.172, 1070.65, 1034.233, 1018.547, 1009.62, 998.847, 1046.572, 1026.97, 1003.235, 1011.987, 1033.818, 1018.394, 1026.377, 1009.214, 1003.708, 999.974, 1025.004, 1029.717, 1042.766, 1030.77, 1003.389, 1033.95, 994.019, 1040.762, 1013.343, 991.96, 996.893, 993.216, 997.943, 1002.917, 988.939, 984.723, 1028.942, 993.775, 981.535, 978.332, 987.503, 976.662, 1001.23, 992.039, 975.856, 980.987]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:56 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:32 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:conv1d out_channels:42 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:20 layer:fc act:selu out_features:200 bias:True input:21 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.0009673374036965584 batch_size:83 epochs:100	100	1000	True	23817.43359		13867132	23	-1	228.27609825134277	{'train_loss': [32693.842, 30222.273, 30222.904, 30225.363, 30220.898, 30223.043, 30220.918, 30222.75, 30228.484, 31094.27, 30331.969, 30242.988, 30241.152, 30280.801, 30252.408, 30247.191, 30351.391, 30391.43, 30501.33, 30398.748, 30816.496, 30638.711, 30249.764, 30243.848, 30323.9, 30375.488, 30323.438, 30299.932, 30375.213, 30358.033, 30400.705, 30316.963, 30313.623, 30383.508, 30239.719, 30418.174, 32483.801, 30657.123, 30242.828, 30377.588, 30384.959, 30298.801, 30337.012, 30244.221, 30319.434, 30283.098, 30350.863, 30333.102, 30270.254, 30268.635, 30262.469, 30250.684, 30255.119, 30300.57, 30257.09, 30379.912, 30313.91, 30397.48, 30419.771, 30278.445, 30261.797, 30276.924, 30258.242, 30316.287, 30314.742, 30322.367, 30333.852, 30339.619, 30293.738, 30362.74, 30446.951, 30250.115, 30256.301, 30270.941, 30279.939, 30346.922, 30362.732, 30268.877, 30433.393, 30354.539, 30265.072, 30259.711, 30256.689, 30248.678, 30246.832, 30298.047, 30279.75, 30267.285, 30271.998, 30269.428, 30265.822, 30283.896, 30246.928, 30233.086, 30247.439, 30224.674, 30280.9, 30267.209, 30246.188, 30218.102], 'val_loss': [23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23492.461, 23448.57, 23495.082, 23495.082, 23495.082, 23498.441, 27082.291, 23495.082, 152939.469, 26577.924, 28578.521, 45863.102, 47379.293, 23495.082, 23495.074, 23495.082, 23495.082, 23495.082, 30664.977, 23495.082, 23422.697, 23495.082, 23495.082, 24110.637, 23495.082, 23495.082, 23467.447, 23502.17, 23495.082, 79707.812, 45694.273, 25667.711, 25173.922, 23495.08, 23495.082, 23379.762, 23356.324, 23886.023, 23483.199, 24003.375, 24345.035, 23495.082, 23495.082, 23494.811, 23494.76, 43621.133, 30061.836, 23462.184, 23851.484, 24028.51, 23664.016, 23494.584, 23494.5, 23495.078, 29711.764, 23495.023, 23495.082, 23910.971, 24061.266, 23475.504, 37649.852, 26401.58, 23495.082, 23495.082, 23495.082, 23488.682, 28651.064, 23443.783, 23494.973, 23654.238, 23495.082, 23493.242, 23492.289, 23495.08, 23484.191, 23628.25, 23791.625, 23491.582, 24064.24, 23621.492, 23675.672, 24515.947, 23491.195, 23495.082, 23494.988, 23495.08, 24307.221, 23734.521, 23495.035, 23393.92, 23476.375]}	100	100	True
