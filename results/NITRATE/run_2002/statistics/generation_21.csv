id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.0009673374036965584 batch_size:49 epochs:100	100	1000	True	1027.51404		748280	21	-1	178.8365352153778	{'train_loss': [5407.141, 3107.101, 2699.601, 2447.168, 2313.814, 2235.645, 2200.972, 2142.507, 2096.316, 2073.019, 2049.102, 1947.643, 1846.576, 1675.227, 1571.199, 1506.592, 1452.997, 1428.628, 1410.066, 1403.724, 1374.673, 1364.438, 1371.142, 1351.823, 1345.733, 1330.753, 1326.719, 1322.734, 1320.879, 1323.254, 1304.758, 1297.993, 1293.13, 1287.885, 1295.966, 1286.398, 1271.589, 1264.997, 1268.67, 1263.755, 1259.979, 1251.239, 1249.426, 1246.461, 1244.79, 1239.474, 1237.295, 1232.003, 1236.202, 1244.433, 1228.841, 1219.46, 1220.225, 1211.507, 1206.648, 1213.487, 1205.259, 1201.885, 1207.828, 1200.817, 1202.778, 1196.299, 1194.72, 1183.526, 1194.07, 1192.993, 1192.162, 1190.152, 1189.752, 1192.587, 1183.24, 1184.831, 1174.763, 1179.629, 1180.997, 1172.039, 1166.056, 1164.547, 1162.946, 1170.158, 1158.476, 1158.183, 1166.668, 1156.42, 1154.947, 1165.569, 1156.34, 1168.522, 1169.875, 1154.308, 1158.57, 1151.718, 1161.769, 1164.48, 1165.337, 1153.022, 1154.226, 1148.466, 1146.476, 1137.083], 'val_loss': [3921.601, 2545.093, 2313.576, 2051.448, 1928.752, 1925.526, 1934.599, 1862.625, 1862.51, 1790.372, 1752.019, 1670.602, 1549.135, 1433.467, 1358.773, 1308.2, 1247.764, 1235.711, 1269.543, 1216.922, 1188.454, 1199.119, 1182.123, 1166.349, 1166.56, 1154.459, 1150.495, 1131.265, 1135.824, 1112.748, 1107.906, 1103.336, 1106.242, 1089.377, 1133.979, 1085.404, 1076.102, 1068.5, 1066.703, 1080.935, 1072.666, 1049.474, 1059.363, 1054.996, 1048.452, 1054.915, 1059.157, 1058.594, 1036.329, 1038.993, 1028.622, 1056.184, 1037.36, 1030.823, 1011.709, 1023.378, 1004.024, 1009.53, 1031.369, 1002.404, 1022.347, 1011.897, 995.424, 1001.708, 1016.356, 1025.263, 1027.98, 1009.673, 987.878, 990.368, 999.807, 986.36, 986.555, 978.517, 1012.114, 978.693, 974.021, 1000.585, 970.514, 973.957, 961.187, 974.705, 969.656, 971.154, 986.221, 962.767, 987.186, 981.087, 994.838, 960.138, 986.509, 984.194, 992.444, 980.671, 982.057, 960.549, 963.796, 971.482, 989.38, 977.757]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:82 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:59 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:88 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:17 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9260223641614682 weight_decay:0.0009673374036965584 batch_size:49 epochs:100	100	1000	True	1020.89832		592096	22	-1	154.6514220237732	{'train_loss': [3419.066, 2473.368, 2270.417, 2170.51, 2092.217, 2057.691, 2046.178, 2013.155, 2010.877, 1962.615, 1971.515, 1960.701, 1948.054, 1944.051, 1928.409, 1939.528, 1941.217, 1928.556, 1908.309, 1837.775, 1674.53, 1585.801, 1533.846, 1500.005, 1472.844, 1440.657, 1420.025, 1403.133, 1380.713, 1372.789, 1374.025, 1353.046, 1391.262, 1356.142, 1368.212, 1355.57, 1331.191, 1327.529, 1325.994, 1314.566, 1304.965, 1316.281, 1303.562, 1302.237, 1294.245, 1293.262, 1298.958, 1272.008, 1288.836, 1278.988, 1279.528, 1277.132, 1267.094, 1259.32, 1249.499, 1255.371, 1249.629, 1262.318, 1256.605, 1245.989, 1239.659, 1230.782, 1225.179, 1238.245, 1234.2, 1235.096, 1235.844, 1224.153, 1235.764, 1226.954, 1225.969, 1219.639, 1224.821, 1213.317, 1210.329, 1215.947, 1214.436, 1214.084, 1216.659, 1217.797, 1214.833, 1205.232, 1209.218, 1202.009, 1202.733, 1197.119, 1195.359, 1195.326, 1197.21, 1199.123, 1197.158, 1198.886, 1189.71, 1187.362, 1191.096, 1188.381, 1196.098, 1185.037, 1190.226, 1180.485], 'val_loss': [3002.023, 2351.85, 2110.253, 1966.3, 1894.564, 1820.651, 1865.629, 1784.526, 1853.396, 1744.251, 1860.167, 1750.229, 1755.94, 1804.647, 1716.541, 1827.03, 1729.271, 1761.049, 1655.646, 1430.243, 1395.691, 1330.759, 1318.683, 1264.971, 1258.438, 1231.595, 1265.308, 1187.153, 1165.016, 1188.074, 1160.872, 1147.055, 1225.484, 1165.005, 1206.878, 1162.979, 1135.48, 1102.936, 1155.695, 1130.904, 1088.183, 1154.86, 1083.425, 1089.55, 1107.836, 1088.458, 1125.753, 1081.855, 1086.682, 1113.059, 1050.571, 1095.561, 1063.863, 1057.61, 1048.96, 1064.092, 1060.756, 1051.618, 1070.069, 1053.566, 1036.472, 1024.06, 1018.692, 1030.327, 1028.219, 1020.519, 1032.872, 1006.148, 1046.035, 1020.806, 1021.471, 1028.411, 1024.908, 1002.375, 1024.984, 998.732, 1005.963, 1011.448, 1016.459, 1026.312, 1021.281, 1006.38, 1014.937, 983.602, 1012.853, 1004.715, 1003.248, 980.278, 1001.325, 1004.58, 1000.006, 999.671, 996.325, 983.365, 987.325, 985.625, 1003.612, 989.126, 991.327, 985.384]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.0009673374036965584 batch_size:49 epochs:100	100	2000	True	997.64172		748280	21	-1	176.15127182006836	{'train_loss': [5591.378, 3315.432, 2849.208, 2605.069, 2389.513, 2250.949, 2058.998, 1821.993, 1678.229, 1565.897, 1502.445, 1458.479, 1410.176, 1383.994, 1360.915, 1340.979, 1331.77, 1320.657, 1310.967, 1298.384, 1291.205, 1284.934, 1276.294, 1281.048, 1272.914, 1273.037, 1265.503, 1271.438, 1257.751, 1257.809, 1249.418, 1268.823, 1238.361, 1224.656, 1225.509, 1226.711, 1229.087, 1226.568, 1221.374, 1218.615, 1212.968, 1205.997, 1210.813, 1203.113, 1210.231, 1204.235, 1206.419, 1204.538, 1203.246, 1196.405, 1195.231, 1196.863, 1197.946, 1196.866, 1189.076, 1180.67, 1187.862, 1197.201, 1178.958, 1170.329, 1171.328, 1165.001, 1174.337, 1175.181, 1169.333, 1176.347, 1169.135, 1166.416, 1158.124, 1156.832, 1149.863, 1171.743, 1156.701, 1146.817, 1142.682, 1140.305, 1147.835, 1190.262, 1159.135, 1147.149, 1146.219, 1147.11, 1141.423, 1140.339, 1143.928, 1139.414, 1131.689, 1128.566, 1133.976, 1158.219, 1149.357, 1135.754, 1144.762, 1133.93, 1133.143, 1124.631, 1127.844, 1131.385, 1129.705, 1135.793], 'val_loss': [3910.851, 2702.958, 2377.321, 2203.504, 1996.513, 1872.36, 1776.986, 1635.17, 1458.202, 1326.007, 1301.556, 1262.764, 1217.366, 1180.65, 1153.773, 1154.299, 1139.066, 1126.175, 1126.823, 1138.533, 1105.683, 1102.42, 1100.916, 1099.625, 1087.75, 1092.624, 1080.07, 1092.2, 1073.703, 1075.057, 1084.526, 1083.172, 1036.854, 1041.277, 1052.693, 1035.412, 1045.869, 1038.896, 1028.376, 1031.813, 1032.657, 1028.367, 1018.337, 1019.786, 1021.281, 1056.188, 1029.668, 1031.799, 1005.721, 1010.727, 1035.018, 1023.109, 1031.285, 1040.015, 1043.818, 1038.925, 1061.385, 1029.659, 1027.054, 1045.14, 1008.625, 1000.636, 1017.032, 1044.963, 1029.097, 1006.818, 1018.352, 1002.425, 1018.252, 992.015, 1015.782, 974.977, 970.532, 972.544, 983.165, 978.597, 1010.206, 1004.07, 973.553, 1021.077, 985.085, 1011.892, 1008.583, 1000.88, 1000.285, 972.666, 997.121, 968.564, 986.529, 1006.42, 977.814, 1005.22, 976.322, 974.482, 972.075, 964.024, 965.15, 965.359, 975.803, 966.42]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:75 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:56 kernel_size:10 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9449585862102097 weight_decay:0.0009673374036965584 batch_size:49 epochs:100	100	2000	True	998.24451		1090508	22	-1	220.17068076133728	{'train_loss': [7687.08, 3843.525, 3223.761, 2840.15, 2507.478, 2330.103, 2253.45, 2199.024, 2162.792, 2104.605, 2058.671, 2050.293, 2016.784, 2001.508, 1985.055, 1986.012, 1981.863, 1953.244, 1936.922, 1923.298, 1934.496, 1908.826, 1906.67, 1901.248, 1898.629, 1880.018, 1862.929, 1860.52, 1864.665, 1850.944, 1835.999, 1824.583, 1824.45, 1783.29, 1722.299, 1627.49, 1542.773, 1480.831, 1429.545, 1392.113, 1375.099, 1361.242, 1338.297, 1311.965, 1305.258, 1300.982, 1292.658, 1285.261, 1270.527, 1264.742, 1255.849, 1252.849, 1251.237, 1246.109, 1242.317, 1240.344, 1237.839, 1235.252, 1224.621, 1222.296, 1216.748, 1210.233, 1214.332, 1201.781, 1205.451, 1214.394, 1198.114, 1229.568, 1201.104, 1199.302, 1199.831, 1192.623, 1192.812, 1194.458, 1195.451, 1179.887, 1187.446, 1195.092, 1180.399, 1192.148, 1173.311, 1182.26, 1172.107, 1166.913, 1175.274, 1181.146, 1167.981, 1160.094, 1163.807, 1159.143, 1161.719, 1163.475, 1156.322, 1159.301, 1156.648, 1151.001, 1155.441, 1151.054, 1151.855, 1144.349], 'val_loss': [4844.924, 3011.978, 2710.261, 2369.635, 2095.437, 1965.153, 1903.823, 1915.132, 1855.423, 1825.251, 1836.963, 1851.182, 1780.337, 1790.475, 1816.711, 1845.779, 1760.779, 1794.943, 1721.54, 1732.234, 1714.524, 1713.172, 1696.035, 1696.175, 1690.809, 1634.969, 1716.071, 1670.832, 1639.004, 1599.896, 1602.809, 1605.324, 1617.776, 1616.926, 1493.463, 1420.085, 1284.482, 1244.576, 1221.503, 1173.712, 1194.925, 1130.59, 1143.897, 1119.648, 1095.979, 1069.063, 1091.523, 1045.387, 1077.536, 1083.003, 1044.81, 1034.5, 1044.192, 1109.594, 1031.334, 1052.624, 1035.17, 1011.969, 1040.124, 1017.159, 1013.714, 1011.972, 1004.132, 1008.827, 1006.275, 990.627, 1041.944, 1027.25, 988.596, 1024.842, 1036.009, 995.382, 994.711, 1016.961, 998.229, 999.06, 997.322, 985.509, 1018.639, 992.495, 983.518, 995.054, 994.787, 991.208, 982.535, 1000.948, 987.225, 987.781, 998.855, 984.097, 988.946, 974.007, 965.966, 989.706, 975.532, 971.269, 976.927, 960.595, 965.239, 961.35]}	0	100	True
