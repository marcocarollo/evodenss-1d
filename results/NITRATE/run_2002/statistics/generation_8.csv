id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:0.0008426385295062967 batch_size:83 epochs:100	100	1000	True	1490.35095		603506	17	-1	205.64551162719727	{'train_loss': [7578.525, 4744.124, 4322.073, 4074.753, 3892.711, 3823.206, 3631.298, 3310.215, 2965.394, 2765.292, 2631.678, 2555.103, 2511.735, 2447.45, 2415.407, 2399.586, 2349.964, 2325.42, 2323.114, 2272.703, 2252.303, 2221.798, 2208.445, 2211.202, 2227.268, 2195.15, 2146.943, 2110.907, 2119.899, 2089.301, 2095.951, 2067.185, 2080.994, 2060.059, 2060.982, 2037.75, 2011.38, 2023.157, 2011.046, 2006.184, 1998.428, 2004.467, 1977.466, 1960.053, 1957.771, 1948.835, 1973.295, 1976.482, 1983.373, 1959.28, 1952.884, 1955.384, 1948.968, 1942.554, 1939.5, 1935.239, 1926.012, 1945.752, 1939.585, 1927.23, 1916.529, 1909.02, 1909.903, 1919.533, 1908.596, 1920.916, 1914.571, 1912.557, 1922.495, 1933.607, 1921.064, 1885.955, 1896.656, 1905.025, 1891.694, 1897.175, 1881.089, 1876.667, 1878.996, 1889.017, 1890.172, 1901.112, 1869.497, 1880.889, 1891.533, 1872.594, 1860.325, 1852.674, 1854.885, 1870.591, 1874.639, 1860.381, 1865.703, 1851.592, 1844.108, 1846.932, 1847.64, 1835.304, 1862.163, 1855.251], 'val_loss': [7243.565, 4194.827, 3848.552, 3639.574, 3349.388, 3291.385, 2927.639, 2551.281, 2442.496, 2222.476, 2244.933, 2166.495, 2094.748, 1981.075, 2065.316, 1922.357, 1869.583, 1928.525, 1843.943, 1787.135, 1773.43, 1725.166, 1725.859, 1730.095, 1755.795, 1689.757, 1634.944, 1614.635, 1644.651, 1610.544, 1600.831, 1597.304, 1598.894, 1566.066, 1613.58, 1568.461, 1570.254, 1551.054, 1604.486, 1537.135, 1589.853, 1585.633, 1508.382, 1529.881, 1527.807, 1532.779, 1533.08, 1549.389, 1585.174, 1521.663, 1519.039, 1543.488, 1495.118, 1529.069, 1526.213, 1539.718, 1527.414, 1504.022, 1481.249, 1549.259, 1508.293, 1475.147, 1507.71, 1493.882, 1481.095, 1498.636, 1493.045, 1542.614, 1508.538, 1541.315, 1474.416, 1448.955, 1471.327, 1481.773, 1544.018, 1456.904, 1517.443, 1442.029, 1474.473, 1531.728, 1531.35, 1457.647, 1490.812, 1498.07, 1474.231, 1468.713, 1470.071, 1438.476, 1427.204, 1468.392, 1513.658, 1484.386, 1553.447, 1428.089, 1490.02, 1519.705, 1425.266, 1426.78, 1494.345, 1440.893]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:79 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:5.9058791991417655e-05 batch_size:83 epochs:100	100	1000	True	1539.88623		617139	18	-1	210.82975435256958	{'train_loss': [7159.616, 4730.854, 4376.175, 4206.77, 3921.568, 3725.489, 3603.197, 3541.682, 3476.062, 3431.878, 3352.805, 3301.631, 3233.893, 3064.786, 2838.057, 2690.629, 2579.918, 2490.236, 2440.582, 2422.786, 2366.832, 2334.167, 2281.991, 2266.665, 2258.513, 2204.42, 2197.322, 2225.018, 2196.737, 2205.019, 2188.915, 2138.603, 2112.563, 2116.296, 2120.274, 2091.011, 2078.51, 2070.842, 2072.11, 2065.292, 2063.592, 2055.801, 2049.568, 2033.54, 2034.744, 2029.466, 2035.539, 2017.19, 2030.483, 2010.493, 1984.837, 1995.076, 1990.466, 1977.148, 1968.765, 1963.598, 1952.128, 1960.2, 1973.591, 1945.219, 1941.631, 1928.874, 1937.086, 1954.689, 1925.244, 1926.098, 1941.432, 1935.109, 1957.402, 1938.254, 1916.257, 1912.115, 1916.098, 1918.121, 1934.572, 1928.168, 1911.756, 1931.59, 1911.357, 1940.753, 1911.325, 1885.506, 1874.665, 1875.971, 1873.08, 1872.08, 1920.508, 1901.317, 1908.785, 1897.364, 1891.985, 1911.286, 1936.152, 1888.06, 1891.783, 1864.665, 1875.856, 1868.26, 1882.99, 1888.796], 'val_loss': [6454.236, 4308.673, 3639.45, 3434.987, 3243.024, 3022.743, 2967.632, 2959.49, 2950.2, 2790.614, 2781.925, 2657.06, 2547.542, 2565.279, 2332.222, 2139.03, 2053.344, 2007.874, 1961.68, 1889.857, 1925.238, 1824.755, 1824.376, 1797.978, 1735.299, 1735.241, 1749.683, 1790.874, 1776.511, 1910.283, 1775.135, 1673.282, 1681.439, 1705.864, 1662.985, 1652.483, 1670.917, 1636.44, 1707.457, 1638.311, 1635.766, 1617.284, 1589.88, 1663.253, 1593.732, 1601.272, 1634.267, 1612.333, 1620.068, 1560.135, 1592.01, 1599.551, 1621.131, 1559.429, 1560.611, 1604.402, 1559.53, 1543.343, 1570.955, 1616.943, 1583.832, 1527.995, 1612.552, 1552.708, 1528.879, 1557.382, 1595.435, 1547.499, 1659.992, 1558.684, 1512.035, 1529.272, 1568.491, 1557.591, 1517.739, 1537.682, 1532.409, 1546.629, 1531.734, 1538.807, 1485.108, 1502.689, 1531.077, 1482.01, 1525.141, 1537.36, 1523.636, 1527.579, 1511.231, 1476.908, 1507.055, 1541.04, 1489.672, 1505.889, 1482.267, 1481.416, 1492.055, 1489.656, 1493.046, 1506.039]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:47 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:0.0006871215647364317 batch_size:83 epochs:100	100	1000	True	1551.91211		538752	18	-1	192.71359276771545	{'train_loss': [6275.952, 4472.244, 4161.471, 3919.696, 3805.249, 3629.863, 3575.725, 3480.821, 3430.804, 3363.596, 3178.09, 2841.439, 2661.372, 2546.868, 2466.688, 2401.804, 2322.482, 2306.489, 2258.287, 2264.777, 2223.248, 2200.071, 2193.908, 2158.283, 2130.115, 2123.786, 2109.419, 2106.993, 2086.091, 2081.82, 2073.477, 2040.927, 2037.854, 2067.595, 2028.57, 2001.54, 1985.033, 2032.113, 2007.845, 1984.058, 1989.327, 1989.771, 1984.358, 1964.864, 1949.948, 1948.554, 1959.96, 1942.2, 1943.33, 1949.601, 1921.812, 1919.932, 1915.87, 1933.034, 1930.043, 1916.314, 1912.226, 1909.703, 1906.394, 1897.927, 1894.164, 1903.817, 1873.497, 1894.506, 1897.974, 1894.34, 1881.056, 1878.441, 1868.38, 1884.711, 1876.746, 1853.21, 1846.392, 1854.243, 1862.019, 1865.535, 1854.82, 1839.193, 1851.326, 1844.761, 1843.677, 1839.097, 1845.787, 1838.736, 1851.18, 1844.105, 1837.293, 1839.528, 1839.26, 1827.36, 1824.205, 1826.074, 1830.028, 1844.489, 1840.368, 1832.208, 1825.289, 1814.83, 1808.032, 1819.971], 'val_loss': [4912.776, 3728.738, 3407.341, 3389.077, 3148.614, 3120.867, 3064.987, 2826.101, 2918.32, 2867.992, 2660.078, 2241.293, 2084.826, 1938.52, 1957.846, 1887.993, 1842.034, 1808.426, 1818.939, 1840.167, 1745.055, 1798.295, 1750.868, 1706.694, 1652.62, 1718.319, 1642.071, 1688.062, 1643.91, 1649.702, 1690.932, 1635.885, 1651.865, 1697.329, 1650.876, 1578.574, 1598.119, 1674.199, 1589.669, 1590.18, 1625.82, 1587.859, 1582.345, 1547.865, 1538.899, 1549.064, 1552.655, 1519.186, 1617.949, 1556.149, 1494.558, 1538.375, 1494.675, 1557.733, 1537.638, 1515.439, 1473.202, 1552.291, 1477.44, 1531.734, 1527.303, 1492.752, 1473.155, 1516.417, 1502.074, 1503.74, 1460.154, 1466.49, 1510.177, 1477.207, 1441.244, 1432.573, 1444.583, 1489.329, 1480.935, 1472.717, 1451.542, 1472.963, 1470.171, 1473.788, 1461.287, 1435.495, 1435.701, 1438.782, 1439.445, 1461.159, 1470.128, 1457.43, 1416.598, 1416.948, 1435.612, 1439.389, 1465.272, 1432.299, 1446.803, 1452.302, 1401.115, 1414.656, 1409.55, 1463.078]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.002460715196435833 beta1:0.8562422222194059 beta2:0.9711402723837911 weight_decay:0.0008426385295062967 batch_size:83 epochs:100	100	1000	True	1497.69751		799205	17	-1	255.49632358551025	{'train_loss': [13632.42, 6577.793, 5491.224, 5215.901, 4997.967, 5503.343, 4458.583, 3706.759, 3479.995, 3078.142, 2961.838, 2736.339, 2612.716, 2647.815, 2469.303, 2438.21, 2377.524, 2330.23, 2304.621, 2290.655, 2257.196, 2192.697, 2203.538, 2198.557, 2166.677, 2144.165, 2123.567, 2144.692, 2126.815, 2095.914, 2084.361, 2085.822, 2061.565, 2042.29, 2051.366, 2044.693, 2028.405, 2024.47, 2028.662, 2016.894, 2002.345, 1988.276, 2010.658, 2001.875, 1964.205, 1965.665, 1957.712, 1960.206, 1953.216, 1961.873, 1943.78, 1937.523, 1937.193, 1929.547, 1927.362, 1923.616, 1929.857, 1928.269, 1905.433, 1910.926, 1907.144, 1906.315, 1913.401, 1898.883, 1890.905, 1887.061, 1872.451, 1878.853, 1882.0, 1877.261, 1871.216, 1874.221, 1883.982, 1878.566, 1874.641, 1886.666, 1887.863, 1878.938, 1887.013, 1851.398, 1853.201, 1862.958, 1841.379, 1840.13, 1858.253, 1842.674, 1842.757, 1836.137, 1825.328, 1832.668, 1820.056, 1835.591, 1829.465, 1829.689, 1830.05, 1822.501, 1827.809, 1819.771, 1830.814, 1827.659], 'val_loss': [7917.707, 5585.917, 5023.725, 5068.071, 6118.576, 5388.287, 3454.332, 3647.751, 3127.34, 2847.223, 2627.854, 2445.596, 2351.694, 2337.216, 2336.429, 2102.642, 2150.712, 2072.358, 2010.964, 2041.753, 1912.806, 1809.532, 1882.47, 1814.536, 1753.274, 1736.525, 1811.26, 1810.874, 1736.177, 1770.639, 1718.647, 1589.31, 1668.061, 1694.407, 1732.743, 1651.879, 1774.548, 1679.768, 1742.034, 1716.723, 1622.412, 1620.42, 1747.544, 1562.892, 1557.209, 1612.723, 1569.841, 1578.149, 1712.575, 1539.961, 1616.826, 1537.55, 1605.648, 1519.057, 1756.98, 1476.431, 1535.066, 1469.12, 1471.179, 1523.375, 1539.145, 1518.925, 1466.408, 1477.879, 1464.968, 1478.032, 1480.697, 1472.55, 1457.979, 1454.57, 1479.543, 1555.73, 1609.665, 1691.401, 1581.414, 1619.251, 1596.988, 1617.156, 1464.289, 1594.66, 1559.296, 1442.744, 1502.593, 1492.839, 1480.681, 1501.871, 1509.72, 1461.181, 1461.736, 1471.279, 1464.366, 1492.245, 1443.991, 1444.578, 1423.641, 1460.428, 1481.273, 1499.183, 1543.334, 1418.671]}	100	100	True
