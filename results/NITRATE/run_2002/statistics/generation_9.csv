id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:0.0008426385295062967 batch_size:83 epochs:100	100	1000	True	1469.96387		603506	17	-1	211.81209015846252	{'train_loss': [7693.933, 4939.768, 4459.5, 4144.596, 3967.109, 3768.966, 3672.557, 3628.545, 3574.746, 3538.911, 3442.657, 3318.244, 3095.142, 2834.173, 2780.651, 2580.16, 2489.812, 2383.912, 2357.836, 2329.319, 2283.882, 2271.058, 2259.673, 2208.565, 2173.93, 2139.934, 2141.307, 2123.273, 2113.022, 2083.557, 2054.359, 2067.915, 2035.212, 2027.908, 2045.987, 2033.332, 2002.233, 1999.026, 1990.336, 2008.437, 2001.36, 1984.99, 1984.863, 1969.192, 1973.944, 1953.505, 1961.076, 1962.629, 1935.808, 1934.131, 1929.63, 1910.59, 1908.48, 1918.292, 1927.946, 1923.443, 1917.735, 1901.206, 1902.695, 1915.312, 1907.028, 1881.834, 1883.545, 1885.108, 1890.924, 1871.058, 1876.158, 1862.079, 1868.072, 1878.676, 1859.437, 1842.633, 1852.701, 1843.973, 1847.426, 1863.544, 1850.951, 1846.207, 1859.748, 1834.3, 1843.459, 1857.182, 1835.292, 1836.276, 1824.966, 1833.533, 1831.303, 1842.905, 1831.368, 1828.23, 1838.788, 1834.367, 1827.534, 1818.338, 1825.341, 1825.033, 1814.036, 1813.696, 1807.464, 1814.318], 'val_loss': [5988.773, 3953.747, 3712.626, 3548.746, 3367.614, 3105.405, 3107.941, 3075.053, 2940.588, 2778.133, 2832.967, 2616.123, 2566.761, 2489.541, 2155.251, 2040.714, 1921.077, 1855.593, 1828.892, 1783.128, 1767.375, 1725.138, 1889.0, 1793.961, 1647.154, 1642.434, 1700.728, 1615.607, 1654.069, 1647.957, 1607.429, 1590.764, 1589.474, 1564.816, 1700.36, 1670.139, 1643.621, 1555.546, 1549.597, 1580.736, 1643.196, 1622.638, 1673.218, 1609.254, 1649.347, 1541.167, 1666.827, 1529.399, 1556.712, 1562.902, 1573.085, 1477.353, 1561.044, 1626.183, 1590.101, 1534.905, 1553.798, 1552.779, 1535.828, 1585.322, 1483.235, 1559.619, 1487.262, 1516.34, 1485.617, 1545.287, 1479.608, 1539.339, 1543.138, 1504.909, 1465.551, 1449.892, 1536.96, 1457.022, 1511.094, 1515.793, 1449.539, 1514.77, 1430.037, 1496.994, 1467.808, 1483.304, 1450.562, 1508.821, 1445.527, 1489.474, 1488.534, 1450.131, 1442.304, 1426.298, 1480.372, 1420.839, 1423.7, 1451.452, 1431.617, 1451.139, 1413.625, 1430.058, 1437.834, 1419.93]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:35 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:0.0008426385295062967 batch_size:49 epochs:100	100	1000	True	1026.96265		516642	18	-1	168.13882327079773	{'train_loss': [3385.737, 2581.956, 2379.372, 2247.536, 2139.687, 2125.221, 2083.061, 2079.284, 2034.386, 2035.324, 1972.101, 1947.334, 1913.566, 1883.78, 1796.166, 1593.562, 1478.24, 1445.962, 1411.998, 1390.402, 1360.742, 1346.045, 1332.608, 1313.935, 1316.809, 1304.036, 1303.099, 1280.599, 1280.003, 1268.099, 1271.818, 1262.185, 1258.854, 1260.15, 1254.646, 1254.445, 1240.631, 1232.966, 1233.432, 1226.37, 1230.13, 1227.837, 1225.37, 1217.428, 1220.88, 1225.862, 1221.82, 1204.64, 1206.25, 1201.525, 1194.475, 1193.912, 1190.112, 1194.989, 1193.816, 1187.733, 1188.416, 1177.98, 1180.075, 1188.87, 1175.08, 1180.042, 1179.893, 1170.654, 1173.563, 1174.002, 1175.661, 1178.952, 1161.628, 1166.844, 1171.109, 1160.251, 1164.487, 1159.357, 1160.081, 1163.846, 1157.131, 1156.999, 1174.96, 1166.71, 1167.237, 1164.091, 1160.486, 1157.591, 1154.338, 1149.205, 1154.02, 1152.834, 1164.426, 1157.166, 1152.266, 1151.627, 1157.31, 1154.442, 1144.799, 1138.609, 1144.373, 1140.61, 1146.489, 1145.792], 'val_loss': [2733.372, 2217.996, 2034.165, 1917.643, 1899.63, 1886.627, 1895.776, 1850.457, 1828.378, 1754.902, 1735.166, 1796.373, 1777.139, 1667.335, 1515.984, 1298.024, 1254.369, 1220.671, 1166.228, 1163.881, 1124.933, 1131.212, 1104.374, 1119.916, 1093.98, 1137.42, 1078.28, 1084.047, 1062.881, 1082.99, 1068.909, 1058.162, 1055.284, 1046.956, 1054.549, 1041.049, 1038.815, 1040.241, 1030.34, 1028.899, 1048.274, 1032.696, 1007.373, 1008.564, 1019.807, 1022.835, 1001.676, 1005.3, 1020.527, 1006.469, 1016.911, 1002.849, 1001.286, 1001.343, 993.612, 997.698, 995.056, 993.978, 986.366, 985.125, 978.727, 992.803, 979.607, 986.223, 994.482, 1000.326, 981.965, 983.981, 977.449, 983.961, 990.457, 972.463, 972.079, 977.267, 975.075, 973.566, 969.72, 970.265, 987.142, 973.604, 974.778, 997.499, 985.374, 973.505, 975.022, 967.38, 965.273, 971.762, 980.082, 970.531, 965.387, 984.755, 972.228, 974.314, 969.151, 959.796, 966.409, 962.009, 965.571, 972.244]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:125 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:0.0004652277044413521 batch_size:83 epochs:100	100	1000	True	1534.73169		739617	18	-1	282.85143852233887	{'train_loss': [11113.979, 6545.551, 5210.349, 4846.933, 4502.191, 4279.954, 4092.912, 3951.134, 3859.164, 3719.485, 3619.613, 3508.154, 3385.533, 3219.695, 3036.416, 2798.949, 2578.499, 2482.828, 2371.334, 2306.789, 2250.6, 2229.838, 2223.204, 2193.598, 2176.812, 2137.936, 2132.608, 2119.228, 2083.856, 2057.378, 2038.401, 2016.412, 2027.509, 2015.489, 2015.098, 1999.413, 1988.542, 1986.168, 1982.595, 1961.29, 1953.263, 1934.617, 1925.828, 1925.487, 1913.748, 1916.22, 1929.183, 1901.758, 1889.253, 1897.876, 1880.565, 1883.741, 1870.659, 1881.984, 1877.432, 1876.194, 1858.656, 1844.422, 1848.604, 1848.605, 1842.118, 1854.19, 1858.852, 1864.276, 1849.208, 1819.793, 1835.577, 1841.804, 1842.936, 1855.839, 1845.499, 1837.348, 1820.319, 1821.459, 1834.839, 1821.436, 1797.07, 1813.482, 1807.084, 1793.04, 1790.659, 1802.562, 1789.41, 1789.896, 1798.853, 1807.253, 1798.004, 1775.615, 1784.562, 1803.813, 1786.425, 1773.712, 1782.321, 1779.215, 1789.511, 1766.013, 1779.294, 1764.113, 1778.332, 1769.332], 'val_loss': [10392.204, 4995.924, 4263.321, 4076.543, 3785.103, 3537.283, 3549.494, 3333.246, 3269.948, 3108.296, 3050.511, 2839.215, 2802.178, 3029.63, 2471.085, 2249.813, 2105.214, 2032.732, 1984.823, 2001.646, 2060.935, 1940.769, 1861.302, 1855.806, 1945.211, 1785.759, 1689.154, 1788.213, 1840.891, 1789.687, 1781.176, 1702.675, 1643.521, 1695.428, 1772.013, 1668.553, 1629.478, 1624.666, 1725.882, 1587.31, 1673.996, 1589.54, 1701.333, 1716.318, 1528.223, 1587.448, 1624.942, 1665.069, 1601.339, 1544.401, 1593.243, 1504.859, 1507.786, 1642.869, 1831.439, 1703.003, 1499.663, 1546.348, 1578.743, 1478.414, 1516.058, 1660.073, 1477.228, 1583.38, 1520.267, 1496.847, 1524.05, 1617.18, 1492.842, 1659.886, 1619.536, 1674.18, 1542.989, 1847.869, 1577.407, 1657.484, 1548.888, 1792.058, 1460.976, 1514.652, 1479.018, 1569.41, 1458.273, 1466.745, 1499.824, 1686.247, 1567.594, 1539.838, 1468.758, 1466.531, 1438.261, 1445.927, 1488.937, 1585.052, 1650.846, 1570.297, 1510.19, 1587.156, 1485.523, 1492.807]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:33 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:deconv1d out_channels:52 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:0.0008426385295062967 batch_size:68 epochs:100	100	1000	True	1517.22534		1042955	18	-1	285.98711252212524	{'train_loss': [14386.407, 10264.971, 8341.748, 7436.528, 6784.873, 6425.259, 6261.313, 5812.11, 4923.568, 3542.037, 2701.987, 2268.022, 2078.647, 1979.886, 1905.754, 1867.172, 1847.352, 1824.01, 1841.611, 1801.348, 1795.96, 1781.696, 1791.308, 1779.611, 1741.56, 1733.077, 1723.356, 1727.972, 1708.066, 1706.111, 1707.641, 1692.222, 1708.17, 1704.136, 1680.701, 1684.074, 1664.889, 1648.04, 1655.175, 1660.339, 1662.037, 1642.005, 1628.939, 1627.126, 1649.194, 1635.624, 1650.111, 1644.271, 1628.229, 1615.046, 1613.225, 1604.594, 1601.885, 1601.344, 1590.732, 1604.399, 1598.584, 1599.08, 1587.084, 1591.135, 1589.677, 1575.241, 1600.854, 1603.292, 1595.089, 1573.429, 1570.085, 1560.214, 1567.158, 1588.365, 1573.574, 1569.165, 1572.524, 1567.077, 1545.763, 1561.887, 1547.458, 1543.147, 1561.121, 1546.568, 1558.157, 1543.736, 1538.533, 1538.476, 1548.839, 1545.286, 1531.108, 1534.233, 1552.116, 1552.73, 1529.755, 1551.738, 1580.362, 1586.813, 1566.412, 1545.749, 1552.123, 1550.505, 1555.751, 1524.303], 'val_loss': [15723.168, 9991.154, 8504.809, 7300.226, 6650.259, 6433.27, 5989.268, 5730.557, 4287.377, 3177.065, 2572.86, 2231.18, 2034.811, 1905.156, 1824.543, 1838.915, 1845.583, 1839.058, 1809.156, 1796.615, 1704.832, 1834.444, 1703.49, 1715.756, 1592.998, 1635.797, 1816.184, 2017.915, 1654.633, 1619.342, 1696.139, 1662.612, 1673.42, 1732.732, 1598.439, 1555.047, 1634.452, 1590.576, 1685.816, 1645.63, 1600.438, 1768.906, 1630.125, 1570.813, 1527.424, 1609.858, 1628.125, 1573.727, 1622.247, 1569.629, 1521.077, 1590.807, 1496.126, 1543.276, 1543.098, 1526.796, 1546.593, 1518.714, 1482.087, 1520.87, 1515.797, 1569.277, 1563.456, 1693.974, 1473.793, 1499.007, 1497.828, 1476.649, 1484.297, 1508.477, 1486.792, 1520.57, 1481.717, 1454.672, 1453.26, 1508.833, 1493.351, 1555.778, 1447.399, 1461.714, 1508.01, 1487.781, 1450.095, 1438.846, 1435.254, 1447.016, 1429.924, 1456.43, 1535.558, 1442.785, 1492.415, 1509.34, 1473.334, 1537.073, 1555.795, 1599.827, 1477.115, 1612.686, 1476.766, 1439.441]}	100	100	True
