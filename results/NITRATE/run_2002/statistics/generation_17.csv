id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:67 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:6.030364562254975e-05 batch_size:49 epochs:100	100	1000	True	1022.00098		457804	21	-1	169.83955192565918	{'train_loss': [3367.615, 2383.763, 2214.268, 2143.385, 2110.166, 2077.683, 2062.633, 2044.442, 2027.843, 2020.622, 1997.364, 1988.889, 1968.118, 1969.248, 1977.577, 1956.391, 1947.742, 1939.844, 1934.753, 1945.254, 1910.875, 1841.645, 1851.07, 1749.838, 1654.686, 1559.063, 1523.984, 1495.262, 1457.485, 1431.295, 1412.129, 1392.251, 1378.76, 1367.069, 1356.986, 1330.815, 1337.423, 1338.723, 1311.18, 1315.349, 1301.053, 1293.346, 1280.583, 1277.793, 1276.0, 1268.041, 1270.349, 1261.111, 1249.695, 1250.32, 1246.122, 1247.214, 1255.988, 1247.245, 1241.858, 1239.364, 1235.603, 1234.986, 1228.887, 1225.023, 1223.849, 1225.097, 1222.34, 1219.964, 1208.149, 1208.321, 1208.055, 1216.504, 1202.64, 1202.232, 1214.543, 1211.117, 1200.83, 1205.8, 1194.798, 1199.078, 1204.194, 1197.992, 1199.24, 1194.13, 1193.6, 1198.155, 1189.552, 1186.258, 1187.727, 1193.057, 1184.096, 1181.434, 1178.208, 1179.584, 1177.12, 1183.535, 1181.15, 1170.462, 1170.345, 1164.215, 1174.848, 1166.17, 1160.323, 1162.588], 'val_loss': [3521.476, 2392.759, 1939.638, 1879.222, 1823.459, 1833.479, 1824.988, 1801.623, 1831.15, 1783.709, 1808.763, 1761.309, 1747.063, 1740.474, 1824.834, 1776.378, 1744.825, 1702.359, 1744.801, 1720.154, 1652.435, 1623.419, 1602.075, 1482.039, 1361.576, 1294.192, 1319.165, 1257.256, 1219.712, 1204.197, 1196.309, 1184.199, 1176.592, 1161.217, 1188.069, 1151.977, 1176.2, 1150.233, 1117.703, 1163.684, 1154.803, 1131.187, 1102.582, 1105.263, 1128.859, 1084.965, 1112.833, 1097.293, 1103.082, 1104.062, 1083.584, 1074.085, 1113.623, 1091.636, 1066.923, 1076.78, 1059.62, 1086.512, 1045.467, 1054.836, 1055.991, 1063.396, 1063.951, 1051.698, 1061.46, 1029.487, 1039.946, 1043.144, 1036.42, 1059.561, 1059.548, 1053.739, 1063.173, 1035.06, 1044.0, 1076.146, 1061.99, 1037.018, 1037.995, 1046.594, 1029.659, 1037.268, 1035.351, 1058.445, 1014.962, 1017.148, 1046.82, 1015.388, 1007.703, 1019.105, 1019.645, 1037.285, 1020.196, 1014.976, 990.213, 999.209, 1027.894, 1005.287, 1007.054, 1000.93]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:31 kernel_size:8 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:deconv1d out_channels:73 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:17 layer:conv1d out_channels:67 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:6.030364562254975e-05 batch_size:74 epochs:100	100	1000	True	1573.5271		512227	22	-1	171.7920000553131	{'train_loss': [5559.688, 3962.85, 3643.736, 3434.068, 3296.785, 3204.861, 3129.414, 3113.319, 3087.686, 3063.147, 3040.066, 3023.942, 3024.442, 2955.654, 2859.211, 2742.62, 2608.311, 2458.036, 2343.126, 2322.065, 2230.711, 2164.712, 2123.49, 2096.097, 2072.709, 2056.625, 2004.125, 1994.104, 1982.588, 1949.834, 1947.943, 1949.217, 1933.752, 1912.299, 1914.977, 1907.709, 1898.495, 1892.794, 1859.793, 1883.193, 1854.993, 1861.895, 1868.937, 1833.577, 1836.575, 1834.681, 1831.34, 1811.042, 1808.754, 1802.645, 1807.209, 1829.316, 1825.278, 1807.53, 1790.787, 1785.81, 1799.706, 1795.866, 1775.265, 1770.337, 1803.498, 1791.959, 1779.586, 1779.628, 1766.332, 1760.26, 1760.852, 1773.371, 1756.895, 1740.888, 1751.534, 1763.288, 1759.395, 1760.645, 1757.493, 1743.293, 1760.041, 1753.351, 1741.558, 1727.828, 1720.1, 1727.959, 1736.582, 1725.859, 1724.165, 1718.588, 1710.357, 1717.956, 1726.766, 1706.472, 1703.028, 1696.227, 1720.303, 1707.421, 1703.698, 1699.924, 1697.648, 1702.748, 1690.081, 1694.219], 'val_loss': [5575.592, 4122.091, 3643.097, 3248.728, 3057.412, 2923.4, 2858.626, 2785.759, 2758.955, 2786.35, 2748.114, 2809.801, 2724.421, 2661.201, 2471.093, 2333.61, 2209.859, 2127.469, 2157.103, 2041.669, 1975.761, 1904.248, 1833.307, 1837.24, 1838.834, 1775.007, 1756.305, 1727.299, 1693.466, 1711.035, 1688.354, 1707.285, 1662.676, 1639.876, 1638.161, 1670.246, 1680.174, 1621.523, 1604.22, 1593.115, 1598.66, 1601.544, 1613.589, 1577.988, 1577.326, 1604.348, 1556.972, 1543.371, 1537.568, 1551.469, 1582.079, 1640.768, 1606.1, 1579.545, 1527.973, 1552.652, 1591.285, 1587.785, 1535.227, 1572.41, 1599.463, 1568.764, 1560.082, 1528.825, 1535.369, 1491.242, 1536.687, 1515.26, 1547.278, 1519.811, 1519.137, 1570.318, 1528.063, 1521.356, 1537.332, 1552.426, 1481.368, 1490.284, 1463.211, 1488.036, 1480.97, 1494.65, 1472.34, 1481.6, 1492.703, 1466.034, 1500.456, 1501.651, 1495.766, 1467.833, 1432.882, 1466.906, 1497.287, 1484.183, 1467.087, 1436.786, 1460.185, 1461.265, 1473.628, 1513.56]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:69 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:121 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:67 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:gradient_descent lr:0.002460715196435833 momentum:0.9171836851752555 weight_decay:6.030364562254975e-05 nesterov:False batch_size:49 epochs:100	100	1000	True	15684.33496		599741	21	-1	153.25073909759521	{'train_loss': [20345.914, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18397.217, 18398.643, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344], 'val_loss': [15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:67 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:deconv1d out_channels:76 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:deconv1d out_channels:99 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:6.030364562254975e-05 batch_size:76 epochs:100	100	1000	True	7340.67871		16176301	22	-1	181.66047477722168	{'train_loss': [42825.695, 28206.191, 28206.191, 28206.191, 28206.191, 28206.191, 28206.191, 28206.754, 28206.191, 28206.191, 28207.234, 28208.406, 28222.479, 28341.195, 28254.723, 29998.275, 28206.191, 28299.301, 28206.945, 28276.488, 28303.465, 28234.41, 28680.24, 29184.383, 28990.832, 28355.039, 28391.586, 28489.521, 28410.824, 28249.438, 28208.23, 28209.312, 28209.828, 28206.836, 28209.082, 28206.848, 28223.039, 28232.92, 28296.299, 28326.811, 28391.566, 28292.117, 28256.242, 28229.945, 28231.039, 28217.262, 28224.713, 28208.371, 28207.219, 28207.811, 28207.363, 28206.176, 28212.514, 28212.832, 28206.707, 28214.264, 28225.875, 28208.631, 28206.842, 28208.848, 28210.031, 28209.764, 28209.426, 28204.469, 28203.611, 28198.428, 28192.48, 28168.59, 28173.484, 28167.045, 28157.959, 28166.725, 28161.105, 28200.506, 28206.531, 28204.584, 28207.195, 28200.271, 28190.742, 28160.691, 28127.535, 28028.684, 27924.691, 27840.445, 27623.631, 27357.363, 26814.004, 26089.752, 25329.027, 24271.107, 23052.037, 21667.465, 20318.895, 18776.068, 17314.184, 15842.894, 14233.54, 12752.378, 11443.898, 10206.043], 'val_loss': [23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.133, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 25232.268, 23495.08, 23495.006, 23495.08, 23643.039, 23495.08, 23492.684, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23476.883, 23493.635, 23494.91, 23562.043, 23495.08, 23495.051, 23493.232, 23495.08, 23492.992, 23495.08, 23495.08, 23495.08, 23495.08, 23495.08, 23362.883, 23489.773, 23495.08, 23494.467, 23495.078, 23495.08, 23495.08, 23495.08, 23494.977, 23495.08, 23495.08, 23494.955, 23495.08, 23487.861, 23482.918, 23494.273, 23468.242, 23382.242, 23384.41, 23352.727, 23492.523, 23492.066, 23494.727, 23484.117, 23495.068, 23495.074, 23494.98, 23495.004, 23490.891, 23456.582, 23325.789, 23127.43, 22959.805, 22528.316, 21723.508, 21415.094, 20348.328, 19400.281, 17521.959, 17935.445, 16214.041, 14894.166, 13373.814, 12284.895, 10987.438, 10312.646, 9321.051, 7957.2, 7039.212]}	100	100	True
