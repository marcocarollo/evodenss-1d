id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:6.030364562254975e-05 batch_size:49 epochs:100	100	2000	True	1005.55048		512391	21	-1	146.4524440765381	{'train_loss': [3539.421, 2596.146, 2373.449, 2237.927, 2170.684, 2110.557, 1950.682, 1663.039, 1540.052, 1490.235, 1453.816, 1415.183, 1395.427, 1382.396, 1358.466, 1335.794, 1328.816, 1315.998, 1307.657, 1285.73, 1292.649, 1275.286, 1269.475, 1259.912, 1252.429, 1251.516, 1238.849, 1232.741, 1232.904, 1224.648, 1223.081, 1218.292, 1208.167, 1206.392, 1206.475, 1199.091, 1189.337, 1198.215, 1201.446, 1200.331, 1193.071, 1189.848, 1186.281, 1183.297, 1169.851, 1178.449, 1168.455, 1175.851, 1179.602, 1170.8, 1163.145, 1163.031, 1173.187, 1160.214, 1164.155, 1155.375, 1163.146, 1148.653, 1143.184, 1144.16, 1152.809, 1141.077, 1140.609, 1139.974, 1140.184, 1131.344, 1135.724, 1141.069, 1137.588, 1144.726, 1136.306, 1125.773, 1131.913, 1132.338, 1128.546, 1134.36, 1130.897, 1120.912, 1127.387, 1123.064, 1116.486, 1116.254, 1126.818, 1127.136, 1115.889, 1111.693, 1118.487, 1115.514, 1112.606, 1117.247, 1114.232, 1118.191, 1115.08, 1110.361, 1102.2, 1111.781, 1110.835, 1107.18, 1119.286, 1110.501], 'val_loss': [2651.632, 2180.48, 2026.409, 1911.109, 1909.482, 1770.292, 1838.68, 1539.287, 1363.918, 1310.445, 1259.267, 1197.918, 1189.655, 1182.9, 1160.467, 1151.894, 1109.059, 1104.974, 1095.091, 1089.496, 1102.236, 1081.476, 1062.14, 1061.609, 1045.619, 1053.497, 1066.48, 1043.874, 1044.7, 1024.527, 1021.049, 1026.99, 1031.403, 1012.829, 1016.732, 1011.516, 1022.286, 1039.668, 1006.854, 1025.959, 1010.454, 997.69, 1000.559, 997.457, 1025.466, 991.488, 992.645, 1009.692, 979.639, 966.712, 1010.125, 984.786, 980.564, 1038.118, 997.275, 964.221, 1006.05, 963.209, 960.792, 959.758, 976.887, 980.888, 969.853, 965.974, 985.619, 947.068, 978.599, 963.836, 968.312, 974.276, 971.911, 956.994, 982.965, 946.751, 964.749, 967.994, 987.867, 956.311, 956.622, 939.516, 956.474, 950.72, 952.195, 953.668, 938.061, 958.726, 938.003, 946.337, 943.692, 947.406, 963.646, 961.975, 942.18, 956.635, 932.011, 937.212, 953.173, 938.941, 937.549, 965.331]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:124 kernel_size:6 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:105 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:deconv1d out_channels:33 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:17 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9152348449086521 weight_decay:6.030364562254975e-05 batch_size:49 epochs:100	100	1000	True	1012.02924		1087870	22	-1	278.5862069129944	{'train_loss': [13060.852, 10198.062, 9270.233, 8574.966, 7880.551, 6838.557, 4139.225, 2042.033, 1664.459, 1502.864, 1430.625, 1374.761, 1341.899, 1310.116, 1307.568, 1278.678, 1277.326, 1275.764, 1268.317, 1241.332, 1225.667, 1221.479, 1215.732, 1213.781, 1202.39, 1194.412, 1197.353, 1190.02, 1187.54, 1188.715, 1181.253, 1171.484, 1164.588, 1157.855, 1154.671, 1157.1, 1147.501, 1140.62, 1146.924, 1147.774, 1139.963, 1137.537, 1145.889, 1143.856, 1136.138, 1128.853, 1130.856, 1149.189, 1136.288, 1128.003, 1144.328, 1128.948, 1119.048, 1126.996, 1118.916, 1117.228, 1112.266, 1123.981, 1111.669, 1103.105, 1102.458, 1102.176, 1100.677, 1101.589, 1101.888, 1091.84, 1101.67, 1095.195, 1116.08, 1111.01, 1104.915, 1112.379, 1119.146, 1113.404, 1113.277, 1090.799, 1090.87, 1085.333, 1088.701, 1102.157, 1090.539, 1081.981, 1083.334, 1080.851, 1085.919, 1083.281, 1088.519, 1093.341, 1080.731, 1086.516, 1080.387, 1088.608, 1100.429, 1081.039, 1080.25, 1078.696, 1086.146, 1095.28, 1080.112, 1085.136], 'val_loss': [10323.697, 8648.98, 7696.189, 6975.049, 6372.636, 5141.305, 2241.475, 1580.929, 1371.332, 1304.814, 1186.799, 1187.423, 1185.151, 1133.575, 1155.418, 1101.136, 1133.375, 1121.512, 1104.66, 1076.883, 1060.821, 1051.649, 1074.694, 1070.211, 1009.441, 1027.281, 1029.137, 1024.229, 1029.106, 989.845, 1010.957, 997.935, 987.289, 981.746, 978.373, 992.221, 1002.25, 983.685, 1020.78, 997.35, 1002.91, 1024.741, 981.373, 961.856, 980.872, 980.883, 1002.564, 954.995, 978.821, 1000.911, 979.461, 965.838, 996.504, 967.385, 971.044, 952.776, 951.33, 957.479, 971.156, 993.946, 968.511, 993.58, 972.503, 950.196, 972.396, 1000.418, 957.795, 948.425, 941.383, 944.017, 979.267, 950.891, 952.73, 940.633, 938.605, 976.473, 956.997, 962.165, 956.369, 940.64, 949.082, 946.0, 965.063, 943.059, 953.317, 932.758, 959.685, 957.963, 958.149, 946.062, 957.334, 960.995, 963.427, 947.453, 938.995, 953.789, 947.497, 946.163, 938.772, 951.62]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:26 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:10 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:34 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:9.901597248723702e-05 batch_size:49 epochs:100	100	2000	True	1008.7099		1039733	22	-1	281.22519969940186	{'train_loss': [11509.119, 7024.081, 6181.818, 5710.69, 5382.566, 5193.991, 4469.465, 3084.08, 2017.973, 1588.57, 1470.24, 1421.787, 1397.359, 1368.015, 1351.605, 1340.654, 1309.774, 1296.969, 1283.895, 1292.073, 1273.758, 1268.74, 1264.335, 1245.453, 1242.935, 1226.765, 1226.612, 1232.234, 1219.884, 1214.086, 1221.001, 1203.947, 1206.713, 1197.96, 1191.199, 1191.424, 1181.996, 1183.501, 1181.875, 1171.175, 1166.367, 1163.647, 1156.997, 1165.011, 1157.851, 1153.937, 1151.348, 1154.582, 1136.12, 1153.129, 1138.7, 1134.583, 1136.37, 1127.438, 1138.762, 1145.88, 1129.748, 1121.004, 1121.073, 1129.584, 1135.264, 1123.57, 1121.131, 1118.607, 1114.853, 1114.746, 1116.235, 1110.352, 1109.76, 1116.38, 1112.736, 1109.813, 1110.933, 1103.737, 1094.312, 1096.413, 1092.043, 1096.643, 1096.916, 1094.536, 1092.793, 1090.991, 1092.496, 1090.482, 1095.009, 1086.027, 1087.373, 1086.915, 1088.941, 1087.098, 1092.505, 1099.71, 1100.378, 1103.854, 1115.836, 1095.952, 1087.668, 1089.246, 1102.107, 1077.979], 'val_loss': [7618.522, 5460.225, 5018.613, 4752.894, 4510.318, 4344.52, 3291.765, 2256.033, 1475.237, 1372.043, 1269.958, 1242.645, 1209.938, 1187.681, 1183.474, 1159.619, 1124.096, 1137.165, 1104.289, 1131.955, 1093.875, 1072.141, 1050.946, 1044.462, 1057.557, 1042.01, 1024.478, 1061.543, 1025.124, 1064.63, 1042.543, 1029.806, 1022.766, 1018.322, 1051.729, 1029.783, 1002.834, 1005.563, 1003.398, 999.084, 980.444, 1003.776, 1005.415, 985.457, 999.85, 995.651, 999.617, 976.48, 1003.207, 1036.191, 990.864, 976.774, 983.305, 966.183, 1019.704, 969.51, 969.285, 971.761, 960.6, 976.323, 974.421, 963.526, 957.573, 968.232, 963.557, 992.183, 963.833, 963.737, 1029.714, 973.582, 952.297, 954.908, 952.347, 953.22, 957.536, 985.9, 984.851, 955.891, 947.559, 944.262, 956.742, 959.735, 944.165, 936.301, 950.31, 953.776, 948.582, 949.023, 951.294, 945.013, 941.556, 950.693, 966.443, 953.994, 952.692, 957.57, 952.289, 1003.905, 945.204, 948.013]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:116 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:127 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:116 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:52 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:17 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9368261487929079 weight_decay:6.030364562254975e-05 batch_size:49 epochs:100	100	1000	True	1027.11633		787662	22	-1	151.08001136779785	{'train_loss': [3755.82, 2634.485, 2363.512, 2222.889, 2150.501, 2097.373, 2041.096, 1992.51, 1921.724, 1783.633, 1668.183, 1545.665, 1470.808, 1444.675, 1413.554, 1420.232, 1397.564, 1373.44, 1365.505, 1358.844, 1343.434, 1323.288, 1307.916, 1303.898, 1299.22, 1297.382, 1293.096, 1277.196, 1271.729, 1259.172, 1246.901, 1252.686, 1243.752, 1233.624, 1240.947, 1233.48, 1227.538, 1218.421, 1225.665, 1225.176, 1214.226, 1205.8, 1200.318, 1192.688, 1198.688, 1186.121, 1186.362, 1204.492, 1199.172, 1194.078, 1186.182, 1190.997, 1181.801, 1182.321, 1185.253, 1183.56, 1184.764, 1174.401, 1178.053, 1182.652, 1190.044, 1173.712, 1165.191, 1167.32, 1158.309, 1166.017, 1165.588, 1153.476, 1152.986, 1138.088, 1145.124, 1155.058, 1171.337, 1159.147, 1160.187, 1151.069, 1157.432, 1157.328, 1155.489, 1153.627, 1149.876, 1143.849, 1151.936, 1146.552, 1144.196, 1143.825, 1131.219, 1138.099, 1145.694, 1142.099, 1139.271, 1136.385, 1129.217, 1135.921, 1131.697, 1130.292, 1130.515, 1125.714, 1133.09, 1137.427], 'val_loss': [2761.32, 2314.094, 2115.303, 1936.042, 1867.971, 1847.436, 1763.891, 1693.521, 1638.957, 1559.725, 1471.444, 1361.013, 1257.843, 1247.176, 1220.543, 1213.353, 1199.305, 1161.117, 1146.849, 1153.997, 1143.15, 1129.843, 1115.454, 1095.774, 1095.077, 1084.584, 1060.161, 1075.276, 1046.142, 1045.409, 1071.505, 1048.731, 1023.647, 1025.084, 1025.79, 1033.3, 1011.382, 1008.593, 1042.818, 1060.589, 1027.128, 1011.492, 1025.761, 998.896, 1003.183, 987.731, 997.183, 1010.792, 993.065, 1016.289, 1018.538, 1003.049, 972.568, 975.941, 988.579, 1012.348, 962.888, 977.626, 980.054, 988.331, 992.435, 979.69, 976.121, 966.489, 992.705, 976.895, 989.324, 987.339, 954.448, 948.241, 948.398, 962.338, 992.595, 973.958, 958.372, 965.419, 979.307, 978.598, 973.365, 975.555, 970.688, 963.241, 969.648, 957.911, 959.844, 965.417, 943.855, 957.107, 948.539, 953.681, 972.438, 963.853, 951.691, 946.895, 945.043, 949.809, 941.828, 941.17, 945.071, 969.638]}	100	100	True
