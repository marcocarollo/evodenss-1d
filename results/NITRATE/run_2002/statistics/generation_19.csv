id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.0009673374036965584 batch_size:49 epochs:100	100	1000	True	989.06152		748280	21	-1	174.89986729621887	{'train_loss': [6088.822, 3215.359, 2779.711, 2461.601, 2294.945, 2223.553, 2167.006, 2100.169, 2066.726, 2039.82, 2017.606, 2009.214, 1985.837, 1964.992, 1961.296, 1929.49, 1826.499, 1642.079, 1513.914, 1443.233, 1397.887, 1368.809, 1353.773, 1343.563, 1339.625, 1303.671, 1289.45, 1284.903, 1268.051, 1268.189, 1262.789, 1249.63, 1239.885, 1232.237, 1226.144, 1226.934, 1220.829, 1217.08, 1210.807, 1210.721, 1203.894, 1206.151, 1216.883, 1205.096, 1201.157, 1199.405, 1191.808, 1182.35, 1178.647, 1185.767, 1199.534, 1192.783, 1178.146, 1182.383, 1185.252, 1186.166, 1168.111, 1169.953, 1172.048, 1169.219, 1170.981, 1181.195, 1170.839, 1176.823, 1168.79, 1164.762, 1161.587, 1159.509, 1159.002, 1168.228, 1151.161, 1149.028, 1157.235, 1151.389, 1147.341, 1146.468, 1144.479, 1145.049, 1153.931, 1150.81, 1139.216, 1142.739, 1135.834, 1134.871, 1127.767, 1129.877, 1126.658, 1127.357, 1144.156, 1134.431, 1136.08, 1129.239, 1133.96, 1131.042, 1131.004, 1135.117, 1121.885, 1124.026, 1139.454, 1127.143], 'val_loss': [4633.943, 2901.364, 2363.442, 2108.066, 1929.934, 1913.835, 1872.339, 1829.789, 1834.232, 1792.744, 1782.295, 1783.637, 1738.133, 1750.703, 1750.577, 1670.834, 1537.781, 1416.029, 1276.707, 1224.129, 1191.253, 1165.371, 1167.988, 1152.041, 1114.927, 1110.816, 1082.41, 1068.53, 1083.965, 1069.624, 1061.759, 1067.708, 1047.791, 1039.67, 1041.939, 1034.905, 1024.041, 1031.224, 1034.768, 1025.146, 1016.523, 1027.879, 1013.076, 1014.835, 1031.911, 1017.776, 1001.042, 1003.155, 1004.661, 1018.237, 1012.794, 990.809, 999.393, 988.075, 1002.305, 984.167, 981.12, 989.638, 992.25, 977.152, 983.965, 972.66, 1011.437, 984.19, 989.284, 980.662, 980.549, 995.115, 990.478, 979.201, 969.753, 965.651, 973.538, 972.696, 985.255, 965.431, 980.798, 961.902, 981.68, 977.927, 974.881, 960.351, 967.438, 951.001, 961.957, 954.182, 958.102, 949.627, 972.274, 982.16, 947.132, 965.477, 971.51, 963.918, 960.963, 964.864, 961.633, 949.302, 970.628, 948.445]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:124 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:6 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:123 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:41 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.000576830538107396 batch_size:49 epochs:100	100	1000	True	999.15106		746024	21	-1	172.45051670074463	{'train_loss': [4862.263, 2847.775, 2543.198, 2375.914, 2237.347, 2117.211, 1957.214, 1799.778, 1658.057, 1595.005, 1515.362, 1471.155, 1434.88, 1421.483, 1397.771, 1396.414, 1372.986, 1353.164, 1343.148, 1334.17, 1326.203, 1315.933, 1303.16, 1299.268, 1287.298, 1302.917, 1281.503, 1293.738, 1281.059, 1277.641, 1267.665, 1265.846, 1264.551, 1258.425, 1259.63, 1247.411, 1249.148, 1249.817, 1238.355, 1234.257, 1232.719, 1230.822, 1235.104, 1239.593, 1221.013, 1209.94, 1214.513, 1211.995, 1220.705, 1211.174, 1198.802, 1193.338, 1201.926, 1190.524, 1191.997, 1194.581, 1189.947, 1191.751, 1177.453, 1176.774, 1180.46, 1184.095, 1200.302, 1174.214, 1175.136, 1175.563, 1176.874, 1176.115, 1175.187, 1172.711, 1178.473, 1174.564, 1163.792, 1167.875, 1157.688, 1156.387, 1155.902, 1152.711, 1155.171, 1158.082, 1153.387, 1145.1, 1166.565, 1164.889, 1149.763, 1150.013, 1148.529, 1141.252, 1150.199, 1157.728, 1143.573, 1133.532, 1135.459, 1140.879, 1161.408, 1144.873, 1143.687, 1141.372, 1156.397, 1141.127], 'val_loss': [4746.354, 2920.331, 2201.371, 2034.803, 1896.346, 1808.056, 1696.129, 1581.984, 1422.978, 1363.203, 1285.278, 1267.083, 1264.344, 1227.852, 1223.655, 1229.629, 1180.461, 1197.993, 1167.151, 1184.242, 1166.447, 1146.455, 1146.638, 1143.761, 1148.439, 1135.502, 1156.553, 1139.393, 1126.243, 1114.055, 1113.118, 1107.795, 1117.326, 1102.252, 1085.903, 1095.419, 1068.922, 1089.262, 1067.406, 1062.384, 1069.621, 1094.449, 1081.294, 1077.552, 1055.676, 1066.595, 1063.811, 1054.19, 1051.341, 1052.484, 1043.216, 1052.765, 1030.691, 1025.793, 1025.584, 1046.094, 1036.106, 1023.909, 1017.136, 1017.756, 1030.58, 1030.802, 1027.11, 1011.88, 1011.034, 1023.17, 1023.991, 1048.484, 1008.477, 1031.541, 1020.956, 1004.05, 1021.366, 1000.402, 1008.653, 1008.374, 999.541, 997.417, 999.82, 1000.312, 994.105, 986.508, 1008.832, 996.572, 1021.383, 999.988, 1002.701, 996.08, 999.377, 997.673, 992.067, 981.233, 989.812, 984.077, 1000.366, 1018.241, 1010.299, 994.841, 1005.166, 991.48]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:rmsprop lr:0.002460715196435833 alpha:0.9403593350358894 weight_decay:0.0009673374036965584 batch_size:49 epochs:100	100	1000	True	1026.13623		735330	20	-1	161.2317509651184	{'train_loss': [17025.924, 9576.771, 3698.258, 3395.708, 3158.5, 2974.481, 2689.457, 2479.879, 2331.987, 2243.025, 2190.655, 2163.042, 2131.799, 2124.181, 2113.313, 2093.168, 1985.639, 1735.978, 1626.604, 1560.477, 1510.13, 1507.772, 1455.704, 1468.214, 1447.153, 1437.9, 1420.659, 1413.161, 1414.608, 1384.791, 1388.956, 1385.051, 1379.027, 1360.635, 1351.599, 1377.97, 1346.472, 1347.307, 1343.177, 1337.788, 1328.605, 1317.558, 1318.305, 1324.311, 1310.904, 1326.987, 1311.692, 1296.378, 1314.399, 1301.02, 1282.313, 1295.777, 1287.822, 1290.904, 1283.072, 1288.476, 1285.708, 1266.888, 1272.12, 1268.146, 1269.495, 1263.013, 1252.955, 1259.488, 1254.967, 1249.926, 1279.271, 1239.347, 1248.124, 1236.44, 1248.801, 1244.661, 1243.368, 1233.839, 1234.979, 1229.758, 1227.208, 1239.369, 1223.176, 1226.06, 1231.884, 1214.429, 1226.407, 1224.322, 1221.441, 1214.992, 1223.446, 1201.261, 1212.41, 1217.968, 1219.856, 1208.326, 1201.292, 1193.462, 1215.326, 1198.073, 1197.793, 1198.177, 1197.33, 1201.327], 'val_loss': [11447.041, 3133.161, 2714.995, 2184.502, 2380.306, 2358.405, 2129.442, 2033.469, 2025.411, 1877.771, 1814.904, 1851.912, 1823.28, 1851.749, 1789.185, 1773.211, 1591.925, 1336.29, 1355.111, 1258.132, 1433.964, 1275.902, 1537.628, 1482.01, 1214.463, 1312.835, 1298.797, 1297.019, 1289.24, 1277.696, 1207.377, 1309.049, 1162.708, 1293.008, 1198.506, 1218.524, 1206.508, 1198.244, 1157.501, 1221.208, 1197.376, 1166.487, 1173.923, 1132.974, 1126.858, 1100.682, 1372.546, 1212.416, 1168.977, 1079.028, 1257.08, 1097.437, 1135.11, 1137.667, 1151.935, 1132.063, 1064.773, 1092.918, 1169.247, 1185.951, 1159.72, 1150.38, 1089.296, 1130.272, 1159.691, 1075.794, 1137.71, 1206.524, 1105.348, 1112.503, 1167.014, 1122.014, 1084.752, 1118.82, 1062.954, 1127.767, 1081.731, 1078.697, 1112.86, 1102.348, 1085.467, 1088.585, 1068.724, 1090.652, 1102.574, 1045.612, 1042.169, 1034.456, 1039.934, 1057.739, 1077.772, 1044.364, 1089.383, 1055.446, 1061.464, 1066.725, 1055.528, 1169.621, 1026.54, 1030.292]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:47 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:52 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.008195831421056341 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.0009673374036965584 batch_size:49 epochs:100	100	1000	True	1498.94678		501988	21	-1	149.26400113105774	{'train_loss': [5618.85, 2732.219, 2264.084, 2080.675, 2037.279, 2019.237, 2005.298, 2020.01, 1962.684, 1970.428, 1974.298, 1937.179, 1934.152, 1921.224, 1931.628, 1998.38, 1915.035, 1904.09, 1893.422, 1881.034, 1854.839, 1904.627, 1872.967, 1919.248, 1878.163, 1901.537, 1853.631, 1874.969, 1856.133, 1853.784, 1853.177, 1850.967, 1837.406, 1871.48, 1853.353, 1844.026, 1852.602, 1841.968, 1898.09, 1920.846, 1817.359, 1816.723, 1867.821, 1845.514, 1850.414, 1813.984, 1838.167, 1816.378, 1869.36, 1798.144, 1817.238, 1811.862, 1824.017, 1802.987, 1805.09, 1783.188, 1811.478, 1780.218, 1813.49, 1851.709, 1815.804, 1801.284, 1831.632, 1853.216, 1819.794, 1790.511, 1754.504, 1799.071, 1793.594, 1783.478, 1793.308, 1759.806, 1736.735, 1764.151, 1795.815, 1769.732, 1809.692, 1869.043, 1818.678, 1791.879, 1803.632, 1824.599, 1791.485, 1765.585, 1762.147, 1788.538, 1770.644, 1773.12, 1784.848, 1793.134, 1803.661, 1869.318, 1736.282, 1718.662, 1749.84, 1791.027, 1778.77, 1742.801, 1761.671, 1757.471], 'val_loss': [4265.434, 3165.063, 1968.691, 1843.501, 1810.644, 1790.806, 1805.126, 1804.987, 1769.032, 1806.337, 1785.31, 1719.734, 1719.366, 1717.786, 1680.117, 1754.214, 1690.459, 1678.624, 1630.802, 1628.242, 1681.905, 1666.118, 1725.988, 1671.883, 1674.316, 1721.782, 1633.172, 1617.931, 1646.937, 1572.507, 1649.691, 1569.269, 1573.676, 1649.618, 1689.867, 1581.602, 1596.565, 1597.753, 1684.761, 1614.932, 1600.607, 1591.061, 1614.159, 1570.882, 1632.51, 1605.202, 1561.55, 1663.864, 1610.495, 1589.132, 1564.894, 1605.699, 1559.996, 1632.481, 1530.624, 1628.871, 1522.696, 1502.278, 1683.876, 1580.846, 1644.271, 1523.496, 1654.237, 1611.18, 1619.547, 1508.906, 1583.034, 1507.828, 1551.933, 1540.354, 1559.332, 1544.075, 1491.951, 1541.344, 1556.965, 1497.28, 1679.63, 1580.719, 1540.997, 1536.063, 1582.12, 1575.126, 1489.399, 1571.365, 1508.69, 1521.805, 1542.28, 1550.602, 1591.542, 1561.16, 1697.09, 1511.792, 1516.773, 1504.973, 1544.891, 1589.623, 1544.049, 1616.997, 1474.113, 1540.933]}	100	100	True
