id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:35 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:126 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:3.381674346430075e-05 batch_size:49 epochs:100	100	1000	True	1018.82947		641438	20	-1	226.61983275413513	{'train_loss': [3795.994, 2592.112, 2374.635, 2266.383, 2190.001, 2011.396, 1814.607, 1680.199, 1587.681, 1515.696, 1476.602, 1444.826, 1434.68, 1399.973, 1383.796, 1352.765, 1349.692, 1354.349, 1332.134, 1320.272, 1315.821, 1290.824, 1287.907, 1280.669, 1262.664, 1263.017, 1264.183, 1266.128, 1254.538, 1249.049, 1252.835, 1243.627, 1232.636, 1226.88, 1233.014, 1225.623, 1225.498, 1219.214, 1214.508, 1212.351, 1210.743, 1196.378, 1207.422, 1209.445, 1203.011, 1193.875, 1183.34, 1186.462, 1182.806, 1189.91, 1189.979, 1187.733, 1183.295, 1180.558, 1181.937, 1171.607, 1177.392, 1166.208, 1165.712, 1162.51, 1169.73, 1162.387, 1167.033, 1173.324, 1156.854, 1158.288, 1153.162, 1151.389, 1156.501, 1152.083, 1146.589, 1156.273, 1150.057, 1145.973, 1153.659, 1147.898, 1143.372, 1141.047, 1144.229, 1144.091, 1141.654, 1139.29, 1138.898, 1143.859, 1138.335, 1138.103, 1135.868, 1142.382, 1142.59, 1144.77, 1130.212, 1138.665, 1135.774, 1128.317, 1132.124, 1128.463, 1127.324, 1128.536, 1129.681, 1125.951], 'val_loss': [2982.039, 2216.935, 2053.596, 1965.035, 1877.245, 1714.261, 1597.966, 1425.979, 1405.46, 1350.839, 1307.078, 1275.299, 1317.484, 1240.239, 1245.446, 1185.095, 1196.634, 1150.853, 1159.535, 1161.757, 1132.389, 1160.642, 1109.085, 1101.932, 1115.381, 1104.535, 1086.672, 1084.549, 1087.248, 1063.958, 1068.26, 1069.839, 1070.455, 1057.245, 1053.026, 1045.35, 1055.45, 1050.903, 1036.787, 1040.129, 1026.112, 1028.901, 1026.753, 1039.617, 1022.89, 1017.383, 1028.466, 1013.379, 1012.304, 1013.461, 1037.659, 1010.238, 1020.013, 1006.4, 1017.626, 1004.835, 997.751, 999.93, 996.487, 1000.064, 1007.351, 997.181, 996.198, 1004.102, 994.704, 997.762, 996.633, 991.394, 994.235, 994.442, 989.101, 1006.985, 996.179, 991.896, 1009.419, 1008.863, 999.947, 992.404, 992.328, 993.403, 993.128, 996.544, 980.194, 995.755, 988.169, 987.207, 983.865, 984.229, 987.544, 987.368, 994.401, 978.42, 990.451, 1014.271, 993.395, 980.852, 980.099, 990.375, 984.696, 984.393]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:79 kernel_size:10 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:6 layer:conv1d out_channels:92 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:35 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:57 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:126 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:17 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:3.381674346430075e-05 batch_size:61 epochs:100	100	1000	True	1020.84204		570408	22	-1	194.86785697937012	{'train_loss': [4463.031, 3014.896, 2801.29, 2668.187, 2580.616, 2325.197, 2108.273, 1959.557, 1888.085, 1840.992, 1823.073, 1783.868, 1755.622, 1737.127, 1710.408, 1709.917, 1686.266, 1662.176, 1659.052, 1642.322, 1646.918, 1640.052, 1627.195, 1637.961, 1607.216, 1613.23, 1607.949, 1602.886, 1594.754, 1589.43, 1575.578, 1577.917, 1588.371, 1597.631, 1601.823, 1586.815, 1586.196, 1560.099, 1570.25, 1579.642, 1567.385, 1571.014, 1576.757, 1557.195, 1544.3, 1542.242, 1534.701, 1532.432, 1533.278, 1532.117, 1524.082, 1523.689, 1516.648, 1523.296, 1508.277, 1508.985, 1488.668, 1500.164, 1510.004, 1478.447, 1482.117, 1489.518, 1491.351, 1473.53, 1492.615, 1498.014, 1490.126, 1482.636, 1475.449, 1464.312, 1477.478, 1472.095, 1486.816, 1468.196, 1466.717, 1473.576, 1465.235, 1475.853, 1458.736, 1453.059, 1449.778, 1442.784, 1442.263, 1449.386, 1438.232, 1429.643, 1428.918, 1436.742, 1425.408, 1426.433, 1438.083, 1427.444, 1444.335, 1438.056, 1420.958, 1425.744, 1422.284, 1431.264, 1420.91, 1432.137], 'val_loss': [3847.789, 2314.162, 2041.708, 1870.1, 1810.079, 1570.695, 1472.014, 1439.073, 1425.268, 1364.433, 1264.042, 1240.62, 1238.939, 1239.477, 1188.989, 1179.104, 1137.059, 1145.717, 1128.255, 1123.91, 1128.771, 1136.817, 1125.516, 1132.134, 1118.781, 1137.287, 1119.121, 1102.244, 1089.575, 1087.779, 1092.902, 1094.383, 1089.518, 1115.906, 1132.464, 1093.187, 1106.43, 1090.3, 1075.692, 1083.495, 1110.338, 1147.394, 1158.771, 1084.978, 1074.917, 1064.872, 1094.498, 1087.12, 1069.827, 1057.648, 1065.924, 1074.656, 1077.97, 1058.513, 1072.892, 1095.714, 1037.454, 1071.537, 1043.948, 1053.699, 1024.515, 1024.344, 1036.751, 1019.554, 1033.017, 1128.489, 1040.019, 1021.952, 1015.715, 1021.882, 1038.582, 1045.83, 1055.804, 1046.168, 1044.395, 1071.933, 1042.904, 1079.417, 1039.298, 1033.35, 1038.067, 1015.85, 1006.928, 1025.28, 1004.999, 1012.364, 1018.86, 989.786, 996.585, 1008.295, 1031.659, 1002.15, 1057.485, 1010.358, 1005.707, 1000.972, 992.156, 1005.816, 991.27, 1005.895]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:34 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:35 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:126 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.002460715196435833 beta1:0.9990019396144919 beta2:0.9711402723837911 weight_decay:3.381674346430075e-05 batch_size:49 epochs:100	100	1000	True	4059.60913		613048	19	-1	208.8758943080902	{'train_loss': [3840.943, 2933.537, 2804.974, 2684.567, 2615.271, 2561.518, 2514.7, 2432.636, 2353.199, 2309.146, 2255.991, 2224.434, 2172.022, 2194.396, 2182.378, 2154.927, 2139.162, 2125.232, 2133.583, 2139.642, 2129.913, 2133.401, 2140.103, 2158.49, 2183.738, 2181.799, 2211.302, 2227.328, 2288.875, 2295.15, 2305.864, 2287.111, 2318.809, 2329.362, 2373.371, 2409.026, 2446.665, 2478.677, 2552.589, 2577.818, 2576.766, 2611.862, 2670.576, 2575.027, 2750.129, 2780.991, 2752.718, 2730.231, 2764.292, 2840.814, 2773.271, 2758.77, 2836.418, 2742.8, 2730.463, 2774.348, 2891.075, 2912.923, 2698.766, 2834.833, 3068.105, 2708.091, 2984.769, 3029.216, 2993.015, 2916.522, 3227.604, 3102.402, 2941.174, 3215.805, 2965.583, 3064.219, 2871.646, 3215.153, 2911.0, 3180.095, 3332.817, 2976.088, 3518.106, 2954.09, 3526.08, 2958.345, 3336.34, 3195.755, 3161.677, 3332.964, 3072.59, 3475.458, 3045.729, 3376.978, 3202.675, 3156.601, 3393.403, 3137.107, 3389.042, 3113.875, 3516.734, 3253.378, 3305.207, 3501.591], 'val_loss': [3266.531, 2817.586, 2582.07, 2466.721, 2393.881, 2345.88, 2225.788, 2130.858, 2087.137, 2112.014, 2054.667, 2224.215, 2099.31, 2184.661, 2205.423, 2149.417, 2095.934, 2064.107, 2007.316, 2156.898, 1963.161, 1973.2, 1878.457, 1875.467, 1857.058, 2135.016, 1875.944, 2039.818, 2070.849, 1863.757, 1880.615, 2173.647, 2249.297, 2115.004, 1960.569, 2052.257, 2299.671, 2535.839, 2003.791, 2468.081, 1996.463, 2569.469, 2070.652, 2300.602, 2700.72, 2572.867, 2250.723, 2080.991, 2114.175, 2113.582, 2044.876, 2146.511, 2393.178, 2612.071, 2521.581, 2273.7, 2100.595, 2413.203, 2691.343, 2126.965, 2362.232, 2616.237, 2229.899, 2282.048, 2770.592, 2633.841, 2443.019, 2859.457, 2365.667, 2728.854, 2459.61, 2771.544, 2257.523, 2471.362, 2917.38, 2361.563, 2853.428, 2889.462, 2509.438, 2645.957, 2516.266, 2620.457, 3021.879, 2309.653, 2830.086, 2731.134, 2679.305, 3075.388, 2360.319, 3189.972, 2356.46, 2994.723, 2925.167, 2582.876, 2500.637, 3092.99, 3208.939, 2889.867, 2941.775, 2754.277]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:6.030364562254975e-05 batch_size:49 epochs:100	100	1000	True	981.44867		512391	21	-1	188.40472292900085	{'train_loss': [3359.946, 2505.287, 2371.866, 2266.402, 2168.048, 2148.606, 2118.088, 2090.273, 2054.849, 2063.749, 2003.666, 2013.716, 1990.69, 1949.335, 1948.973, 1938.218, 1829.433, 1651.446, 1523.739, 1468.308, 1419.562, 1388.556, 1362.742, 1365.665, 1350.962, 1346.956, 1314.034, 1298.026, 1290.199, 1290.171, 1284.368, 1268.141, 1274.32, 1260.017, 1245.464, 1250.911, 1240.285, 1244.906, 1248.929, 1247.218, 1233.383, 1224.238, 1225.608, 1220.498, 1221.469, 1206.073, 1206.716, 1205.78, 1193.847, 1193.034, 1198.957, 1205.8, 1182.29, 1179.879, 1180.831, 1189.69, 1187.009, 1206.107, 1200.006, 1191.399, 1177.14, 1179.806, 1184.718, 1185.104, 1182.425, 1170.041, 1164.749, 1167.829, 1166.26, 1165.247, 1154.501, 1165.794, 1174.684, 1173.35, 1175.968, 1157.998, 1149.676, 1148.946, 1150.111, 1147.624, 1148.476, 1151.641, 1151.279, 1148.341, 1156.916, 1143.643, 1138.391, 1143.078, 1142.997, 1141.039, 1145.551, 1133.563, 1132.014, 1134.207, 1138.966, 1144.303, 1129.023, 1122.659, 1136.255, 1137.018], 'val_loss': [2692.565, 2248.877, 2087.304, 1949.685, 1937.003, 1928.351, 1860.171, 1841.602, 1873.462, 1839.931, 1772.561, 1762.803, 1749.355, 1746.248, 1717.369, 1675.714, 1505.741, 1362.782, 1287.664, 1251.262, 1218.965, 1188.143, 1174.507, 1144.003, 1143.489, 1138.095, 1101.795, 1119.183, 1094.289, 1101.589, 1078.724, 1101.741, 1073.693, 1065.862, 1067.106, 1103.309, 1054.49, 1060.075, 1048.387, 1049.164, 1035.095, 1030.169, 1044.161, 1010.206, 1018.183, 1020.166, 1036.817, 1023.189, 1019.256, 1008.753, 1022.055, 1005.967, 991.747, 994.71, 1012.228, 1011.475, 998.276, 1011.18, 1015.993, 1010.423, 986.495, 980.904, 1002.295, 989.165, 998.453, 977.572, 975.942, 987.103, 983.959, 981.763, 974.526, 999.302, 983.329, 972.004, 1012.164, 973.924, 972.594, 970.357, 973.685, 977.843, 964.009, 972.169, 981.006, 982.678, 983.071, 960.077, 964.305, 970.607, 959.883, 998.01, 961.696, 955.822, 967.49, 961.528, 971.324, 970.854, 945.15, 955.215, 973.669, 936.906]}	100	100	True
