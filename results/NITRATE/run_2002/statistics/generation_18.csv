id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:67 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:6.030364562254975e-05 batch_size:49 epochs:100	100	1000	True	1049.27295		457804	21	-1	132.38634371757507	{'train_loss': [3337.447, 2435.9, 2253.143, 2113.849, 2093.916, 2062.268, 2051.637, 2030.08, 2007.56, 2007.414, 2005.568, 2004.941, 1975.658, 1965.371, 1964.83, 1938.803, 1944.205, 1930.157, 1915.941, 1924.647, 1909.97, 1895.658, 1929.754, 1878.783, 1872.235, 1851.289, 1811.646, 1801.259, 1795.372, 1792.048, 1738.691, 1685.936, 1582.827, 1553.959, 1480.938, 1448.508, 1411.895, 1387.349, 1377.131, 1351.639, 1339.084, 1320.143, 1301.315, 1309.98, 1295.309, 1301.926, 1278.129, 1283.46, 1271.717, 1258.111, 1258.082, 1253.955, 1254.333, 1247.67, 1237.803, 1233.875, 1232.162, 1225.962, 1226.235, 1217.422, 1233.606, 1218.706, 1223.512, 1207.901, 1206.566, 1216.365, 1202.49, 1203.968, 1197.453, 1202.376, 1192.964, 1197.286, 1193.188, 1195.894, 1190.677, 1189.251, 1181.946, 1177.346, 1180.902, 1184.245, 1181.834, 1178.851, 1175.346, 1173.728, 1163.603, 1161.786, 1166.795, 1166.234, 1170.453, 1160.034, 1158.335, 1161.093, 1157.307, 1165.474, 1164.399, 1163.291, 1160.932, 1159.92, 1160.848, 1161.305], 'val_loss': [4301.902, 2940.106, 2083.472, 1850.952, 1821.234, 1867.083, 1793.371, 1790.1, 1810.616, 1772.732, 1798.662, 1804.436, 1763.789, 1743.022, 1772.736, 1736.404, 1720.609, 1759.63, 1690.143, 1743.989, 1699.213, 1706.653, 1702.806, 1668.355, 1679.344, 1593.305, 1537.881, 1562.835, 1657.569, 1516.955, 1544.616, 1398.088, 1349.828, 1319.33, 1259.787, 1234.418, 1194.456, 1178.945, 1152.704, 1145.93, 1126.262, 1159.631, 1092.03, 1103.968, 1144.532, 1098.743, 1080.972, 1106.326, 1074.008, 1055.748, 1073.599, 1061.148, 1084.318, 1051.766, 1072.033, 1028.163, 1041.96, 1040.554, 1047.147, 1014.491, 1055.117, 1005.152, 1034.255, 1009.751, 1049.311, 1020.318, 1038.84, 1010.979, 1044.872, 1012.06, 1009.706, 1028.082, 1019.824, 1009.15, 1015.949, 1022.032, 994.368, 984.434, 985.894, 992.042, 1005.002, 992.254, 1007.665, 974.697, 971.226, 968.232, 981.895, 995.288, 982.174, 968.451, 985.457, 1009.823, 972.349, 999.99, 977.568, 988.905, 971.515, 988.36, 991.014, 976.195]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.0009673374036965584 batch_size:49 epochs:100	100	1000	True	1015.25269		748280	21	-1	146.5376694202423	{'train_loss': [5357.452, 2987.756, 2655.364, 2485.424, 2370.292, 2288.095, 2206.374, 2134.9, 2084.456, 2092.099, 2059.761, 2060.669, 2086.012, 2007.809, 1997.329, 1978.325, 1981.581, 1954.337, 1919.156, 1808.344, 1658.85, 1527.02, 1460.602, 1417.039, 1398.2, 1378.815, 1372.744, 1358.473, 1330.642, 1325.032, 1305.41, 1300.861, 1287.678, 1285.392, 1280.623, 1264.442, 1268.849, 1265.447, 1244.029, 1238.78, 1237.944, 1234.735, 1228.079, 1228.418, 1222.485, 1222.986, 1225.685, 1217.439, 1209.547, 1203.057, 1200.905, 1193.337, 1199.578, 1190.69, 1194.708, 1190.025, 1193.87, 1190.721, 1186.831, 1176.285, 1168.71, 1175.215, 1169.97, 1166.67, 1168.202, 1177.6, 1165.714, 1163.636, 1162.865, 1164.025, 1171.271, 1168.505, 1165.801, 1154.586, 1153.386, 1149.408, 1146.758, 1153.422, 1140.425, 1139.608, 1144.128, 1140.222, 1140.777, 1135.832, 1129.951, 1139.591, 1166.136, 1145.925, 1139.557, 1136.975, 1133.883, 1125.111, 1136.376, 1133.025, 1130.177, 1133.98, 1139.284, 1121.316, 1128.317, 1125.181], 'val_loss': [3711.583, 2497.728, 2254.398, 2121.851, 2072.993, 1934.978, 1871.993, 1823.563, 1840.187, 1826.964, 1807.596, 1868.306, 1789.874, 1813.571, 1766.48, 1761.795, 1757.965, 1728.355, 1713.656, 1499.508, 1488.29, 1341.86, 1262.184, 1214.826, 1179.343, 1231.258, 1194.056, 1144.611, 1135.393, 1123.054, 1107.772, 1093.75, 1098.973, 1112.928, 1099.371, 1096.447, 1096.564, 1089.59, 1067.798, 1082.062, 1084.69, 1066.69, 1056.511, 1060.699, 1071.202, 1044.414, 1047.265, 1054.945, 1012.698, 1044.517, 1027.953, 1018.093, 1004.737, 1026.114, 1008.684, 1010.236, 1007.985, 1018.069, 990.97, 982.517, 998.234, 976.768, 983.152, 986.364, 993.259, 983.453, 972.513, 980.983, 977.481, 993.339, 990.088, 1029.677, 985.685, 979.161, 980.81, 961.815, 953.903, 989.46, 952.479, 973.092, 973.339, 956.789, 957.124, 965.211, 957.769, 984.503, 968.457, 961.994, 975.881, 953.163, 965.037, 945.378, 973.437, 951.559, 979.99, 967.667, 970.07, 974.85, 959.248, 941.55]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:38 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:67 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:rmsprop lr:0.002460715196435833 alpha:0.8264060461189374 weight_decay:6.030364562254975e-05 batch_size:49 epochs:100	100	1000	True	1023.93201		598456	21	-1	223.0232229232788	{'train_loss': [9201.79, 8636.821, 5355.977, 4063.425, 3105.885, 2789.87, 2599.825, 2451.769, 2366.289, 2263.535, 2230.465, 2116.583, 1960.031, 1839.344, 1723.683, 1678.346, 1612.05, 1560.116, 1536.727, 1497.937, 1481.254, 1452.545, 1421.367, 1412.875, 1394.684, 1387.215, 1377.576, 1360.228, 1362.152, 1352.073, 1334.701, 1330.306, 1324.204, 1321.146, 1322.849, 1312.101, 1306.016, 1302.876, 1300.362, 1293.654, 1293.622, 1282.17, 1276.55, 1275.755, 1276.217, 1266.082, 1268.191, 1265.854, 1267.138, 1255.248, 1255.953, 1251.859, 1249.369, 1250.937, 1244.078, 1247.471, 1241.653, 1232.293, 1233.755, 1237.392, 1229.252, 1232.38, 1228.171, 1230.234, 1218.745, 1222.389, 1217.947, 1214.931, 1213.34, 1210.204, 1211.922, 1208.406, 1208.177, 1203.758, 1201.872, 1203.323, 1192.572, 1205.324, 1191.321, 1191.425, 1196.05, 1194.516, 1187.296, 1190.186, 1189.325, 1185.047, 1182.628, 1184.236, 1181.978, 1182.148, 1179.567, 1180.529, 1174.236, 1171.65, 1173.224, 1171.826, 1175.014, 1167.666, 1171.275, 1165.93], 'val_loss': [7924.357, 5090.229, 3164.31, 2312.613, 3223.083, 2028.863, 2052.53, 2320.477, 2018.88, 2311.513, 2341.641, 2065.514, 2664.824, 1521.281, 1519.003, 1990.255, 1401.638, 1372.492, 1474.945, 1432.226, 1597.151, 1223.053, 1456.73, 1301.367, 1445.158, 1226.087, 1176.051, 1203.504, 1335.973, 1227.872, 1130.43, 1167.279, 1148.169, 1111.045, 1132.088, 1195.484, 1087.945, 1076.99, 1403.071, 1153.285, 1192.68, 1071.381, 1092.945, 1183.148, 1087.44, 1092.091, 1050.483, 1112.605, 1142.602, 1112.063, 1165.546, 1150.977, 1150.842, 1048.077, 1131.171, 1014.692, 1048.225, 1141.428, 1171.098, 1145.173, 1145.051, 1172.549, 1193.867, 1211.147, 1044.433, 1072.109, 1038.898, 1072.206, 1141.72, 1232.987, 1109.211, 1236.334, 1125.638, 1128.881, 1187.934, 1164.004, 1159.496, 1027.841, 1005.027, 1140.113, 1127.353, 1059.332, 1134.195, 996.526, 1040.598, 1026.259, 1077.981, 1077.877, 1058.533, 1042.114, 1106.177, 1058.535, 1076.643, 1112.532, 1001.055, 1130.75, 1063.885, 1046.908, 1055.631, 1033.525]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:83 kernel_size:7 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:90 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:75 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:17 layer:conv1d out_channels:67 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:6.030364562254975e-05 batch_size:81 epochs:100	100	1000	True	1577.31921		614274	22	-1	172.06217694282532	{'train_loss': [6225.065, 4411.342, 4176.617, 3905.406, 3693.57, 3476.04, 3353.391, 3210.327, 3052.894, 2884.51, 2748.372, 2592.97, 2547.518, 2501.802, 2467.421, 2435.786, 2372.865, 2325.018, 2299.718, 2282.32, 2239.458, 2231.362, 2219.552, 2199.346, 2167.89, 2141.23, 2147.681, 2127.583, 2131.763, 2131.272, 2121.72, 2110.383, 2086.515, 2071.014, 2035.933, 2027.991, 2056.409, 2040.273, 2011.814, 2019.106, 2009.644, 2013.315, 2004.297, 1979.253, 1998.453, 1985.591, 1972.469, 1974.188, 1968.163, 1960.423, 1963.192, 1951.443, 1934.727, 1934.685, 1929.407, 1942.766, 1928.979, 1922.61, 1934.211, 1934.587, 1923.578, 1911.842, 1915.294, 1922.719, 1907.95, 1903.217, 1929.023, 1913.578, 1910.38, 1912.146, 1932.534, 1955.525, 1920.39, 1899.58, 1887.549, 1899.761, 1908.582, 1928.107, 1930.37, 1881.97, 1892.863, 1879.517, 1885.587, 1888.865, 1920.25, 1903.365, 1887.679, 1901.155, 1891.42, 1870.801, 1865.165, 1861.507, 1858.252, 1868.181, 1871.151, 1870.641, 1881.723, 1859.625, 1876.035, 1897.314], 'val_loss': [6613.707, 4677.285, 3847.046, 3444.806, 3089.218, 2756.598, 2755.469, 3368.667, 2391.118, 2502.965, 2172.018, 2130.042, 2058.058, 2009.951, 2008.835, 1918.158, 2112.539, 1875.219, 1769.065, 1949.085, 1805.949, 1845.398, 1795.0, 1792.735, 1773.18, 1718.448, 1693.887, 1678.845, 1765.103, 1710.865, 1705.574, 1671.018, 1669.053, 1635.436, 1567.93, 1563.073, 1649.341, 1582.476, 1588.367, 1564.477, 1664.792, 1661.615, 1539.697, 1598.693, 1575.534, 1592.064, 1563.773, 1556.138, 1538.337, 1493.142, 1536.316, 1499.823, 1534.408, 1522.901, 1525.415, 1547.608, 1504.31, 1466.184, 1544.71, 1496.313, 1496.876, 1481.448, 1488.323, 1446.852, 1462.437, 1511.978, 1481.351, 1551.641, 1460.22, 1459.333, 1561.727, 1448.843, 1466.896, 1463.611, 1459.052, 1455.431, 1476.76, 1447.546, 1500.764, 1459.198, 1450.356, 1449.755, 1449.426, 1490.05, 1454.856, 1427.543, 1440.864, 1433.137, 1422.564, 1459.285, 1433.148, 1410.346, 1420.959, 1439.744, 1459.097, 1470.626, 1440.577, 1427.265, 1447.928, 1509.867]}	100	100	True
