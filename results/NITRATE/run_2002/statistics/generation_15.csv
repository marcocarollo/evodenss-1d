id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:6.030364562254975e-05 batch_size:49 epochs:100	100	2000	True	1051.78113		512391	21	-1	175.47094249725342	{'train_loss': [3528.694, 2494.378, 2269.89, 2165.255, 2115.644, 2065.57, 1934.219, 1749.057, 1636.223, 1570.06, 1520.728, 1470.478, 1447.085, 1440.373, 1403.596, 1384.112, 1376.403, 1361.258, 1361.205, 1350.819, 1334.45, 1335.161, 1307.363, 1300.174, 1298.323, 1291.136, 1291.903, 1279.244, 1279.462, 1271.057, 1280.062, 1278.587, 1272.753, 1256.929, 1255.911, 1255.856, 1238.058, 1232.477, 1228.04, 1233.713, 1224.816, 1223.688, 1212.47, 1210.258, 1210.323, 1208.707, 1212.189, 1216.583, 1209.36, 1216.489, 1206.44, 1213.778, 1197.853, 1197.281, 1193.373, 1211.151, 1208.329, 1188.482, 1193.112, 1194.77, 1201.114, 1191.249, 1192.832, 1183.421, 1177.902, 1181.627, 1171.883, 1171.85, 1171.908, 1179.005, 1174.635, 1169.329, 1162.697, 1165.238, 1171.851, 1171.112, 1158.279, 1161.459, 1160.183, 1161.055, 1166.393, 1168.217, 1159.061, 1160.308, 1162.901, 1162.081, 1159.002, 1161.111, 1160.162, 1155.779, 1152.243, 1152.335, 1149.192, 1148.795, 1147.763, 1142.945, 1143.053, 1144.683, 1138.018, 1146.74], 'val_loss': [2759.427, 2199.214, 1919.942, 1912.691, 1907.049, 1785.202, 1658.871, 1479.026, 1363.924, 1355.024, 1413.096, 1245.848, 1270.475, 1248.197, 1266.561, 1185.253, 1219.666, 1175.359, 1186.755, 1156.628, 1114.417, 1111.793, 1118.534, 1112.188, 1118.604, 1098.187, 1107.797, 1065.965, 1054.144, 1079.831, 1078.631, 1080.646, 1049.226, 1052.954, 1052.948, 1027.585, 1005.335, 1030.207, 1026.188, 1025.12, 1006.835, 1021.851, 1005.113, 999.718, 1000.161, 997.803, 1023.523, 1013.763, 1008.457, 1008.206, 999.652, 1025.822, 1003.138, 1008.383, 985.936, 1080.693, 1029.641, 994.576, 1002.755, 993.721, 1043.42, 1024.441, 1020.75, 1024.459, 1018.481, 997.733, 995.749, 985.768, 1019.611, 1051.684, 1046.968, 1005.243, 985.887, 987.142, 1006.527, 1044.188, 995.912, 992.822, 989.536, 1008.174, 997.925, 1028.57, 1006.63, 1013.151, 1000.06, 1014.189, 1003.273, 1020.526, 1020.746, 980.972, 994.428, 969.577, 980.181, 983.196, 976.559, 983.852, 977.069, 979.489, 978.438, 1032.662]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:57 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:14 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:52 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:17 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:6.030364562254975e-05 batch_size:67 epochs:100	100	1000	True	1484.49683		768509	22	-1	302.5919487476349	{'train_loss': [8557.382, 4744.968, 3918.928, 3643.042, 3312.555, 3169.366, 3069.93, 3011.338, 2968.503, 2896.841, 2791.641, 2601.846, 2383.197, 2234.188, 2142.124, 2024.334, 1970.825, 1926.403, 1913.111, 1873.442, 1841.766, 1849.9, 1843.921, 1834.725, 1800.032, 1779.37, 1774.196, 1758.215, 1740.858, 1720.115, 1723.106, 1734.874, 1723.653, 1743.037, 1706.711, 1695.799, 1700.228, 1695.205, 1694.526, 1651.782, 1647.349, 1666.661, 1651.999, 1651.617, 1635.182, 1653.393, 1623.022, 1624.672, 1640.716, 1623.158, 1619.656, 1593.676, 1615.349, 1596.771, 1600.082, 1607.324, 1595.111, 1589.21, 1575.678, 1584.151, 1587.178, 1579.626, 1584.699, 1583.598, 1580.603, 1565.568, 1580.236, 1549.622, 1565.709, 1568.709, 1557.879, 1553.893, 1558.445, 1554.946, 1541.172, 1539.734, 1540.831, 1555.824, 1567.22, 1555.008, 1538.594, 1540.067, 1536.938, 1549.901, 1549.262, 1542.382, 1555.99, 1553.98, 1526.38, 1523.305, 1534.11, 1531.61, 1527.903, 1533.492, 1530.768, 1521.876, 1537.394, 1532.97, 1518.751, 1512.775], 'val_loss': [7082.248, 4475.575, 3859.485, 3521.475, 3272.153, 3045.319, 2972.127, 2955.036, 2902.708, 2736.708, 2701.507, 2576.156, 2403.431, 2236.49, 2112.592, 1998.867, 1919.146, 1873.094, 1865.656, 1779.13, 1785.594, 1768.017, 1781.666, 1751.518, 1752.145, 1691.202, 1719.259, 1694.737, 1749.995, 1695.521, 1652.585, 1678.65, 1702.559, 1744.117, 1635.182, 1631.419, 1695.893, 1642.122, 1735.63, 1606.255, 1588.732, 1681.557, 1597.971, 1680.587, 1636.598, 1682.983, 1590.256, 1595.172, 1600.812, 1640.663, 1670.191, 1562.354, 1569.124, 1615.808, 1676.686, 1604.613, 1582.532, 1608.948, 1543.081, 1668.287, 1584.775, 1590.086, 1570.126, 1640.841, 1582.452, 1538.926, 1585.117, 1538.064, 1548.775, 1572.217, 1552.141, 1535.588, 1551.915, 1520.862, 1539.304, 1533.81, 1512.618, 1518.472, 1610.693, 1558.564, 1543.376, 1527.318, 1521.777, 1568.968, 1520.029, 1500.53, 1546.476, 1513.994, 1509.018, 1494.123, 1504.644, 1522.519, 1573.367, 1566.769, 1525.036, 1525.024, 1533.347, 1519.82, 1515.854, 1497.64]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:64 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:17 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:5.560855664373183e-05 batch_size:49 epochs:100	100	1000	True	1016.50659		464799	22	-1	159.80461406707764	{'train_loss': [3477.124, 2481.671, 2298.221, 2176.401, 2099.216, 2041.21, 2022.422, 1989.455, 1955.42, 1880.781, 1772.811, 1660.53, 1555.548, 1508.371, 1473.149, 1440.447, 1429.455, 1414.272, 1397.375, 1397.054, 1387.896, 1364.499, 1362.53, 1350.909, 1335.368, 1345.635, 1341.508, 1325.652, 1315.002, 1316.156, 1304.798, 1302.284, 1302.881, 1303.508, 1288.795, 1291.159, 1294.138, 1291.711, 1294.862, 1288.106, 1284.378, 1282.733, 1266.038, 1267.778, 1263.454, 1272.194, 1262.874, 1257.744, 1255.255, 1255.787, 1263.848, 1252.778, 1251.799, 1247.026, 1246.794, 1240.767, 1249.516, 1235.069, 1244.823, 1228.787, 1225.291, 1232.266, 1234.569, 1233.556, 1226.005, 1225.435, 1219.089, 1212.017, 1220.444, 1214.758, 1207.424, 1207.778, 1205.531, 1202.907, 1210.419, 1199.34, 1195.175, 1200.65, 1196.032, 1202.877, 1199.387, 1195.019, 1191.182, 1193.422, 1186.976, 1184.682, 1185.628, 1190.784, 1182.276, 1187.097, 1183.777, 1185.925, 1189.365, 1176.685, 1177.776, 1178.144, 1173.445, 1179.51, 1180.79, 1175.641], 'val_loss': [2805.451, 2190.675, 1996.505, 1860.635, 1879.429, 1764.246, 1851.391, 1690.898, 1699.412, 1840.899, 1616.0, 1400.316, 1356.967, 1314.321, 1275.791, 1296.152, 1266.354, 1239.926, 1276.796, 1257.213, 1241.184, 1218.65, 1225.697, 1195.393, 1218.573, 1206.115, 1160.537, 1159.372, 1159.9, 1154.146, 1172.113, 1137.503, 1151.254, 1142.425, 1135.571, 1125.909, 1157.454, 1160.164, 1162.904, 1140.609, 1122.001, 1159.702, 1116.338, 1097.193, 1107.781, 1103.791, 1081.163, 1108.512, 1112.301, 1100.483, 1113.329, 1087.903, 1077.695, 1081.789, 1096.274, 1099.529, 1108.497, 1094.198, 1114.233, 1066.078, 1057.178, 1056.34, 1053.79, 1067.29, 1074.033, 1067.645, 1059.714, 1042.719, 1049.829, 1045.812, 1032.394, 1038.096, 1042.936, 1035.896, 1039.018, 1035.902, 1027.344, 1033.973, 1045.65, 1072.841, 1036.411, 1058.302, 1028.019, 1027.336, 1033.615, 1029.129, 1037.005, 1028.43, 1031.48, 1042.376, 1021.152, 1012.242, 1031.64, 1012.207, 1003.901, 1019.16, 1021.669, 1012.395, 1014.03, 1006.115]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:34 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:61 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:67 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:6.030364562254975e-05 batch_size:49 epochs:100	100	1000	True	1016.01379		457804	21	-1	164.79526090621948	{'train_loss': [3387.667, 2509.819, 2299.091, 2167.445, 2110.776, 2094.724, 2083.13, 2050.276, 2049.036, 2041.884, 2024.068, 2006.491, 2003.925, 1973.498, 1963.84, 1953.882, 1916.554, 1857.088, 1764.525, 1697.198, 1621.486, 1530.194, 1493.295, 1447.534, 1443.03, 1408.625, 1386.917, 1386.329, 1356.405, 1352.875, 1350.083, 1329.88, 1317.717, 1324.698, 1310.61, 1311.179, 1299.505, 1288.409, 1287.327, 1277.631, 1273.03, 1269.698, 1268.365, 1262.513, 1262.273, 1242.332, 1247.831, 1242.107, 1234.628, 1240.799, 1228.042, 1232.055, 1235.779, 1226.694, 1221.61, 1221.8, 1227.748, 1218.376, 1208.702, 1200.423, 1200.459, 1209.348, 1208.171, 1211.213, 1204.188, 1198.566, 1190.646, 1186.403, 1191.515, 1187.843, 1191.392, 1192.94, 1183.514, 1178.042, 1183.742, 1181.701, 1186.718, 1181.237, 1176.478, 1170.804, 1175.078, 1178.845, 1175.7, 1189.809, 1185.266, 1174.842, 1169.866, 1170.569, 1167.916, 1164.806, 1166.589, 1163.408, 1162.272, 1161.325, 1163.031, 1162.27, 1166.651, 1163.476, 1168.305, 1160.812], 'val_loss': [3079.758, 2329.927, 2057.545, 1874.626, 1883.036, 1897.507, 1882.687, 1853.477, 1860.754, 1817.577, 1828.932, 1797.545, 1831.491, 1767.249, 1757.899, 1771.044, 1738.683, 1574.884, 1521.63, 1419.452, 1381.085, 1295.438, 1255.319, 1265.727, 1228.022, 1193.825, 1197.827, 1180.019, 1153.479, 1168.105, 1171.065, 1112.179, 1143.856, 1164.755, 1129.039, 1138.718, 1122.339, 1088.991, 1117.473, 1116.181, 1083.563, 1106.071, 1075.535, 1094.765, 1071.182, 1054.311, 1062.546, 1037.753, 1070.623, 1061.453, 1054.317, 1048.963, 1066.233, 1033.364, 1066.0, 1041.289, 1068.84, 1030.782, 1041.564, 1038.429, 1038.254, 1039.779, 1053.114, 1055.498, 1034.04, 1041.307, 1020.802, 1017.446, 1037.339, 1033.204, 1041.991, 1037.719, 1006.86, 1011.783, 1014.767, 1043.097, 1035.794, 1013.474, 1030.354, 1001.298, 1031.605, 1031.072, 1037.17, 1011.314, 1027.909, 1017.119, 1006.887, 1019.831, 1010.135, 1007.005, 1022.94, 1009.561, 1014.958, 998.152, 1013.267, 1022.207, 1028.789, 1015.574, 1023.823, 998.132]}	100	100	True
