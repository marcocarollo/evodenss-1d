id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	1519.01367		403446	16	-1	143.0257318019867	{'train_loss': [6081.194, 4234.111, 4069.646, 3816.634, 3690.167, 3544.13, 3453.018, 3440.507, 3396.448, 3338.198, 3329.936, 3308.844, 3287.192, 3237.097, 3197.627, 3030.782, 2877.684, 2755.868, 2695.6, 2589.318, 2515.15, 2489.789, 2433.335, 2439.551, 2382.68, 2343.49, 2327.918, 2299.654, 2309.272, 2280.725, 2285.433, 2257.806, 2247.268, 2242.574, 2206.827, 2211.752, 2221.602, 2200.023, 2159.174, 2183.458, 2145.703, 2151.233, 2125.967, 2114.267, 2140.492, 2142.689, 2119.037, 2101.984, 2093.083, 2091.659, 2092.546, 2088.669, 2077.781, 2064.264, 2058.532, 2085.837, 2075.567, 2058.805, 2081.05, 2069.079, 2060.081, 2050.397, 2037.461, 2074.186, 2045.754, 2034.287, 2022.655, 2019.735, 2036.64, 2011.91, 2017.338, 2019.066, 2013.246, 1998.392, 1990.573, 2018.407, 2012.319, 1995.049, 1993.319, 1991.512, 1991.914, 1994.269, 1980.662, 1984.804, 1988.543, 1996.455, 2006.768, 1981.152, 1988.022, 1983.095, 1995.479, 1974.632, 1977.975, 1959.152, 1995.02, 1970.49, 1954.198, 1965.235, 1963.628, 1959.892], 'val_loss': [5427.319, 3962.896, 3509.731, 3203.464, 3062.318, 2869.053, 2855.372, 2928.789, 2800.543, 2820.854, 2725.946, 2802.594, 2688.904, 2801.277, 2598.167, 2376.836, 2243.914, 2152.106, 2262.289, 2078.527, 2092.035, 2125.988, 1989.386, 2076.665, 1965.719, 1897.17, 1909.183, 1782.936, 1920.683, 1757.734, 1799.271, 1832.351, 1757.344, 1787.014, 1711.13, 1763.007, 1838.837, 1717.329, 1699.83, 1712.006, 1712.587, 1736.922, 1696.593, 1720.787, 1660.808, 1745.218, 1660.818, 1664.62, 1618.771, 1652.563, 1639.632, 1678.549, 1683.588, 1587.921, 1593.39, 1710.627, 1665.824, 1610.833, 1698.664, 1581.349, 1644.286, 1576.26, 1642.256, 1680.518, 1662.746, 1584.647, 1602.759, 1588.458, 1656.076, 1535.567, 1594.285, 1637.934, 1593.179, 1550.616, 1561.993, 1615.195, 1586.76, 1602.346, 1574.83, 1529.429, 1614.534, 1545.336, 1534.087, 1548.982, 1561.077, 1572.878, 1513.924, 1537.292, 1540.533, 1592.695, 1543.113, 1569.134, 1527.471, 1559.411, 1545.182, 1527.796, 1520.142, 1546.408, 1521.951, 1499.71]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	2000	True	1532.56372		403446	16	-1	142.04819107055664	{'train_loss': [5907.53, 4250.927, 4027.599, 3803.044, 3639.768, 3517.39, 3421.891, 3376.347, 3327.404, 3305.841, 3280.823, 3255.874, 3233.642, 3233.505, 3172.655, 3108.752, 3068.843, 2977.491, 2964.551, 2909.982, 2844.722, 2866.764, 2742.042, 2671.673, 2579.213, 2578.59, 2509.418, 2513.4, 2452.047, 2394.599, 2362.715, 2322.853, 2299.937, 2283.984, 2280.039, 2283.228, 2264.395, 2243.517, 2203.235, 2200.848, 2195.798, 2178.061, 2185.65, 2151.815, 2122.386, 2128.653, 2119.183, 2114.172, 2110.975, 2139.01, 2116.421, 2111.084, 2119.271, 2128.276, 2078.413, 2089.301, 2058.444, 2047.113, 2053.417, 2046.454, 2044.692, 2048.012, 2040.746, 2017.574, 2013.773, 1994.81, 2016.416, 2006.233, 1990.604, 1992.441, 1992.005, 1995.771, 1990.691, 1985.403, 1979.777, 1970.936, 1985.669, 1968.803, 1957.246, 1967.15, 1955.464, 1965.396, 1950.441, 1942.995, 1958.03, 1952.929, 1942.981, 1950.181, 1948.067, 1931.545, 1935.516, 1946.265, 1943.291, 1926.993, 1929.294, 1925.476, 1928.272, 1938.984, 1930.359, 1920.196], 'val_loss': [5198.579, 3695.481, 3304.63, 3079.342, 2863.883, 2847.571, 2816.435, 2810.926, 2810.217, 2757.939, 2737.204, 2773.328, 2621.835, 2596.006, 2505.554, 2416.982, 2468.63, 2453.047, 2462.713, 2273.765, 2514.762, 2145.743, 2373.99, 2280.371, 2169.585, 2107.882, 2032.031, 2023.959, 1963.969, 2003.506, 1870.664, 1902.479, 1886.904, 1803.482, 1851.553, 1901.95, 1799.98, 1860.809, 1780.572, 1728.77, 1761.038, 1793.608, 1717.436, 1706.673, 1668.276, 1681.414, 1747.397, 1657.485, 1690.514, 1732.164, 1668.003, 1763.786, 1734.539, 1830.664, 1755.898, 1646.591, 1603.176, 1680.192, 1673.223, 1625.441, 1620.798, 1637.107, 1650.146, 1652.86, 1656.987, 1644.263, 1681.173, 1662.676, 1591.656, 1640.125, 1579.438, 1611.912, 1545.547, 1636.072, 1572.861, 1546.269, 1563.438, 1601.235, 1577.911, 1566.15, 1590.995, 1575.104, 1573.136, 1583.704, 1526.14, 1575.184, 1496.433, 1595.726, 1569.281, 1514.716, 1530.997, 1598.727, 1544.042, 1536.489, 1518.224, 1520.339, 1580.664, 1514.978, 1537.982, 1516.39]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:60 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:29 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8559414101327206 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	1582.72314		383650	17	-1	123.34138059616089	{'train_loss': [6434.984, 4304.161, 4122.854, 3998.235, 3789.487, 3688.185, 3560.945, 3456.527, 3415.691, 3366.288, 3343.64, 3302.719, 3296.466, 3203.146, 3115.706, 3004.427, 2966.36, 2881.297, 2773.705, 2733.574, 2668.693, 2584.961, 2527.324, 2510.215, 2481.583, 2463.829, 2440.596, 2388.088, 2386.704, 2384.11, 2356.016, 2321.206, 2313.283, 2296.03, 2288.14, 2282.257, 2259.055, 2251.621, 2257.233, 2281.873, 2288.6, 2240.791, 2244.126, 2220.085, 2225.901, 2195.414, 2195.319, 2169.388, 2164.251, 2159.899, 2156.732, 2143.784, 2139.083, 2125.996, 2129.721, 2123.379, 2102.468, 2109.902, 2126.417, 2121.467, 2098.663, 2093.623, 2092.341, 2087.302, 2097.215, 2083.108, 2073.172, 2063.075, 2060.964, 2057.722, 2053.443, 2062.219, 2071.426, 2057.032, 2063.414, 2052.014, 2056.796, 2044.792, 2030.55, 2030.388, 2041.409, 2029.181, 2039.968, 2024.485, 2032.595, 2021.271, 2027.567, 2016.471, 2027.422, 2021.603, 2007.636, 2003.794, 2014.92, 2010.946, 1997.092, 1988.232, 1998.984, 1998.613, 1998.5, 1986.794], 'val_loss': [7185.669, 4862.417, 3865.58, 3254.719, 3088.524, 2991.535, 2830.441, 2804.781, 2726.798, 2717.517, 2690.469, 2782.323, 2688.913, 2823.477, 2379.416, 2269.731, 2224.843, 2195.423, 2231.333, 2236.512, 2028.764, 1959.072, 1991.218, 1959.436, 2049.146, 1923.558, 1867.126, 1828.187, 1907.032, 1901.448, 1795.646, 1794.92, 1942.679, 1768.126, 1887.146, 1739.554, 1813.986, 1715.713, 1871.339, 1863.781, 1728.124, 1764.991, 1731.222, 1741.209, 1703.371, 1728.969, 1722.388, 1713.295, 1684.791, 1679.282, 1665.049, 1670.069, 1678.985, 1670.062, 1668.768, 1646.673, 1638.591, 1662.918, 1677.633, 1595.678, 1603.114, 1610.15, 1635.796, 1622.813, 1619.483, 1597.35, 1588.587, 1565.819, 1585.977, 1602.507, 1573.355, 1624.86, 1563.038, 1581.25, 1563.188, 1563.007, 1575.915, 1547.662, 1543.029, 1571.326, 1589.191, 1578.068, 1574.104, 1564.89, 1586.651, 1549.797, 1549.99, 1577.636, 1563.448, 1544.879, 1536.733, 1535.058, 1533.265, 1531.417, 1545.422, 1570.929, 1567.55, 1541.757, 1541.23, 1539.779]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:12 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:42 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:14 layer:conv1d out_channels:85 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.0010291034297796682 beta1:0.9460386875373169 beta2:0.9711402723837911 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	23256.32617		7342146	18	-1	191.20651245117188	{'train_loss': [35036.977, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30223.371, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30747.857, 30404.906, 30221.164, 30225.445, 30220.1, 30218.482, 30220.918, 30220.641, 30220.918, 30220.918, 30221.357, 30227.637, 30232.816, 30220.395, 30220.549, 30220.699, 30232.643, 30220.977, 30232.348, 30238.096, 30232.246, 30220.918, 30220.918, 30220.918, 30220.918, 30220.918, 30239.098, 30227.883, 30281.287, 30220.906, 30221.648, 30215.613, 30268.393, 30261.986, 30219.148, 30275.229, 30266.875, 30240.373, 30250.332, 30248.113, 30139.367, 30088.393, 30100.455, 30109.057, 30123.863, 30157.287, 30122.381, 30085.703, 30204.066, 30151.514, 30059.695, 30001.939, 29965.822, 29961.91, 29941.277, 29950.498, 29995.748, 29931.062, 29958.873, 29917.785, 29921.611, 29889.662, 29777.896], 'val_loss': [23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23730.328, 23478.672, 23494.754, 23495.08, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.07, 23494.012, 23493.197, 23495.082, 23382.859, 23890.391, 23495.082, 23495.082, 23722.578, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23495.082, 23386.379, 24046.051, 23495.082, 23374.586, 23495.082, 23495.082, 23463.123, 23495.082, 23495.08, 23802.184, 23747.824, 23603.781, 23423.613, 24255.531, 23352.293, 23459.66, 23436.17, 23429.48, 23483.742, 23422.139, 23357.715, 23480.496, 23491.562, 23355.705, 23345.609, 23344.781, 23351.746, 23286.184, 23319.125, 23369.602, 23275.918, 23352.098, 23190.508, 23214.203, 23251.678, 23198.66, 23079.852]}	100	100	True
