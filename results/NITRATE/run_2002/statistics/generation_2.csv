id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:47 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.08835175881075448 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:83 epochs:100	100	1000	True	2305.96729		424039	15	-1	108.18141794204712	{'train_loss': [11638.83, 4396.895, 4161.377, 3972.601, 3954.263, 3948.698, 3951.861, 3952.875, 3955.821, 3959.964, 3962.791, 3963.25, 3970.417, 3945.096, 3947.432, 3967.759, 3967.083, 3948.158, 3788.449, 3722.95, 3788.9, 3573.809, 3541.432, 3530.334, 3522.766, 3519.825, 3472.118, 3510.986, 3445.621, 3471.437, 3415.505, 3374.806, 3454.389, 3458.996, 3471.388, 3454.949, 3497.987, 3404.377, 3438.354, 3415.209, 3437.56, 3353.562, 3374.655, 3369.984, 3294.008, 3319.103, 3301.915, 3438.577, 3541.764, 3393.132, 3339.268, 3269.474, 3353.164, 3256.586, 3341.849, 3287.927, 3546.268, 3428.617, 3365.895, 3314.543, 3279.56, 3281.007, 3258.389, 3203.539, 3289.142, 3376.352, 3427.556, 3855.253, 3973.53, 3460.962, 3343.192, 3217.112, 3295.315, 3264.704, 3399.681, 3307.839, 3190.12, 3203.471, 3335.523, 3252.92, 3381.732, 3245.246, 3102.974, 3068.83, 3253.588, 3092.78, 3080.607, 3152.016, 3161.767, 3208.338, 3139.316, 3115.684, 3064.141, 3060.542, 3033.614, 3074.838, 3099.014, 3032.461, 3027.333, 3067.439], 'val_loss': [3986.881, 3439.69, 3181.506, 3138.144, 3134.602, 3137.168, 3138.777, 3146.195, 3167.686, 3151.432, 3146.5, 3172.629, 3130.251, 3129.377, 3176.024, 3176.796, 3197.564, 3083.025, 3052.193, 3376.87, 2998.897, 2914.599, 2900.303, 3067.489, 2799.709, 2958.467, 2870.622, 2821.699, 2793.008, 2783.735, 2778.447, 2703.753, 2860.93, 2775.82, 3045.904, 2811.385, 2782.386, 2826.878, 2696.729, 2890.552, 2747.396, 2705.929, 2860.985, 2662.102, 2626.775, 2711.553, 2724.561, 3460.577, 2841.818, 2887.775, 2843.907, 2707.295, 2752.059, 2738.835, 2793.452, 3095.454, 3406.177, 2830.042, 2662.194, 2854.141, 2616.345, 2683.168, 2550.232, 2635.672, 2858.33, 2806.344, 3917.623, 3845.724, 2713.004, 2802.175, 2711.346, 2750.645, 2675.817, 2888.578, 3052.323, 2725.027, 2673.319, 2715.921, 2654.166, 2902.012, 2626.594, 2538.406, 2483.279, 2897.412, 2514.273, 2446.091, 2401.901, 2917.474, 2584.716, 2857.721, 2650.444, 2491.513, 2432.533, 2367.38, 2351.443, 2581.633, 2524.425, 2325.257, 2426.774, 2342.402]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:17 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:45 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:47 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.08835175881075448 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:90 epochs:100	100	1000	True	2381.05762		466735	16	-1	111.10259532928467	{'train_loss': [19246.088, 7209.47, 5279.532, 4615.694, 4470.71, 4436.934, 4430.01, 4425.069, 4420.495, 4416.42, 4417.63, 4417.13, 4424.633, 4437.705, 4431.851, 4409.621, 4402.517, 4390.081, 4375.59, 4341.875, 4284.867, 4231.587, 4228.104, 4183.477, 4128.109, 4259.079, 4158.49, 4050.761, 4074.667, 4080.608, 3973.944, 3991.75, 3927.677, 3823.648, 3813.972, 3831.05, 3677.875, 3615.385, 3695.835, 3743.352, 3593.555, 3571.055, 3589.971, 3611.691, 3562.142, 3541.952, 3596.657, 3544.966, 3539.579, 3516.14, 3567.197, 3554.286, 3679.793, 3510.981, 3474.71, 3493.77, 3522.146, 3529.156, 3498.613, 3468.372, 3476.571, 3489.288, 3475.376, 3519.213, 3445.907, 3486.041, 3490.225, 3421.522, 3478.622, 3435.508, 3480.325, 3476.596, 3449.815, 3465.673, 3406.705, 3407.527, 3449.769, 3415.234, 3377.423, 3407.937, 3472.865, 3548.375, 3468.968, 3454.322, 3375.355, 3441.301, 3412.447, 3362.388, 3348.855, 3437.12, 3527.042, 3407.131, 3445.741, 3394.417, 3452.218, 3408.465, 3381.286, 3488.477, 3454.189, 3417.726], 'val_loss': [6085.897, 4601.528, 3387.524, 3194.01, 3154.359, 3145.193, 3132.866, 3126.068, 3127.897, 3127.144, 3125.438, 3125.807, 3126.393, 3137.465, 3136.552, 3118.428, 3109.518, 3099.83, 3052.93, 3042.617, 2974.836, 3065.796, 2973.572, 3034.935, 2918.192, 3048.612, 2902.485, 2935.359, 3032.85, 2897.222, 2962.07, 2880.862, 2798.177, 2766.015, 2865.723, 2651.392, 2659.674, 2654.599, 2724.619, 2615.885, 2579.813, 2602.188, 2658.797, 2641.207, 2619.264, 2615.543, 2594.942, 2533.871, 2573.766, 2558.429, 2579.14, 2656.71, 2544.721, 2522.2, 2536.836, 2538.919, 2492.752, 2498.219, 2495.556, 2463.495, 2517.986, 2498.091, 2500.752, 2502.323, 2426.262, 2414.986, 2450.511, 2414.337, 2410.676, 2407.28, 2500.556, 2479.113, 2433.02, 2459.768, 2408.866, 2432.391, 2423.45, 2415.899, 2439.615, 2411.898, 2527.47, 2493.28, 2370.716, 2375.128, 2334.394, 2385.587, 2392.821, 2323.462, 2446.067, 2444.781, 2422.729, 2387.211, 2356.574, 2414.705, 2390.403, 2351.987, 2473.7, 2409.919, 2515.33, 2406.53]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:31 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:47 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:114 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:47 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:123 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.08835175881075448 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:53 epochs:100	100	1000	True	16676.84375		41291141	16	-1	205.03117299079895	{'train_loss': [492346.375, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738, 19678.738], 'val_loss': [15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389, 15663.389]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:94 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:47 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.08835175881075448 beta1:0.9460386875373169 beta2:0.9975553842663847 weight_decay:3.3008957210522833e-05 batch_size:64 epochs:100	100	1000	True	2613.54639		428255	14	-1	112.33004713058472	{'train_loss': [9854.911, 3351.043, 3245.86, 3093.263, 3075.999, 3063.559, 2975.236, 2850.118, 2837.587, 2736.013, 2711.626, 2633.473, 2615.637, 2604.91, 2635.088, 2582.599, 2609.182, 2637.365, 2592.715, 2604.969, 2662.353, 2659.571, 2768.571, 2655.321, 2582.096, 2608.718, 2534.185, 2485.988, 2535.337, 2646.322, 3470.002, 2742.493, 2669.036, 2471.445, 2497.68, 2540.664, 2613.535, 2575.043, 2488.888, 2482.779, 2494.053, 2632.333, 2494.561, 2454.687, 2435.552, 2441.403, 2427.713, 2463.832, 2535.902, 2527.855, 2507.352, 2605.827, 2500.624, 2570.581, 2492.293, 2549.234, 2465.8, 2548.468, 2453.718, 2520.505, 2498.045, 2436.721, 2432.23, 2468.472, 2416.086, 2452.194, 2728.947, 2439.748, 2379.71, 2386.023, 2374.604, 2418.556, 2480.293, 2661.624, 2896.244, 3483.502, 2951.106, 2971.024, 2780.883, 2697.255, 2533.837, 2490.704, 2341.622, 2374.817, 2387.691, 2744.994, 2445.573, 2354.413, 2379.844, 2423.629, 2509.519, 2470.828, 2406.88, 2435.788, 2376.586, 2363.841, 2409.662, 2507.492, 2582.019, 2761.091], 'val_loss': [4781.714, 3580.383, 3168.743, 3136.345, 3140.552, 3213.977, 3001.965, 3047.77, 2884.712, 2927.015, 2733.355, 2757.038, 2843.227, 2901.569, 2725.244, 2763.926, 2951.188, 2745.312, 2786.412, 2844.95, 2900.36, 3427.917, 2808.241, 2849.281, 2733.735, 2581.448, 2551.481, 2656.668, 2665.484, 4699.711, 3130.337, 3159.646, 2744.187, 2523.845, 2576.01, 2524.399, 2711.746, 2663.142, 2536.952, 2581.132, 3010.438, 2709.054, 2701.154, 2571.573, 2916.978, 2538.198, 2654.636, 2705.554, 2881.038, 2672.667, 3665.557, 2651.951, 2907.536, 2546.127, 3289.236, 2800.963, 3052.564, 2434.994, 2565.974, 2686.931, 2552.307, 2382.382, 2529.984, 2564.147, 2375.93, 2793.167, 2935.881, 2400.627, 2363.829, 2392.466, 2407.559, 2356.25, 2775.412, 2936.175, 4679.351, 3530.099, 3786.178, 3214.887, 2946.1, 2985.386, 2793.009, 2405.01, 2536.247, 2427.226, 3250.099, 2805.943, 2443.299, 2468.763, 2520.823, 3113.47, 2486.64, 2702.512, 2492.097, 2470.616, 2385.533, 2554.442, 2678.739, 3741.265, 3515.453, 2639.487]}	100	100	True
