id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:97 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:114 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:30 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:19 layer:fc act:selu out_features:200 bias:True input:20 learning:adam lr:0.002460715196435833 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.000436441315793302 batch_size:49 epochs:100	100	1000	True	1058.08313		1798512	22	-1	279.0618987083435	{'train_loss': [15402.123, 12989.915, 11774.241, 11320.337, 11047.104, 10931.425, 9471.363, 4974.997, 2886.311, 2272.938, 2104.603, 2039.215, 2027.887, 2001.68, 1982.698, 1994.066, 1970.775, 1955.14, 1906.95, 1855.437, 1756.192, 1717.495, 1636.394, 1584.52, 1528.226, 1460.364, 1435.993, 1412.224, 1402.105, 1390.914, 1365.009, 1343.835, 1347.041, 1336.087, 1315.342, 1313.952, 1289.723, 1294.36, 1291.222, 1286.858, 1272.204, 1276.295, 1286.58, 1266.823, 1255.729, 1255.426, 1254.926, 1241.313, 1248.5, 1230.676, 1238.258, 1228.392, 1229.873, 1222.266, 1231.087, 1224.963, 1212.719, 1208.99, 1214.559, 1217.715, 1231.468, 1214.804, 1208.273, 1212.769, 1205.828, 1203.256, 1209.885, 1203.284, 1188.868, 1191.449, 1205.888, 1194.878, 1194.511, 1189.31, 1173.831, 1167.265, 1167.028, 1175.641, 1171.324, 1175.248, 1173.289, 1167.843, 1181.292, 1171.949, 1157.783, 1171.504, 1149.672, 1149.862, 1151.327, 1154.986, 1150.866, 1162.195, 1144.099, 1158.132, 1145.132, 1152.219, 1153.276, 1145.132, 1143.139, 1154.328], 'val_loss': [13888.703, 10475.639, 9851.401, 9451.91, 9340.171, 9216.784, 6131.025, 2907.46, 2101.93, 1925.769, 1820.033, 1800.002, 1841.179, 1747.611, 1771.656, 1751.946, 1784.779, 1763.359, 1665.569, 1629.94, 1591.72, 1509.26, 1404.573, 1364.128, 1298.138, 1263.107, 1223.327, 1205.506, 1227.584, 1226.681, 1170.009, 1146.047, 1159.739, 1144.709, 1133.841, 1138.557, 1111.583, 1104.8, 1104.249, 1112.145, 1097.283, 1097.366, 1097.473, 1078.055, 1076.569, 1077.595, 1062.969, 1069.145, 1041.742, 1072.033, 1087.164, 1055.948, 1024.022, 1029.458, 1058.887, 1045.167, 1030.076, 1019.482, 1038.019, 1032.54, 1066.117, 1032.86, 1032.153, 1006.303, 1022.541, 1015.402, 1059.57, 1020.605, 1002.091, 1027.705, 1006.155, 1002.76, 991.39, 1002.57, 1008.455, 1007.576, 1004.7, 1008.718, 993.246, 998.146, 1007.531, 1003.754, 1027.9, 1026.968, 989.651, 970.175, 995.818, 1010.548, 992.823, 996.034, 963.737, 980.479, 996.806, 985.876, 967.044, 997.798, 1007.604, 1036.826, 1033.65, 1006.192]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:85 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:99 kernel_size:5 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:97 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:114 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:85 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:30 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:17 layer:deconv1d out_channels:26 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:deconv1d out_channels:99 kernel_size:5 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:19 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:20 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:21 layer:fc act:selu out_features:200 bias:True input:22 learning:adam lr:0.08015360775213085 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.000436441315793302 batch_size:49 epochs:100	12	1000	True	19089.02539		41030365	24	-1	1015.7188675403595	{'train_loss': [20679.756, 18395.344, 18395.344, 349942.531, 38539.953, 18395.344, 18395.344, 3997222.0, 23142668.0, 52715888.0, 18395.344, 18395.344], 'val_loss': [15663.388, 15663.388, 15663.388, 200140.953, 15663.388, 15663.388, 15663.388, 1123248.625, 40164828.0, 15663.388, 15663.388, 15663.388]}	12	12	False
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:97 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:114 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:91 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:30 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:deconv1d out_channels:116 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:conv1d out_channels:123 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:19 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:20 layer:conv1d out_channels:49 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:21 layer:fc act:selu out_features:200 bias:True input:22 learning:adam lr:0.004252519629845999 beta1:0.9460386875373169 beta2:0.8692540522544694 weight_decay:0.000436441315793302 batch_size:49 epochs:100	100	1000	True	16440.3125		32022369	24	-1	604.8051142692566	{'train_loss': [127129.32, 18395.344, 18395.344, 18395.344, 18395.344, 18395.344, 18654.643, 25560.494, 21659.701, 19362.963, 19714.447, 28462.977, 48739.062, 97026.422, 93918.992, 20989.508, 19606.656, 19676.447, 22660.768, 21064.238, 22384.611, 21440.715, 27042.479, 19716.969, 20396.287, 30686.188, 36728.324, 22113.777, 20403.781, 19798.635, 18980.457, 18684.695, 19620.037, 23878.32, 24347.328, 19770.002, 18714.848, 18719.268, 19346.953, 18805.834, 18565.18, 18554.891, 18635.297, 18447.217, 18585.711, 18529.648, 18722.928, 18495.164, 18478.678, 18507.039, 18452.387, 18449.016, 18470.789, 18525.033, 18482.705, 18462.586, 18459.531, 18581.525, 18502.713, 18483.258, 18444.43, 18561.809, 18484.234, 18519.652, 18594.988, 18580.076, 18554.293, 18415.324, 18451.711, 18441.24, 18423.621, 18482.682, 18444.992, 18497.162, 18444.146, 18422.551, 18469.027, 18431.748, 18448.799, 18465.287, 18439.637, 18434.967, 18478.16, 18438.414, 18451.219, 18563.229, 18449.625, 18423.371, 18427.523, 18453.016, 18433.127, 18431.906, 18414.312, 18449.668, 18429.314, 18434.879, 18437.035, 18416.117, 18410.477, 18455.752], 'val_loss': [18800.91, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15644.932, 30688.496, 15663.388, 15663.388, 51562.855, 15663.388, 15663.388, 794659.188, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 27108.844, 68171.836, 24856.166, 15663.385, 331104.688, 15663.388, 17554.414, 15663.388, 15663.388, 15663.388, 15663.388, 157355.406, 15663.388, 15663.388, 80896.922, 86735.328, 16613.939, 15663.388, 15663.388, 15663.388, 268705.188, 3207188.0, 15663.388, 15663.388, 3089664.0, 279818.312, 15663.388, 15663.388, 15663.388, 66270.883, 15663.388, 15663.937, 15663.388, 15663.388, 31743.111, 34514.449, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 18989.053, 15663.388, 247212.75, 517878.812, 15663.388, 65968.461, 15663.388, 15663.388, 15663.388, 15663.388, 23358.922, 15663.388, 15663.388, 16656.738, 16266.151, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 16346.152, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388, 15663.388]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:97 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:114 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:75 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:125 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:conv1d out_channels:42 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:85 kernel_size:3 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:deconv1d out_channels:30 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:deconv1d out_channels:109 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:deconv1d out_channels:4 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:18 layer:deconv1d out_channels:26 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:19 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:20 layer:fc act:selu out_features:200 bias:True input:21 learning:adadelta lr:0.002460715196435833 batch_size:49 epochs:100	100	1000	True	1668.14136		781317	23	-1	174.25047254562378	{'train_loss': [8731.53, 6979.035, 4655.865, 3786.465, 3431.645, 3215.221, 3086.773, 2993.763, 2926.876, 2875.929, 2835.673, 2800.075, 2771.423, 2746.937, 2728.992, 2711.469, 2701.58, 2688.301, 2674.085, 2665.466, 2657.778, 2648.416, 2637.491, 2631.226, 2624.836, 2619.044, 2612.812, 2610.287, 2607.689, 2600.039, 2596.06, 2591.451, 2586.253, 2584.008, 2580.541, 2577.32, 2575.047, 2569.094, 2569.047, 2565.785, 2561.704, 2556.74, 2551.05, 2547.316, 2538.956, 2534.239, 2522.769, 2513.198, 2500.676, 2481.845, 2474.211, 2460.022, 2442.986, 2436.232, 2421.313, 2407.417, 2404.942, 2394.064, 2381.882, 2375.924, 2368.093, 2358.571, 2347.621, 2348.034, 2341.719, 2336.282, 2327.473, 2323.543, 2313.547, 2312.608, 2303.036, 2309.378, 2298.973, 2289.896, 2284.55, 2284.94, 2275.99, 2270.537, 2261.012, 2256.824, 2253.347, 2246.785, 2246.116, 2235.6, 2231.9, 2230.412, 2221.126, 2222.179, 2213.823, 2218.347, 2207.073, 2208.679, 2202.631, 2189.374, 2190.892, 2191.945, 2191.727, 2183.506, 2185.508, 2182.065], 'val_loss': [5093.573, 4711.288, 3730.398, 3071.819, 2780.375, 2613.499, 2498.393, 2428.924, 2368.845, 2326.931, 2297.477, 2277.446, 2251.821, 2246.751, 2229.448, 2220.615, 2212.452, 2213.868, 2201.703, 2195.673, 2191.524, 2187.081, 2175.617, 2179.714, 2168.203, 2161.819, 2158.107, 2152.711, 2149.042, 2148.186, 2143.858, 2136.268, 2137.494, 2132.8, 2132.721, 2126.061, 2121.34, 2124.097, 2122.274, 2117.46, 2117.014, 2114.955, 2111.973, 2105.97, 2098.146, 2098.383, 2084.309, 2075.638, 2065.077, 2053.164, 2052.316, 2039.957, 2031.021, 2011.517, 2003.947, 2004.47, 1996.729, 1987.306, 1982.331, 1972.74, 1973.829, 1963.16, 1961.636, 1957.307, 1953.394, 1951.691, 1943.154, 1938.417, 1930.34, 1922.952, 1924.083, 1912.474, 1903.44, 1897.305, 1889.265, 1886.43, 1887.748, 1871.922, 1863.367, 1863.903, 1856.334, 1844.004, 1839.451, 1837.18, 1834.166, 1822.302, 1815.274, 1815.225, 1809.731, 1804.631, 1801.549, 1794.914, 1795.586, 1789.408, 1796.294, 1785.903, 1782.634, 1774.57, 1781.735, 1782.739]}	100	100	True
