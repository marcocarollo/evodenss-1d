id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4329.01416		452251	14	-1	178.30941200256348	{'train_loss': [332723.25, 169562.797, 149529.547, 141234.547, 135041.719, 128027.445, 123107.164, 119044.445, 115814.461, 113233.242, 111135.961, 109044.367, 107137.766, 106222.602, 104636.953, 103425.5, 101928.992, 101197.602, 99691.477, 99031.406, 98149.656, 96993.547, 95973.008, 96562.391, 94476.789, 93908.508, 94104.531, 93472.617, 92916.562, 91739.703, 91841.078, 90380.883, 89926.227, 89480.102, 89414.188, 88556.492, 88947.664, 87376.508, 87359.867, 87134.75, 87554.852, 85870.062, 86392.422, 86159.016, 85909.086, 85257.578, 84339.602, 84999.07, 84065.25, 84276.594, 83203.328, 82603.445, 83278.023, 83436.125, 82265.953, 81904.266, 82399.531, 81886.539, 82184.914, 81120.391, 81732.312, 80661.086, 80322.484, 80466.078, 79769.547, 80698.148, 79653.258, 79915.766, 79063.781, 78863.031, 79257.555, 78217.164, 78932.055, 78453.922, 78691.203, 78156.453, 78030.445, 78105.281, 77812.602, 77222.766, 76414.695, 77469.148, 77440.172, 76761.086, 76734.43, 77096.703, 76752.852, 75892.375, 75987.141, 76318.711, 76317.469, 75757.477, 74980.492, 75397.797, 75640.609, 75096.477, 75961.0, 74583.469, 75392.477, 74970.992], 'val_loss': [2190.376, 1829.963, 1710.574, 1645.723, 1604.79, 1623.404, 1645.848, 1634.335, 1658.94, 1606.633, 1594.471, 1645.936, 1512.72, 1477.487, 1484.238, 1458.18, 1409.573, 1429.332, 1369.854, 1427.817, 1318.399, 1344.582, 1350.104, 1286.652, 1286.412, 1292.628, 1273.11, 1332.882, 1286.642, 1247.681, 1232.946, 1268.996, 1257.438, 1259.644, 1199.622, 1192.899, 1242.229, 1293.97, 1217.724, 1204.875, 1211.82, 1182.328, 1184.566, 1173.702, 1159.023, 1211.941, 1184.544, 1210.999, 1220.68, 1175.179, 1155.688, 1124.833, 1131.661, 1136.938, 1152.882, 1145.606, 1158.021, 1131.58, 1105.447, 1136.461, 1150.506, 1081.839, 1113.472, 1116.818, 1057.956, 1103.065, 1090.571, 1122.188, 1082.048, 1069.487, 1030.009, 1038.388, 1035.205, 1066.855, 1009.495, 1015.231, 1096.822, 1075.505, 1062.397, 1038.817, 1060.811, 1060.579, 1035.571, 1048.602, 1016.611, 1085.099, 1054.458, 1007.645, 1020.511, 1065.344, 1145.368, 1094.109, 1083.518, 1027.156, 1069.646, 1024.323, 1121.964, 1047.362, 1005.668, 989.968]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:125 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	4388.86816		461105	15	-1	187.57960963249207	{'train_loss': [394119.125, 313488.781, 194567.078, 143789.812, 133078.922, 127683.156, 122399.461, 118487.727, 115334.102, 112214.523, 109435.359, 106637.664, 105908.758, 104369.672, 102762.602, 101579.094, 100785.406, 99619.773, 97851.195, 96892.883, 96048.977, 96177.312, 94321.352, 94616.312, 93847.609, 93675.383, 93247.234, 92856.367, 93078.156, 91741.492, 91492.492, 91534.898, 91391.328, 90247.289, 89783.414, 89720.352, 89941.273, 88785.891, 88941.758, 88207.016, 88187.641, 87747.633, 88029.469, 88050.758, 87019.172, 87052.992, 85924.258, 85491.047, 86234.055, 86189.953, 86054.266, 84997.328, 86041.328, 84353.461, 84634.922, 84863.875, 83859.094, 83634.141, 82357.609, 82777.711, 83133.938, 82640.539, 82137.953, 81942.82, 83059.07, 82659.008, 81646.82, 81452.938, 82046.875, 81557.133, 80906.859, 81222.742, 80694.977, 80897.492, 80534.57, 79946.25, 80548.938, 79027.117, 79664.719, 80221.148, 79831.055, 78988.805, 79069.422, 79806.828, 78871.812, 79176.68, 78607.195, 78385.922, 78442.383, 77544.711, 77896.688, 77757.586, 77539.773, 77543.766, 77223.648, 77186.883, 77364.422, 76899.664, 76428.594, 76751.336], 'val_loss': [4836.208, 3150.02, 2516.117, 2103.899, 2028.36, 1778.05, 1770.598, 1765.257, 1637.023, 1535.4, 1635.501, 1505.763, 1465.509, 1446.108, 1408.461, 1430.466, 1333.348, 1343.434, 1368.773, 1320.919, 1347.955, 1378.442, 1279.901, 1258.969, 1315.602, 1295.68, 1267.555, 1323.874, 1276.93, 1233.801, 1199.792, 1331.394, 1226.822, 1252.148, 1214.744, 1234.355, 1249.485, 1201.359, 1249.365, 1297.147, 1217.02, 1225.255, 1269.193, 1229.282, 1201.668, 1183.714, 1192.153, 1222.776, 1234.38, 1135.828, 1150.322, 1137.719, 1231.671, 1150.976, 1143.849, 1168.25, 1150.188, 1139.54, 1136.405, 1143.126, 1137.957, 1121.016, 1095.089, 1114.108, 1084.613, 1114.369, 1188.856, 1126.745, 1133.86, 1136.37, 1113.509, 1117.553, 1144.651, 1154.786, 1067.361, 1065.167, 1097.606, 1109.824, 1089.704, 1053.922, 1072.141, 1058.945, 1127.575, 1092.147, 1110.954, 1072.69, 1068.487, 1094.051, 1107.176, 1095.874, 1043.028, 1058.427, 1049.193, 1039.377, 1094.703, 1016.11, 1134.038, 1036.245, 1061.676, 1060.654]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:conv1d out_channels:86 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	44537.46875		3782107	13	-1	170.75269889831543	{'train_loss': [893159.312, 804482.438, 805602.062, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	4105.6875		470815	15	-1	185.31744241714478	{'train_loss': [392665.094, 291440.531, 164094.734, 144656.031, 136383.188, 128571.484, 124563.414, 120523.773, 116419.469, 113027.445, 111464.578, 109499.922, 108494.797, 107220.789, 104813.758, 103807.648, 101481.625, 100458.5, 99602.562, 98805.32, 97085.555, 95660.852, 95630.102, 94925.992, 94558.633, 93225.523, 93729.781, 92601.953, 91902.992, 91416.781, 91192.875, 91688.203, 89492.617, 90109.992, 88945.195, 88299.031, 88096.719, 88691.578, 87803.5, 87828.586, 86526.914, 86025.227, 86843.062, 84933.633, 85241.594, 85677.367, 85241.969, 83788.773, 84422.969, 83794.242, 83562.43, 84533.484, 83309.203, 82597.883, 82466.711, 81956.844, 82761.922, 82080.938, 81449.375, 82085.477, 81780.336, 80667.867, 80334.703, 81256.242, 80517.789, 80147.953, 79772.758, 79798.602, 79555.344, 79313.336, 79445.531, 78355.992, 78802.891, 78690.172, 78827.711, 78210.93, 78184.367, 78498.992, 77579.211, 77775.688, 78668.695, 77358.562, 77919.281, 77387.875, 77187.375, 77018.734, 76106.867, 76183.188, 76562.625, 75558.977, 76272.461, 75970.07, 76503.0, 76175.648, 75924.422, 76258.188, 75677.914, 75549.969, 74735.539, 75096.547], 'val_loss': [4906.395, 2180.641, 1924.424, 1874.633, 2113.681, 1773.985, 1772.241, 1597.088, 1683.086, 1687.831, 1482.299, 1496.819, 1553.596, 1511.328, 1630.74, 1465.56, 1424.247, 1486.722, 1447.26, 1458.595, 1461.908, 1422.709, 1433.501, 1307.501, 1464.622, 1330.639, 1370.455, 1409.895, 1255.894, 1356.806, 1367.667, 1331.37, 1357.153, 1426.187, 1287.039, 1395.259, 1356.438, 1215.101, 1315.67, 1255.885, 1248.653, 1165.376, 1198.99, 1295.38, 1190.899, 1203.414, 1169.08, 1154.373, 1131.242, 1152.275, 1188.363, 1143.493, 1121.355, 1108.902, 1063.073, 1129.013, 1105.162, 1085.573, 1121.948, 1086.12, 1073.544, 1072.486, 1100.903, 1051.312, 1060.021, 1087.747, 1060.013, 1163.035, 1065.534, 1074.182, 1075.487, 1048.435, 1070.959, 1140.314, 1054.09, 1103.523, 1049.89, 1056.1, 1049.885, 1089.109, 1049.271, 1035.385, 1150.594, 1024.27, 1066.898, 1041.12, 1033.439, 1055.586, 1026.789, 1016.058, 1035.887, 1035.618, 1047.333, 1059.616, 1005.231, 1018.425, 1013.484, 987.151, 1035.588, 1017.222]}	100	100	True
