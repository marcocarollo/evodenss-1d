id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	2000	True	4139.92529		470815	15	-1	188.70591235160828	{'train_loss': [389886.5, 367119.562, 286918.219, 212385.375, 193998.641, 181223.266, 178609.0, 176585.156, 174361.094, 171266.266, 168401.578, 164406.281, 135610.266, 117449.07, 110520.508, 106933.172, 103660.086, 101201.469, 99836.594, 98729.43, 97174.188, 95905.094, 96989.484, 95269.82, 94067.273, 92800.055, 93712.664, 92781.648, 92062.25, 90710.258, 90908.891, 89655.102, 89291.156, 88813.312, 88893.836, 87793.555, 87679.383, 87517.68, 87873.617, 87675.961, 86298.609, 85712.945, 87163.82, 85495.438, 85778.555, 84264.828, 85416.484, 84977.438, 83797.742, 83986.93, 83943.414, 83640.453, 82977.156, 83710.789, 82373.383, 82700.727, 82879.586, 82812.578, 81798.484, 81853.195, 82210.781, 81515.398, 80973.023, 81350.805, 80971.5, 81197.789, 80136.367, 80688.367, 80474.055, 80835.195, 80111.492, 79255.695, 79796.328, 79357.742, 79926.141, 78823.562, 79251.578, 79653.461, 79209.461, 78159.133, 79046.422, 79001.422, 78603.969, 78441.477, 78028.641, 78367.562, 77682.312, 77830.953, 77402.828, 77635.266, 77722.18, 77695.141, 77399.711, 77414.234, 77266.914, 77492.055, 76612.336, 75585.078, 76187.031, 77113.531], 'val_loss': [5330.597, 5054.458, 3337.93, 2595.907, 2904.539, 2591.216, 2572.989, 2446.142, 2347.308, 2435.122, 2338.472, 2575.482, 1896.049, 1728.172, 1675.324, 1579.744, 1620.013, 1567.762, 1608.615, 1510.237, 1424.991, 1318.021, 1383.784, 1513.647, 1413.702, 1305.716, 1321.764, 1292.001, 1299.338, 1272.308, 1299.488, 1415.772, 1272.71, 1383.359, 1263.56, 1255.433, 1223.384, 1218.14, 1213.021, 1189.33, 1195.023, 1238.682, 1198.437, 1164.273, 1221.012, 1195.825, 1173.599, 1231.348, 1163.354, 1204.092, 1196.815, 1147.884, 1163.484, 1167.405, 1243.372, 1158.968, 1139.934, 1145.044, 1244.473, 1313.339, 1132.125, 1133.084, 1140.972, 1167.292, 1204.851, 1109.718, 1130.241, 1191.084, 1096.461, 1144.64, 1124.483, 1140.732, 1083.213, 1070.283, 1112.849, 1129.807, 1206.208, 1138.868, 1092.383, 1153.411, 1093.745, 1148.241, 1117.677, 1098.028, 1120.723, 1094.263, 1133.881, 1068.717, 1097.39, 1082.228, 1103.817, 1107.27, 1119.974, 1130.656, 1167.105, 1077.67, 1053.827, 1075.832, 1080.452, 1051.527]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	2000	True	4105.6875		470815	15	-1	185.83865213394165	{'train_loss': [392665.094, 291440.531, 164094.734, 144656.031, 136383.188, 128571.484, 124563.414, 120523.773, 116419.469, 113027.445, 111464.578, 109499.922, 108494.797, 107220.789, 104813.758, 103807.648, 101481.625, 100458.5, 99602.562, 98805.32, 97085.555, 95660.852, 95630.102, 94925.992, 94558.633, 93225.523, 93729.781, 92601.953, 91902.992, 91416.781, 91192.875, 91688.203, 89492.617, 90109.992, 88945.195, 88299.031, 88096.719, 88691.578, 87803.5, 87828.586, 86526.914, 86025.227, 86843.062, 84933.633, 85241.594, 85677.367, 85241.969, 83788.773, 84422.969, 83794.242, 83562.43, 84533.484, 83309.203, 82597.883, 82466.711, 81956.844, 82761.922, 82080.938, 81449.375, 82085.477, 81780.336, 80667.867, 80334.703, 81256.242, 80517.789, 80147.953, 79772.758, 79798.602, 79555.344, 79313.336, 79445.531, 78355.992, 78802.891, 78690.172, 78827.711, 78210.93, 78184.367, 78498.992, 77579.211, 77775.688, 78668.695, 77358.562, 77919.281, 77387.875, 77187.375, 77018.734, 76106.867, 76183.188, 76562.625, 75558.977, 76272.461, 75970.07, 76503.0, 76175.648, 75924.422, 76258.188, 75677.914, 75549.969, 74735.539, 75096.547], 'val_loss': [4906.395, 2180.641, 1924.424, 1874.633, 2113.681, 1773.985, 1772.241, 1597.088, 1683.086, 1687.831, 1482.299, 1496.819, 1553.596, 1511.328, 1630.74, 1465.56, 1424.247, 1486.722, 1447.26, 1458.595, 1461.908, 1422.709, 1433.501, 1307.501, 1464.622, 1330.639, 1370.455, 1409.895, 1255.894, 1356.806, 1367.667, 1331.37, 1357.153, 1426.187, 1287.039, 1395.259, 1356.438, 1215.101, 1315.67, 1255.885, 1248.653, 1165.376, 1198.99, 1295.38, 1190.899, 1203.414, 1169.08, 1154.373, 1131.242, 1152.275, 1188.363, 1143.493, 1121.355, 1108.902, 1063.073, 1129.013, 1105.162, 1085.573, 1121.948, 1086.12, 1073.544, 1072.486, 1100.903, 1051.312, 1060.021, 1087.747, 1060.013, 1163.035, 1065.534, 1074.182, 1075.487, 1048.435, 1070.959, 1140.314, 1054.09, 1103.523, 1049.89, 1056.1, 1049.885, 1089.109, 1049.271, 1035.385, 1150.594, 1024.27, 1066.898, 1041.12, 1033.439, 1055.586, 1026.789, 1016.058, 1035.887, 1035.618, 1047.333, 1059.616, 1005.231, 1018.425, 1013.484, 987.151, 1035.588, 1017.222]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:4 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:123 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:conv1d out_channels:112 kernel_size:7 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:125 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:32 epochs:100	100	1000	True	4550.50879		513782	16	-1	180.9700379371643	{'train_loss': [342222.344, 202958.391, 160917.594, 140829.219, 130061.133, 125065.938, 120563.867, 116851.828, 115594.055, 111942.875, 110047.891, 109484.016, 106821.594, 106046.383, 104616.461, 102986.773, 102707.281, 101848.562, 100472.742, 99673.703, 99960.906, 99758.305, 98557.656, 97880.602, 98071.328, 96405.383, 96396.805, 95373.242, 94995.727, 94652.812, 94062.898, 93429.562, 93397.547, 93425.789, 93308.617, 92909.281, 92350.789, 91395.633, 91316.328, 91553.578, 90310.242, 90201.531, 90333.789, 89991.117, 89675.844, 89086.477, 88757.773, 89109.383, 88556.492, 88433.781, 88217.102, 87683.859, 88215.383, 87361.414, 86663.758, 86644.352, 87290.227, 85838.891, 86377.758, 86867.391, 85450.508, 86003.336, 84564.484, 85359.508, 85307.898, 85357.344, 84557.117, 84954.641, 84148.234, 84079.875, 83774.961, 84350.586, 84112.789, 83746.609, 83665.305, 83079.031, 83610.445, 83371.383, 83508.344, 83468.82, 83115.195, 83143.422, 82833.898, 83260.766, 81956.766, 82633.031, 83032.789, 82460.234, 81905.078, 82374.117, 81963.844, 82740.586, 81893.367, 81921.789, 81920.867, 80905.508, 81767.117, 81097.047, 80763.5, 81741.828], 'val_loss': [3421.543, 2772.07, 2356.267, 2132.766, 1917.727, 1839.897, 1787.707, 1685.163, 1670.161, 1580.874, 1527.139, 1454.01, 1447.772, 1571.124, 1385.852, 1377.983, 1422.945, 1421.519, 1347.206, 1359.155, 1312.89, 1342.423, 1379.613, 1281.214, 1294.541, 1318.871, 1251.847, 1281.531, 1227.573, 1180.714, 1260.888, 1270.745, 1219.066, 1236.858, 1184.499, 1220.921, 1189.92, 1162.234, 1240.343, 1266.747, 1186.53, 1213.218, 1143.807, 1131.557, 1160.64, 1185.484, 1197.291, 1231.619, 1110.2, 1160.367, 1235.031, 1154.354, 1136.659, 1164.999, 1140.083, 1134.552, 1155.436, 1158.062, 1204.887, 1115.589, 1075.396, 1135.733, 1081.212, 1121.816, 1131.597, 1133.107, 1105.371, 1103.085, 1125.384, 1059.857, 1181.791, 1146.194, 1065.556, 1090.505, 1153.232, 1133.518, 1107.124, 1090.343, 1117.376, 1083.799, 1066.017, 1059.174, 1151.379, 1100.997, 1049.905, 1069.246, 1082.936, 1054.664, 1035.501, 1072.126, 1088.731, 1072.828, 1051.372, 1067.413, 1092.97, 1074.307, 1065.81, 1011.651, 1045.014, 1058.783]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:47 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:62 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:121 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:110 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:rmsprop lr:0.08999775859637592 alpha:0.8672516374535759 weight_decay:7.681325204836857e-05 batch_size:32 epochs:100	100	1000	True	44540.25391		3045961	16	-1	850.7912621498108	{'train_loss': [848646.062, 804482.438, 1954948.5, 4869866.5, 804482.438, 1845650.75, 804482.438, 4376543.0, 804482.438, 3233914.25, 804482.438, 2140690.5, 804482.438, 3165161.25, 2372394.75, 804482.438, 2972717.75, 1417593.5, 4368087.5, 1046087.312, 871086.125, 2713679.25, 804482.438, 2834876.25, 804482.438, 2699232.75, 2270757.5, 845074.062, 1117199.625, 1498519.875, 916269.312, 2058400.625, 1241176.875, 1296468.25, 1123117.125, 1185664.5, 1514821.0, 1100056.25, 1283933.0, 1679998.75, 963645.125, 917921.5, 1569947.5, 1015350.188, 1101885.625, 1488477.25, 1041823.688, 1118659.875, 1110362.75, 1104718.25, 1119044.375, 1349316.875, 912759.0, 1378859.25, 1117694.75, 1285955.5, 1222636.75, 1252627.75, 1092954.25, 1256273.75, 1302795.875, 1116110.0, 1213203.875, 1177386.375, 1135556.25, 1417697.75, 1127561.125, 1010093.688, 1363187.375, 1493808.25, 1259959.75, 1162319.625, 1429854.75, 1131213.375, 1122291.375, 1333307.5, 1349259.125, 947676.75, 1284607.875, 979018.625, 1192677.625, 1162636.25, 1414855.75, 1074807.125, 1239840.75, 1274731.625, 1163464.25, 1044503.438, 1209507.0, 1376803.625, 1393955.0, 1297004.75, 1064486.875, 1210682.125, 1199011.75, 1346623.875, 1247589.5, 1288362.625, 1019579.688, 1190591.0], 'val_loss': [11044.456, 11044.456, 11044.455, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 27195.566, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 228757.828, 11044.456, 11044.456, 11044.456, 11044.456, 21049.572, 22006.248, 17318.299, 11044.456, 11044.456, 27159.898, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 21026.451, 11044.456, 12114.977, 11044.456, 11044.456, 11044.456, 11044.456, 11044.232, 11044.456, 11044.456, 22467.141, 46328.082, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 13395.096, 15932.406, 11044.456, 11044.456, 11044.456, 11044.456, 62561.926, 21259.268, 11326.811, 11044.455, 11044.445, 11044.456, 11044.456, 11044.455, 11044.456, 11044.456, 11044.414, 18557.803, 11044.403, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 17728.035, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 14960.0, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 23936.764, 11044.456, 11044.456, 11044.456]}	100	100	True
