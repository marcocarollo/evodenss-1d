id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4079.34131		407937	14	-1	189.43184328079224	{'train_loss': [436485.75, 317578.281, 160040.656, 137760.453, 128025.508, 123289.43, 119607.422, 114598.094, 111583.516, 110215.5, 107404.164, 105155.898, 102659.547, 100875.672, 100628.477, 98215.719, 97388.805, 96466.102, 94152.812, 93471.633, 92762.648, 91125.602, 89817.086, 89826.664, 89140.141, 88091.086, 86764.984, 85763.609, 85607.797, 85854.461, 85480.461, 84239.422, 83685.219, 83840.758, 84403.805, 81888.406, 82969.445, 80943.344, 81408.07, 81125.703, 81319.391, 81336.492, 79560.867, 80021.141, 80049.547, 79447.984, 79398.672, 78923.875, 77727.711, 78563.078, 77752.734, 77948.977, 77364.375, 77260.961, 76672.734, 77009.414, 75961.656, 76517.977, 76895.062, 76566.297, 75023.891, 74499.922, 75752.852, 75107.711, 76137.492, 74594.859, 75177.781, 74554.203, 74843.82, 74055.031, 74531.516, 74173.516, 73633.172, 73578.562, 73630.188, 73860.0, 74457.922, 73322.539, 73332.195, 72595.055, 72803.906, 73111.148, 72606.414, 72420.461, 71544.492, 72346.055, 72774.984, 71860.031, 72366.969, 71318.375, 71469.68, 71830.852, 71992.93, 71499.078, 72202.922, 70529.398, 71688.352, 71218.234, 70971.062, 70485.445], 'val_loss': [6616.528, 3107.388, 1963.463, 2068.22, 1801.706, 1797.984, 1619.783, 1618.554, 1731.672, 1556.399, 1515.334, 1449.352, 1459.001, 1455.47, 1383.95, 1414.639, 1418.897, 1348.75, 1420.77, 1293.042, 1278.58, 1308.515, 1342.621, 1316.323, 1230.766, 1256.828, 1219.266, 1224.694, 1259.612, 1256.916, 1172.648, 1168.256, 1217.483, 1206.117, 1146.071, 1234.455, 1223.852, 1202.66, 1192.294, 1176.967, 1192.073, 1123.643, 1144.763, 1119.236, 1150.173, 1200.398, 1093.89, 1097.425, 1109.592, 1032.15, 1089.755, 1109.571, 1074.738, 1056.519, 1085.415, 1103.169, 1068.668, 1062.628, 1052.996, 1034.655, 1040.709, 1037.606, 1055.331, 1038.492, 1019.415, 1045.383, 1050.469, 1015.791, 1143.651, 1023.112, 1097.776, 1077.119, 1053.54, 1027.381, 1008.365, 1010.69, 1033.873, 981.336, 1012.913, 995.612, 969.213, 1015.621, 1004.001, 948.957, 973.472, 963.997, 971.666, 1024.781, 977.672, 1054.529, 981.128, 1000.377, 972.774, 990.419, 965.617, 963.317, 983.412, 1039.277, 979.069, 970.763]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:deconv1d out_channels:39 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:32 epochs:100	100	1000	True	44574.33984		41758139	16	-1	416.99367332458496	{'train_loss': [2243475.75, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	3998.26489		479265	14	-1	254.27419209480286	{'train_loss': [546900.062, 362740.0, 166007.188, 146466.359, 133876.094, 125511.266, 119676.867, 115638.914, 112069.242, 109516.805, 106469.758, 105276.812, 103226.305, 101585.68, 99101.805, 99275.195, 96828.594, 95888.266, 94985.07, 94315.383, 92834.008, 91974.695, 91408.188, 89969.008, 90476.828, 89147.219, 89142.734, 87688.383, 86967.523, 86087.891, 85123.617, 85889.469, 84413.672, 85439.93, 83776.047, 83633.961, 83653.477, 83283.922, 81445.406, 82062.508, 81628.742, 80832.18, 80539.031, 80433.414, 80507.789, 79992.016, 79500.805, 78607.336, 78701.586, 79345.195, 78493.555, 78424.141, 78292.531, 77049.148, 77460.266, 76475.211, 76208.867, 77724.469, 75640.789, 76559.539, 75981.773, 76527.258, 75924.297, 75991.57, 75101.07, 75352.594, 75108.055, 75494.773, 74490.82, 74125.477, 74673.898, 75591.547, 75008.555, 74545.852, 74336.742, 73837.93, 73581.766, 73801.312, 72882.758, 72721.641, 74141.32, 73024.922, 72274.461, 72910.312, 72468.812, 72538.352, 71783.586, 72677.844, 71419.016, 70255.836, 71111.234, 71511.188, 70351.375, 71216.891, 70512.477, 71275.086, 70351.938, 70688.047, 70478.32, 70559.094], 'val_loss': [5188.317, 3006.902, 2341.079, 1903.889, 1674.464, 1913.326, 1693.307, 1654.933, 1603.226, 1596.374, 1523.871, 1396.186, 1496.263, 1445.121, 1435.629, 1353.415, 1408.841, 1458.731, 1331.594, 1322.885, 1293.633, 1306.971, 1321.204, 1242.219, 1244.439, 1203.838, 1300.544, 1178.396, 1251.097, 1174.168, 1198.335, 1142.879, 1182.088, 1234.194, 1197.682, 1177.415, 1254.207, 1191.867, 1209.824, 1165.835, 1095.973, 1130.277, 1103.758, 1036.535, 1150.289, 1114.46, 1179.422, 1102.543, 1050.885, 1063.258, 1091.875, 1127.491, 1039.758, 1066.083, 1069.9, 1054.059, 1032.04, 1013.181, 1056.495, 991.428, 1049.134, 1044.928, 1070.301, 1060.398, 1012.702, 1041.431, 1037.15, 1052.44, 1098.859, 1022.184, 1060.954, 1037.804, 1058.787, 986.776, 986.536, 973.286, 987.653, 1010.044, 1010.328, 980.689, 979.689, 954.658, 962.835, 954.997, 986.749, 952.446, 988.972, 948.91, 977.1, 964.46, 964.778, 937.022, 931.951, 959.644, 925.758, 974.451, 913.61, 961.163, 951.311, 926.651]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:36 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4507.61719		398945	14	-1	198.90681791305542	{'train_loss': [438308.281, 309778.438, 165719.672, 144026.812, 134192.719, 127469.188, 122906.969, 119134.219, 115655.117, 112712.906, 111044.547, 109824.039, 109013.359, 106477.891, 105065.797, 104463.141, 103200.727, 102722.5, 101108.18, 100261.641, 99707.188, 98163.828, 97958.703, 97925.766, 97240.938, 96660.922, 95627.914, 95042.258, 95619.695, 94640.328, 94950.945, 93143.906, 93112.461, 92596.766, 92301.062, 91353.57, 92333.352, 90013.43, 91286.57, 89513.617, 91218.023, 89455.289, 88132.859, 88630.672, 88018.398, 88392.688, 88083.961, 86822.844, 86862.406, 87529.203, 85750.266, 86417.656, 87232.25, 85696.391, 85945.844, 85670.852, 85098.844, 85524.969, 84598.359, 85786.977, 82969.648, 84999.055, 83348.664, 82883.562, 83235.047, 83682.141, 83657.297, 83358.281, 83240.562, 82742.734, 82994.242, 82370.562, 82177.828, 82453.172, 81419.992, 81155.422, 82377.203, 81459.016, 81042.586, 80423.773, 80911.727, 79834.352, 80501.516, 80388.836, 79906.484, 80441.758, 80210.336, 79234.539, 80449.914, 79747.0, 79908.148, 78753.055, 80290.898, 79260.891, 79061.352, 79220.945, 78801.227, 79308.906, 78318.125, 78832.719], 'val_loss': [5681.624, 2139.294, 1708.224, 1693.914, 1711.269, 1633.962, 1707.567, 1700.35, 1581.381, 1569.545, 1522.587, 1545.556, 1496.109, 1460.441, 1469.89, 1398.117, 1366.275, 1365.605, 1380.699, 1380.485, 1372.127, 1348.879, 1402.321, 1432.512, 1328.024, 1386.209, 1381.937, 1343.306, 1398.198, 1378.954, 1343.523, 1276.592, 1353.17, 1281.05, 1291.754, 1336.289, 1239.842, 1233.375, 1311.881, 1247.162, 1339.28, 1286.325, 1274.603, 1285.01, 1275.473, 1236.184, 1178.47, 1253.295, 1220.022, 1240.984, 1254.025, 1267.844, 1175.885, 1203.304, 1263.71, 1234.984, 1193.367, 1224.975, 1229.785, 1196.683, 1161.499, 1176.038, 1153.696, 1190.511, 1237.31, 1274.984, 1201.212, 1210.648, 1216.763, 1144.796, 1130.118, 1226.59, 1169.494, 1180.079, 1117.53, 1117.443, 1099.703, 1154.865, 1122.226, 1138.732, 1062.517, 1137.12, 1078.322, 1111.03, 1094.625, 1113.78, 1137.202, 1151.139, 1130.46, 1093.584, 1083.947, 1035.033, 1083.89, 1087.128, 1119.128, 1063.29, 1096.441, 1128.464, 1109.928, 1169.76]}	100	100	True
