id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	2000	True	4099.23291		470815	15	-1	196.11040329933167	{'train_loss': [339538.375, 167924.109, 151358.234, 140690.0, 134466.656, 128496.773, 122917.016, 118981.117, 116451.469, 113652.367, 111242.414, 109922.594, 108056.125, 106292.32, 105180.516, 103842.312, 102811.273, 101659.938, 100299.68, 99606.383, 97784.984, 97284.445, 95911.305, 95467.688, 95909.516, 93468.852, 93149.477, 92244.008, 91272.828, 91844.5, 90940.578, 90230.047, 89808.945, 89486.18, 90721.109, 88827.039, 89298.742, 87460.438, 88739.484, 86361.141, 87185.578, 87877.188, 85690.258, 86035.727, 85668.625, 85214.781, 85593.531, 84528.367, 85410.617, 83393.078, 82779.867, 83663.867, 82620.68, 82310.844, 83225.82, 81891.07, 81912.898, 81685.578, 81472.852, 81142.883, 80761.891, 81119.109, 79464.297, 81659.352, 80553.898, 80326.094, 79874.711, 79151.719, 79003.789, 78858.68, 79426.773, 79647.398, 79349.633, 79428.891, 78548.242, 78510.375, 78652.469, 78165.688, 77565.367, 77721.68, 77432.656, 77902.344, 77572.086, 76892.703, 77282.305, 77605.812, 77766.336, 77050.844, 76852.773, 77159.062, 76915.227, 76093.922, 75698.172, 76755.766, 76459.062, 76252.453, 75609.266, 75684.953, 75964.43, 75566.562], 'val_loss': [2827.462, 2053.794, 1895.848, 1913.49, 1640.748, 1650.208, 1584.811, 1701.76, 1706.081, 1568.412, 1626.299, 1557.033, 1604.481, 1628.056, 1622.891, 1525.65, 1582.963, 1485.923, 1528.08, 1405.043, 1415.445, 1476.196, 1355.683, 1380.121, 1400.042, 1356.931, 1392.827, 1392.228, 1364.962, 1341.376, 1491.058, 1297.141, 1270.708, 1343.214, 1304.811, 1261.066, 1304.816, 1262.801, 1259.616, 1282.238, 1192.128, 1164.248, 1238.109, 1158.15, 1188.015, 1170.46, 1232.212, 1125.124, 1179.903, 1262.673, 1152.74, 1136.21, 1199.198, 1157.452, 1114.013, 1100.609, 1130.184, 1087.862, 1134.785, 1110.661, 1097.583, 1101.586, 1080.821, 1117.114, 1093.866, 1153.58, 1105.598, 1090.516, 1078.794, 1110.707, 1147.942, 1118.748, 1081.984, 1101.221, 1074.423, 1112.116, 1119.546, 1080.371, 1100.689, 1061.158, 1064.281, 1048.251, 1040.84, 1054.121, 1083.758, 1029.387, 1034.198, 1066.249, 1054.194, 1028.224, 1043.053, 1086.102, 1031.834, 1016.891, 1034.244, 1060.314, 1051.752, 1038.273, 1054.075, 1103.154]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:126 kernel_size:8 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:101 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:32 epochs:100	100	1000	True	4124.30859		624353	16	-1	180.78519916534424	{'train_loss': [243818.469, 170104.797, 155750.172, 148229.641, 140202.016, 134496.453, 129850.93, 127449.633, 122876.617, 118814.141, 116110.242, 114709.156, 112265.188, 109554.555, 108141.734, 106740.703, 105454.844, 103392.164, 103085.672, 100476.711, 98702.195, 97564.102, 96305.32, 95910.758, 94479.812, 94376.68, 92580.008, 92758.703, 91785.516, 89771.555, 89919.992, 88170.242, 89553.219, 88397.414, 87060.711, 87909.258, 86508.789, 86702.031, 85616.977, 84940.477, 85339.375, 84888.078, 84353.852, 84255.844, 83721.555, 84377.633, 82951.945, 84075.75, 82816.812, 82627.078, 82130.703, 81862.609, 81845.289, 81842.016, 81402.516, 81037.203, 80728.117, 81011.648, 81012.109, 80413.664, 79346.094, 80184.391, 79551.016, 79285.562, 79784.844, 79285.297, 79313.562, 79273.875, 78200.414, 79520.086, 79048.062, 78260.438, 77904.398, 78177.0, 77507.617, 76957.133, 77402.172, 77069.68, 76579.656, 76747.445, 76487.531, 76339.578, 76074.742, 76283.43, 75740.031, 76081.523, 75473.57, 75884.156, 75760.133, 75306.93, 75333.281, 74861.031, 74953.906, 74654.906, 75810.555, 74084.445, 74707.555, 74422.18, 73897.109, 74483.633], 'val_loss': [2718.731, 1973.861, 1660.501, 1717.536, 1682.995, 1630.015, 1615.804, 1549.022, 1509.425, 1545.733, 1528.726, 1427.471, 1500.664, 1364.717, 1456.624, 1454.173, 1357.752, 1387.344, 1352.972, 1338.417, 1332.47, 1365.128, 1352.445, 1372.596, 1325.822, 1384.925, 1347.665, 1341.494, 1312.393, 1302.148, 1235.972, 1271.0, 1208.688, 1223.595, 1195.517, 1220.919, 1249.565, 1171.916, 1228.538, 1286.076, 1183.633, 1126.663, 1146.352, 1187.146, 1111.47, 1190.508, 1247.982, 1123.952, 1137.762, 1109.117, 1200.309, 1133.048, 1089.005, 1098.247, 1148.819, 1083.214, 1095.95, 1058.276, 1114.725, 1024.279, 1169.267, 1050.851, 1083.983, 1097.968, 1058.0, 1082.502, 1015.54, 1006.73, 1064.187, 1068.035, 1049.151, 1092.152, 1051.265, 1003.869, 1050.105, 1023.617, 1045.698, 1075.22, 973.012, 983.371, 1043.372, 991.285, 1034.815, 997.732, 1059.013, 1023.579, 984.111, 1009.35, 981.611, 976.616, 1026.496, 1010.956, 973.622, 985.579, 999.766, 997.744, 1005.143, 999.426, 983.134, 989.755]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	3000	True	4412.42041		470815	15	-1	197.16718316078186	{'train_loss': [393440.094, 368826.844, 257612.031, 198649.812, 184882.203, 177443.609, 174090.438, 162143.25, 137868.516, 123253.414, 116141.492, 112118.148, 108685.055, 107096.797, 107651.867, 103555.297, 101062.516, 100309.773, 98505.023, 97363.016, 95149.531, 95394.398, 94774.891, 93275.07, 92265.477, 91839.898, 91242.93, 90570.555, 90452.117, 89149.031, 89615.539, 88533.398, 87519.094, 88294.023, 86876.938, 87623.688, 86520.086, 85093.766, 84965.836, 85088.195, 84085.594, 84031.539, 84537.961, 83378.375, 83687.008, 82697.938, 82036.914, 82820.711, 82204.125, 82078.008, 80884.945, 81486.891, 81595.734, 80810.578, 81168.125, 80021.008, 80043.102, 80050.641, 79825.609, 79396.18, 79103.07, 79746.055, 79644.008, 78722.266, 77632.828, 77938.18, 78051.898, 78584.367, 78235.438, 77845.648, 77790.25, 77481.836, 77219.461, 77360.75, 76896.0, 76929.133, 77225.234, 77104.617, 75849.969, 76617.617, 75671.5, 76079.344, 75908.781, 75301.672, 76195.242, 74735.086, 74419.641, 74564.781, 75219.617, 75079.531, 74411.617, 74454.344, 74881.406, 74595.734, 74154.055, 73910.43, 73329.406, 73809.898, 74162.492, 72310.766], 'val_loss': [5354.752, 4943.657, 3019.19, 3178.562, 2503.413, 2374.505, 2830.62, 2257.185, 2056.062, 1874.421, 1657.483, 1907.955, 1642.297, 1578.165, 1657.691, 1690.356, 1461.961, 1390.773, 1558.892, 1353.94, 1456.773, 1295.278, 1336.697, 1427.47, 1266.729, 1354.167, 1283.955, 1294.811, 1336.828, 1294.681, 1280.651, 1278.049, 1283.351, 1338.042, 1345.379, 1171.527, 1236.278, 1243.798, 1216.288, 1234.69, 1231.678, 1186.767, 1373.151, 1201.372, 1220.126, 1181.009, 1199.093, 1209.894, 1198.469, 1163.462, 1151.521, 1152.42, 1090.108, 1156.825, 1208.578, 1124.422, 1259.938, 1159.774, 1065.223, 1136.258, 1098.19, 1094.612, 1117.43, 1153.802, 1112.047, 1107.557, 1129.053, 1120.494, 1058.992, 1074.711, 1097.122, 1040.777, 1100.765, 1072.064, 1102.505, 1048.991, 1140.826, 1045.258, 1083.23, 1055.058, 1034.873, 1012.871, 1058.048, 1053.83, 1064.331, 1066.104, 1119.564, 1011.098, 1023.726, 1031.617, 1018.169, 1046.799, 1093.569, 1121.331, 1094.132, 1038.171, 1087.992, 1037.51, 1015.464, 1104.575]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4087.95898		407937	14	-1	188.77144122123718	{'train_loss': [437462.844, 330517.625, 215187.016, 143824.328, 132840.875, 124327.578, 119579.469, 113890.75, 109745.969, 108046.477, 103325.461, 101516.75, 99491.703, 98050.531, 96948.586, 95031.641, 94384.945, 92364.148, 91686.195, 90772.992, 90921.078, 90246.578, 89104.203, 87725.859, 87476.016, 86938.195, 87121.414, 86139.531, 85771.242, 86239.234, 85114.469, 84224.062, 84678.641, 84417.195, 83876.023, 82576.445, 83242.336, 82789.891, 82058.016, 82583.414, 82026.219, 81746.203, 81213.203, 80431.211, 81180.328, 80322.102, 80268.281, 80524.164, 79603.609, 78708.016, 79581.312, 79106.703, 79047.273, 78391.398, 78570.5, 78872.891, 77817.844, 78323.812, 77789.195, 77800.852, 77755.844, 77133.727, 77409.172, 76727.836, 76212.289, 76977.938, 76119.484, 76251.789, 76920.664, 75797.75, 76037.359, 74866.484, 75239.539, 74591.258, 75474.82, 75515.344, 74433.484, 74831.555, 75216.336, 73936.305, 74954.703, 73795.984, 73841.5, 73988.695, 72941.039, 73478.797, 73414.789, 72994.438, 72989.203, 72456.484, 72407.305, 72771.312, 72643.844, 71516.008, 71810.773, 72312.305, 71216.453, 71848.531, 70878.875, 71519.469], 'val_loss': [5321.233, 3249.536, 2299.951, 1890.228, 1699.924, 1657.84, 1423.247, 1547.603, 1468.825, 1383.825, 1450.051, 1333.71, 1358.108, 1275.307, 1298.673, 1210.457, 1197.839, 1261.895, 1241.802, 1173.535, 1121.889, 1190.06, 1182.12, 1170.39, 1142.154, 1115.923, 1160.569, 1112.527, 1144.523, 1108.306, 1073.169, 1127.031, 1135.916, 1116.289, 1100.859, 1116.297, 1014.193, 1091.63, 1071.768, 1033.016, 1002.177, 1052.855, 1029.828, 1097.326, 1006.577, 1055.377, 1029.528, 1021.199, 1026.548, 1039.629, 1029.578, 1085.543, 1061.473, 1022.199, 1033.451, 1010.587, 992.247, 990.506, 1126.02, 988.865, 1077.78, 1034.948, 1008.565, 1058.782, 996.758, 1003.204, 968.682, 963.779, 991.761, 1040.665, 1029.0, 975.785, 1025.579, 1028.293, 1001.18, 989.758, 979.061, 989.105, 1054.627, 991.039, 975.506, 988.107, 979.111, 1002.195, 1007.828, 979.009, 972.395, 1026.356, 1026.697, 993.759, 990.086, 983.612, 1055.365, 961.073, 964.534, 989.584, 989.218, 1024.405, 939.017, 993.918]}	100	100	True
