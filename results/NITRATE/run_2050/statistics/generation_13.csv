id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	3000	True	4003.51855		491663	14	-1	283.1952950954437	{'train_loss': [521305.062, 287340.188, 227997.391, 203960.5, 187191.891, 153421.938, 131728.484, 121792.062, 116215.93, 112781.297, 109693.68, 105546.969, 104992.508, 103617.555, 100380.438, 99516.891, 97459.336, 97830.562, 95857.68, 95412.188, 94081.625, 93579.008, 92527.297, 91563.453, 89971.023, 88492.844, 88836.461, 88242.562, 88095.445, 87345.656, 86039.961, 86124.133, 86200.555, 85470.117, 85001.672, 83619.18, 84015.594, 82881.312, 83046.078, 81935.633, 82577.742, 80882.977, 81096.508, 80968.656, 80928.719, 80170.148, 78878.938, 80231.852, 80645.719, 77595.844, 79672.82, 78445.086, 78074.164, 77131.164, 77947.125, 77784.758, 77375.812, 76617.391, 76705.344, 76957.211, 76584.836, 75703.031, 75852.508, 74367.492, 75958.594, 74050.57, 74406.336, 75205.242, 74682.992, 73939.508, 73997.227, 72453.688, 73504.32, 74286.594, 72815.922, 73716.203, 74195.281, 73058.594, 73071.469, 72608.477, 72087.695, 72308.836, 71966.156, 73998.078, 71389.273, 70927.953, 70546.039, 72271.773, 70734.039, 70820.305, 71516.273, 70371.18, 70753.602, 69584.117, 70179.656, 70280.898, 70027.195, 70328.047, 68442.516, 69456.078], 'val_loss': [4872.791, 3544.679, 2739.069, 2625.229, 2984.518, 1940.058, 1926.773, 1717.813, 1693.922, 1632.074, 1578.009, 1610.44, 1489.796, 1505.671, 1525.099, 1431.079, 1459.426, 1439.927, 1471.125, 1489.386, 1537.389, 1358.896, 1307.906, 1405.215, 1404.85, 1271.883, 1427.097, 1382.657, 1339.906, 1403.644, 1320.004, 1396.059, 1435.056, 1307.502, 1352.164, 1309.527, 1347.41, 1176.267, 1315.063, 1409.345, 1192.613, 1170.179, 1312.697, 1411.078, 1213.021, 1116.494, 1158.842, 1314.002, 1196.486, 1244.166, 1235.466, 1338.084, 1247.249, 1207.875, 1191.266, 1276.736, 1269.958, 1181.905, 1211.788, 1291.234, 1177.103, 1368.51, 1192.619, 1306.561, 1285.475, 1212.97, 1194.867, 1355.899, 1159.376, 1148.155, 1075.108, 1197.524, 1279.826, 1203.535, 1110.228, 1200.059, 1153.489, 1052.231, 1154.625, 1206.712, 1039.056, 1096.558, 1109.403, 1138.923, 1115.007, 1177.493, 1143.924, 1103.231, 1096.705, 1037.618, 1135.668, 1054.755, 1103.901, 1100.816, 1037.55, 1097.449, 1121.424, 1047.408, 1100.583, 1104.051]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:99 kernel_size:8 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	4218.45215		502944	15	-1	266.5170884132385	{'train_loss': [568820.625, 429815.312, 333071.812, 224110.984, 191322.688, 136377.875, 124157.242, 116501.664, 112759.922, 109757.094, 107549.32, 104295.055, 102789.984, 100717.414, 99080.047, 98031.672, 97325.242, 96123.203, 95967.352, 94570.43, 93113.078, 92793.883, 92787.133, 91701.711, 90983.422, 91533.648, 90340.461, 89038.703, 89041.812, 88010.93, 87034.969, 88377.086, 86608.953, 87436.469, 85706.273, 85694.734, 85131.523, 83655.328, 84344.383, 82943.938, 83010.43, 82103.789, 82059.188, 81833.828, 81070.914, 81592.914, 80891.383, 80189.477, 80251.25, 79759.023, 79849.32, 79258.062, 78794.906, 79220.539, 78727.609, 78555.875, 77882.984, 79086.18, 77380.211, 77721.438, 77468.289, 77273.633, 78291.961, 76515.117, 76816.18, 76443.508, 77371.109, 76049.438, 76258.578, 76164.891, 76476.883, 74789.312, 74844.852, 76811.18, 76271.797, 74863.508, 75814.539, 74492.062, 74497.836, 75388.781, 75097.5, 74711.344, 74301.492, 73732.523, 72988.422, 74125.516, 74487.898, 73347.883, 73116.539, 73439.656, 73829.688, 73330.102, 73046.641, 72527.211, 72579.75, 72208.727, 72613.914, 72171.055, 72242.094, 72230.133], 'val_loss': [5381.334, 5285.53, 3418.525, 2795.299, 2157.847, 1867.772, 1660.542, 1588.691, 1427.907, 1437.258, 1505.419, 1417.661, 1464.535, 1353.983, 1361.297, 1427.244, 1340.52, 1324.294, 1272.011, 1241.533, 1245.787, 1256.391, 1227.146, 1215.224, 1202.283, 1238.37, 1189.908, 1162.865, 1228.29, 1231.568, 1217.39, 1217.322, 1199.414, 1179.169, 1158.548, 1262.132, 1165.988, 1258.517, 1124.598, 1150.531, 1119.214, 1211.494, 1198.347, 1107.279, 1130.072, 1126.648, 1121.843, 1164.922, 1212.757, 1137.281, 1161.296, 1087.426, 1138.006, 1141.132, 1087.478, 1118.179, 1191.837, 1072.395, 1125.802, 1137.879, 1193.848, 1132.671, 1088.399, 1106.253, 1088.9, 1166.128, 1070.215, 1011.524, 1071.598, 1110.06, 1040.106, 1100.333, 1187.912, 1128.327, 1114.083, 1090.289, 1049.058, 1053.235, 1064.764, 1055.129, 1176.656, 1069.95, 1036.749, 1082.916, 1109.217, 1106.347, 1063.683, 1068.953, 1050.277, 1043.823, 1042.596, 1053.351, 1069.992, 1015.932, 1057.966, 989.381, 1039.426, 1038.338, 1088.064, 1032.222]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	3000	True	3871.68359		491663	14	-1	270.9908912181854	{'train_loss': [536777.062, 310867.844, 224851.141, 197864.875, 183615.844, 165162.703, 134514.672, 118531.867, 113233.82, 108628.93, 105724.688, 101722.234, 99944.938, 97417.164, 96303.812, 95542.508, 94605.414, 93441.211, 91418.773, 92539.766, 91195.781, 90329.406, 89294.891, 89018.742, 88443.477, 87613.312, 86035.258, 85167.836, 85481.664, 85186.18, 85264.891, 84231.18, 82686.266, 82462.758, 83258.172, 81729.188, 81224.906, 80799.836, 80409.578, 79406.328, 80912.336, 79190.695, 79345.617, 79091.461, 78341.664, 78613.438, 77160.766, 78150.711, 78314.664, 76670.898, 77314.797, 77157.875, 76102.539, 75693.914, 76009.016, 74468.0, 75141.758, 75630.906, 74839.086, 74634.945, 74090.422, 74138.656, 74501.305, 73851.836, 73271.641, 73369.281, 73401.25, 73027.859, 73385.969, 72985.273, 73369.867, 72498.961, 71777.039, 71968.953, 72632.438, 71069.789, 71794.328, 71613.445, 70098.195, 72393.969, 72032.32, 71741.984, 70916.305, 70306.805, 71024.117, 71482.273, 69903.969, 70543.898, 69870.703, 70655.773, 69370.891, 70526.484, 69311.031, 68793.547, 70223.898, 69827.555, 68825.312, 69187.07, 69004.797, 69953.281], 'val_loss': [5119.25, 3085.652, 2899.946, 2533.311, 2846.021, 2562.421, 2041.956, 1903.069, 1676.147, 1757.548, 1495.679, 1497.21, 1566.86, 1592.587, 1551.599, 1634.656, 1510.025, 1342.162, 1376.389, 1397.125, 1384.76, 1328.79, 1346.681, 1237.417, 1184.957, 1241.788, 1175.488, 1145.162, 1172.633, 1176.682, 1169.589, 1156.177, 1317.493, 1070.252, 1131.275, 1186.901, 1124.672, 1140.328, 1114.527, 1260.201, 1087.435, 1113.794, 1135.651, 1054.944, 1245.787, 1101.659, 1054.859, 1096.095, 1094.041, 1051.835, 1040.822, 1068.77, 1029.834, 1056.949, 1084.467, 1007.207, 1081.511, 1031.698, 1075.516, 1079.458, 1013.429, 987.475, 1009.505, 1018.544, 999.412, 997.78, 1045.0, 1030.729, 950.44, 992.493, 1052.784, 1016.866, 979.477, 977.599, 998.312, 993.033, 970.93, 990.313, 993.926, 948.525, 936.814, 957.148, 993.093, 958.535, 939.207, 1038.021, 988.054, 1002.6, 1016.269, 931.797, 946.082, 931.175, 993.128, 971.837, 926.073, 986.721, 967.651, 950.222, 1006.622, 961.064]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:64 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:12 epochs:100	100	3000	True	4012.01855		661343	15	-1	436.1900668144226	{'train_loss': [487860.438, 172198.953, 135715.812, 124488.891, 117634.734, 115172.0, 111358.227, 107479.242, 106022.641, 103266.539, 101853.555, 101788.32, 99415.266, 97527.273, 96570.773, 95281.398, 94107.164, 93358.773, 92301.516, 91174.328, 90616.0, 90447.938, 89051.805, 88548.5, 88115.469, 87445.508, 86936.758, 86984.133, 86216.078, 86381.547, 85354.539, 84904.312, 84807.641, 83549.93, 84316.039, 81627.508, 82191.164, 82192.062, 81742.094, 81487.555, 80456.586, 80494.609, 79723.625, 79375.961, 79819.344, 80045.391, 78942.438, 79080.336, 78991.766, 78571.391, 77540.523, 78810.023, 76930.75, 77395.359, 77737.398, 76463.844, 76230.742, 76648.094, 75836.281, 76032.703, 76556.594, 74681.258, 74885.672, 74881.352, 74312.867, 75320.523, 74201.414, 73993.844, 74338.164, 73470.578, 73600.242, 73157.734, 73254.336, 72587.016, 71853.016, 73265.438, 71871.57, 73005.07, 73223.797, 71892.023, 72778.594, 71008.422, 72620.242, 71061.648, 70875.047, 70427.859, 70486.398, 71763.82, 70989.375, 71279.812, 71650.422, 70689.797, 70864.156, 70403.688, 69808.406, 70211.578, 70160.023, 70013.664, 69514.844, 69606.25], 'val_loss': [885.563, 642.117, 632.584, 600.702, 594.742, 523.609, 542.124, 523.414, 517.484, 478.155, 479.623, 479.694, 502.98, 456.592, 448.886, 460.645, 467.672, 434.426, 423.384, 423.209, 418.818, 430.129, 430.761, 403.328, 419.038, 410.804, 394.522, 416.138, 415.403, 384.48, 425.416, 407.675, 406.89, 385.056, 393.374, 402.571, 439.874, 392.699, 386.642, 435.908, 419.595, 389.438, 380.469, 377.987, 400.719, 407.329, 418.035, 382.05, 397.169, 377.646, 407.572, 365.709, 401.583, 365.701, 387.63, 404.236, 387.046, 369.012, 380.639, 392.472, 360.189, 380.001, 390.701, 385.36, 438.099, 377.42, 392.599, 380.586, 370.051, 364.75, 364.949, 352.751, 358.198, 362.779, 375.155, 359.744, 376.052, 363.048, 358.418, 366.336, 363.64, 342.776, 357.087, 348.296, 362.666, 376.358, 382.379, 372.463, 374.842, 367.407, 372.522, 359.085, 363.365, 350.906, 351.868, 373.468, 371.197, 353.76, 373.457, 353.679]}	0	100	True
