id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	2000	True	3888.75415		479265	14	-1	260.37436413764954	{'train_loss': [522761.594, 242462.625, 172209.5, 154468.984, 139012.188, 131634.016, 124974.008, 121295.523, 117056.438, 115380.242, 112224.867, 111383.297, 108385.508, 106194.477, 105880.555, 103103.164, 102038.703, 101287.555, 99853.367, 97881.422, 97197.227, 95956.43, 95290.977, 92687.594, 91995.727, 92613.188, 90335.891, 90767.859, 89603.164, 88678.008, 87809.148, 88019.992, 86854.055, 87471.203, 86143.359, 85552.578, 85006.352, 85025.188, 83788.219, 84042.258, 82277.297, 83559.156, 82938.414, 82475.461, 82187.148, 80444.172, 80481.414, 80916.461, 80570.688, 79295.203, 78437.195, 80244.32, 77923.789, 78513.461, 77307.375, 78041.633, 76850.773, 77158.93, 78067.422, 77988.602, 76499.789, 75903.117, 76154.867, 76897.062, 76002.977, 74715.438, 75495.289, 73986.508, 74887.867, 74959.18, 74684.953, 73742.352, 74288.57, 73507.852, 74362.523, 72225.383, 71912.336, 72471.578, 72530.852, 72222.016, 72148.516, 72716.828, 72280.484, 71687.945, 70031.914, 71587.672, 71367.742, 71608.758, 70687.969, 70508.148, 71161.875, 70759.5, 70306.641, 70507.883, 70849.992, 70034.0, 70208.023, 69806.336, 69641.859, 70067.43], 'val_loss': [4589.632, 2067.991, 1797.303, 1592.688, 1495.445, 1504.401, 1582.933, 1434.338, 1437.504, 1389.688, 1380.637, 1317.397, 1309.962, 1338.279, 1492.436, 1330.804, 1280.875, 1374.81, 1279.97, 1296.509, 1239.532, 1224.596, 1253.931, 1275.702, 1195.432, 1296.622, 1218.409, 1264.167, 1164.391, 1193.874, 1207.243, 1174.546, 1159.327, 1159.064, 1136.647, 1276.963, 1174.102, 1143.291, 1162.147, 1179.081, 1111.11, 1167.152, 1129.224, 1075.094, 1126.187, 1132.148, 1198.327, 1209.132, 1136.36, 1106.244, 1092.335, 1085.905, 1110.071, 1090.284, 1110.26, 1092.308, 1112.852, 1071.733, 1067.242, 1070.06, 1091.116, 1061.639, 1042.033, 1119.0, 1115.913, 1034.467, 1100.121, 1072.381, 1074.166, 1092.083, 1036.835, 1052.412, 1072.313, 1071.228, 1071.715, 1047.366, 1060.264, 1064.701, 1057.77, 1031.513, 1127.786, 1063.634, 1092.311, 1009.845, 1033.333, 1045.52, 1168.34, 996.621, 1036.3, 1039.84, 1038.504, 1013.596, 1040.672, 1089.147, 1021.608, 980.63, 1060.77, 993.311, 1043.536, 987.342]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:77 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	2000	True	4051.82568		456070	14	-1	268.2007496356964	{'train_loss': [557672.5, 251248.797, 178586.062, 158377.516, 142103.578, 133613.625, 126484.391, 120502.664, 117747.367, 114199.906, 112769.281, 111028.656, 107189.141, 105916.031, 103897.797, 102864.547, 101349.641, 99761.039, 99417.875, 97664.125, 96903.875, 95827.219, 95194.328, 94399.102, 92942.641, 93099.25, 91190.93, 91484.617, 90583.414, 91515.477, 92130.555, 90953.586, 88903.086, 89068.227, 89125.688, 87081.539, 87404.859, 86149.602, 87009.242, 86500.992, 85851.461, 85088.445, 84160.758, 84662.875, 84479.398, 83868.703, 83315.953, 82431.281, 83264.211, 82699.703, 81789.219, 81679.227, 81632.609, 81684.031, 80829.461, 80245.297, 80270.258, 79051.805, 79686.039, 79263.562, 79517.172, 78841.531, 77813.242, 78470.859, 77420.258, 78015.664, 76718.219, 77064.461, 77672.367, 76963.727, 76711.352, 76125.25, 75329.047, 75214.82, 75910.984, 75096.484, 75177.57, 75927.766, 75058.523, 74084.172, 75654.578, 74801.312, 74484.117, 73594.773, 72972.195, 74242.531, 75014.508, 73110.0, 72566.82, 74787.758, 71365.133, 72755.945, 72513.82, 72260.719, 73380.656, 71957.625, 72977.031, 72119.125, 71590.75, 71482.883], 'val_loss': [7390.735, 3064.667, 1794.587, 1646.317, 1728.499, 1525.102, 1574.767, 1587.069, 1511.998, 1661.926, 1612.031, 1483.702, 1479.392, 1420.294, 1410.963, 1502.488, 1377.644, 1364.624, 1451.404, 1313.782, 1372.602, 1316.248, 1308.783, 1286.858, 1607.278, 1286.022, 1304.877, 1294.299, 1271.921, 1295.43, 1260.857, 1207.95, 1297.669, 1234.022, 1211.863, 1191.749, 1232.024, 1200.717, 1130.294, 1262.269, 1164.598, 1236.92, 1163.778, 1274.394, 1186.508, 1230.317, 1135.044, 1244.711, 1152.793, 1090.493, 1208.28, 1117.16, 1126.312, 1240.491, 1145.367, 1165.937, 1116.069, 1073.43, 1129.061, 1135.106, 1116.096, 1152.105, 1170.895, 1089.264, 1133.102, 1061.116, 1118.301, 1085.868, 1106.326, 1047.149, 1057.098, 1128.992, 1119.596, 1079.648, 1087.457, 1080.571, 1067.826, 1030.26, 1055.136, 1059.824, 1096.794, 1128.785, 1056.049, 1051.157, 1087.384, 1057.312, 1075.997, 1110.922, 1132.29, 1048.488, 1024.946, 1072.722, 1014.505, 1079.9, 1025.651, 1079.181, 1040.089, 987.417, 1005.352, 1073.786]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:96 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4187.5293		485385	14	-1	209.47683382034302	{'train_loss': [438685.344, 267944.938, 161923.094, 147749.297, 135115.25, 126963.625, 121996.836, 117771.156, 114769.844, 111580.281, 109060.586, 106393.0, 104669.977, 103693.555, 101899.68, 100982.164, 98500.867, 98280.172, 97424.047, 97004.203, 95610.836, 95430.133, 93993.898, 93273.445, 92651.328, 91792.391, 91615.109, 91868.867, 90261.922, 89832.719, 89721.047, 88052.352, 88903.859, 88507.516, 88630.117, 87291.203, 86592.789, 86799.883, 87012.344, 85979.039, 86507.031, 84932.328, 85739.5, 85963.648, 83979.539, 84760.891, 84100.883, 83477.656, 83563.094, 82777.078, 83318.266, 82316.898, 81825.609, 81861.664, 81126.07, 81634.156, 81391.938, 81204.719, 79758.344, 79914.555, 79238.141, 79734.625, 79468.586, 78692.945, 78096.383, 78681.188, 78733.148, 77666.062, 76920.07, 78319.859, 78004.68, 77185.188, 77147.742, 76258.258, 76206.633, 76520.188, 76180.055, 75737.227, 75605.891, 75466.383, 76049.617, 74155.305, 74822.367, 75199.852, 73633.516, 75025.406, 74084.953, 74498.273, 74235.727, 74931.188, 72836.281, 72886.008, 73832.039, 72847.805, 73506.875, 72884.711, 73544.352, 72648.008, 72526.312, 72499.258], 'val_loss': [5957.728, 4327.979, 1754.0, 1629.084, 1724.763, 1566.327, 1734.264, 1556.31, 1593.622, 1525.632, 1523.265, 1470.952, 1517.054, 1530.082, 1455.386, 1403.833, 1491.993, 1399.07, 1451.045, 1471.255, 1335.398, 1398.387, 1314.877, 1440.94, 1483.184, 1369.976, 1331.869, 1305.487, 1263.801, 1227.521, 1196.539, 1272.142, 1261.108, 1324.597, 1230.783, 1248.803, 1136.761, 1179.427, 1224.753, 1116.939, 1227.598, 1174.835, 1125.957, 1103.519, 1261.323, 1182.688, 1118.786, 1084.583, 1176.238, 1150.148, 1085.63, 1096.012, 1169.044, 1164.714, 1114.408, 1194.997, 1124.081, 1060.553, 1094.042, 1175.071, 1174.194, 1170.598, 1138.731, 1090.506, 1124.819, 1028.751, 1075.467, 999.14, 1119.455, 1193.334, 1065.78, 1118.962, 1023.893, 1194.472, 1124.446, 1029.471, 1031.789, 1067.895, 1019.959, 1106.365, 1050.478, 1050.096, 1116.032, 1042.029, 1030.127, 1071.082, 1039.726, 1053.369, 1065.004, 1030.521, 1099.361, 1049.941, 1029.221, 1035.245, 1090.354, 1098.606, 1054.517, 1037.19, 1095.446, 1002.247]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:84 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	4249.14355		473100	13	-1	205.32545256614685	{'train_loss': [343211.375, 183496.156, 164626.297, 151363.188, 140964.906, 132498.422, 126990.102, 121606.055, 119578.938, 118347.781, 114573.984, 112984.57, 110371.656, 109083.375, 107482.0, 106899.828, 105363.242, 104654.562, 104373.656, 102593.367, 101571.969, 100860.148, 100263.867, 99097.367, 98995.469, 98230.836, 97739.602, 96201.656, 95682.289, 95209.422, 94942.094, 94373.734, 93520.742, 93000.742, 92184.211, 91997.125, 90940.891, 91390.734, 90808.359, 90161.562, 89570.688, 88812.062, 88751.469, 87703.898, 87060.727, 87178.93, 87690.414, 86687.406, 86317.461, 86057.297, 85200.008, 85239.133, 85079.617, 84303.719, 84345.25, 84086.609, 83856.453, 83327.789, 83645.297, 82896.922, 82425.203, 82316.422, 82501.953, 81828.383, 80687.531, 81442.805, 80055.828, 81161.594, 80524.695, 80357.023, 79470.203, 79202.594, 79513.211, 79416.008, 79809.453, 78342.875, 79509.359, 78691.07, 79375.547, 78109.453, 77402.43, 77865.609, 78430.398, 76952.594, 77059.883, 76623.0, 76241.805, 76134.344, 77438.422, 75962.82, 75405.812, 75338.508, 76609.281, 75438.719, 75325.961, 75965.883, 74654.672, 75369.961, 74689.672, 73876.328], 'val_loss': [2128.002, 1845.002, 1967.709, 1924.736, 1756.116, 1784.43, 1780.068, 1752.44, 1683.92, 1607.575, 1568.097, 1520.761, 1492.766, 1510.793, 1514.767, 1511.231, 1467.207, 1511.86, 1424.698, 1469.941, 1451.188, 1399.857, 1395.636, 1388.309, 1334.99, 1416.291, 1433.855, 1401.762, 1422.698, 1460.056, 1370.027, 1388.902, 1332.156, 1399.146, 1336.747, 1347.969, 1297.187, 1329.68, 1249.552, 1290.275, 1305.185, 1345.112, 1210.678, 1325.91, 1263.789, 1250.194, 1190.439, 1158.618, 1214.979, 1317.735, 1216.658, 1178.23, 1139.785, 1170.735, 1199.599, 1180.461, 1179.558, 1293.836, 1165.626, 1212.87, 1180.288, 1163.702, 1217.336, 1208.718, 1095.573, 1294.238, 1161.883, 1274.72, 1108.408, 1101.681, 1152.16, 1178.868, 1160.773, 1191.463, 1134.532, 1164.282, 1163.759, 1196.227, 1253.243, 1189.171, 1128.345, 1080.7, 1090.173, 1122.651, 1170.668, 1148.409, 1104.974, 1109.551, 1204.695, 1070.294, 1081.038, 1085.717, 1180.347, 1072.62, 1068.958, 1142.446, 1148.771, 1121.631, 1055.696, 1051.17]}	100	100	True
