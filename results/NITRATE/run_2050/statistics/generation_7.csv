id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	3855.66748		479265	14	-1	259.70214891433716	{'train_loss': [505903.156, 270224.688, 220274.75, 199415.531, 154065.703, 133619.547, 124044.914, 117959.742, 113200.203, 109412.352, 105728.531, 103422.789, 100778.516, 98685.055, 97089.422, 95472.734, 94327.375, 92518.102, 92359.414, 90680.773, 89601.25, 88154.0, 88644.93, 87321.984, 86493.945, 85804.352, 84413.25, 85127.32, 84273.914, 83274.688, 82744.234, 82950.578, 82253.43, 81974.086, 80335.359, 81220.398, 80292.047, 80501.883, 80005.047, 79054.727, 77792.367, 78563.281, 78433.609, 77947.625, 78399.047, 77607.32, 76994.836, 77396.648, 77406.797, 76695.875, 76803.664, 76739.477, 75379.312, 75104.297, 76189.461, 75427.383, 74822.445, 74576.078, 75297.719, 74465.188, 75444.93, 74200.148, 74169.984, 74197.938, 73139.0, 73305.117, 73079.289, 73271.852, 72763.078, 71974.336, 73095.258, 73440.25, 72376.586, 71746.836, 71311.023, 72451.633, 72273.406, 72657.648, 71879.25, 71370.602, 71221.703, 70751.961, 70630.438, 71682.359, 70043.82, 70717.227, 71160.586, 71279.992, 71190.461, 70233.102, 69823.828, 70555.758, 69548.547, 69975.031, 69852.711, 70659.188, 69772.391, 69397.578, 70091.766, 70558.75], 'val_loss': [3764.716, 3166.103, 3094.96, 3055.299, 2112.193, 1998.963, 1757.744, 1717.285, 1602.969, 1540.471, 1546.421, 1415.241, 1453.875, 1448.03, 1589.75, 1495.705, 1388.942, 1320.358, 1259.499, 1368.383, 1238.605, 1281.72, 1300.912, 1297.341, 1322.415, 1207.626, 1288.1, 1228.933, 1239.154, 1180.807, 1175.486, 1248.412, 1146.199, 1320.934, 1172.415, 1083.66, 1142.192, 1089.302, 1111.076, 1152.476, 1107.328, 1183.901, 1153.744, 1164.331, 1054.683, 1062.413, 1155.502, 1041.043, 1089.696, 1066.748, 1113.49, 1120.793, 1091.928, 1109.322, 1139.429, 1151.799, 1216.153, 1166.057, 1082.183, 1137.153, 1095.812, 1116.22, 1209.622, 1044.893, 1143.282, 1143.5, 1037.391, 1043.855, 1151.572, 1074.057, 1118.715, 1025.261, 1097.547, 1012.268, 1073.63, 1030.755, 1109.907, 999.029, 1014.673, 1122.084, 1007.075, 1011.149, 1084.809, 1079.087, 1058.038, 1027.303, 1056.546, 1073.356, 1048.678, 1004.631, 1058.635, 1035.881, 1032.802, 1043.216, 1101.506, 1001.687, 974.431, 1028.427, 1099.812, 979.075]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:20 kernel_size:6 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:67 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	4158.2124		305697	13	-1	197.47661423683167	{'train_loss': [276413.5, 152135.5, 138436.281, 131197.828, 123920.328, 118869.078, 115358.82, 111715.93, 108666.883, 106409.516, 105083.656, 103213.719, 101602.133, 100023.695, 99683.789, 97428.477, 97508.648, 96033.766, 95038.727, 94212.922, 94318.094, 92873.211, 92278.945, 92236.148, 90114.367, 91018.133, 90323.031, 88926.172, 89270.547, 88061.312, 87750.969, 87755.57, 86941.086, 86710.109, 86504.492, 86661.516, 86151.742, 85418.914, 84827.555, 85592.461, 84018.203, 84723.445, 84442.0, 83865.156, 83060.641, 82891.055, 82897.039, 82989.391, 81772.664, 82007.703, 83035.453, 83328.656, 82326.008, 81902.305, 81762.477, 80977.758, 80692.094, 80944.719, 80739.031, 80803.82, 80586.461, 79628.188, 79640.289, 80105.211, 79732.312, 79315.586, 79154.547, 80060.25, 79041.172, 79263.375, 77854.594, 78922.484, 78844.945, 78898.68, 78566.055, 77134.273, 77582.867, 78154.758, 77297.523, 77334.469, 77562.664, 76901.633, 77209.352, 76670.117, 77620.211, 76466.586, 77389.195, 75664.688, 76069.75, 75635.68, 75695.414, 75805.812, 76275.367, 75167.391, 75506.57, 74925.836, 75463.875, 75142.047, 75862.648, 74713.727], 'val_loss': [2104.418, 1760.046, 1682.634, 1641.916, 1532.857, 1469.442, 1454.431, 1443.269, 1396.955, 1399.279, 1433.959, 1368.461, 1357.786, 1335.609, 1350.288, 1309.892, 1330.475, 1307.662, 1279.629, 1282.631, 1271.462, 1302.591, 1284.394, 1233.643, 1324.324, 1222.218, 1163.056, 1243.913, 1178.55, 1168.579, 1135.758, 1151.473, 1187.021, 1172.547, 1137.0, 1229.881, 1156.002, 1196.079, 1169.199, 1152.013, 1077.071, 1154.626, 1127.759, 1140.098, 1188.649, 1116.201, 1146.715, 1158.701, 1104.432, 1150.235, 1141.07, 1164.62, 1125.706, 1176.383, 1132.081, 1117.115, 1072.287, 1091.032, 1060.451, 1103.925, 1136.474, 1088.922, 1148.787, 1138.334, 1141.693, 1080.227, 1152.72, 1160.09, 1054.53, 1106.733, 1189.078, 1105.992, 1157.515, 1136.579, 1113.818, 1118.119, 1087.048, 1087.429, 1122.675, 1118.452, 1103.135, 1121.654, 1075.913, 1098.306, 1120.049, 1105.045, 1063.268, 1078.608, 1087.918, 1082.659, 1110.749, 1120.102, 1049.239, 1097.314, 1137.324, 1103.891, 1069.248, 1140.221, 1100.469, 1080.296]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4084.54297		497865	14	-1	273.61170291900635	{'train_loss': [507041.875, 231516.922, 174172.656, 153776.781, 137239.234, 129709.688, 123544.852, 118823.508, 114107.695, 111028.43, 109001.617, 105923.773, 105324.469, 102036.883, 101331.789, 99616.719, 97938.633, 97117.609, 96779.391, 95654.281, 93777.234, 93477.617, 93031.383, 92360.625, 91621.844, 90796.594, 90041.555, 89477.477, 89265.094, 88925.766, 87595.164, 87526.07, 86948.586, 86925.289, 86158.453, 85185.305, 85548.703, 84446.562, 83088.438, 83209.586, 83150.758, 83252.406, 82378.758, 82058.555, 82254.43, 81187.305, 81292.578, 81282.336, 79705.617, 79838.562, 80366.344, 79185.359, 78982.047, 78746.398, 78633.141, 77904.617, 79345.898, 78889.953, 77981.266, 77427.711, 75948.5, 77567.672, 76604.781, 76059.742, 75780.938, 76486.953, 75971.211, 76010.914, 75199.523, 75916.977, 74297.75, 75134.391, 75784.273, 74883.906, 74557.633, 73326.281, 73622.086, 74136.188, 73750.008, 73657.602, 73703.039, 72206.055, 72561.188, 72639.875, 71542.688, 72370.961, 71617.883, 72815.539, 71764.453, 71616.164, 70753.859, 71169.672, 70475.289, 72208.305, 70258.297, 71020.836, 71595.453, 70143.078, 70865.008, 70413.781], 'val_loss': [4350.695, 2499.564, 2297.91, 2017.026, 1927.156, 1907.441, 1823.575, 1695.432, 1633.598, 1557.648, 1522.938, 1527.135, 1446.997, 1466.231, 1362.656, 1406.316, 1387.648, 1399.871, 1352.627, 1338.902, 1333.191, 1372.336, 1324.069, 1273.839, 1337.827, 1270.287, 1254.162, 1316.616, 1300.086, 1238.218, 1158.783, 1256.093, 1206.093, 1187.71, 1181.884, 1136.744, 1080.473, 1099.5, 1082.965, 1132.297, 1106.818, 1111.37, 1107.603, 1048.73, 1059.698, 1098.532, 1084.272, 1111.816, 1059.795, 1039.345, 1059.032, 1110.106, 1048.264, 1063.206, 1079.427, 1086.719, 1046.299, 1052.607, 1018.818, 997.338, 1012.838, 1084.316, 992.729, 1045.467, 971.243, 1008.964, 1033.506, 1009.19, 1073.001, 1013.665, 1037.56, 969.495, 1014.524, 1001.697, 992.343, 983.929, 1015.188, 1036.212, 970.64, 1016.679, 966.561, 968.535, 1030.467, 989.441, 1036.209, 984.84, 934.268, 955.281, 997.538, 971.332, 961.489, 932.113, 956.676, 1002.685, 974.044, 1004.67, 966.31, 1009.817, 922.226, 979.516]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:69 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:16 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:123 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:110 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:125 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.2135450477008342 alpha:0.8638461035606795 weight_decay:9.563310947024901e-05 batch_size:32 epochs:100	100	1000	True	44596.19531		18467756	15	-1	299.92389011383057	{'train_loss': [996343.562, 11079682.0, 1137902.875, 1716462.75, 57616392.0, 804482.438, 27831466.0, 804482.438, 938556.375, 55587896.0, 1987213.5, 25396968.0, 804568.562, 16142341.0, 804482.438, 18324756.0, 46806084.0, 804482.438, 93513032.0, 4341934.5, 52984016.0, 804482.438, 804482.438, 83987136.0, 804482.438, 94994464.0, 804482.438, 2324649.0, 38127280.0, 13253025.0, 2163717.0, 1846592.375, 59396064.0, 804482.438, 47165016.0, 804482.438, 6374110.0, 30135408.0, 2619910.25, 30356716.0, 804482.438, 18212754.0, 864303.312, 2178275.5, 19166868.0, 10186331.0, 15908719.0, 976328.688, 10291663.0, 820806.375, 18251730.0, 804520.188, 12970036.0, 19005118.0, 3700295.75, 13943037.0, 8071033.5, 9267986.0, 3581482.25, 6878250.0, 28808152.0, 944514.438, 21210728.0, 804482.438, 2445863.5, 9628213.0, 804482.438, 28365702.0, 804482.438, 4547688.5, 20913838.0, 1517342.375, 19798942.0, 804482.438, 17083484.0, 804482.438, 19563392.0, 804482.438, 2376782.0, 1552818.0, 26661290.0, 2570428.75, 10733271.0, 34419560.0, 804482.438, 40106868.0, 804482.438, 1800227.5, 35953692.0, 804482.438, 24516652.0, 804559.062, 4318326.0, 28834138.0, 804482.438, 11138532.0, 15022625.0, 13114624.0, 804482.438, 14546191.0], 'val_loss': [11044.456, 329646752.0, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 19608.957, 11044.456, 295068.875, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 158768.719, 11044.456, 11044.456, 11044.456, 11044.456, 8592113.0, 11044.456, 11044.456, 11044.456, 11044.456, 71347.562, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 1092733.25, 11044.456, 11830.721, 11044.456, 11044.456, 24001.898, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 720035.562, 11044.456, 11044.456, 11044.456, 11044.456, 25995.346, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 620657.0, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 17964.293, 11044.456, 11044.456, 11044.456, 514987.094, 11044.456, 11044.456, 11044.456]}	100	100	True
