id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4216.91602		479265	14	-1	260.11168026924133	{'train_loss': [544937.0, 309608.75, 175835.312, 150383.0, 134993.656, 126989.312, 120497.984, 116740.281, 112902.039, 110214.031, 107882.625, 105422.062, 102829.852, 102400.867, 100275.188, 99068.219, 98073.758, 96844.992, 95888.93, 94953.414, 94022.492, 92320.25, 92060.43, 90963.18, 90842.586, 90051.125, 90547.391, 89143.711, 88044.508, 88011.75, 87168.5, 87527.672, 86167.984, 85767.523, 84441.602, 84907.609, 84344.922, 84481.57, 83847.656, 83380.688, 82499.383, 81138.477, 81714.094, 81824.672, 82073.75, 81217.336, 80996.172, 80102.211, 79830.297, 79899.125, 79587.344, 78522.398, 79766.18, 78699.672, 77729.938, 78366.438, 78938.609, 77404.266, 78092.977, 77202.391, 77921.188, 76385.461, 76746.117, 75924.461, 75407.477, 77169.203, 74854.977, 76032.992, 75696.523, 75516.18, 75263.57, 74482.578, 75437.047, 74637.297, 73819.422, 74470.969, 74443.156, 75208.266, 73770.859, 74012.43, 73213.492, 74045.75, 73729.43, 72781.109, 72219.992, 73125.555, 72918.562, 73313.633, 71995.773, 71929.617, 72884.602, 71644.758, 72399.328, 71927.797, 70817.828, 70912.211, 72055.305, 71892.258, 71723.289, 70065.594], 'val_loss': [6327.924, 2595.564, 1945.103, 1900.868, 1877.848, 1680.999, 1658.975, 1609.546, 1587.194, 1533.287, 1500.222, 1478.699, 1546.481, 1498.373, 1453.499, 1457.308, 1397.374, 1359.237, 1377.868, 1429.61, 1282.946, 1224.783, 1340.57, 1249.893, 1298.119, 1298.431, 1302.811, 1258.287, 1217.676, 1200.694, 1209.789, 1147.939, 1209.062, 1192.524, 1184.875, 1204.12, 1163.247, 1160.255, 1153.568, 1143.78, 1147.111, 1077.168, 1177.079, 1206.933, 1109.703, 1156.849, 1122.091, 1117.238, 1106.077, 1108.528, 1086.404, 1144.803, 1102.625, 1105.951, 1113.787, 1120.018, 1140.37, 1093.516, 1133.087, 1143.831, 1086.715, 1137.78, 1088.208, 1060.727, 1082.821, 1025.131, 1048.209, 1086.152, 1060.155, 1081.087, 1020.806, 1056.373, 1031.77, 1006.425, 1115.668, 1041.133, 1144.537, 1030.264, 1048.134, 1133.84, 1014.771, 975.009, 972.48, 1026.245, 1018.683, 1011.142, 1031.796, 1087.588, 968.221, 1073.741, 1014.399, 1014.306, 983.765, 948.02, 966.987, 978.896, 991.698, 1037.957, 952.149, 1022.451]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:50 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.09836173756243491 alpha:0.9521498633582561 weight_decay:5.185796622822519e-05 batch_size:32 epochs:100	100	1000	True	44535.94141		484801	15	-1	257.38398265838623	{'train_loss': [1733846.875, 814723.0, 881572.562, 1498044.5, 804482.438, 804482.438, 804482.438, 1325345.375, 804786.062, 804482.438, 804482.438, 1514372.5, 804718.0, 804482.312, 821881.438, 804482.438, 804482.438, 1073331.875, 827805.812, 806623.688, 810931.562, 804482.438, 987071.688, 804482.438, 804482.438, 824327.438, 804471.812, 804482.438, 869098.312, 836865.188, 886528.312, 805831.688, 822784.938, 804482.438, 820063.0, 848642.812, 870872.812, 824037.125, 842557.375, 841751.375, 837287.688, 824056.812, 829665.312, 833108.688, 848928.812, 837296.188, 844956.75, 831679.062, 836016.938, 837066.5, 857893.688, 829729.875, 835977.812, 847445.25, 825434.25, 860643.625, 830327.75, 826195.25, 868906.0, 847898.375, 838027.875, 825994.188, 836162.0, 842744.938, 818349.312, 840520.938, 875076.938, 848130.75, 934385.375, 806812.875, 840082.062, 804482.438, 830042.438, 805092.312, 804497.562, 881731.938, 886083.5, 826741.875, 818500.0, 808173.312, 860361.938, 821629.5, 836176.625, 829665.938, 854337.375, 871411.75, 809186.125, 820618.0, 878052.5, 834783.125, 844228.312, 825323.25, 845334.188, 851667.375, 815669.0, 841808.188, 844122.938, 828142.125, 832033.562, 846177.125], 'val_loss': [11043.669, 11332.934, 11044.455, 11044.456, 11044.456, 11044.456, 11042.786, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.455, 11044.455, 11044.456, 11044.456, 11044.456, 11043.422, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11016.508, 11042.336, 11068.641, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11036.069, 11044.456, 11044.456, 11164.522, 11044.456, 12591.598, 11044.455, 11044.455, 11015.792, 11044.456, 11044.456, 11691.994, 11044.456, 11069.939, 11679.18, 12003.115, 11044.456, 11044.456, 11044.455, 14254.762, 11044.445, 11044.456, 11044.456, 11044.456, 11475.144, 11044.456, 11044.456, 11044.318, 11044.437, 11044.456, 11044.456, 11035.863, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.455, 12863.699, 11044.456, 11044.455, 11044.456, 11044.362, 11044.456, 11044.456, 11391.1, 11044.456, 11044.456, 11044.456, 11044.338, 11044.456, 17833.584, 11044.456, 11044.456, 11044.426, 11044.451, 11044.417, 11044.456, 11409.213, 12306.616, 11044.26, 11044.455]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	2000	True	3855.66748		479265	14	-1	260.3374729156494	{'train_loss': [505903.156, 270224.688, 220274.75, 199415.531, 154065.703, 133619.547, 124044.914, 117959.742, 113200.203, 109412.352, 105728.531, 103422.789, 100778.516, 98685.055, 97089.422, 95472.734, 94327.375, 92518.102, 92359.414, 90680.773, 89601.25, 88154.0, 88644.93, 87321.984, 86493.945, 85804.352, 84413.25, 85127.32, 84273.914, 83274.688, 82744.234, 82950.578, 82253.43, 81974.086, 80335.359, 81220.398, 80292.047, 80501.883, 80005.047, 79054.727, 77792.367, 78563.281, 78433.609, 77947.625, 78399.047, 77607.32, 76994.836, 77396.648, 77406.797, 76695.875, 76803.664, 76739.477, 75379.312, 75104.297, 76189.461, 75427.383, 74822.445, 74576.078, 75297.719, 74465.188, 75444.93, 74200.148, 74169.984, 74197.938, 73139.0, 73305.117, 73079.289, 73271.852, 72763.078, 71974.336, 73095.258, 73440.25, 72376.586, 71746.836, 71311.023, 72451.633, 72273.406, 72657.648, 71879.25, 71370.602, 71221.703, 70751.961, 70630.438, 71682.359, 70043.82, 70717.227, 71160.586, 71279.992, 71190.461, 70233.102, 69823.828, 70555.758, 69548.547, 69975.031, 69852.711, 70659.188, 69772.391, 69397.578, 70091.766, 70558.75], 'val_loss': [3764.716, 3166.103, 3094.96, 3055.299, 2112.193, 1998.963, 1757.744, 1717.285, 1602.969, 1540.471, 1546.421, 1415.241, 1453.875, 1448.03, 1589.75, 1495.705, 1388.942, 1320.358, 1259.499, 1368.383, 1238.605, 1281.72, 1300.912, 1297.341, 1322.415, 1207.626, 1288.1, 1228.933, 1239.154, 1180.807, 1175.486, 1248.412, 1146.199, 1320.934, 1172.415, 1083.66, 1142.192, 1089.302, 1111.076, 1152.476, 1107.328, 1183.901, 1153.744, 1164.331, 1054.683, 1062.413, 1155.502, 1041.043, 1089.696, 1066.748, 1113.49, 1120.793, 1091.928, 1109.322, 1139.429, 1151.799, 1216.153, 1166.057, 1082.183, 1137.153, 1095.812, 1116.22, 1209.622, 1044.893, 1143.282, 1143.5, 1037.391, 1043.855, 1151.572, 1074.057, 1118.715, 1025.261, 1097.547, 1012.268, 1073.63, 1030.755, 1109.907, 999.029, 1014.673, 1122.084, 1007.075, 1011.149, 1084.809, 1079.087, 1058.038, 1027.303, 1056.546, 1073.356, 1048.678, 1004.631, 1058.635, 1035.881, 1032.802, 1043.216, 1101.506, 1001.687, 974.431, 1028.427, 1099.812, 979.075]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:31 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	2000	True	3988.16748		373204	13	-1	208.16901063919067	{'train_loss': [430912.812, 318911.062, 225081.484, 202845.516, 188443.719, 174882.484, 154837.641, 142729.125, 132535.141, 124752.391, 119874.148, 115411.555, 113900.055, 110171.289, 108729.055, 106172.805, 104438.969, 102636.664, 101005.727, 99670.367, 98703.906, 96802.75, 95414.547, 95030.258, 94482.039, 92910.742, 92555.305, 91633.484, 90494.891, 89791.977, 88491.352, 88408.445, 87208.82, 85494.508, 87188.406, 85516.664, 85030.812, 84135.492, 84495.641, 83505.57, 83437.344, 83192.375, 83062.812, 82639.484, 81484.469, 80942.586, 80843.781, 81253.133, 81226.164, 80783.109, 80091.266, 79985.469, 79392.133, 78850.57, 78994.781, 78397.398, 78684.5, 77945.375, 77408.68, 77142.664, 77529.461, 77059.93, 76691.328, 76810.867, 76168.117, 76661.891, 75726.672, 75579.188, 75009.898, 75361.562, 75460.039, 74142.461, 75140.766, 73989.203, 75353.984, 74851.102, 73869.156, 74148.008, 73688.984, 73962.344, 73230.664, 73015.023, 72195.414, 72874.344, 73214.297, 72362.695, 71519.422, 71929.367, 72054.0, 71327.797, 71152.484, 71052.266, 71402.227, 72096.102, 71563.492, 69801.023, 72438.727, 70769.156, 70509.539, 71257.602], 'val_loss': [4908.684, 3399.323, 3007.44, 2626.688, 3121.355, 2434.988, 2228.309, 2064.108, 2108.615, 1942.54, 1729.239, 1764.244, 1822.251, 1606.01, 1512.946, 1863.499, 1555.973, 1676.093, 1564.712, 1473.952, 1319.842, 1469.385, 1476.662, 1408.697, 1454.525, 1381.293, 1440.797, 1408.668, 1313.948, 1335.653, 1264.735, 1292.389, 1304.988, 1424.703, 1368.481, 1178.609, 1227.722, 1252.55, 1265.016, 1251.038, 1257.593, 1218.519, 1181.591, 1187.279, 1209.224, 1124.632, 1160.325, 1151.182, 1249.289, 1152.16, 1169.376, 1120.117, 1160.847, 1123.493, 1169.081, 1182.804, 1110.429, 1090.352, 1099.104, 1065.136, 1093.882, 1072.667, 1147.499, 1111.235, 1022.506, 1066.702, 1056.878, 1097.476, 1068.858, 1077.983, 1053.032, 1080.263, 1046.184, 1035.849, 1049.383, 1032.407, 1036.653, 1062.931, 985.759, 1053.065, 992.305, 961.026, 1004.476, 1087.928, 1032.544, 945.87, 1035.298, 1015.67, 1032.105, 984.073, 1051.823, 1004.361, 994.407, 1010.622, 1040.309, 1040.537, 950.089, 988.506, 964.073, 997.027]}	0	100	True
