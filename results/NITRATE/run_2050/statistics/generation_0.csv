id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4211.83447		452251	14	-1	173.4645607471466	{'train_loss': [288463.938, 166663.734, 147986.984, 138603.875, 130668.758, 125318.281, 119989.5, 116020.211, 112564.75, 110116.203, 107496.258, 106396.078, 103880.352, 102200.414, 101667.703, 101484.414, 98632.102, 98960.695, 96463.422, 94643.258, 94196.531, 92963.797, 92546.148, 92502.32, 91596.117, 91150.25, 90282.398, 90620.867, 88890.469, 88526.633, 88972.305, 88117.836, 88249.633, 87250.047, 86284.609, 86560.68, 87063.266, 85766.648, 85940.961, 85267.711, 85178.516, 84780.156, 83687.867, 83395.18, 84001.414, 82962.93, 83880.727, 82879.289, 83011.047, 83672.312, 82745.914, 81866.891, 82152.094, 82057.75, 80947.461, 81407.383, 80632.281, 80880.742, 81452.445, 81305.344, 80669.031, 80412.82, 81048.93, 80415.516, 79863.328, 79028.117, 79319.492, 80226.07, 78779.469, 78720.281, 78855.789, 78456.5, 78426.547, 78376.281, 78345.078, 77783.156, 77946.453, 78152.641, 77758.758, 78119.406, 77509.031, 77821.711, 77436.664, 76597.461, 76800.391, 77252.094, 76296.914, 76111.289, 76650.289, 76714.25, 76222.836, 75790.852, 76517.578, 75286.836, 75600.859, 74973.82, 75271.078, 75917.531, 75502.062, 75609.906], 'val_loss': [1795.911, 1690.84, 1836.528, 1653.932, 1668.787, 1646.385, 1541.041, 1555.926, 1602.982, 1447.811, 1509.13, 1502.941, 1427.679, 1478.583, 1472.731, 1409.322, 1494.208, 1320.987, 1332.64, 1411.412, 1369.01, 1282.182, 1268.284, 1220.672, 1197.581, 1229.955, 1240.812, 1207.547, 1205.774, 1249.976, 1174.631, 1149.868, 1177.166, 1171.512, 1109.197, 1107.41, 1107.663, 1150.618, 1144.368, 1090.192, 1129.605, 1103.865, 1087.272, 1095.048, 1083.206, 1107.166, 1136.054, 1117.948, 1118.632, 1098.19, 1081.0, 1077.832, 1088.601, 1107.586, 1083.29, 1113.668, 1073.583, 1066.985, 1112.39, 1060.055, 1077.899, 1058.502, 1061.937, 1084.032, 1124.576, 1065.437, 1086.477, 1056.442, 1071.564, 1079.061, 1062.603, 1059.776, 1057.856, 1039.353, 1048.779, 1050.162, 1052.43, 1054.048, 1069.372, 1066.781, 1072.255, 1036.227, 1042.399, 1024.756, 1027.502, 1066.496, 1045.019, 1043.671, 1044.04, 999.047, 1024.835, 1016.133, 1068.004, 1054.945, 1096.977, 1010.927, 1037.688, 1039.931, 1032.906, 1032.253]}	100	100	True
