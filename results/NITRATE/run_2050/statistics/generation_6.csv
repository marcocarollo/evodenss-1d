id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	3977.33008		479265	14	-1	259.5763249397278	{'train_loss': [500779.75, 261586.906, 173552.984, 150708.0, 135149.859, 127016.758, 120769.094, 115143.781, 112544.117, 109981.281, 107559.375, 104620.719, 102877.336, 100593.258, 99374.883, 97807.0, 96015.758, 95690.648, 94788.188, 93251.109, 91193.977, 91098.688, 89666.664, 89036.438, 87806.977, 87241.578, 85738.758, 86169.0, 85359.602, 84355.367, 84258.0, 82820.211, 84054.609, 83423.82, 82669.875, 80759.359, 81588.109, 80227.789, 80734.555, 80476.898, 80045.406, 79426.203, 78811.445, 78706.773, 77993.195, 77350.375, 76689.875, 76789.641, 77170.883, 76490.328, 76576.117, 75645.672, 76474.867, 75760.781, 75214.945, 75349.602, 75105.938, 74863.289, 74381.25, 73941.938, 74168.719, 75168.5, 73927.797, 73017.242, 74490.781, 73213.062, 72858.797, 72359.938, 72612.367, 73133.328, 72386.57, 71905.172, 71788.555, 71465.383, 71510.086, 71558.57, 71101.102, 70488.898, 71062.367, 71753.953, 70372.695, 70772.898, 70739.008, 69919.344, 69553.109, 68963.398, 69827.5, 70843.734, 69324.32, 70303.086, 69779.469, 69058.0, 70175.938, 68955.992, 68739.062, 69741.961, 68964.938, 68845.914, 69360.727, 68689.172], 'val_loss': [4327.873, 2089.393, 2177.695, 2058.078, 1969.679, 1792.309, 1661.842, 1578.581, 1472.262, 1435.344, 1450.818, 1412.62, 1408.1, 1395.057, 1350.819, 1318.945, 1294.739, 1323.705, 1312.955, 1318.644, 1290.751, 1225.807, 1254.917, 1192.685, 1195.862, 1242.399, 1146.904, 1160.833, 1158.939, 1154.22, 1152.317, 1161.393, 1236.7, 1112.557, 1129.679, 1128.685, 1183.847, 1118.917, 1134.266, 1107.858, 1145.901, 1085.346, 1121.261, 1070.325, 1064.765, 1087.429, 1080.605, 1066.646, 1068.883, 1110.1, 1096.911, 1112.144, 1072.177, 1185.142, 1072.367, 1095.931, 1054.851, 1075.155, 1096.713, 1044.38, 1046.137, 1082.573, 1032.362, 1093.498, 1030.206, 1087.817, 1002.653, 1047.712, 1053.137, 1080.759, 1000.09, 993.32, 1034.189, 997.852, 982.274, 982.287, 1032.337, 1059.869, 1024.715, 1014.201, 972.858, 1021.101, 1006.687, 973.409, 972.044, 1024.784, 1053.964, 1026.993, 1019.285, 1016.267, 1099.661, 1030.203, 1029.856, 986.4, 1009.472, 1047.93, 992.873, 1025.312, 1052.266, 1037.515]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:33 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4080.34082		394476	14	-1	217.9898328781128	{'train_loss': [436818.656, 324189.438, 219927.656, 190133.562, 178418.469, 162740.422, 139584.531, 128777.742, 122035.344, 117200.047, 113701.945, 110813.492, 108570.156, 107121.289, 104813.0, 103012.359, 102115.055, 100136.172, 98680.227, 98708.688, 96226.047, 95650.492, 94919.477, 93919.312, 92151.453, 91845.18, 90927.977, 90481.586, 90194.992, 89194.898, 89164.312, 88181.578, 88176.203, 87250.398, 87975.789, 86634.266, 85100.492, 85266.914, 83957.336, 83410.523, 83141.492, 82975.656, 81924.32, 82449.883, 82637.344, 82030.938, 79910.703, 80514.102, 81013.438, 79228.266, 80083.562, 79895.578, 79169.977, 78436.562, 79268.414, 77908.406, 77934.273, 77054.43, 77252.047, 77233.766, 77350.781, 76219.938, 75698.305, 74934.461, 75682.469, 75898.242, 75465.781, 75192.766, 75642.305, 74004.609, 74804.68, 74861.617, 73339.016, 74578.867, 72764.742, 72462.836, 72920.984, 73416.281, 72940.984, 73251.578, 71922.164, 72459.258, 72675.672, 72117.672, 71768.961, 72062.062, 71820.719, 71316.672, 72134.773, 71634.602, 70925.969, 71069.047, 71170.977, 70643.234, 70364.617, 70557.773, 70794.383, 70321.875, 70128.797, 69943.07], 'val_loss': [5825.81, 3031.004, 2585.667, 2511.074, 2512.435, 2440.322, 1917.592, 1776.185, 1753.125, 1585.0, 1508.867, 1503.92, 1480.533, 1447.552, 1391.074, 1341.408, 1381.574, 1361.4, 1315.382, 1274.337, 1334.301, 1302.672, 1254.78, 1293.266, 1241.971, 1197.18, 1191.509, 1236.17, 1161.159, 1178.084, 1179.126, 1135.913, 1204.209, 1201.364, 1278.884, 1113.172, 1172.321, 1193.693, 1139.751, 1134.451, 1056.782, 1074.721, 1112.145, 1062.407, 1172.793, 1111.981, 1060.123, 1157.379, 1059.975, 1055.734, 1130.369, 1109.573, 1054.415, 1040.196, 1055.257, 1119.179, 1071.553, 1033.374, 1146.856, 1036.233, 1039.942, 1075.68, 1030.744, 1025.291, 1044.558, 1055.867, 1008.814, 1047.959, 996.516, 1012.338, 984.23, 1020.457, 978.267, 982.498, 1003.239, 971.612, 983.087, 991.706, 979.336, 970.941, 998.339, 1004.542, 965.747, 1001.525, 1007.746, 1073.034, 1011.587, 1007.904, 972.491, 964.957, 990.56, 996.228, 970.536, 980.873, 988.403, 983.534, 999.274, 1000.969, 986.367, 994.805]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:18 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	4419.24365		589541	13	-1	284.61169624328613	{'train_loss': [702590.062, 315202.188, 220558.906, 179573.266, 159756.484, 150668.547, 138825.219, 130685.68, 123584.523, 117996.281, 113176.625, 109907.195, 107146.188, 105527.938, 102177.062, 102415.539, 101121.828, 97816.906, 97906.406, 95451.68, 96392.875, 94637.617, 94822.688, 92794.773, 93021.562, 91819.594, 90311.484, 91100.195, 90941.055, 89764.547, 89697.172, 89384.188, 87609.789, 87761.523, 88459.992, 88438.578, 86960.539, 86056.969, 85040.32, 85204.195, 85442.117, 85250.422, 84259.32, 83322.672, 84145.039, 82149.57, 83140.836, 81141.516, 82385.25, 81540.688, 82073.82, 83144.258, 80357.781, 81033.312, 79982.57, 79550.93, 81083.18, 79497.297, 80321.258, 79379.062, 80755.336, 78385.625, 78166.562, 78953.5, 79299.094, 77839.086, 79153.914, 78337.906, 77501.391, 78219.141, 77718.133, 76042.883, 75223.211, 77818.461, 76831.773, 76927.086, 76719.984, 76909.555, 75631.398, 74800.781, 76670.953, 76218.172, 75070.906, 74434.992, 73997.211, 75044.883, 75713.117, 73607.758, 74618.125, 74285.602, 74753.195, 73979.555, 72984.859, 73707.992, 74017.445, 72372.43, 73729.938, 73290.094, 73375.57, 73762.828], 'val_loss': [6582.844, 2525.967, 2115.777, 2013.433, 2014.651, 1892.474, 1803.462, 1597.577, 1633.197, 1646.321, 1796.821, 1493.728, 1492.73, 1534.5, 1686.271, 1442.159, 1340.859, 1330.668, 1339.192, 1336.47, 1316.841, 1539.869, 1305.5, 1269.799, 1504.631, 1447.198, 1340.684, 1266.383, 1197.469, 1378.581, 1365.377, 1233.381, 1352.111, 1284.746, 1199.15, 1295.844, 1321.124, 1344.687, 1325.37, 1185.076, 1192.737, 1217.735, 1208.902, 1215.893, 1256.379, 1200.71, 1191.305, 1218.417, 1140.63, 1156.682, 1098.608, 1132.49, 1138.469, 1183.584, 1188.047, 1157.219, 1130.649, 1189.776, 1069.884, 1175.926, 1136.979, 1209.202, 1135.822, 1178.015, 1103.127, 1155.568, 1049.07, 1081.732, 1072.022, 1142.527, 1156.534, 1092.933, 1117.156, 1146.095, 1049.498, 1057.692, 1166.584, 1105.773, 1116.258, 1142.074, 1094.836, 1132.321, 1031.716, 1101.05, 1139.841, 1145.461, 1065.645, 1038.133, 1150.309, 1100.531, 1107.509, 1107.465, 1144.22, 1095.683, 1159.979, 1114.953, 1041.181, 1041.635, 1046.517, 1072.522]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:conv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.23298109086050128 alpha:0.8270843344196558 weight_decay:8.645712235475656e-05 batch_size:32 epochs:100	100	1000	True	44549.67969		7091285	15	-1	227.01468443870544	{'train_loss': [1306853.5, 39301500.0, 804482.438, 34995848.0, 804482.438, 23944806.0, 896903.562, 10971615.0, 4147773.75, 2335772.25, 18482430.0, 906064.5, 3621879.25, 6535830.5, 935256.812, 4640470.5, 2769639.0, 3228722.25, 5633379.5, 3248366.0, 5173211.0, 3148069.25, 4772393.5, 8817884.0, 2728771.0, 6245727.0, 4618046.5, 3182249.25, 5098824.5, 3353466.25, 4234298.0, 3897980.0, 7457282.0, 1854482.125, 8310509.5, 2837125.75, 5862174.0, 5567662.0, 2609919.75, 7476190.0, 2526364.25, 3848760.25, 2821788.5, 4558339.5, 3982841.5, 4479971.0, 3188757.25, 7976393.0, 7432336.5, 904111.062, 6882946.5, 806456.562, 7118780.0, 804813.688, 7012646.5, 2800457.0, 3690312.25, 5208463.5, 3423583.75, 3868699.0, 3834830.75, 7424845.0, 2209965.0, 8726802.0, 2946307.75, 2868237.75, 4964647.5, 3287997.0, 4098690.75, 2384454.75, 4988603.5, 2539410.75, 4948363.5, 2893453.0, 4808326.5, 2662716.0, 5995037.0, 1963629.5, 5367943.5, 1869110.75, 5614434.0, 2349797.25, 5160169.0, 2776926.25, 4665117.0, 2176261.5, 6636517.0, 2121757.25, 4226554.0, 2288986.75, 4905530.0, 2317498.25, 8029559.5, 1832907.25, 5281405.5, 2286644.25, 3477628.0, 2532642.0, 3383332.25, 2667062.5], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 137980.562, 11044.456, 11044.456, 11044.456, 18962.885, 11044.456, 11044.456, 1089207.75, 11044.456, 16255.218, 11044.456, 35818.246, 11044.456, 58737.922, 11044.456, 29185.328, 11044.456, 111463.93, 11059.166, 11044.456, 11044.456, 96439.57, 11044.456, 11044.456, 11044.456, 43318.789, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 57897.246, 11044.456, 11044.456, 149390.75, 11044.456, 11044.456, 11044.456, 11044.456, 45223.086, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11116.809, 11044.456, 11044.309, 250846.5, 11044.455, 679571.25, 11044.456, 40273.062, 11044.456, 11044.456, 37663.609, 11044.021, 11044.456, 11044.456, 11044.456, 11772.793, 149768.469, 11044.456, 11044.456, 11044.456, 57245.949, 11044.456, 13944.655, 11044.456, 11044.456, 11044.456, 248250.031, 61613.34, 11044.456, 11044.456, 11044.456, 11044.426, 11044.456, 15980.202, 11044.456, 33658.344, 26731.305, 88788.734, 11044.278, 34901.414, 11044.014]}	100	100	True
