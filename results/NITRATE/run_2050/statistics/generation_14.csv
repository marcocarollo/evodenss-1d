id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	4000	True	4073.32129		491663	14	-1	283.07387614250183	{'train_loss': [473430.906, 220407.547, 172489.078, 151787.328, 136286.578, 126390.508, 120923.719, 116978.844, 114361.375, 111313.305, 110291.805, 107594.852, 106046.812, 104895.984, 102412.492, 101525.133, 99520.648, 99171.0, 98002.227, 96930.141, 95703.516, 94271.742, 94323.664, 93539.75, 92261.602, 91042.391, 91332.57, 89459.656, 89821.984, 88048.055, 89860.414, 88954.195, 87156.188, 85576.086, 85726.742, 85176.18, 84237.141, 83631.648, 83258.477, 82316.797, 81507.961, 81751.961, 80823.375, 80238.164, 79822.195, 79611.852, 78765.953, 78402.164, 78933.469, 77347.828, 76985.758, 77506.508, 77485.609, 75250.297, 76541.055, 76087.68, 77262.008, 75054.93, 76817.203, 75462.375, 75081.156, 75415.492, 74740.07, 74270.789, 73452.328, 73896.914, 74940.758, 73945.617, 74529.547, 73598.781, 72319.555, 73444.938, 72700.156, 72513.289, 72652.094, 71513.75, 72029.977, 71202.852, 70955.398, 71062.656, 71286.852, 71177.406, 70562.32, 70926.141, 69569.805, 69649.562, 69667.867, 69921.797, 70329.711, 68606.445, 69930.07, 70167.719, 69562.82, 69383.484, 68681.781, 68515.633, 68469.789, 69033.086, 67698.539, 69113.82], 'val_loss': [4316.214, 1979.197, 1911.734, 1774.885, 1637.548, 1605.699, 1617.701, 1578.352, 1571.169, 1526.616, 1555.496, 1452.455, 1442.865, 1455.504, 1445.39, 1475.672, 1465.296, 1443.849, 1394.982, 1398.399, 1279.856, 1307.108, 1326.186, 1311.232, 1269.672, 1284.156, 1322.947, 1406.376, 1265.34, 1198.392, 1176.657, 1211.113, 1286.696, 1133.492, 1184.567, 1160.563, 1139.587, 1147.303, 1192.535, 1108.488, 1128.971, 1072.322, 1076.02, 1059.604, 1091.463, 1105.699, 1109.292, 1135.293, 1120.104, 1053.647, 1042.732, 1123.969, 995.227, 1032.731, 1104.741, 1161.668, 1031.357, 1064.686, 1061.504, 1046.965, 1178.969, 1044.519, 1021.261, 1052.985, 1055.879, 1022.902, 1063.344, 1114.397, 1032.915, 1033.53, 1078.888, 1056.584, 981.035, 1060.618, 1045.068, 1037.102, 1053.539, 1070.021, 1044.469, 1035.874, 999.049, 999.851, 1077.315, 992.414, 1015.443, 1010.331, 954.375, 1007.938, 1057.544, 1027.253, 1015.268, 1000.158, 975.315, 1035.135, 979.59, 1047.337, 998.701, 973.236, 997.266, 1020.602]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	4000	True	3871.68359		491663	14	-1	271.66326904296875	{'train_loss': [536777.062, 310867.844, 224851.141, 197864.875, 183615.844, 165162.703, 134514.672, 118531.867, 113233.82, 108628.93, 105724.688, 101722.234, 99944.938, 97417.164, 96303.812, 95542.508, 94605.414, 93441.211, 91418.773, 92539.766, 91195.781, 90329.406, 89294.891, 89018.742, 88443.477, 87613.312, 86035.258, 85167.836, 85481.664, 85186.18, 85264.891, 84231.18, 82686.266, 82462.758, 83258.172, 81729.188, 81224.906, 80799.836, 80409.578, 79406.328, 80912.336, 79190.695, 79345.617, 79091.461, 78341.664, 78613.438, 77160.766, 78150.711, 78314.664, 76670.898, 77314.797, 77157.875, 76102.539, 75693.914, 76009.016, 74468.0, 75141.758, 75630.906, 74839.086, 74634.945, 74090.422, 74138.656, 74501.305, 73851.836, 73271.641, 73369.281, 73401.25, 73027.859, 73385.969, 72985.273, 73369.867, 72498.961, 71777.039, 71968.953, 72632.438, 71069.789, 71794.328, 71613.445, 70098.195, 72393.969, 72032.32, 71741.984, 70916.305, 70306.805, 71024.117, 71482.273, 69903.969, 70543.898, 69870.703, 70655.773, 69370.891, 70526.484, 69311.031, 68793.547, 70223.898, 69827.555, 68825.312, 69187.07, 69004.797, 69953.281], 'val_loss': [5119.25, 3085.652, 2899.946, 2533.311, 2846.021, 2562.421, 2041.956, 1903.069, 1676.147, 1757.548, 1495.679, 1497.21, 1566.86, 1592.587, 1551.599, 1634.656, 1510.025, 1342.162, 1376.389, 1397.125, 1384.76, 1328.79, 1346.681, 1237.417, 1184.957, 1241.788, 1175.488, 1145.162, 1172.633, 1176.682, 1169.589, 1156.177, 1317.493, 1070.252, 1131.275, 1186.901, 1124.672, 1140.328, 1114.527, 1260.201, 1087.435, 1113.794, 1135.651, 1054.944, 1245.787, 1101.659, 1054.859, 1096.095, 1094.041, 1051.835, 1040.822, 1068.77, 1029.834, 1056.949, 1084.467, 1007.207, 1081.511, 1031.698, 1075.516, 1079.458, 1013.429, 987.475, 1009.505, 1018.544, 999.412, 997.78, 1045.0, 1030.729, 950.44, 992.493, 1052.784, 1016.866, 979.477, 977.599, 998.312, 993.033, 970.93, 990.313, 993.926, 948.525, 936.814, 957.148, 993.093, 958.535, 939.207, 1038.021, 988.054, 1002.6, 1016.269, 931.797, 946.082, 931.175, 993.128, 971.837, 926.073, 986.721, 967.651, 950.222, 1006.622, 961.064]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	4000	True	3871.68359		491663	14	-1	271.69135642051697	{'train_loss': [536777.062, 310867.844, 224851.141, 197864.875, 183615.844, 165162.703, 134514.672, 118531.867, 113233.82, 108628.93, 105724.688, 101722.234, 99944.938, 97417.164, 96303.812, 95542.508, 94605.414, 93441.211, 91418.773, 92539.766, 91195.781, 90329.406, 89294.891, 89018.742, 88443.477, 87613.312, 86035.258, 85167.836, 85481.664, 85186.18, 85264.891, 84231.18, 82686.266, 82462.758, 83258.172, 81729.188, 81224.906, 80799.836, 80409.578, 79406.328, 80912.336, 79190.695, 79345.617, 79091.461, 78341.664, 78613.438, 77160.766, 78150.711, 78314.664, 76670.898, 77314.797, 77157.875, 76102.539, 75693.914, 76009.016, 74468.0, 75141.758, 75630.906, 74839.086, 74634.945, 74090.422, 74138.656, 74501.305, 73851.836, 73271.641, 73369.281, 73401.25, 73027.859, 73385.969, 72985.273, 73369.867, 72498.961, 71777.039, 71968.953, 72632.438, 71069.789, 71794.328, 71613.445, 70098.195, 72393.969, 72032.32, 71741.984, 70916.305, 70306.805, 71024.117, 71482.273, 69903.969, 70543.898, 69870.703, 70655.773, 69370.891, 70526.484, 69311.031, 68793.547, 70223.898, 69827.555, 68825.312, 69187.07, 69004.797, 69953.281], 'val_loss': [5119.25, 3085.652, 2899.946, 2533.311, 2846.021, 2562.421, 2041.956, 1903.069, 1676.147, 1757.548, 1495.679, 1497.21, 1566.86, 1592.587, 1551.599, 1634.656, 1510.025, 1342.162, 1376.389, 1397.125, 1384.76, 1328.79, 1346.681, 1237.417, 1184.957, 1241.788, 1175.488, 1145.162, 1172.633, 1176.682, 1169.589, 1156.177, 1317.493, 1070.252, 1131.275, 1186.901, 1124.672, 1140.328, 1114.527, 1260.201, 1087.435, 1113.794, 1135.651, 1054.944, 1245.787, 1101.659, 1054.859, 1096.095, 1094.041, 1051.835, 1040.822, 1068.77, 1029.834, 1056.949, 1084.467, 1007.207, 1081.511, 1031.698, 1075.516, 1079.458, 1013.429, 987.475, 1009.505, 1018.544, 999.412, 997.78, 1045.0, 1030.729, 950.44, 992.493, 1052.784, 1016.866, 979.477, 977.599, 998.312, 993.033, 970.93, 990.313, 993.926, 948.525, 936.814, 957.148, 993.093, 958.535, 939.207, 1038.021, 988.054, 1002.6, 1016.269, 931.797, 946.082, 931.175, 993.128, 971.837, 926.073, 986.721, 967.651, 950.222, 1006.622, 961.064]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.18413406064586726 alpha:0.9816476580053541 weight_decay:5.122958504786313e-05 batch_size:32 epochs:100	100	4000	True	46356.92188		688718	14	-1	310.45848965644836	{'train_loss': [1439629.25, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 1966748.75, 804482.438, 838299.062, 804470.812, 7565047.5, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 7502482.0, 804482.438, 804482.438, 849675.312, 804482.438, 804482.438, 1236525.5, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 11961217.0, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 6115485.0, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 2281917.25, 804482.438, 2746217.25, 804482.438, 804482.438, 804482.438, 1353454.5, 804482.438, 804482.438, 804482.438, 804482.438, 868087.188, 804482.438, 804482.438, 804482.438, 9470342.0, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11772.377, 11290.318, 11111.314, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11443.733, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 18616.156, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11492.431]}	0	100	True
