id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	2000	True	3990.96924		491663	14	-1	271.11447763442993	{'train_loss': [579052.688, 433125.625, 256877.094, 200881.109, 185277.438, 160871.625, 131114.719, 124312.328, 118964.172, 115053.211, 111103.953, 108310.164, 105095.398, 102919.781, 100272.016, 99192.883, 98315.18, 96181.055, 95754.438, 93632.695, 92566.648, 92314.461, 90819.508, 89749.766, 90054.109, 87899.898, 88229.586, 88770.57, 87128.492, 86185.5, 85740.68, 85289.836, 85775.82, 84513.469, 83600.328, 83967.586, 83917.555, 82540.789, 82498.891, 81356.414, 82254.109, 81702.805, 81341.977, 81586.641, 80234.562, 81282.43, 79522.328, 80004.461, 79727.945, 79648.25, 79706.016, 79625.984, 79425.43, 78284.781, 78523.836, 78537.32, 77354.992, 77185.758, 77520.438, 77536.648, 77323.586, 77398.062, 77478.234, 75654.891, 76244.82, 76443.164, 76156.461, 75345.789, 75050.859, 75636.555, 75122.227, 75777.648, 75718.195, 74131.18, 75886.57, 74502.664, 74762.914, 73726.242, 73892.203, 74213.0, 73351.438, 74536.008, 73680.492, 73126.992, 74249.203, 73364.945, 72976.125, 72815.852, 71652.406, 72481.227, 72534.5, 72813.758, 71514.18, 72409.219, 71936.031, 71803.008, 72597.164, 71481.227, 71361.5, 71840.648], 'val_loss': [5940.86, 3602.901, 2812.781, 2687.996, 2536.729, 2358.398, 1816.636, 1678.728, 1512.444, 1442.056, 1478.456, 1387.288, 1333.425, 1273.69, 1246.044, 1285.626, 1301.826, 1290.596, 1204.872, 1217.926, 1130.439, 1161.839, 1148.172, 1114.118, 1130.234, 1170.281, 1150.066, 1185.645, 1071.035, 1099.182, 1061.614, 1156.857, 1127.757, 1079.193, 1078.764, 1116.536, 1067.447, 1096.61, 1024.358, 1064.523, 1024.378, 1111.62, 1122.839, 1027.503, 1047.885, 1079.524, 1016.389, 1005.587, 1015.495, 968.467, 1073.147, 1006.484, 1039.929, 1096.552, 1012.351, 1063.838, 1077.69, 994.828, 996.846, 984.479, 1000.912, 990.723, 1022.039, 953.518, 1012.767, 986.333, 985.866, 1028.589, 1011.732, 970.4, 992.937, 1006.691, 1029.756, 992.86, 1002.727, 994.047, 995.682, 1023.047, 1001.664, 951.156, 962.895, 979.667, 1010.203, 1000.843, 989.108, 931.54, 1037.155, 926.443, 979.298, 970.887, 960.395, 938.584, 963.349, 1001.403, 941.869, 955.198, 996.325, 924.914, 1005.578, 981.305]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	2000	True	3871.68359		491663	14	-1	270.31976294517517	{'train_loss': [536777.062, 310867.844, 224851.141, 197864.875, 183615.844, 165162.703, 134514.672, 118531.867, 113233.82, 108628.93, 105724.688, 101722.234, 99944.938, 97417.164, 96303.812, 95542.508, 94605.414, 93441.211, 91418.773, 92539.766, 91195.781, 90329.406, 89294.891, 89018.742, 88443.477, 87613.312, 86035.258, 85167.836, 85481.664, 85186.18, 85264.891, 84231.18, 82686.266, 82462.758, 83258.172, 81729.188, 81224.906, 80799.836, 80409.578, 79406.328, 80912.336, 79190.695, 79345.617, 79091.461, 78341.664, 78613.438, 77160.766, 78150.711, 78314.664, 76670.898, 77314.797, 77157.875, 76102.539, 75693.914, 76009.016, 74468.0, 75141.758, 75630.906, 74839.086, 74634.945, 74090.422, 74138.656, 74501.305, 73851.836, 73271.641, 73369.281, 73401.25, 73027.859, 73385.969, 72985.273, 73369.867, 72498.961, 71777.039, 71968.953, 72632.438, 71069.789, 71794.328, 71613.445, 70098.195, 72393.969, 72032.32, 71741.984, 70916.305, 70306.805, 71024.117, 71482.273, 69903.969, 70543.898, 69870.703, 70655.773, 69370.891, 70526.484, 69311.031, 68793.547, 70223.898, 69827.555, 68825.312, 69187.07, 69004.797, 69953.281], 'val_loss': [5119.25, 3085.652, 2899.946, 2533.311, 2846.021, 2562.421, 2041.956, 1903.069, 1676.147, 1757.548, 1495.679, 1497.21, 1566.86, 1592.587, 1551.599, 1634.656, 1510.025, 1342.162, 1376.389, 1397.125, 1384.76, 1328.79, 1346.681, 1237.417, 1184.957, 1241.788, 1175.488, 1145.162, 1172.633, 1176.682, 1169.589, 1156.177, 1317.493, 1070.252, 1131.275, 1186.901, 1124.672, 1140.328, 1114.527, 1260.201, 1087.435, 1113.794, 1135.651, 1054.944, 1245.787, 1101.659, 1054.859, 1096.095, 1094.041, 1051.835, 1040.822, 1068.77, 1029.834, 1056.949, 1084.467, 1007.207, 1081.511, 1031.698, 1075.516, 1079.458, 1013.429, 987.475, 1009.505, 1018.544, 999.412, 997.78, 1045.0, 1030.729, 950.44, 992.493, 1052.784, 1016.866, 979.477, 977.599, 998.312, 993.033, 970.93, 990.313, 993.926, 948.525, 936.814, 957.148, 993.093, 958.535, 939.207, 1038.021, 988.054, 1002.6, 1016.269, 931.797, 946.082, 931.175, 993.128, 971.837, 926.073, 986.721, 967.651, 950.222, 1006.622, 961.064]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:43 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:89 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:79 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.09632825939233194 alpha:0.905160758358956 weight_decay:9.450726909087942e-05 batch_size:32 epochs:100	100	1000	True	26721.07422		408364	14	-1	200.95285201072693	{'train_loss': [908932.812, 761013.688, 742216.312, 694705.125, 723578.188, 711493.062, 747411.562, 635202.438, 786736.75, 708329.875, 655045.438, 624985.125, 647295.0, 667235.125, 765452.188, 825015.25, 641854.062, 617049.938, 654530.0, 749544.938, 663249.062, 574567.938, 775159.562, 664346.188, 720942.5, 640913.5, 626593.562, 626489.875, 737769.188, 630277.25, 650989.5, 589843.438, 879809.062, 908532.25, 601521.812, 734986.438, 730878.812, 615878.688, 668194.562, 756754.125, 750432.125, 724921.0, 604209.75, 626905.75, 711307.688, 750317.938, 653853.125, 902411.5, 665094.125, 697872.625, 804838.812, 646475.562, 787413.0, 859006.625, 601026.062, 720311.188, 652369.625, 607312.438, 651843.188, 625740.5, 675814.562, 608426.438, 771964.625, 711347.062, 617996.75, 595171.25, 780652.25, 698956.062, 759911.0, 881344.438, 567123.0, 675638.188, 639580.375, 809153.25, 634431.25, 686012.438, 867683.062, 729396.688, 522005.0, 763246.438, 933134.25, 608058.875, 647907.562, 680798.688, 594766.375, 622735.188, 663655.375, 627219.938, 626808.812, 649462.688, 638204.875, 635798.625, 645226.25, 816715.875, 734805.812, 828709.062, 685943.0, 555738.75, 778321.312, 818457.875], 'val_loss': [6689.301, 6474.5, 6703.988, 10793.664, 12456.226, 10719.838, 6363.949, 4678.652, 11517.417, 5640.349, 9818.621, 4708.389, 5217.804, 4772.475, 12426.327, 10604.995, 4872.193, 5434.316, 10065.888, 7764.747, 6695.057, 11632.916, 5686.735, 4836.713, 10434.242, 5008.409, 5085.034, 7003.419, 5569.311, 4678.205, 5391.533, 11042.744, 10724.584, 9704.378, 11504.875, 11009.852, 5008.85, 5433.218, 9095.545, 10961.107, 10987.471, 5129.572, 4685.261, 4718.141, 11008.145, 4940.334, 11038.337, 6212.389, 8910.99, 5240.802, 6425.422, 11330.017, 10643.441, 5012.309, 4703.608, 9141.829, 6341.228, 7651.356, 4632.869, 4686.312, 5478.924, 4978.536, 5693.859, 4778.259, 10654.662, 4829.233, 7521.477, 11043.059, 11044.332, 10295.469, 27427.104, 25535.566, 4642.129, 5134.226, 4638.038, 11043.377, 10979.938, 5314.43, 4640.151, 11132.061, 7104.164, 4648.513, 7424.371, 4823.483, 4740.493, 10993.107, 4988.492, 5256.472, 4802.213, 5425.573, 7176.034, 9569.928, 10917.143, 6826.14, 6081.35, 15849.291, 7565.392, 7645.934, 5892.642, 6301.767]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:35 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	4341.75586		376288	15	-1	201.99869060516357	{'train_loss': [306175.562, 172760.266, 156187.828, 145987.844, 138277.016, 131163.281, 128140.477, 124100.484, 120470.93, 117606.57, 116562.695, 114744.25, 112397.5, 110447.031, 110135.938, 109055.469, 107135.758, 105629.141, 104283.711, 103313.477, 102526.352, 101973.352, 100476.07, 99400.352, 98625.57, 99136.945, 97907.047, 97095.203, 96452.945, 95966.609, 95625.062, 95368.242, 94847.016, 94891.578, 94111.477, 93671.789, 93924.898, 93134.0, 92637.562, 91005.234, 91620.266, 91593.508, 91698.195, 90735.18, 89945.547, 89478.742, 89182.055, 89157.727, 89290.227, 88379.695, 88469.07, 87223.852, 88373.078, 87499.562, 87812.875, 87762.836, 87004.438, 86202.227, 85864.719, 86282.0, 86106.023, 86012.367, 85937.969, 86153.289, 85116.594, 84425.188, 84516.82, 84509.781, 84579.609, 84555.078, 84726.57, 83941.547, 83554.023, 84609.562, 83439.766, 82869.117, 82996.547, 83521.758, 83259.195, 83636.039, 81593.352, 81937.656, 82079.469, 81923.961, 82164.531, 81959.969, 82134.016, 81489.914, 81414.312, 81641.055, 81571.641, 81507.633, 80452.266, 81497.297, 80571.633, 80542.109, 80215.875, 80298.352, 80865.438, 81025.531], 'val_loss': [2776.419, 1765.599, 1815.804, 1805.249, 1824.007, 1683.496, 1655.358, 1654.833, 1639.294, 1620.706, 1610.967, 1537.999, 1475.634, 1499.423, 1484.703, 1391.059, 1383.605, 1353.846, 1450.956, 1343.52, 1357.731, 1425.542, 1382.764, 1415.743, 1429.461, 1372.845, 1395.638, 1307.768, 1337.017, 1377.583, 1379.344, 1252.086, 1285.03, 1343.923, 1301.469, 1241.93, 1281.962, 1299.612, 1272.722, 1207.042, 1197.126, 1306.454, 1296.642, 1217.762, 1265.507, 1197.146, 1223.253, 1213.658, 1208.694, 1193.53, 1295.933, 1250.262, 1208.107, 1241.178, 1233.547, 1237.001, 1194.202, 1154.325, 1170.368, 1196.677, 1144.63, 1183.631, 1135.136, 1165.444, 1167.31, 1173.325, 1120.658, 1126.554, 1144.295, 1181.019, 1115.012, 1065.119, 1157.051, 1159.491, 1122.489, 1113.374, 1109.441, 1166.78, 1153.448, 1144.093, 1080.149, 1066.559, 1142.405, 1133.549, 1107.373, 1100.097, 1086.955, 1084.16, 1116.604, 1108.203, 1137.556, 1131.322, 1181.771, 1123.125, 1097.423, 1083.606, 1063.831, 1062.516, 1123.635, 1067.071]}	100	100	True
