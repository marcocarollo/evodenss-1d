id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:52 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:116 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:12 epochs:100	100	1000	True	3713.64624		499172	15	-1	346.0227608680725	{'train_loss': [373369.875, 218977.109, 183305.938, 153983.125, 132318.891, 120990.203, 114999.672, 112369.875, 108278.812, 106022.258, 103852.742, 103042.234, 99731.523, 99099.531, 95925.477, 94899.695, 94152.594, 91917.484, 91314.938, 91134.758, 89843.352, 88851.914, 88615.992, 88139.234, 86038.844, 86340.188, 85831.594, 84956.023, 85061.641, 82732.695, 83425.547, 82814.578, 83140.648, 81907.75, 81870.367, 81529.062, 80906.383, 79984.117, 80518.695, 79418.398, 79518.844, 79195.406, 77907.281, 78180.969, 77886.375, 76914.844, 77017.508, 76586.086, 76819.062, 75942.477, 76548.695, 75688.18, 75547.328, 75316.844, 74936.617, 75014.836, 74067.625, 73967.5, 73792.203, 74048.305, 74906.32, 73937.531, 73355.219, 73419.172, 73245.383, 72917.445, 72972.945, 73027.836, 72501.539, 72757.188, 72597.57, 72511.383, 71419.609, 71859.383, 71541.094, 71165.758, 70738.219, 71026.68, 71093.453, 70812.656, 70824.391, 70562.352, 70759.43, 70648.211, 70076.977, 70091.0, 69691.648, 70723.773, 69172.289, 70197.25, 69630.352, 68876.078, 68630.633, 68476.234, 69157.227, 68588.648, 68456.453, 68801.188, 68074.711, 68284.688], 'val_loss': [1203.963, 951.555, 854.617, 640.334, 585.168, 547.35, 516.9, 512.889, 512.177, 480.934, 482.119, 481.05, 470.594, 453.599, 485.624, 459.336, 446.335, 434.458, 437.253, 430.351, 429.689, 423.391, 405.242, 403.618, 404.714, 401.695, 408.896, 389.315, 414.2, 393.433, 388.735, 379.833, 375.429, 384.149, 370.673, 377.484, 369.375, 369.239, 367.392, 355.885, 368.518, 363.873, 377.187, 362.668, 362.408, 366.73, 373.071, 352.419, 356.243, 361.759, 358.136, 351.006, 348.261, 348.518, 364.626, 342.129, 341.96, 340.222, 349.178, 356.405, 338.876, 341.873, 351.383, 334.966, 359.201, 335.167, 335.391, 344.45, 341.342, 327.618, 340.142, 330.927, 349.35, 343.093, 338.647, 345.261, 351.628, 334.981, 329.83, 349.893, 339.713, 343.937, 330.436, 333.699, 347.891, 331.553, 331.722, 339.303, 333.935, 334.158, 326.85, 327.607, 320.085, 334.162, 326.577, 337.092, 328.191, 334.22, 325.333, 325.171]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:52 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:61 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:107 kernel_size:10 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.10107500073194553 alpha:0.9361335603304262 weight_decay:0.00046194219725101057 batch_size:12 epochs:100	100	1000	True	44535.89844		686408	14	-1	362.50502133369446	{'train_loss': [1105936.625, 804482.375, 1339645.75, 917629.0, 893268.188, 1147388.375, 946719.812, 1067158.25, 1232816.125, 1124184.625, 1125577.25, 1322332.125, 1257033.625, 1405915.125, 1360992.25, 1032911.75, 1133303.125, 991383.562, 1141160.125, 887549.125, 1132828.25, 984020.5, 1052825.125, 986140.625, 914572.75, 858809.438, 863341.25, 919219.688, 870987.375, 921786.875, 842965.938, 891879.125, 862064.312, 888040.062, 868674.75, 882006.875, 860943.5, 889356.5, 881997.188, 919114.312, 911781.062, 856073.5, 910890.125, 853452.312, 916718.562, 862619.875, 890518.312, 872989.0, 893405.375, 876508.25, 893504.75, 871211.938, 900313.562, 873740.938, 899485.438, 873803.5, 904470.0, 860390.188, 919883.438, 856948.0, 909736.438, 837392.375, 909160.812, 861933.312, 886445.438, 906039.5, 859944.25, 917696.438, 854038.75, 903578.0, 839336.188, 937004.875, 846935.562, 982309.688, 864890.688, 906841.312, 863079.5, 918841.75, 882452.938, 878374.125, 984862.688, 846768.312, 960463.375, 842307.125, 907380.188, 863934.188, 860016.188, 888262.375, 887670.938, 880524.188, 883066.75, 874452.312, 887890.875, 860833.938, 908902.312, 846523.188, 915484.0, 852913.25, 880400.312, 869863.875], 'val_loss': [4016.166, 4016.166, 4016.166, 4016.166, 4016.127, 4016.166, 4016.166, 4016.162, 4016.166, 4016.166, 4016.166, 4413.324, 4016.165, 4016.166, 4016.166, 4016.166, 4016.166, 4525.893, 4016.166, 4016.166, 22073.016, 4660.405, 4007.457, 4016.166, 4015.781, 4034.847, 4016.166, 4903.901, 4016.166, 4016.166, 4016.166, 4016.166, 4016.166, 4016.166, 4016.166, 4122.234, 4016.166, 4016.161, 4016.166, 4016.166, 4016.164, 4016.166, 4016.166, 4558.156, 4016.078, 4016.139, 4016.165, 4016.166, 4016.166, 4016.061, 4016.162, 4016.166, 4524.539, 4523.687, 4016.166, 4002.732, 4802.788, 4016.166, 4016.154, 4016.165, 4367.785, 4016.166, 4727.722, 4016.166, 4016.166, 4016.166, 4015.979, 4016.166, 4016.145, 4014.958, 4307.871, 4016.166, 4016.166, 4016.159, 4153.747, 4016.166, 4016.166, 4016.165, 9449.68, 4016.166, 4016.166, 4016.166, 4016.166, 4016.165, 4016.166, 4016.166, 4016.166, 4016.166, 4319.063, 4016.166, 4016.164, 4016.166, 4014.906, 4008.593, 4016.166, 4016.166, 4016.163, 4016.166, 4016.166, 4016.166]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:52 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:116 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:12 epochs:100	100	2000	True	3729.64062		499172	15	-1	341.87276792526245	{'train_loss': [356260.25, 209438.172, 167792.312, 134306.984, 123313.047, 116830.992, 114663.008, 109518.875, 107680.758, 104389.133, 103227.305, 100474.883, 98117.984, 98007.016, 95684.461, 94947.445, 93320.266, 92275.586, 90654.594, 90212.281, 88719.273, 88067.477, 86979.688, 87305.812, 86353.812, 85584.734, 84185.914, 84270.367, 83174.648, 82803.547, 83215.484, 82210.719, 82792.453, 81285.711, 81080.844, 81078.008, 80380.172, 79053.516, 79498.648, 78945.68, 78686.375, 78457.297, 78031.125, 77613.719, 77244.219, 76864.5, 76457.273, 76350.086, 75926.086, 75846.352, 75630.406, 75387.594, 74939.234, 74760.039, 74215.148, 74269.133, 74027.141, 73776.695, 73782.953, 73089.539, 73448.969, 73308.633, 73016.75, 72483.312, 72669.914, 72092.758, 72146.164, 72029.625, 71853.602, 71658.656, 71728.523, 71635.094, 71099.75, 70775.398, 71397.008, 70549.641, 70557.828, 70286.016, 70377.68, 70669.375, 70439.055, 70135.734, 69967.641, 69334.883, 70398.32, 69727.828, 69954.703, 69321.273, 69668.719, 68759.43, 68897.766, 68118.258, 69304.828, 68520.203, 68882.461, 68845.844, 68618.164, 68405.477, 67904.203, 68544.734], 'val_loss': [1025.931, 1103.695, 689.706, 596.292, 560.556, 536.099, 531.457, 505.793, 519.445, 502.81, 492.338, 485.047, 463.996, 464.875, 454.257, 441.51, 432.396, 427.195, 424.071, 438.157, 411.888, 413.628, 410.573, 393.964, 392.953, 388.114, 384.585, 384.52, 383.227, 370.132, 400.527, 377.844, 357.289, 364.715, 363.813, 373.27, 354.324, 367.935, 356.377, 357.437, 350.647, 357.203, 349.787, 352.717, 361.858, 339.591, 355.717, 352.837, 341.164, 337.548, 350.188, 344.579, 336.269, 339.892, 346.293, 350.549, 341.641, 349.09, 348.181, 345.884, 347.423, 336.709, 346.913, 336.779, 350.225, 346.225, 336.417, 344.61, 331.348, 343.875, 324.151, 338.507, 343.474, 347.746, 338.816, 333.916, 335.51, 342.679, 356.14, 339.919, 335.739, 343.069, 334.876, 324.752, 329.356, 324.417, 332.916, 337.934, 326.049, 328.658, 329.927, 333.5, 331.848, 331.736, 332.104, 329.241, 327.509, 334.788, 327.602, 330.666]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:85 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:52 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:60 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:116 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:12 epochs:100	100	1000	True	3969.22168		489722	15	-1	349.2309091091156	{'train_loss': [225157.141, 145233.078, 127330.344, 119328.328, 113863.227, 109333.164, 106347.258, 104074.094, 102490.297, 101815.453, 99276.789, 97686.18, 97155.477, 95025.523, 94022.953, 92917.461, 92905.75, 91387.984, 89324.102, 89586.883, 89215.805, 87907.391, 87232.969, 86584.547, 86261.742, 84867.18, 84733.227, 83612.227, 83193.508, 82440.422, 82494.562, 82206.477, 81843.164, 81060.445, 81124.539, 80730.492, 79757.906, 78853.617, 79413.516, 79195.734, 78086.867, 79117.156, 77917.172, 77849.719, 77374.617, 77214.875, 76861.086, 76653.875, 76453.156, 76312.664, 75466.867, 75342.688, 75191.25, 75055.562, 74746.258, 74899.562, 74340.312, 74900.852, 74187.32, 74456.18, 73928.117, 74453.375, 73670.273, 73668.25, 73505.031, 72632.57, 72811.312, 72399.281, 72030.727, 72323.117, 72003.359, 71587.766, 72070.188, 72137.703, 72363.562, 71671.125, 71900.375, 70871.93, 70915.664, 71290.094, 70859.812, 70446.938, 70699.945, 69789.422, 70364.703, 70533.141, 70705.914, 69542.039, 70703.141, 70320.688, 70182.984, 70722.875, 70126.102, 70260.484, 70098.633, 69153.891, 68805.047, 69846.594, 68901.148, 69410.789], 'val_loss': [660.264, 592.931, 540.575, 552.753, 506.778, 515.622, 529.926, 498.448, 499.796, 506.162, 512.954, 510.492, 488.428, 468.25, 456.319, 475.998, 466.893, 467.912, 464.004, 440.419, 471.362, 424.567, 447.992, 450.739, 437.226, 458.607, 395.612, 472.371, 407.119, 428.645, 391.927, 407.024, 406.764, 400.679, 396.831, 395.464, 389.931, 388.677, 409.536, 407.638, 367.403, 408.596, 392.204, 393.205, 391.659, 368.332, 378.175, 400.712, 408.252, 382.242, 396.649, 370.16, 356.295, 372.675, 386.35, 387.657, 405.211, 407.059, 366.48, 379.454, 365.841, 365.202, 372.33, 360.154, 374.99, 384.663, 345.848, 357.558, 383.895, 352.074, 373.114, 358.786, 359.428, 397.74, 359.313, 372.026, 371.635, 353.868, 366.658, 388.752, 353.717, 359.374, 340.611, 356.079, 344.442, 353.703, 352.07, 355.387, 356.12, 353.071, 369.461, 343.859, 354.567, 366.186, 349.821, 365.756, 343.725, 343.618, 361.359, 343.156]}	100	100	True
