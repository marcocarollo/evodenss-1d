id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	5000	True	4082.49414		491663	14	-1	280.3789358139038	{'train_loss': [524694.688, 361072.719, 225518.703, 187933.594, 160793.531, 135341.703, 126096.219, 120830.461, 117003.312, 113846.516, 111275.719, 110088.703, 107975.523, 104529.375, 103033.594, 102956.141, 101650.047, 100613.164, 99933.797, 98619.211, 97435.016, 96858.016, 95838.602, 96168.266, 95351.875, 94909.812, 93240.617, 93839.195, 93494.188, 91989.43, 90992.719, 90857.898, 90369.789, 89648.828, 89719.617, 89029.047, 87929.117, 88376.484, 87135.781, 87788.547, 86580.375, 86945.086, 86699.992, 85485.852, 85632.305, 85472.531, 84268.648, 83018.914, 83920.93, 85536.508, 83867.648, 83846.766, 82521.344, 82975.312, 82478.961, 81509.969, 81699.0, 80582.688, 81820.688, 80694.914, 81034.289, 79580.352, 79445.617, 79198.391, 78344.898, 78274.625, 78353.719, 79678.172, 77744.719, 77876.211, 78004.828, 77981.352, 75815.891, 76414.578, 75836.633, 77636.969, 75058.078, 77130.664, 75738.383, 74515.844, 74265.328, 74254.234, 74651.125, 74160.68, 74210.984, 73723.82, 74567.602, 73356.023, 74449.125, 73218.922, 73373.508, 73124.25, 72538.805, 72177.781, 72709.539, 72737.25, 72289.508, 72270.773, 72394.992, 71363.109], 'val_loss': [6280.817, 3333.959, 2965.55, 3082.871, 1998.903, 2368.093, 1988.925, 1924.557, 1946.586, 1799.461, 1718.318, 1534.668, 1563.318, 1501.532, 1455.422, 1444.63, 1593.741, 1461.34, 1271.985, 1375.104, 1361.375, 1429.069, 1223.618, 1255.403, 1276.922, 1232.615, 1288.055, 1241.746, 1330.47, 1229.564, 1310.623, 1213.391, 1199.789, 1212.96, 1201.763, 1182.441, 1248.899, 1197.889, 1204.421, 1174.804, 1169.264, 1175.621, 1217.049, 1229.512, 1184.205, 1237.688, 1175.91, 1148.379, 1196.223, 1143.872, 1139.348, 1237.495, 1199.12, 1195.802, 1105.613, 1186.942, 1174.589, 1144.709, 1163.594, 1140.701, 1133.491, 1136.889, 1070.754, 1137.408, 1073.872, 1104.41, 1104.875, 1072.797, 1173.596, 1071.202, 1108.192, 1065.147, 1033.726, 1062.87, 1083.165, 1042.202, 1067.984, 1047.16, 1038.23, 1020.319, 1039.993, 1035.718, 1054.001, 1109.795, 1016.211, 1039.812, 1059.209, 1046.479, 1038.736, 1034.609, 1014.246, 1061.838, 1048.466, 1044.937, 1021.432, 1014.347, 1031.822, 1007.338, 997.921, 1026.428]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	5000	True	3871.68359		491663	14	-1	272.36099433898926	{'train_loss': [536777.062, 310867.844, 224851.141, 197864.875, 183615.844, 165162.703, 134514.672, 118531.867, 113233.82, 108628.93, 105724.688, 101722.234, 99944.938, 97417.164, 96303.812, 95542.508, 94605.414, 93441.211, 91418.773, 92539.766, 91195.781, 90329.406, 89294.891, 89018.742, 88443.477, 87613.312, 86035.258, 85167.836, 85481.664, 85186.18, 85264.891, 84231.18, 82686.266, 82462.758, 83258.172, 81729.188, 81224.906, 80799.836, 80409.578, 79406.328, 80912.336, 79190.695, 79345.617, 79091.461, 78341.664, 78613.438, 77160.766, 78150.711, 78314.664, 76670.898, 77314.797, 77157.875, 76102.539, 75693.914, 76009.016, 74468.0, 75141.758, 75630.906, 74839.086, 74634.945, 74090.422, 74138.656, 74501.305, 73851.836, 73271.641, 73369.281, 73401.25, 73027.859, 73385.969, 72985.273, 73369.867, 72498.961, 71777.039, 71968.953, 72632.438, 71069.789, 71794.328, 71613.445, 70098.195, 72393.969, 72032.32, 71741.984, 70916.305, 70306.805, 71024.117, 71482.273, 69903.969, 70543.898, 69870.703, 70655.773, 69370.891, 70526.484, 69311.031, 68793.547, 70223.898, 69827.555, 68825.312, 69187.07, 69004.797, 69953.281], 'val_loss': [5119.25, 3085.652, 2899.946, 2533.311, 2846.021, 2562.421, 2041.956, 1903.069, 1676.147, 1757.548, 1495.679, 1497.21, 1566.86, 1592.587, 1551.599, 1634.656, 1510.025, 1342.162, 1376.389, 1397.125, 1384.76, 1328.79, 1346.681, 1237.417, 1184.957, 1241.788, 1175.488, 1145.162, 1172.633, 1176.682, 1169.589, 1156.177, 1317.493, 1070.252, 1131.275, 1186.901, 1124.672, 1140.328, 1114.527, 1260.201, 1087.435, 1113.794, 1135.651, 1054.944, 1245.787, 1101.659, 1054.859, 1096.095, 1094.041, 1051.835, 1040.822, 1068.77, 1029.834, 1056.949, 1084.467, 1007.207, 1081.511, 1031.698, 1075.516, 1079.458, 1013.429, 987.475, 1009.505, 1018.544, 999.412, 997.78, 1045.0, 1030.729, 950.44, 992.493, 1052.784, 1016.866, 979.477, 977.599, 998.312, 993.033, 970.93, 990.313, 993.926, 948.525, 936.814, 957.148, 993.093, 958.535, 939.207, 1038.021, 988.054, 1002.6, 1016.269, 931.797, 946.082, 931.175, 993.128, 971.837, 926.073, 986.721, 967.651, 950.222, 1006.622, 961.064]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:39 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:123 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:61 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.22797119647001352 alpha:0.8046659495825146 weight_decay:7.136769455232232e-05 batch_size:32 epochs:100	100	1000	True	15342.59766		389221	14	-1	191.8295726776123	{'train_loss': [446386.031, 437502.906, 432556.0, 406126.5, 398306.625, 414714.812, 421383.094, 407106.344, 396271.375, 400647.656, 362453.438, 361161.844, 339386.344, 341582.562, 340399.406, 343225.969, 348125.25, 332179.312, 341895.594, 328663.531, 319430.531, 321990.969, 332077.344, 324115.125, 319274.812, 320794.5, 326820.438, 307013.0, 317823.688, 306397.812, 306875.344, 312076.281, 313570.938, 307763.594, 289754.0, 312035.219, 299223.844, 307724.562, 297044.281, 298799.938, 312293.75, 297441.031, 305429.906, 293058.344, 313768.125, 297568.125, 315024.688, 309611.375, 307742.688, 294718.125, 333251.594, 313198.906, 314091.438, 303067.875, 311713.0, 320644.219, 313579.781, 304473.062, 300406.688, 316004.875, 297885.438, 299742.906, 299226.344, 295112.844, 316923.812, 289931.469, 296273.75, 304455.094, 296195.0, 308690.562, 302152.969, 300071.875, 293865.938, 293560.438, 286982.25, 302005.031, 302806.031, 293090.156, 284960.75, 291481.312, 286258.938, 299807.062, 274799.25, 293399.438, 285600.906, 306322.656, 295691.562, 296641.875, 292826.125, 291964.656, 299451.062, 291789.344, 296475.156, 294319.562, 288333.938, 284410.656, 285655.094, 286075.312, 288119.375, 292771.719], 'val_loss': [9921.469, 5148.869, 6301.06, 74869.32, 5209.547, 4830.792, 5645.101, 6216.114, 6126.957, 4713.52, 4185.983, 3863.66, 13336.535, 4276.993, 2895.959, 4218.07, 3549.045, 4521.211, 3499.499, 3398.236, 4159.966, 3084.965, 3424.367, 3284.781, 3243.668, 7391.595, 4444.685, 3873.086, 3341.12, 3590.908, 2901.739, 3425.849, 2701.724, 3516.193, 3708.884, 3922.903, 6499.174, 4464.98, 4886.531, 7494.441, 3156.018, 3850.044, 5390.29, 6844.954, 2890.135, 2762.313, 3104.123, 2969.151, 4915.634, 3506.394, 5925.405, 3366.506, 4399.337, 5606.373, 5012.908, 11936.297, 4826.983, 4250.514, 3218.073, 2549.01, 7804.278, 2510.482, 3468.955, 2566.723, 2445.202, 4897.567, 3564.86, 2888.956, 4780.362, 3361.021, 3334.732, 4050.78, 14066.029, 4091.489, 4188.61, 4037.872, 3284.176, 4515.071, 2930.886, 5311.458, 3456.948, 9806.346, 6034.699, 4356.346, 7070.762, 3007.46, 3001.051, 10419.145, 3665.598, 2524.968, 7037.519, 2705.742, 2911.288, 3386.678, 3432.027, 3005.591, 3539.549, 3717.355, 10057.412, 4081.997]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	5000	True	4046.01172		417269	15	-1	221.91490364074707	{'train_loss': [261775.234, 192674.266, 170052.031, 154301.719, 144270.641, 133842.156, 127047.234, 121389.484, 117654.867, 113478.477, 112064.414, 108394.273, 108312.977, 106801.828, 105037.992, 104158.633, 103063.672, 101218.656, 99892.312, 99316.086, 98490.852, 97407.898, 97189.32, 96414.844, 95103.484, 94160.312, 93913.586, 93413.172, 91879.445, 91142.625, 90285.867, 91061.883, 90165.188, 89391.047, 89784.391, 88772.25, 87681.164, 87200.359, 87091.773, 86478.922, 85572.969, 85077.633, 85392.445, 85086.508, 85024.703, 84852.211, 84151.328, 83321.109, 83207.594, 83330.969, 82731.242, 82131.039, 81366.859, 81371.141, 81044.172, 81474.203, 80826.516, 81243.516, 80972.734, 80556.461, 79575.359, 79998.297, 79960.734, 79633.164, 79531.125, 79769.758, 78878.859, 78509.602, 78859.164, 78168.977, 78011.828, 78507.68, 77968.781, 77376.562, 77756.148, 77422.609, 77534.664, 76344.5, 76746.25, 76943.75, 76296.633, 76034.938, 76325.32, 75326.25, 76198.062, 75541.734, 76290.789, 75792.75, 76015.758, 75120.711, 74559.445, 75009.898, 74181.234, 75662.906, 73969.422, 75078.523, 74320.102, 74010.078, 73772.891, 73507.133], 'val_loss': [2379.76, 2003.302, 1772.447, 1658.45, 1873.419, 1768.473, 1903.473, 1628.721, 1521.341, 1552.806, 1527.389, 1462.909, 1617.557, 1566.148, 1428.212, 1469.978, 1390.283, 1514.505, 1406.958, 1388.33, 1284.533, 1248.183, 1339.04, 1213.869, 1306.235, 1251.534, 1358.02, 1215.881, 1243.854, 1207.721, 1228.521, 1165.318, 1193.508, 1237.825, 1227.488, 1156.916, 1194.283, 1162.618, 1113.398, 1115.158, 1105.471, 1236.112, 1060.782, 1089.651, 1124.867, 1130.427, 1112.482, 1176.66, 1069.095, 1084.626, 1157.006, 1117.639, 1097.46, 1171.269, 1062.666, 1087.007, 1096.671, 1116.87, 1091.628, 1036.55, 1085.614, 1035.215, 1135.355, 1089.094, 1050.845, 1135.183, 1104.24, 1070.346, 1040.751, 1072.826, 1039.562, 1012.954, 1087.58, 1086.303, 1047.078, 1024.254, 1041.843, 1055.208, 1089.043, 1052.175, 1016.657, 1044.59, 1083.403, 1081.459, 1090.322, 1038.198, 1066.374, 1035.26, 1064.196, 978.831, 1028.528, 1004.351, 1063.683, 1024.26, 1036.058, 1029.653, 1042.975, 969.503, 1040.646, 990.337]}	0	100	True
