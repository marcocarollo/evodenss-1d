id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	2000	True	4351.54541		470815	15	-1	196.63490462303162	{'train_loss': [393440.094, 368826.844, 257612.031, 198649.812, 184882.203, 177443.609, 174090.438, 162143.25, 137868.516, 123253.414, 116141.492, 112118.148, 108685.055, 107096.797, 107651.867, 103555.297, 101062.516, 100309.773, 98505.023, 97363.016, 95149.531, 95394.398, 94774.891, 93275.07, 92265.477, 91839.898, 91242.93, 90570.555, 90452.117, 89149.031, 89615.539, 88533.398, 87519.094, 88294.023, 86876.938, 87623.688, 86520.086, 85093.766, 84965.836, 85088.195, 84085.594, 84031.539, 84537.961, 83378.375, 83687.008, 82697.938, 82036.914, 82820.711, 82204.125, 82078.008, 80884.945, 81486.891, 81595.734, 80810.578, 81168.125, 80021.008, 80043.102, 80050.641, 79825.609, 79396.18, 79103.07, 79746.055, 79644.008, 78722.266, 77632.828, 77938.18, 78051.898, 78584.367, 78235.438, 77845.648, 77790.25, 77481.836, 77219.461, 77360.75, 76896.0, 76929.133, 77225.234, 77104.617, 75849.969, 76617.617, 75671.5, 76079.344, 75908.781, 75301.672, 76195.242, 74735.086, 74419.641, 74564.781, 75219.617, 75079.531, 74411.617, 74454.344, 74881.406, 74595.734, 74154.055, 73910.43, 73329.406, 73809.898, 74162.492, 72310.766], 'val_loss': [5354.752, 4943.657, 3019.19, 3178.562, 2503.413, 2374.505, 2830.62, 2257.185, 2056.062, 1874.421, 1657.483, 1907.955, 1642.297, 1578.165, 1657.691, 1690.356, 1461.961, 1390.773, 1558.892, 1353.94, 1456.773, 1295.278, 1336.697, 1427.47, 1266.729, 1354.167, 1283.955, 1294.811, 1336.828, 1294.681, 1280.651, 1278.049, 1283.351, 1338.042, 1345.379, 1171.527, 1236.278, 1243.798, 1216.288, 1234.69, 1231.678, 1186.767, 1373.151, 1201.372, 1220.126, 1181.009, 1199.093, 1209.894, 1198.469, 1163.462, 1151.521, 1152.42, 1090.108, 1156.825, 1208.578, 1124.422, 1259.938, 1159.774, 1065.223, 1136.258, 1098.19, 1094.612, 1117.43, 1153.802, 1112.047, 1107.557, 1129.053, 1120.494, 1058.992, 1074.711, 1097.122, 1040.777, 1100.765, 1072.064, 1102.505, 1048.991, 1140.826, 1045.258, 1083.23, 1055.058, 1034.873, 1012.871, 1058.048, 1053.83, 1064.331, 1066.104, 1119.564, 1011.098, 1023.726, 1031.617, 1018.169, 1046.799, 1093.569, 1121.331, 1094.132, 1038.171, 1087.992, 1037.51, 1015.464, 1104.575]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:95 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:48 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:49 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.16699114326444553 beta1:0.9182447616573843 beta2:0.9332052311473158 weight_decay:9.155926996571642e-05 batch_size:32 epochs:100	100	1000	True	44539.80859		1025119	15	-1	666.1042683124542	{'train_loss': [815272.562, 804482.438, 804482.438, 1122575.25, 811444.812, 804482.438, 804482.438, 807160.75, 1064250.625, 830147.438, 811754.562, 950356.312, 807168.312, 836601.375, 824649.75, 823256.312, 1184089.25, 804772.438, 807251.938, 839514.188, 818085.625, 814015.438, 899444.625, 821826.188, 833411.188, 817404.375, 824384.438, 841920.938, 837907.25, 1030827.562, 839693.375, 856907.438, 808657.062, 812126.375, 870785.625, 821318.062, 863081.188, 848734.5, 830887.812, 818595.875, 865567.375, 889678.812, 824355.188, 813208.188, 816629.375, 896322.5, 816733.688, 839739.438, 810740.438, 1125563.125, 843366.188, 818516.438, 848718.562, 806623.25, 894353.062, 811652.25, 820341.188, 859200.562, 808188.938, 850294.562, 838430.688, 838463.062, 820594.312, 818774.562, 865423.125, 831643.938, 824133.875, 888560.688, 888454.438, 838027.688, 819576.062, 988391.25, 833635.688, 842443.562, 823749.188, 813104.25, 847907.188, 882289.312, 882655.375, 819542.375, 808959.438, 907335.312, 813475.062, 813969.438, 822764.562, 856110.25, 815850.062, 821543.188, 854245.062, 854535.938, 815239.688, 850867.688, 875726.125, 985157.0, 822233.375, 842298.938, 894296.688, 818060.562, 846521.25, 868437.5], 'val_loss': [11044.456, 11044.456, 11044.456, 16908.111, 11044.456, 11044.456, 176153.562, 11044.456, 27413.102, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 15052.479, 11044.456, 11044.456, 14129.154, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11670.283, 11245.022, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11042.638, 11044.456, 11044.456, 11044.456, 11044.456, 11044.393, 11044.456, 11044.456, 13381.143, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11148.836, 11044.456, 11044.456, 11043.699, 12861.541, 11044.456, 11044.456, 11044.456, 11044.456, 11044.402, 11044.456, 11044.456, 11044.456, 11044.456, 11016.916, 11044.456, 11044.456, 13128.181, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.432, 11044.456, 11044.453, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:86 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	44536.33203		2608740	14	-1	197.09123182296753	{'train_loss': [1936417.875, 805486.312, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 840452.812, 804482.438, 804482.438, 807899.625, 806159.25, 865180.938, 808238.438, 804482.438, 806264.438, 813441.688, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804689.562, 804482.438, 805725.375, 805742.312, 804482.438, 804715.938, 805138.812, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 847513.188, 805519.938, 805031.688, 805008.812, 804482.438, 804482.438, 804933.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438], 'val_loss': [11044.193, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11551.222, 11044.451, 11044.455, 11044.456, 11044.456, 11042.307, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.455, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:49 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:49 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:117 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:53 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta batch_size:32 epochs:100	100	2000	True	4745.60205		1708103	16	-1	666.919260263443	{'train_loss': [892782.375, 839628.25, 832830.812, 820873.688, 814236.875, 815293.375, 396075.094, 248890.047, 184139.938, 152099.047, 134848.25, 125592.039, 119809.672, 118264.164, 113312.117, 112956.117, 109757.141, 106819.227, 105797.57, 103428.438, 103051.117, 102451.336, 99336.883, 98821.016, 99196.336, 96707.469, 96978.484, 95631.453, 94872.93, 94737.484, 93374.531, 94370.641, 92814.906, 92344.516, 91377.719, 92829.273, 91814.836, 89760.656, 88613.945, 90820.055, 88986.188, 89047.094, 90079.898, 88973.422, 87608.562, 88402.172, 87310.18, 86771.328, 87871.938, 89540.133, 88760.18, 87697.57, 86704.758, 86692.102, 86139.766, 87712.414, 89129.648, 86851.602, 87560.18, 85081.828, 84719.469, 84695.008, 86513.078, 83811.945, 86327.883, 84963.141, 84182.688, 84816.18, 84744.469, 85188.516, 83737.727, 84938.914, 82051.367, 82352.352, 82204.242, 82087.5, 82609.367, 84969.664, 82222.531, 81376.172, 81980.469, 82516.477, 84818.898, 81538.977, 81211.93, 82268.938, 81175.227, 80856.156, 82470.602, 79186.539, 80462.766, 79849.242, 79086.531, 80974.164, 79436.938, 79614.969, 78579.969, 79535.383, 77815.734, 80264.625], 'val_loss': [11043.082, 11043.732, 11035.776, 11034.709, 11038.023, 7922.617, 4432.211, 3325.61, 2158.944, 2412.982, 1733.258, 1678.392, 1587.543, 1604.3, 1635.35, 1471.967, 1539.503, 1551.258, 1559.876, 1497.266, 1544.72, 1506.489, 1518.852, 1552.831, 1515.796, 1470.114, 1454.367, 1495.515, 1468.94, 1355.118, 1433.449, 1441.394, 1227.015, 1325.469, 1467.043, 1355.663, 1262.963, 1394.082, 1180.47, 1281.553, 1362.839, 1337.604, 1190.204, 1269.522, 1193.302, 1419.568, 1151.897, 1330.341, 1306.385, 1200.308, 1241.06, 1242.678, 1302.811, 1204.708, 1239.825, 1526.627, 1253.76, 1301.072, 1150.416, 1210.674, 1318.373, 1155.235, 1216.418, 1384.315, 1222.226, 1145.29, 1184.16, 1202.168, 1117.893, 1218.952, 1373.888, 1169.75, 1237.45, 1229.621, 1103.593, 1136.43, 1201.406, 1210.911, 1387.566, 1230.29, 1325.239, 1214.74, 1077.18, 1115.537, 1156.195, 1212.969, 1200.356, 1256.91, 1138.93, 1183.194, 1190.003, 1186.598, 1134.526, 1100.955, 1162.192, 1239.595, 1109.796, 1132.5, 1157.668, 1133.885]}	0	100	True
