id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	5000	True	4110.28418		491663	14	-1	282.37253999710083	{'train_loss': [546199.75, 315002.438, 185490.031, 151999.281, 136969.938, 129439.234, 124618.844, 119254.656, 116039.109, 113319.719, 110395.406, 108419.172, 106793.727, 103911.344, 102959.852, 100751.266, 100028.789, 98507.719, 97965.844, 96982.266, 95828.875, 94669.93, 94459.984, 94058.852, 92471.414, 92249.688, 92180.086, 92029.219, 90051.953, 89933.297, 89170.727, 88315.695, 87604.633, 86608.508, 86552.961, 86486.633, 86115.461, 85921.516, 84904.555, 84685.047, 83473.633, 83827.203, 82766.867, 83071.328, 83151.297, 81906.117, 81770.664, 81097.555, 81156.289, 81313.953, 81020.852, 80363.742, 80263.695, 80346.609, 79529.875, 78964.68, 79493.219, 78782.609, 78353.641, 77738.805, 77775.578, 78041.422, 78004.781, 77053.828, 77929.922, 76911.594, 77042.414, 76254.148, 75418.117, 75486.773, 75377.68, 75738.812, 74082.977, 74695.18, 74500.352, 73590.312, 74490.844, 73854.516, 73677.531, 73677.781, 73744.734, 73127.0, 72998.164, 73052.031, 72148.195, 72080.344, 73176.742, 72364.781, 72047.133, 72105.492, 72130.953, 72636.219, 70836.234, 72273.453, 70673.914, 70790.68, 71128.008, 71095.008, 70222.281, 70328.336], 'val_loss': [7087.618, 3862.651, 2049.313, 1756.348, 1836.826, 1574.53, 1517.083, 1528.246, 1453.025, 1438.923, 1375.802, 1419.729, 1338.172, 1377.644, 1379.899, 1305.827, 1334.223, 1293.041, 1319.245, 1338.393, 1263.136, 1257.231, 1296.575, 1294.867, 1298.125, 1264.581, 1278.033, 1266.49, 1244.318, 1216.756, 1243.676, 1242.741, 1232.754, 1146.383, 1239.768, 1148.746, 1164.813, 1181.731, 1160.561, 1128.035, 1145.064, 1258.886, 1180.44, 1114.546, 1116.538, 1087.639, 1106.679, 1129.987, 1107.878, 1259.925, 1103.335, 1086.973, 1094.866, 1091.17, 1086.14, 1261.342, 1069.477, 1055.533, 1126.198, 1036.088, 1053.379, 1047.234, 1001.093, 1050.638, 1059.1, 1079.028, 1035.935, 1021.197, 1003.969, 1082.0, 1028.278, 1044.774, 1063.261, 1029.589, 992.688, 1048.27, 1016.725, 1016.732, 1045.08, 1085.135, 1025.909, 1018.042, 989.714, 1000.82, 992.099, 987.041, 1015.197, 985.629, 1018.163, 1013.31, 1001.242, 1007.898, 999.445, 1009.209, 997.723, 1018.837, 976.357, 1010.276, 1031.868, 1029.688]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:54 kernel_size:9 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4573.9585		458617	14	-1	244.41098403930664	{'train_loss': [435094.031, 350816.031, 223426.734, 184485.594, 154871.125, 135136.359, 126426.031, 120511.062, 116884.062, 114261.367, 112499.867, 110550.352, 107333.438, 105015.852, 104539.883, 102531.938, 101462.5, 100499.453, 98840.57, 97711.242, 98430.383, 97997.164, 96469.414, 96205.711, 95053.773, 93937.633, 93218.016, 93128.719, 92607.297, 91560.805, 90766.461, 90558.688, 89418.469, 89592.922, 89299.086, 88836.852, 88470.734, 90045.906, 87284.141, 87155.891, 86465.008, 87072.695, 88015.773, 85434.219, 85554.961, 85249.75, 84819.688, 84385.641, 83063.766, 83449.594, 82304.961, 82516.492, 82594.523, 81655.32, 82476.789, 81678.648, 81988.359, 80906.43, 79792.742, 80446.312, 79558.781, 80443.641, 79582.258, 79419.273, 78561.242, 77625.344, 78632.656, 78902.805, 78699.977, 78108.711, 78183.508, 77170.578, 77614.281, 76973.695, 77878.445, 77112.453, 77315.047, 76267.289, 76482.633, 76738.305, 76340.367, 76075.828, 75179.047, 76626.5, 75121.477, 75499.922, 75022.25, 75677.086, 75265.055, 74916.117, 75120.453, 74193.531, 74950.93, 74673.445, 75148.148, 74119.281, 74925.039, 73997.258, 74341.984, 73658.516], 'val_loss': [5700.063, 3000.29, 2659.83, 2674.687, 2343.935, 2328.994, 2090.025, 2182.192, 2049.078, 1947.471, 1847.865, 1769.616, 1673.702, 1701.358, 1675.38, 1555.061, 1584.302, 1588.639, 1472.215, 1508.351, 1536.726, 1488.667, 1487.078, 1461.477, 1414.427, 1421.316, 1394.182, 1441.562, 1415.497, 1472.201, 1345.267, 1375.247, 1295.037, 1359.91, 1302.228, 1316.676, 1364.293, 1241.848, 1389.553, 1349.206, 1294.216, 1376.649, 1202.514, 1225.018, 1259.091, 1226.147, 1208.482, 1215.553, 1239.767, 1241.117, 1214.712, 1204.908, 1178.046, 1235.946, 1122.4, 1172.668, 1127.522, 1107.896, 1246.394, 1251.76, 1180.962, 1163.659, 1188.562, 1207.118, 1240.285, 1214.982, 1181.767, 1105.788, 1188.175, 1150.528, 1172.606, 1234.12, 1022.732, 1114.789, 1187.992, 1151.251, 1168.76, 1142.574, 1213.542, 1095.38, 1141.578, 1176.231, 1105.783, 1061.174, 1135.193, 1070.783, 1183.069, 1142.137, 1088.365, 1071.813, 1102.826, 1129.054, 1107.733, 1050.612, 1163.03, 1118.314, 1115.118, 1059.861, 1060.77, 1081.023]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:8 epochs:100	100	1000	True	4489.67188		436095	13	-1	404.7241060733795	{'train_loss': [325853.875, 143766.047, 125478.07, 117742.32, 112414.758, 109524.375, 106975.188, 105454.219, 104117.742, 101775.609, 100130.25, 99544.945, 98164.5, 96942.172, 95046.281, 94059.859, 94201.594, 92760.516, 91733.836, 91066.719, 90058.438, 89467.391, 88873.016, 88690.086, 87199.469, 86743.812, 86983.797, 86202.648, 85485.945, 84892.5, 85454.695, 85040.289, 84941.516, 84457.688, 83725.148, 83181.523, 82775.336, 82973.438, 81796.656, 82339.898, 81758.258, 81921.922, 81049.148, 80541.148, 81124.055, 80570.969, 79286.016, 79960.789, 79209.523, 79645.844, 78258.906, 78437.953, 78677.383, 77128.188, 78149.906, 77865.406, 76178.867, 77714.875, 76820.352, 77253.977, 76240.422, 76604.188, 75689.242, 76093.648, 75913.469, 75311.266, 75475.383, 75076.234, 75139.617, 75039.828, 74419.258, 74774.078, 74429.703, 73949.961, 74108.438, 73585.891, 73547.133, 73613.094, 73325.273, 73187.117, 72733.273, 72359.281, 73620.133, 72445.898, 72927.375, 72889.406, 72989.781, 71888.555, 72495.977, 73074.539, 71907.57, 71871.602, 72031.422, 71812.867, 72349.32, 71606.203, 71707.727, 70917.477, 71223.242, 71126.172], 'val_loss': [507.991, 414.047, 379.261, 377.516, 373.334, 345.371, 343.724, 331.964, 330.143, 324.265, 330.376, 339.591, 334.914, 341.834, 320.876, 323.325, 316.835, 314.15, 307.439, 312.928, 296.406, 307.4, 313.883, 300.704, 287.487, 275.694, 312.852, 274.405, 310.898, 285.219, 287.262, 319.419, 288.497, 281.344, 283.648, 280.249, 290.97, 257.769, 272.533, 268.799, 254.7, 261.288, 280.474, 267.865, 263.14, 262.449, 297.339, 274.959, 266.371, 274.522, 260.382, 266.346, 267.734, 260.447, 251.962, 252.937, 257.987, 253.925, 254.382, 259.805, 251.587, 256.602, 267.679, 265.098, 270.48, 258.688, 258.323, 260.444, 273.157, 251.359, 276.25, 286.177, 245.435, 246.714, 284.986, 258.962, 255.43, 253.293, 271.315, 242.111, 259.63, 250.851, 271.341, 257.288, 251.856, 250.916, 243.999, 267.339, 253.104, 250.466, 249.353, 257.673, 258.395, 253.794, 247.013, 281.629, 265.463, 260.842, 266.733, 272.336]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:52 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:116 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:12 epochs:100	100	1000	True	3729.64062		499172	15	-1	341.1631257534027	{'train_loss': [356260.25, 209438.172, 167792.312, 134306.984, 123313.047, 116830.992, 114663.008, 109518.875, 107680.758, 104389.133, 103227.305, 100474.883, 98117.984, 98007.016, 95684.461, 94947.445, 93320.266, 92275.586, 90654.594, 90212.281, 88719.273, 88067.477, 86979.688, 87305.812, 86353.812, 85584.734, 84185.914, 84270.367, 83174.648, 82803.547, 83215.484, 82210.719, 82792.453, 81285.711, 81080.844, 81078.008, 80380.172, 79053.516, 79498.648, 78945.68, 78686.375, 78457.297, 78031.125, 77613.719, 77244.219, 76864.5, 76457.273, 76350.086, 75926.086, 75846.352, 75630.406, 75387.594, 74939.234, 74760.039, 74215.148, 74269.133, 74027.141, 73776.695, 73782.953, 73089.539, 73448.969, 73308.633, 73016.75, 72483.312, 72669.914, 72092.758, 72146.164, 72029.625, 71853.602, 71658.656, 71728.523, 71635.094, 71099.75, 70775.398, 71397.008, 70549.641, 70557.828, 70286.016, 70377.68, 70669.375, 70439.055, 70135.734, 69967.641, 69334.883, 70398.32, 69727.828, 69954.703, 69321.273, 69668.719, 68759.43, 68897.766, 68118.258, 69304.828, 68520.203, 68882.461, 68845.844, 68618.164, 68405.477, 67904.203, 68544.734], 'val_loss': [1025.931, 1103.695, 689.706, 596.292, 560.556, 536.099, 531.457, 505.793, 519.445, 502.81, 492.338, 485.047, 463.996, 464.875, 454.257, 441.51, 432.396, 427.195, 424.071, 438.157, 411.888, 413.628, 410.573, 393.964, 392.953, 388.114, 384.585, 384.52, 383.227, 370.132, 400.527, 377.844, 357.289, 364.715, 363.813, 373.27, 354.324, 367.935, 356.377, 357.437, 350.647, 357.203, 349.787, 352.717, 361.858, 339.591, 355.717, 352.837, 341.164, 337.548, 350.188, 344.579, 336.269, 339.892, 346.293, 350.549, 341.641, 349.09, 348.181, 345.884, 347.423, 336.709, 346.913, 336.779, 350.225, 346.225, 336.417, 344.61, 331.348, 343.875, 324.151, 338.507, 343.474, 347.746, 338.816, 333.916, 335.51, 342.679, 356.14, 339.919, 335.739, 343.069, 334.876, 324.752, 329.356, 324.417, 332.916, 337.934, 326.049, 328.658, 329.927, 333.5, 331.848, 331.736, 332.104, 329.241, 327.509, 334.788, 327.602, 330.666]}	100	100	True
