id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	3000	True	4229.20166		479265	14	-1	264.9484746456146	{'train_loss': [488703.875, 224944.062, 177168.594, 149710.703, 135797.609, 128258.641, 121879.008, 117097.609, 113739.992, 110572.633, 108130.141, 105754.594, 103744.398, 102329.531, 101464.188, 99301.531, 97463.258, 96718.031, 95297.766, 94488.609, 94098.703, 91978.68, 91375.516, 90765.023, 90540.578, 90533.844, 89383.695, 89140.961, 88375.219, 87538.578, 87175.883, 87144.914, 86114.0, 86396.344, 84301.211, 84690.367, 84271.867, 82992.477, 83599.852, 83568.883, 83219.102, 82795.5, 82638.016, 82280.82, 81068.555, 80730.688, 80414.883, 80050.102, 80002.734, 79731.43, 79173.023, 79402.625, 79153.219, 78762.82, 78074.352, 79060.375, 77929.961, 77442.211, 77416.812, 76909.375, 76167.305, 76685.711, 78598.195, 76089.469, 75682.75, 76811.422, 75664.922, 75288.875, 75294.227, 74998.922, 75517.391, 75317.5, 73484.211, 75671.891, 74271.055, 74004.289, 73693.344, 74015.945, 73520.992, 73406.445, 74037.953, 72511.188, 73884.656, 73174.906, 73295.719, 72427.617, 73130.844, 72550.898, 72332.906, 71570.312, 72133.328, 72612.555, 71072.773, 71138.336, 71219.875, 71388.312, 70885.094, 71552.102, 70254.812, 71268.297], 'val_loss': [4013.807, 2341.717, 2149.139, 2083.202, 1833.22, 1723.77, 1673.155, 1548.855, 1535.503, 1531.908, 1464.735, 1507.159, 1496.586, 1403.873, 1396.996, 1361.985, 1448.026, 1340.003, 1324.978, 1359.221, 1589.027, 1255.381, 1264.102, 1286.393, 1264.112, 1258.506, 1204.619, 1359.684, 1200.956, 1245.453, 1463.884, 1247.116, 1320.774, 1305.123, 1225.806, 1204.721, 1203.679, 1205.092, 1167.947, 1218.112, 1360.495, 1185.588, 1215.565, 1163.346, 1121.928, 1238.748, 1169.036, 1202.118, 1089.426, 1156.581, 1165.273, 1090.12, 1210.366, 1067.542, 1136.648, 1167.941, 1162.09, 1209.975, 1145.941, 1154.198, 1173.503, 1173.377, 1093.715, 1093.547, 1123.252, 1218.661, 1079.87, 1123.57, 1078.636, 1131.23, 1075.543, 1130.045, 1109.385, 1125.177, 1142.785, 1017.173, 1085.965, 1112.55, 1195.798, 1030.757, 1027.02, 1093.132, 1115.464, 1026.513, 1002.34, 1032.668, 1064.06, 1125.669, 994.844, 1083.157, 1074.503, 1008.252, 1004.757, 1080.961, 1032.005, 996.433, 1084.46, 1009.452, 980.004, 1040.685]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:58 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:98 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:conv1d out_channels:47 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	44548.59375		15242591	15	-1	424.0392596721649	{'train_loss': [850757.688, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438, 804482.438], 'val_loss': [11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456, 11044.456]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:32 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	4365.41113		391097	15	-1	209.37448143959045	{'train_loss': [376851.188, 183495.984, 151312.531, 140892.703, 133868.938, 127330.109, 122080.781, 117462.836, 114468.734, 112342.594, 109228.203, 107604.258, 106328.641, 104277.477, 101855.43, 100975.531, 100186.18, 99253.461, 97404.336, 97693.5, 96665.422, 96352.406, 94874.031, 94668.867, 94125.312, 93920.094, 92991.953, 92579.766, 92032.969, 91506.867, 90633.984, 90038.641, 90452.047, 90617.961, 89167.828, 88660.688, 87912.039, 87714.203, 87895.25, 87730.094, 87032.5, 86278.992, 85817.992, 85915.75, 85124.117, 84961.164, 83775.711, 85050.984, 84114.648, 83570.148, 84158.016, 83318.453, 83863.883, 83179.07, 82127.719, 82115.25, 82875.227, 82032.891, 81981.289, 81470.547, 81091.664, 81814.547, 80709.391, 81002.195, 80948.242, 80359.852, 79427.578, 80254.977, 80150.672, 78890.906, 79866.203, 79783.992, 78999.555, 78848.875, 78991.82, 79017.562, 78196.156, 78782.359, 78678.633, 78602.703, 78259.875, 78128.094, 77214.805, 77482.422, 76600.812, 77956.477, 77251.703, 76657.828, 76769.523, 77325.641, 76939.711, 76207.695, 76860.109, 76343.688, 76048.82, 75793.734, 75577.0, 74956.438, 75685.117, 76378.609], 'val_loss': [5185.215, 1929.856, 1765.984, 1744.834, 1789.615, 1709.601, 1571.338, 1522.253, 1436.45, 1523.442, 1554.845, 1432.717, 1441.578, 1399.244, 1374.39, 1397.031, 1290.247, 1319.297, 1347.403, 1258.293, 1318.183, 1256.314, 1325.082, 1206.605, 1206.121, 1260.235, 1222.727, 1223.75, 1246.886, 1289.57, 1198.65, 1261.145, 1214.578, 1187.317, 1215.612, 1181.721, 1190.131, 1248.338, 1194.582, 1250.863, 1162.588, 1150.713, 1182.379, 1173.577, 1134.984, 1164.917, 1217.664, 1098.164, 1123.831, 1147.79, 1069.536, 1209.662, 1188.005, 1099.982, 1167.156, 1146.326, 1104.933, 1099.541, 1077.496, 1110.813, 1108.538, 1154.344, 1108.625, 1185.422, 1097.322, 1180.486, 1107.238, 1182.856, 1128.432, 1113.193, 1139.989, 1196.86, 1094.697, 1132.33, 1133.464, 1099.936, 1171.68, 1121.002, 1138.255, 1057.744, 1118.553, 1103.727, 1094.708, 1126.687, 1120.609, 1111.435, 1148.076, 1056.229, 1062.517, 1082.167, 1066.665, 1106.519, 1068.633, 1065.762, 1023.609, 1079.526, 1060.574, 1133.003, 1085.538, 1112.81]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:107 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	3871.68359		491663	14	-1	269.6929876804352	{'train_loss': [536777.062, 310867.844, 224851.141, 197864.875, 183615.844, 165162.703, 134514.672, 118531.867, 113233.82, 108628.93, 105724.688, 101722.234, 99944.938, 97417.164, 96303.812, 95542.508, 94605.414, 93441.211, 91418.773, 92539.766, 91195.781, 90329.406, 89294.891, 89018.742, 88443.477, 87613.312, 86035.258, 85167.836, 85481.664, 85186.18, 85264.891, 84231.18, 82686.266, 82462.758, 83258.172, 81729.188, 81224.906, 80799.836, 80409.578, 79406.328, 80912.336, 79190.695, 79345.617, 79091.461, 78341.664, 78613.438, 77160.766, 78150.711, 78314.664, 76670.898, 77314.797, 77157.875, 76102.539, 75693.914, 76009.016, 74468.0, 75141.758, 75630.906, 74839.086, 74634.945, 74090.422, 74138.656, 74501.305, 73851.836, 73271.641, 73369.281, 73401.25, 73027.859, 73385.969, 72985.273, 73369.867, 72498.961, 71777.039, 71968.953, 72632.438, 71069.789, 71794.328, 71613.445, 70098.195, 72393.969, 72032.32, 71741.984, 70916.305, 70306.805, 71024.117, 71482.273, 69903.969, 70543.898, 69870.703, 70655.773, 69370.891, 70526.484, 69311.031, 68793.547, 70223.898, 69827.555, 68825.312, 69187.07, 69004.797, 69953.281], 'val_loss': [5119.25, 3085.652, 2899.946, 2533.311, 2846.021, 2562.421, 2041.956, 1903.069, 1676.147, 1757.548, 1495.679, 1497.21, 1566.86, 1592.587, 1551.599, 1634.656, 1510.025, 1342.162, 1376.389, 1397.125, 1384.76, 1328.79, 1346.681, 1237.417, 1184.957, 1241.788, 1175.488, 1145.162, 1172.633, 1176.682, 1169.589, 1156.177, 1317.493, 1070.252, 1131.275, 1186.901, 1124.672, 1140.328, 1114.527, 1260.201, 1087.435, 1113.794, 1135.651, 1054.944, 1245.787, 1101.659, 1054.859, 1096.095, 1094.041, 1051.835, 1040.822, 1068.77, 1029.834, 1056.949, 1084.467, 1007.207, 1081.511, 1031.698, 1075.516, 1079.458, 1013.429, 987.475, 1009.505, 1018.544, 999.412, 997.78, 1045.0, 1030.729, 950.44, 992.493, 1052.784, 1016.866, 979.477, 977.599, 998.312, 993.033, 970.93, 990.313, 993.926, 948.525, 936.814, 957.148, 993.093, 958.535, 939.207, 1038.021, 988.054, 1002.6, 1016.269, 931.797, 946.082, 931.175, 993.128, 971.837, 926.073, 986.721, 967.651, 950.222, 1006.622, 961.064]}	100	100	True
