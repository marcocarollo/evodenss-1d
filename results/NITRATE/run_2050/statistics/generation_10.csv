id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	3000	True	3987.87207		479265	14	-1	265.03835582733154	{'train_loss': [556976.812, 373084.375, 198830.469, 150913.828, 137889.641, 128394.047, 121457.688, 116138.219, 112486.281, 110154.797, 106815.211, 104800.594, 103462.016, 101137.219, 99893.461, 98203.133, 97711.539, 96583.375, 94514.305, 94318.133, 94536.289, 93216.75, 92650.836, 90868.836, 90429.898, 89692.016, 90384.758, 88987.148, 87635.125, 88279.57, 86814.734, 87302.133, 85485.344, 85539.039, 86564.992, 84077.367, 84625.133, 84288.469, 83022.031, 83615.906, 82912.203, 81912.109, 81054.242, 82311.734, 81987.258, 81153.281, 81478.625, 81536.516, 80311.594, 79527.156, 79717.602, 78753.656, 79438.25, 79987.539, 79303.266, 78530.008, 78907.367, 78055.0, 78531.938, 77917.352, 77442.852, 76933.977, 76854.109, 77194.023, 75650.977, 77208.984, 76960.258, 76380.328, 75568.188, 75002.281, 75929.578, 74952.266, 75813.992, 75348.414, 74823.32, 75334.727, 74922.586, 75439.0, 73990.68, 73297.508, 73996.43, 74528.594, 74718.812, 73302.164, 72600.617, 73583.961, 72728.148, 73310.742, 72852.352, 72780.758, 72844.867, 73232.922, 73034.086, 74143.062, 71397.359, 72784.602, 71712.117, 71429.461, 71818.102, 71733.75], 'val_loss': [6477.371, 3688.229, 2706.388, 1924.311, 1892.136, 1677.622, 1751.599, 1635.765, 1585.025, 1577.293, 1730.683, 1545.46, 1528.726, 1513.353, 1430.324, 1447.524, 1471.677, 1503.016, 1508.729, 1442.432, 1458.327, 1414.967, 1409.989, 1448.984, 1393.508, 1526.944, 1380.912, 1336.104, 1387.395, 1344.203, 1349.491, 1370.368, 1252.836, 1283.597, 1278.243, 1319.531, 1276.573, 1192.03, 1296.535, 1317.372, 1302.008, 1208.758, 1308.911, 1239.522, 1289.647, 1236.993, 1199.834, 1189.506, 1246.569, 1189.563, 1266.581, 1173.383, 1240.448, 1102.22, 1325.183, 1186.303, 1099.013, 1214.562, 1165.765, 1160.926, 1186.434, 1099.412, 1099.424, 1173.997, 1169.554, 1125.602, 1116.152, 1109.828, 1076.541, 1118.167, 1127.074, 1077.719, 1107.482, 1161.045, 1133.15, 1059.482, 1112.525, 1073.911, 1096.333, 1049.695, 1087.016, 1066.511, 1068.819, 1029.586, 1045.68, 1068.47, 1018.402, 1068.756, 1060.887, 1092.475, 1022.605, 1076.607, 1086.183, 1064.121, 1009.947, 1123.874, 1102.363, 1091.817, 1035.469, 1068.674]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	3000	True	3888.75415		479265	14	-1	261.0308475494385	{'train_loss': [522761.594, 242462.625, 172209.5, 154468.984, 139012.188, 131634.016, 124974.008, 121295.523, 117056.438, 115380.242, 112224.867, 111383.297, 108385.508, 106194.477, 105880.555, 103103.164, 102038.703, 101287.555, 99853.367, 97881.422, 97197.227, 95956.43, 95290.977, 92687.594, 91995.727, 92613.188, 90335.891, 90767.859, 89603.164, 88678.008, 87809.148, 88019.992, 86854.055, 87471.203, 86143.359, 85552.578, 85006.352, 85025.188, 83788.219, 84042.258, 82277.297, 83559.156, 82938.414, 82475.461, 82187.148, 80444.172, 80481.414, 80916.461, 80570.688, 79295.203, 78437.195, 80244.32, 77923.789, 78513.461, 77307.375, 78041.633, 76850.773, 77158.93, 78067.422, 77988.602, 76499.789, 75903.117, 76154.867, 76897.062, 76002.977, 74715.438, 75495.289, 73986.508, 74887.867, 74959.18, 74684.953, 73742.352, 74288.57, 73507.852, 74362.523, 72225.383, 71912.336, 72471.578, 72530.852, 72222.016, 72148.516, 72716.828, 72280.484, 71687.945, 70031.914, 71587.672, 71367.742, 71608.758, 70687.969, 70508.148, 71161.875, 70759.5, 70306.641, 70507.883, 70849.992, 70034.0, 70208.023, 69806.336, 69641.859, 70067.43], 'val_loss': [4589.632, 2067.991, 1797.303, 1592.688, 1495.445, 1504.401, 1582.933, 1434.338, 1437.504, 1389.688, 1380.637, 1317.397, 1309.962, 1338.279, 1492.436, 1330.804, 1280.875, 1374.81, 1279.97, 1296.509, 1239.532, 1224.596, 1253.931, 1275.702, 1195.432, 1296.622, 1218.409, 1264.167, 1164.391, 1193.874, 1207.243, 1174.546, 1159.327, 1159.064, 1136.647, 1276.963, 1174.102, 1143.291, 1162.147, 1179.081, 1111.11, 1167.152, 1129.224, 1075.094, 1126.187, 1132.148, 1198.327, 1209.132, 1136.36, 1106.244, 1092.335, 1085.905, 1110.071, 1090.284, 1110.26, 1092.308, 1112.852, 1071.733, 1067.242, 1070.06, 1091.116, 1061.639, 1042.033, 1119.0, 1115.913, 1034.467, 1100.121, 1072.381, 1074.166, 1092.083, 1036.835, 1052.412, 1072.313, 1071.228, 1071.715, 1047.366, 1060.264, 1064.701, 1057.77, 1031.513, 1127.786, 1063.634, 1092.311, 1009.845, 1033.333, 1045.52, 1168.34, 996.621, 1036.3, 1039.84, 1038.504, 1013.596, 1040.672, 1089.147, 1021.608, 980.63, 1060.77, 993.311, 1043.536, 987.342]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:67 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:86 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:110 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.11855004590233206 alpha:0.8605675500729606 weight_decay:3.308733034334262e-05 batch_size:32 epochs:100	100	1000	True	29398.86719		443760	15	-1	201.82670187950134	{'train_loss': [820824.688, 969578.5, 905906.062, 812279.688, 822251.5, 808390.75, 863253.312, 838412.938, 794132.812, 815296.688, 798021.438, 867077.438, 841432.5, 817792.25, 821494.25, 838445.625, 817824.0, 871890.5, 808453.625, 856695.312, 823549.125, 840553.25, 856267.25, 874202.625, 837368.312, 807184.688, 841532.375, 846258.375, 838170.188, 871289.312, 831004.562, 828987.0, 782341.188, 862130.25, 753137.5, 856455.562, 841037.438, 759739.312, 703584.938, 832705.875, 883874.062, 765602.375, 930608.5, 813809.062, 857681.5, 815372.688, 813790.938, 781332.688, 713457.688, 777906.625, 830056.875, 818225.625, 841923.625, 764640.562, 826152.438, 808957.438, 856794.562, 776938.812, 836769.312, 824650.375, 818086.5, 847285.0, 804322.812, 860766.75, 888046.75, 799766.5, 838387.125, 731839.25, 806477.625, 841278.688, 805228.688, 868566.375, 807359.938, 832386.688, 801228.625, 836120.188, 800233.625, 822265.625, 805645.5, 830249.812, 813621.25, 823870.562, 767440.188, 846299.188, 800085.75, 800815.625, 832611.938, 803880.625, 855490.812, 869400.312, 800976.0, 822344.812, 814903.438, 866808.062, 782341.812, 770412.25, 767089.5, 843563.5, 839548.562, 823375.25], 'val_loss': [10991.666, 7736.61, 8486.078, 8186.947, 7698.91, 9208.58, 8499.235, 9428.389, 8577.079, 9427.342, 9356.615, 10844.392, 6239.201, 9456.734, 8330.967, 8975.76, 8896.523, 8160.777, 10690.356, 10207.479, 10217.833, 10650.693, 9829.189, 10057.968, 8727.977, 8932.209, 9891.66, 8635.368, 10646.988, 10818.236, 7504.327, 8277.561, 7363.264, 9051.748, 5709.131, 6453.267, 8434.17, 11043.82, 33933.023, 5389.022, 8024.279, 6851.807, 8415.771, 10780.781, 8092.144, 9474.545, 6254.991, 10106.405, 6013.201, 6052.532, 9364.795, 8901.404, 8004.182, 9810.583, 9605.924, 8434.209, 7664.856, 10273.969, 9784.525, 8759.208, 9699.959, 8315.869, 8298.436, 10903.248, 9858.837, 13386.472, 5835.21, 6184.197, 5386.668, 6676.725, 10847.414, 8923.552, 10964.343, 9309.191, 7202.578, 9370.709, 8428.057, 7477.679, 11575.83, 8399.075, 7543.422, 6017.965, 9341.266, 9660.31, 10071.569, 7976.546, 8583.433, 7958.503, 9209.245, 9884.533, 7443.326, 9629.624, 11283.334, 7362.391, 7819.41, 7624.65, 9112.54, 10090.482, 9597.771, 7060.337]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:86 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:126 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:17 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	3000	True	4255.05371		1595415	14	-1	373.0728361606598	{'train_loss': [842702.938, 849131.75, 841511.75, 735247.688, 317621.625, 239540.922, 193322.141, 165289.531, 146429.422, 135700.047, 128290.008, 122662.766, 119447.938, 115557.117, 114316.938, 111396.047, 111436.328, 106328.453, 106525.125, 105161.797, 104339.617, 102816.0, 99944.516, 101694.906, 97802.93, 100333.109, 98715.492, 97374.195, 96181.953, 96851.906, 96037.25, 97621.477, 94916.203, 96940.758, 94101.547, 93141.656, 92434.18, 93266.25, 91423.016, 91313.234, 90851.734, 91316.688, 90932.602, 89327.906, 91963.164, 90524.633, 90062.0, 87942.289, 88775.133, 87525.336, 88099.43, 87341.273, 87005.203, 88837.609, 86592.391, 86221.602, 89172.828, 84570.836, 86840.359, 84094.703, 86065.102, 85441.625, 84185.945, 84910.898, 84882.977, 83971.758, 83171.633, 82539.297, 85672.219, 83805.109, 88414.508, 81694.188, 81532.234, 83243.047, 82089.188, 81702.75, 81157.547, 80414.281, 86600.469, 79675.836, 80909.812, 80278.82, 79055.625, 80512.703, 79456.734, 79282.305, 78635.742, 79578.164, 79803.406, 77218.875, 79484.812, 79890.25, 77463.039, 79391.438, 78508.07, 76041.406, 77488.078, 79704.688, 78315.586, 78372.75], 'val_loss': [11044.416, 11019.406, 10888.361, 5085.981, 3822.22, 2506.27, 2200.074, 1935.12, 1756.339, 1718.931, 1679.564, 1474.467, 1544.207, 1565.489, 1493.968, 1545.08, 1447.612, 1472.308, 1459.31, 1540.982, 1479.125, 1457.646, 1584.292, 1492.443, 1575.653, 1374.567, 1346.827, 1408.913, 1511.232, 1445.073, 1490.403, 1517.297, 1449.766, 1346.026, 1463.902, 1399.53, 1382.905, 1321.594, 1332.254, 1334.67, 1472.008, 1352.404, 1427.921, 1329.996, 1545.876, 1192.637, 1236.876, 1294.333, 1334.104, 1374.713, 1365.359, 1159.472, 1226.35, 1359.916, 1455.739, 1094.819, 1061.754, 1108.392, 1582.486, 1306.787, 1450.701, 1274.727, 1360.736, 1204.767, 1082.91, 1034.874, 1242.202, 1169.263, 1077.398, 1295.4, 1080.36, 1119.909, 1267.349, 1371.778, 1223.084, 1095.94, 1069.271, 1146.452, 1031.118, 1166.781, 1036.256, 1119.385, 1060.083, 1031.252, 1089.237, 1138.469, 1032.653, 1117.285, 1137.701, 1090.037, 1312.103, 1246.748, 1083.274, 997.615, 1029.209, 955.441, 1116.539, 1013.503, 1012.237, 1049.951]}	0	100	True
