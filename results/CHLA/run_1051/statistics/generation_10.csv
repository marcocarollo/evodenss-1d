id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	2000	True	24620.35938		619010	12	-1	208.92149925231934	{'train_loss': [613045.125, 423918.625, 310849.938, 271935.875, 262225.406, 256595.922, 253709.312, 250058.891, 247656.906, 245871.453, 243079.328, 241712.312, 239674.031, 237718.141, 237062.016, 235279.688, 233694.547, 232728.734, 230861.703, 229682.297, 228262.359, 227482.609, 224892.688, 224143.172, 221947.281, 221845.469, 220602.453, 220613.344, 219206.875, 218818.062, 218001.562, 217557.5, 215985.75, 216645.531, 215673.391, 214165.641, 215600.344, 214835.844, 213316.891, 212372.75, 212935.688, 212212.875, 212037.453, 211875.609, 210871.266, 211255.75, 209875.594, 209377.984, 209549.516, 208652.016, 209338.938, 207760.0, 208160.031, 207722.859, 206876.844, 207543.047, 206699.766, 206285.875, 205822.844, 205556.359, 205687.219, 205483.5, 204997.953, 205405.031, 204846.875, 204406.328, 204396.625, 204320.844, 203328.922, 203891.172, 203498.516, 202683.719, 202359.062, 203189.734, 202404.953, 202897.891, 201758.641, 202045.844, 201923.281, 202487.609, 201445.375, 201080.375, 202334.0, 201487.219, 200918.141, 199541.109, 200046.547, 201066.141, 199876.031, 199653.328, 200518.562, 199158.438, 199311.844, 198677.938, 199985.047, 198440.547, 198949.094, 198284.484, 198950.766, 198663.625], 'val_loss': [4033.739, 3205.803, 2806.439, 2762.94, 2657.509, 2595.763, 2631.328, 2637.464, 2532.317, 2509.779, 2495.064, 2495.083, 2478.196, 2473.934, 2458.355, 2427.989, 2431.298, 2419.673, 2415.688, 2395.964, 2410.462, 2399.159, 2364.721, 2364.583, 2379.317, 2357.276, 2364.06, 2364.907, 2340.786, 2331.189, 2332.757, 2323.601, 2314.679, 2292.694, 2293.992, 2326.983, 2304.076, 2330.422, 2272.955, 2286.148, 2298.544, 2285.81, 2293.978, 2295.384, 2266.361, 2274.413, 2274.757, 2291.897, 2257.476, 2233.804, 2243.129, 2250.829, 2261.983, 2236.371, 2220.976, 2225.136, 2209.086, 2226.467, 2225.814, 2241.371, 2222.214, 2207.989, 2215.174, 2198.555, 2211.456, 2192.137, 2196.784, 2209.542, 2198.462, 2176.722, 2197.323, 2212.818, 2185.169, 2192.271, 2191.977, 2186.395, 2180.774, 2188.425, 2186.753, 2184.074, 2175.77, 2165.882, 2191.946, 2178.056, 2173.413, 2196.25, 2190.129, 2171.237, 2166.137, 2170.974, 2151.084, 2164.623, 2151.155, 2155.852, 2166.549, 2151.607, 2143.514, 2161.386, 2171.47, 2147.308]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.244673639468736 beta1:0.9458874151210942 beta2:0.9140374963263671 weight_decay:0.0008895747654529963 batch_size:32 epochs:100	100	1000	True	131122.20312		794882	11	-1	214.22696113586426	{'train_loss': [1083760.125, 1103085.625, 1090329.375, 1171678.5, 1127243.75, 1188942.25, 1107480.625, 1126472.875, 1110766.625, 1211734.5, 1111794.25, 1131135.125, 1121224.5, 1149268.125, 1131513.5, 1116532.875, 1115928.625, 1138831.0, 1123762.0, 1114677.75, 1110094.0, 1142290.5, 1133309.375, 1098706.125, 1107333.5, 1106359.125, 1088648.75, 1106682.5, 1093508.75, 1100875.0, 1107781.75, 1086396.875, 1090899.25, 1096996.625, 1081937.375, 1098981.25, 1081474.125, 1081499.625, 1098740.625, 1086342.0, 1082633.25, 1088268.125, 1076259.125, 1083513.375, 1086215.375, 1085000.875, 1084912.0, 1098389.75, 1079307.625, 1079355.125, 1081115.75, 1079390.625, 1087013.75, 1078112.5, 1081451.5, 1077966.25, 1091372.5, 1080616.625, 1100123.5, 1076783.125, 1083349.625, 1091179.875, 1080153.5, 1081206.25, 1082419.625, 1080585.0, 1078777.625, 1079514.0, 1073272.75, 1075974.875, 1082930.75, 1081933.5, 1084853.0, 1081658.25, 1093363.375, 1077652.625, 1076926.5, 1092194.625, 1076592.625, 1081584.125, 1112654.25, 1082101.125, 1075040.75, 1080326.625, 1082418.625, 1070624.0, 1085497.625, 1071719.25, 1089389.5, 1084898.875, 1085468.0, 1083106.25, 1081342.125, 1077656.125, 1082125.0, 1076027.625, 1089310.125, 1082939.625, 1079252.75, 1083353.75], 'val_loss': [11110.434, 11110.434, 11110.434, 34505.648, 11110.434, 39867.387, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.344, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:107 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:87 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	2000	True	25751.90234		612942	14	-1	230.82651281356812	{'train_loss': [522374.5, 356976.906, 292266.406, 270774.188, 262164.062, 257893.969, 254704.781, 251828.391, 250603.516, 248625.031, 247380.094, 245834.391, 244303.234, 242945.484, 242139.25, 240568.234, 239057.031, 238062.922, 237809.984, 236473.953, 236009.016, 234449.641, 234221.562, 233302.609, 232825.547, 231075.25, 231249.75, 230787.234, 229336.312, 229153.797, 229387.156, 228061.0, 226924.688, 227048.547, 227056.391, 225108.922, 225610.953, 224213.484, 223361.297, 223543.438, 221861.172, 222887.516, 221656.078, 221241.312, 220819.938, 220705.547, 219750.406, 220058.141, 219522.594, 218514.562, 218579.734, 217672.656, 217332.078, 217889.922, 217377.719, 216502.922, 216876.359, 216405.906, 215433.828, 216078.547, 214498.438, 214541.281, 214610.688, 213690.125, 213379.0, 213345.891, 212377.656, 212748.078, 212643.875, 212267.531, 211435.688, 211631.578, 211963.406, 211183.516, 210699.703, 210409.0, 210594.75, 210068.969, 210034.172, 210722.969, 209275.594, 209200.062, 209166.469, 208275.625, 208080.047, 208886.703, 208026.312, 207878.844, 207458.281, 207346.953, 207326.0, 207647.672, 207612.844, 205645.594, 207114.234, 207196.391, 206468.422, 205290.125, 205098.453, 206857.344], 'val_loss': [4090.057, 3050.872, 2920.502, 2878.599, 2779.713, 2810.373, 2666.873, 2700.734, 2654.332, 2556.428, 2575.002, 2536.833, 2552.782, 2533.275, 2513.547, 2494.924, 2498.849, 2478.557, 2472.122, 2469.522, 2463.015, 2464.614, 2421.35, 2459.081, 2417.339, 2420.445, 2479.446, 2416.797, 2410.661, 2385.0, 2403.665, 2405.878, 2367.164, 2390.637, 2377.891, 2353.734, 2350.125, 2375.105, 2426.299, 2382.417, 2451.293, 2309.565, 2323.026, 2345.306, 2339.159, 2342.908, 2382.832, 2365.372, 2378.24, 2309.233, 2323.778, 2363.259, 2345.468, 2324.661, 2312.378, 2343.558, 2355.888, 2316.91, 2303.743, 2288.062, 2327.559, 2260.695, 2273.142, 2324.69, 2327.025, 2331.968, 2278.678, 2329.35, 2381.748, 2349.716, 2294.019, 2316.093, 2247.801, 2243.371, 2276.082, 2343.593, 2296.439, 2247.266, 2335.325, 2330.603, 2266.092, 2311.582, 2241.706, 2283.752, 2320.572, 2257.194, 2269.106, 2219.102, 2308.151, 2271.013, 2269.952, 2273.029, 2251.444, 2309.116, 2273.516, 2261.857, 2219.085, 2229.128, 2277.684, 2240.748]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	3000	True	24876.03906		619010	12	-1	208.5432951450348	{'train_loss': [636241.875, 405001.812, 306666.219, 270735.75, 258067.625, 250956.094, 247048.656, 243263.797, 240947.859, 239006.828, 237334.5, 234918.953, 234410.062, 232977.734, 231692.766, 229778.312, 229405.75, 228032.031, 226037.797, 225971.172, 224697.172, 223969.672, 222584.203, 222047.625, 220902.922, 220750.828, 219353.266, 218245.062, 218281.047, 217221.547, 216613.984, 216527.141, 215937.438, 214484.594, 213699.266, 213537.734, 213316.859, 211929.234, 212526.219, 211734.344, 211916.297, 210686.406, 211241.219, 210597.281, 210172.312, 209413.484, 209443.016, 208978.672, 209032.812, 208039.016, 207727.484, 207141.969, 206883.469, 207163.812, 206122.75, 206801.75, 205623.656, 205753.453, 204592.984, 205340.391, 204405.312, 203812.484, 203835.109, 204035.953, 203946.359, 203839.625, 202771.906, 202254.344, 202284.984, 202642.531, 201173.984, 200554.219, 201163.359, 201437.062, 201532.234, 200565.828, 199295.266, 199890.297, 198571.516, 200472.172, 199855.078, 198813.438, 198486.938, 198127.641, 198848.906, 198797.516, 198342.984, 197546.438, 198194.078, 196570.672, 196745.922, 196341.875, 195802.875, 196400.938, 195771.516, 196953.938, 195606.25, 196381.656, 195606.656, 194726.891], 'val_loss': [4370.647, 3374.07, 2833.447, 2813.476, 2682.862, 2589.664, 2566.591, 2600.5, 2546.072, 2541.939, 2497.559, 2474.26, 2483.701, 2461.237, 2469.587, 2515.504, 2440.49, 2489.743, 2456.848, 2431.44, 2386.666, 2362.983, 2394.466, 2382.789, 2355.654, 2361.148, 2365.178, 2354.252, 2345.098, 2343.439, 2371.432, 2360.339, 2322.174, 2347.194, 2344.613, 2328.869, 2308.321, 2331.152, 2305.755, 2298.643, 2329.139, 2273.618, 2307.133, 2301.43, 2320.414, 2292.841, 2291.406, 2308.682, 2274.087, 2291.838, 2313.881, 2298.627, 2275.746, 2285.334, 2286.193, 2265.11, 2309.668, 2299.232, 2276.625, 2269.548, 2272.117, 2258.876, 2239.307, 2264.117, 2263.516, 2250.619, 2259.577, 2224.795, 2241.468, 2247.547, 2276.691, 2223.68, 2283.541, 2228.462, 2250.689, 2209.382, 2246.729, 2239.437, 2213.944, 2195.93, 2190.92, 2180.682, 2198.0, 2225.602, 2207.742, 2192.586, 2213.263, 2204.687, 2198.19, 2206.157, 2215.299, 2184.421, 2195.868, 2160.584, 2170.978, 2205.229, 2156.636, 2188.173, 2190.526, 2181.107]}	0	100	True
