id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:26 epochs:100	100	1000	True	24407.41992		468105	9	-1	215.27670454978943	{'train_loss': [566357.562, 332697.062, 293011.969, 278190.062, 267335.312, 261386.438, 258081.812, 254213.062, 252166.406, 248823.766, 246870.594, 244573.188, 242483.703, 240582.5, 238431.266, 237001.5, 235185.797, 234720.469, 233150.047, 232306.25, 230282.312, 229521.25, 228210.203, 227254.328, 226021.109, 225270.469, 224893.641, 224126.219, 223189.031, 223503.656, 222440.547, 221590.953, 220591.391, 220087.359, 220684.531, 218730.625, 218738.812, 217336.484, 217774.484, 217101.734, 216539.859, 216473.828, 215441.641, 214995.219, 214914.375, 213979.234, 213598.906, 213413.953, 212313.047, 212956.859, 212192.188, 211712.609, 210496.859, 211123.734, 210846.266, 209012.703, 209796.812, 209078.922, 209110.156, 209041.312, 208473.906, 207911.891, 207487.219, 207466.484, 206969.969, 206747.141, 206146.562, 205984.766, 205002.172, 205561.266, 205413.75, 205042.078, 204110.25, 204689.156, 203814.656, 204365.656, 204660.641, 204506.844, 203103.0, 203649.297, 203102.641, 202686.984, 202417.172, 203404.531, 202338.25, 201431.484, 202007.688, 201977.891, 202034.062, 201436.562, 201776.484, 201042.047, 200380.047, 199986.609, 200603.688, 199640.922, 200296.359, 200189.625, 200022.094, 200148.719], 'val_loss': [2857.406, 2399.861, 2270.948, 2179.947, 2128.999, 2089.365, 2057.023, 2051.832, 2018.285, 2010.589, 1993.912, 1985.801, 2002.477, 1966.811, 1960.344, 1951.663, 1935.986, 1928.358, 1915.249, 1912.713, 1913.876, 1911.729, 1897.383, 1893.972, 1890.918, 1872.786, 1870.42, 1870.443, 1857.596, 1867.033, 1865.548, 1869.498, 1861.024, 1869.065, 1861.922, 1852.196, 1833.301, 1839.352, 1834.886, 1839.446, 1827.83, 1818.938, 1828.593, 1826.748, 1824.037, 1827.98, 1820.663, 1801.961, 1810.371, 1800.647, 1806.31, 1838.604, 1808.962, 1798.872, 1805.867, 1782.993, 1788.807, 1792.422, 1791.704, 1790.729, 1786.692, 1765.156, 1778.832, 1781.848, 1777.393, 1790.116, 1762.566, 1768.826, 1774.635, 1766.222, 1764.715, 1765.682, 1759.194, 1766.761, 1752.554, 1761.229, 1760.332, 1751.564, 1755.781, 1768.94, 1751.045, 1738.557, 1735.97, 1754.899, 1738.303, 1739.485, 1750.6, 1738.425, 1741.206, 1738.167, 1736.977, 1730.541, 1723.024, 1730.21, 1718.679, 1734.043, 1729.371, 1736.047, 1724.921, 1712.547]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:26 epochs:100	100	2000	True	24484.92578		468105	9	-1	210.07577395439148	{'train_loss': [604980.812, 333457.156, 293678.562, 277017.125, 268181.531, 263216.188, 258949.141, 255909.031, 252771.703, 250511.953, 247779.828, 245726.078, 243830.031, 241764.969, 240247.359, 238703.094, 237539.906, 236181.391, 234828.703, 233822.922, 232514.234, 230619.688, 230356.109, 229610.594, 228150.453, 228216.078, 227279.5, 226346.984, 225448.75, 224087.125, 224041.391, 224026.047, 223124.141, 222525.172, 222148.453, 221023.0, 221332.625, 219769.422, 219257.297, 217988.828, 219090.984, 218046.859, 216992.297, 217584.625, 217120.969, 216468.047, 216793.141, 216140.891, 215519.172, 214379.109, 215337.484, 214299.438, 214081.031, 214283.891, 213545.719, 213898.766, 212595.688, 213579.594, 212668.391, 213059.453, 212408.156, 211555.062, 211787.562, 211363.844, 210569.109, 211413.734, 210877.844, 210699.266, 209398.312, 209728.625, 210149.016, 208876.281, 208701.797, 209046.766, 208184.234, 208496.438, 208113.188, 208405.562, 207153.484, 207679.875, 207641.344, 207949.766, 207644.969, 206768.938, 205684.672, 205932.5, 206927.781, 206136.297, 205316.156, 205166.328, 205730.188, 204924.109, 206049.422, 205997.438, 204284.156, 205210.188, 204812.594, 204452.406, 203859.266, 204518.188], 'val_loss': [2832.6, 2425.256, 2242.494, 2177.96, 2154.131, 2108.522, 2075.682, 2049.608, 2034.598, 2030.208, 2016.449, 2005.463, 1993.245, 1978.782, 1972.859, 1944.703, 1954.725, 1934.541, 1922.72, 1929.56, 1912.288, 1919.754, 1902.772, 1897.23, 1902.49, 1886.151, 1872.699, 1876.641, 1866.554, 1884.663, 1869.14, 1858.785, 1854.811, 1861.844, 1850.595, 1861.45, 1856.526, 1843.59, 1846.701, 1832.47, 1829.792, 1830.724, 1834.695, 1825.134, 1824.512, 1814.844, 1817.497, 1820.073, 1805.787, 1825.616, 1813.304, 1817.363, 1801.206, 1794.847, 1801.0, 1804.875, 1790.87, 1799.204, 1798.85, 1787.944, 1791.758, 1793.03, 1794.797, 1781.536, 1785.679, 1790.893, 1778.984, 1805.782, 1777.054, 1782.089, 1764.973, 1779.93, 1764.898, 1769.112, 1752.255, 1765.3, 1757.918, 1765.567, 1765.704, 1772.31, 1774.314, 1749.082, 1765.375, 1763.179, 1750.901, 1765.323, 1757.823, 1759.309, 1765.362, 1747.218, 1750.613, 1746.965, 1766.942, 1736.843, 1740.304, 1739.721, 1736.722, 1755.901, 1729.643, 1737.765]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:rmsprop lr:0.08146319621832485 alpha:0.9029536701678864 weight_decay:3.217453007887132e-05 batch_size:26 epochs:100	100	1000	True	131121.98438		657232	9	-1	211.33632850646973	{'train_loss': [1152941.875, 1351803.0, 1062567.25, 1139861.375, 1134294.625, 1145830.75, 1117175.375, 1146378.5, 1081913.125, 1103723.75, 1076283.875, 1088156.125, 1092272.5, 1097850.375, 1080860.0, 1096876.25, 1082376.875, 1094901.625, 1088636.0, 1096663.25, 1096527.25, 1089708.375, 1096745.75, 1084745.75, 1094536.375, 1093819.625, 1078222.875, 1095584.875, 1082356.875, 1098634.625, 1100920.125, 1091417.375, 1088510.875, 1085776.625, 1086136.375, 1084077.0, 1095969.125, 1089707.875, 1086360.75, 1094573.125, 1096278.625, 1085417.625, 1092806.875, 1096135.75, 1094031.75, 1072925.5, 1088518.625, 1088862.25, 1095852.375, 1085665.0, 1092466.375, 1091519.875, 1092600.25, 1096251.25, 1089659.25, 1095005.75, 1095554.125, 1099540.375, 1080195.375, 1087506.0, 1096206.875, 1089955.625, 1085197.75, 1091471.375, 1086165.875, 1080654.25, 1094582.875, 1090589.75, 1099615.5, 1088122.125, 1091464.125, 1092545.125, 1088295.375, 1086991.875, 1100537.625, 1089541.625, 1086216.875, 1090634.75, 1088693.625, 1086492.375, 1093161.5, 1086768.75, 1092666.25, 1089659.875, 1086212.25, 1089796.875, 1094204.875, 1098055.125, 1081860.75, 1101637.375, 1085346.875, 1097721.125, 1084760.25, 1084782.5, 1094923.75, 1091148.625, 1096727.5, 1089641.0, 1086267.75, 1083713.625], 'val_loss': [8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 11674.08, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 9392.799, 8888.349, 9066.557, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8921.327, 8888.349, 13302.966, 8888.349, 8888.349, 8888.349, 8888.349, 10574.05, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 9072.651, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.328, 8888.349, 8888.349, 8888.349, 8883.114, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 9102.473, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 10079.206, 8888.349, 8888.349, 8888.349, 8864.941, 8888.349]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:60 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:104 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:27 epochs:100	100	1000	True	25575.15039		784394	9	-1	209.75680899620056	{'train_loss': [983945.188, 533984.375, 351612.031, 302079.188, 284339.906, 275029.469, 267673.094, 261375.438, 257138.406, 252616.266, 249687.453, 247245.828, 244679.266, 241986.75, 239460.25, 238469.219, 236149.016, 235025.875, 233885.344, 232966.781, 230955.734, 229895.938, 229329.906, 228278.828, 227078.203, 226466.141, 226137.984, 224678.156, 224700.391, 223919.391, 223003.797, 221621.125, 221688.219, 220720.984, 220075.625, 219890.328, 219277.859, 218629.422, 218612.078, 217720.016, 217138.828, 216262.203, 216554.953, 215759.75, 215270.812, 214642.672, 213821.641, 212450.672, 214437.953, 212837.188, 212156.453, 212358.438, 212144.484, 210898.641, 210636.078, 210010.0, 209051.719, 210386.0, 209339.5, 209890.875, 209302.422, 208255.0, 208316.812, 208018.203, 207843.609, 206897.0, 207503.969, 206454.953, 206531.391, 206185.062, 205351.281, 205590.156, 204922.469, 205070.656, 205300.156, 205048.562, 204896.562, 205022.875, 204461.109, 203477.062, 203077.25, 202706.547, 203565.812, 202953.797, 202758.703, 202682.188, 202370.531, 201959.453, 202763.453, 201813.406, 201888.859, 200870.547, 201257.906, 200109.203, 199951.812, 200997.625, 199488.234, 200072.438, 199673.031, 199570.938], 'val_loss': [5447.775, 3335.981, 2762.622, 2539.577, 2477.184, 2402.891, 2343.148, 2280.707, 2235.55, 2192.593, 2181.308, 2165.061, 2131.881, 2132.425, 2137.888, 2106.621, 2117.028, 2100.899, 2131.895, 2096.238, 2091.67, 2076.628, 2056.253, 2085.599, 2072.666, 2057.073, 2053.263, 2084.234, 2045.736, 2041.917, 2058.84, 2036.262, 2043.562, 2037.268, 2038.264, 2019.556, 2036.017, 2011.659, 2016.53, 2027.467, 1991.82, 2020.893, 2002.899, 1998.716, 2003.493, 1994.824, 2005.259, 1992.314, 1971.594, 2009.555, 1982.392, 1997.328, 1983.133, 1976.78, 1964.45, 1974.457, 1986.884, 1963.423, 1957.941, 1951.934, 1964.544, 1951.487, 1935.979, 1948.611, 1947.649, 1932.74, 1950.294, 1933.728, 1941.945, 1981.505, 1933.605, 1950.494, 1954.141, 1951.381, 1942.86, 1913.009, 1930.594, 1920.419, 1930.374, 1940.188, 1931.955, 1917.123, 1914.31, 1935.574, 1925.097, 1935.063, 1933.466, 1920.123, 1930.53, 1928.251, 1940.192, 1934.887, 1923.88, 1907.479, 1890.533, 1948.796, 1902.55, 1913.184, 1902.747, 1923.984]}	100	100	True
