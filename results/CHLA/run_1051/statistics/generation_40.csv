id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	2000	True	23705.10547		416732	11	-1	242.36492228507996	{'train_loss': [489588.438, 321490.281, 283271.094, 267599.0, 261081.766, 257130.922, 253387.234, 248775.391, 246633.672, 243330.859, 240691.172, 238882.047, 236761.219, 234712.547, 232599.875, 231683.734, 229804.094, 228094.391, 227284.281, 225166.469, 223304.875, 222940.062, 221576.875, 220614.562, 220134.297, 218423.391, 217531.391, 216813.75, 216119.438, 214559.609, 214382.391, 214532.938, 214228.281, 212719.203, 212982.406, 212054.438, 211457.219, 211056.812, 210590.453, 211555.594, 209518.25, 209058.359, 209213.5, 208144.5, 208471.438, 208156.25, 207176.594, 206515.953, 206909.719, 206404.328, 205575.156, 206282.609, 204832.578, 205246.141, 204874.547, 204186.641, 203130.641, 203095.438, 203170.984, 202935.672, 202411.875, 201549.031, 201966.891, 200676.422, 201267.203, 200789.562, 201316.5, 201105.094, 199842.016, 200764.266, 200137.719, 199524.797, 199356.609, 198553.547, 198351.312, 197982.953, 197965.625, 197970.172, 196958.141, 196959.281, 197224.594, 197306.375, 196151.344, 197054.891, 196330.25, 196936.078, 195826.062, 196161.25, 195033.25, 195434.094, 195196.922, 194457.953, 194546.484, 194024.812, 193894.969, 192720.859, 193936.922, 193271.0, 192444.422, 192948.609], 'val_loss': [3033.361, 2394.234, 2248.51, 2147.02, 2102.614, 2062.01, 2048.231, 2037.408, 2020.374, 1972.844, 1960.676, 1945.339, 1933.8, 1946.859, 1949.926, 1943.24, 1908.843, 1884.855, 1901.077, 1887.166, 1870.952, 1874.157, 1856.446, 1839.729, 1822.091, 1823.47, 1819.085, 1829.992, 1810.269, 1797.409, 1800.606, 1794.038, 1775.669, 1795.181, 1782.609, 1786.931, 1794.332, 1766.864, 1762.212, 1779.342, 1791.463, 1746.11, 1756.973, 1764.056, 1751.188, 1737.805, 1745.111, 1744.954, 1749.487, 1732.111, 1723.053, 1739.484, 1723.151, 1716.697, 1732.822, 1720.012, 1728.934, 1714.584, 1713.684, 1708.274, 1713.235, 1708.566, 1709.694, 1699.192, 1710.631, 1706.516, 1700.428, 1700.299, 1704.88, 1713.882, 1681.906, 1688.661, 1678.912, 1686.195, 1700.296, 1690.755, 1678.215, 1681.24, 1674.359, 1672.734, 1673.434, 1663.584, 1670.298, 1666.884, 1683.948, 1667.823, 1673.287, 1673.699, 1670.454, 1650.264, 1674.839, 1688.144, 1652.217, 1661.679, 1658.189, 1662.63, 1673.189, 1647.511, 1645.319, 1665.202]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adam lr:0.0932107970732484 beta1:0.8170822629037018 beta2:0.8596491070580785 weight_decay:7.972890942319735e-05 batch_size:26 epochs:100	100	1000	True	56148.88281		467204	12	-1	257.9181921482086	{'train_loss': [1086353.25, 1064673.875, 1065361.875, 1065242.875, 773477.438, 442572.781, 431727.906, 439921.938, 442978.625, 435363.688, 451437.719, 438131.844, 443883.438, 442537.125, 447372.625, 442152.688, 439031.812, 440172.438, 443325.125, 440503.219, 446305.031, 443126.281, 444578.062, 439161.5, 437565.125, 445371.469, 442220.188, 444825.75, 446992.188, 453279.812, 446647.406, 444287.75, 446464.062, 441755.625, 441265.5, 441539.75, 443513.438, 441670.031, 443223.344, 439620.469, 443466.812, 439487.25, 440514.656, 438943.938, 442075.469, 443065.875, 441169.0, 446997.5, 441962.25, 437906.688, 447271.375, 452618.812, 441459.188, 441458.031, 444567.438, 443407.625, 445443.188, 450026.625, 440167.594, 445658.0, 442705.281, 445362.719, 437273.281, 442907.0, 448319.25, 445054.875, 445461.938, 440060.094, 441795.562, 456389.938, 444147.938, 440977.594, 448748.938, 466375.375, 440780.062, 440720.562, 446846.0, 439647.969, 442475.688, 447647.406, 440191.438, 447331.062, 443075.719, 443608.062, 447736.75, 442104.531, 447250.656, 454132.812, 447316.0, 442075.094, 450698.0, 445316.875, 443070.312, 442503.719, 446424.844, 458060.938, 447347.875, 451086.188, 437544.875, 436994.719], 'val_loss': [9034.855, 8856.389, 9062.938, 8884.227, 4254.386, 3797.067, 4255.351, 4898.379, 3606.696, 4495.672, 3791.475, 4256.019, 4532.062, 3761.008, 4508.577, 4565.432, 4025.188, 3978.296, 3691.104, 3692.164, 4113.54, 4226.725, 3640.261, 3772.91, 4014.259, 4461.506, 3757.48, 4789.486, 4570.795, 3508.977, 4069.974, 5186.333, 3698.667, 3941.608, 4084.702, 3979.299, 3963.102, 4048.508, 3801.574, 4483.917, 4084.117, 4354.169, 4217.005, 4026.798, 4726.188, 3386.248, 3939.203, 5205.133, 3880.87, 3787.017, 5646.772, 4132.657, 4378.756, 3808.999, 3558.648, 4447.569, 3959.596, 3741.54, 3868.703, 3667.038, 3527.474, 3786.476, 3641.986, 3754.956, 4790.529, 4014.958, 5242.062, 3879.371, 6636.168, 4642.072, 3646.118, 4175.959, 3771.68, 3687.665, 4396.806, 3846.763, 3662.833, 3333.514, 3908.329, 3491.181, 3647.463, 3586.225, 3942.906, 4584.233, 3804.737, 3802.557, 4431.225, 3755.965, 4851.386, 4072.014, 3533.062, 3763.105, 4019.831, 3571.56, 4195.94, 4433.072, 6090.585, 3764.257, 4016.028, 3787.38]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:73 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:80 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	23691.16602		345804	11	-1	237.3366343975067	{'train_loss': [417688.5, 303641.031, 281278.719, 272195.531, 266732.344, 260653.609, 256169.188, 252101.094, 249122.938, 245376.016, 242203.984, 240661.766, 237698.531, 234880.453, 232956.438, 231660.703, 229187.406, 227708.672, 226130.953, 225276.969, 224287.875, 223446.438, 221038.406, 220995.031, 219503.172, 218812.25, 218647.375, 217691.078, 217340.438, 216594.422, 216261.0, 215694.453, 214020.375, 214627.766, 213756.859, 214028.203, 212504.203, 212192.234, 211876.641, 211490.156, 209623.125, 209931.328, 209967.484, 208769.078, 209069.375, 208993.922, 207758.641, 207483.953, 207197.172, 206873.984, 206379.016, 205468.891, 205445.5, 205736.781, 204874.781, 204582.328, 204073.297, 203759.5, 203735.719, 202172.609, 202612.375, 202526.469, 202245.438, 201122.562, 201706.906, 201443.469, 200549.047, 200673.625, 200534.125, 200406.141, 199014.969, 198889.969, 199956.156, 198695.141, 198346.234, 199024.469, 197804.891, 197858.156, 197707.875, 196811.422, 197354.219, 195947.562, 196478.828, 196498.844, 195735.578, 194860.875, 195393.906, 195170.734, 194679.062, 194003.578, 193313.875, 193241.531, 194186.938, 192760.828, 193397.812, 193221.656, 192527.922, 192353.531, 192055.484, 193260.484], 'val_loss': [2676.634, 2526.783, 2224.52, 2190.099, 2181.222, 2095.419, 2109.955, 2055.096, 2037.79, 2012.563, 1991.437, 1935.88, 1922.758, 1915.075, 1887.836, 1880.175, 1860.597, 1851.218, 1851.353, 1845.09, 1824.435, 1834.384, 1819.335, 1821.436, 1812.182, 1809.569, 1827.106, 1793.972, 1815.27, 1792.306, 1808.736, 1793.89, 1822.71, 1792.132, 1779.624, 1785.032, 1773.465, 1763.956, 1775.291, 1759.184, 1782.441, 1761.847, 1768.452, 1751.591, 1758.804, 1759.379, 1746.152, 1756.465, 1750.172, 1749.722, 1729.11, 1734.853, 1740.744, 1740.594, 1733.05, 1724.601, 1723.403, 1724.867, 1724.176, 1717.313, 1711.028, 1718.052, 1719.304, 1703.551, 1716.896, 1716.552, 1719.205, 1710.849, 1704.427, 1713.61, 1705.844, 1721.645, 1715.713, 1717.246, 1713.008, 1700.431, 1705.142, 1688.175, 1694.702, 1711.942, 1695.29, 1697.858, 1689.35, 1686.143, 1686.066, 1692.342, 1676.011, 1694.166, 1683.176, 1683.989, 1692.795, 1686.18, 1661.942, 1666.619, 1697.759, 1672.529, 1676.235, 1669.257, 1664.112, 1677.288]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	3000	True	23818.17383		416732	11	-1	241.98718237876892	{'train_loss': [484864.812, 323467.031, 280940.625, 265004.469, 256034.625, 250660.859, 246261.781, 243589.234, 240656.531, 238259.734, 236794.234, 235393.672, 233490.297, 232539.016, 230954.75, 229678.516, 228327.703, 226917.094, 226324.484, 225393.234, 224851.125, 224448.703, 223149.406, 222289.234, 222655.562, 220162.688, 220104.656, 220047.703, 218258.266, 218426.438, 217016.047, 217463.734, 216487.625, 214902.578, 215699.906, 214557.641, 214884.391, 213595.953, 213490.875, 213513.969, 213524.031, 212574.266, 211861.25, 210806.859, 211365.281, 209372.344, 210383.875, 209440.422, 209108.812, 208237.891, 208209.047, 208015.047, 207621.469, 207864.891, 207282.391, 206768.016, 206138.562, 205853.188, 206564.031, 205766.438, 204501.266, 205008.688, 205232.766, 204110.953, 203654.344, 204222.844, 203788.391, 203866.688, 203138.75, 202207.594, 203014.516, 202383.828, 201944.469, 201811.422, 201079.797, 201356.078, 200747.156, 200866.234, 200257.75, 200024.516, 199510.375, 199455.688, 200121.891, 199273.141, 198783.641, 198647.094, 199031.844, 198430.688, 197692.875, 198205.266, 198014.484, 196613.438, 197050.438, 196786.516, 197091.031, 196561.094, 196337.062, 196320.047, 195968.938, 195633.016], 'val_loss': [2868.734, 2311.157, 2222.113, 2103.792, 2058.394, 2016.446, 1983.951, 1975.886, 1952.94, 1959.132, 1945.835, 1928.837, 1926.684, 1906.991, 1915.157, 1884.026, 1884.686, 1872.845, 1876.807, 1849.246, 1856.952, 1850.081, 1847.485, 1856.392, 1813.825, 1826.312, 1824.066, 1810.899, 1816.564, 1806.801, 1802.533, 1800.568, 1814.33, 1789.924, 1785.446, 1793.422, 1787.636, 1792.759, 1784.518, 1771.781, 1788.443, 1801.572, 1766.052, 1771.976, 1772.85, 1764.993, 1757.679, 1757.356, 1767.944, 1745.045, 1762.657, 1760.621, 1773.568, 1773.418, 1763.696, 1751.712, 1752.028, 1767.78, 1747.422, 1742.331, 1750.609, 1754.196, 1728.754, 1739.847, 1746.099, 1735.363, 1726.215, 1726.597, 1729.94, 1730.108, 1734.468, 1719.736, 1723.308, 1738.623, 1721.308, 1724.386, 1727.368, 1729.129, 1725.545, 1708.502, 1714.544, 1715.891, 1715.12, 1726.203, 1709.729, 1705.391, 1722.417, 1714.786, 1717.73, 1700.49, 1708.016, 1711.614, 1709.618, 1710.93, 1715.484, 1722.446, 1698.777, 1690.979, 1699.225, 1696.476]}	0	100	True
