id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	24526.70117		619010	12	-1	213.62084555625916	{'train_loss': [622171.188, 412929.875, 316571.781, 276238.25, 261707.578, 252668.578, 247679.016, 243750.625, 240331.141, 238367.281, 235993.688, 233980.5, 232870.391, 229353.484, 228192.766, 227626.422, 226154.406, 224988.641, 222890.469, 223438.938, 221634.359, 221653.984, 219880.031, 219894.766, 218331.203, 218841.656, 217022.719, 217253.391, 216188.688, 215361.297, 214687.047, 214456.125, 213393.391, 212703.594, 212586.641, 212087.016, 212458.297, 211468.75, 211501.016, 210406.891, 210816.094, 210163.609, 209899.094, 208703.266, 209513.391, 209076.672, 209045.375, 207328.859, 207780.188, 207254.188, 206644.266, 206969.312, 205771.641, 206064.75, 205714.562, 205592.016, 204324.406, 204325.0, 204592.062, 203662.531, 204422.391, 203726.875, 203198.906, 202533.516, 202246.328, 202548.891, 202425.844, 201227.172, 201851.906, 200383.703, 201453.562, 200639.594, 200821.5, 200899.703, 199983.594, 200138.234, 198972.75, 199058.219, 199106.578, 198516.984, 198910.094, 198380.875, 198471.156, 198035.703, 197639.984, 196759.578, 197415.719, 196365.812, 196608.047, 196123.172, 196952.0, 197193.781, 196189.562, 195471.172, 196766.266, 195542.75, 195807.703, 195411.094, 195451.844, 194618.25], 'val_loss': [4521.193, 3357.875, 2935.356, 2724.941, 2648.982, 2593.979, 2549.054, 2572.632, 2493.934, 2519.0, 2483.933, 2454.765, 2419.965, 2431.775, 2403.576, 2416.271, 2380.417, 2387.071, 2392.69, 2369.749, 2377.542, 2345.627, 2339.755, 2362.071, 2346.681, 2358.46, 2343.705, 2333.312, 2324.931, 2322.198, 2330.953, 2340.36, 2355.593, 2310.901, 2322.096, 2310.568, 2285.704, 2290.155, 2286.078, 2296.015, 2266.522, 2283.284, 2273.57, 2268.044, 2288.979, 2296.591, 2267.686, 2310.113, 2263.063, 2255.638, 2260.301, 2263.483, 2236.974, 2251.111, 2246.047, 2235.871, 2249.353, 2229.195, 2252.824, 2237.34, 2231.244, 2259.005, 2232.872, 2230.428, 2233.936, 2239.557, 2231.517, 2224.345, 2221.05, 2219.204, 2227.335, 2211.868, 2197.501, 2237.178, 2228.987, 2210.539, 2226.346, 2200.332, 2223.725, 2197.003, 2214.737, 2205.516, 2203.218, 2208.458, 2202.766, 2206.219, 2230.513, 2203.86, 2216.603, 2201.275, 2213.857, 2183.888, 2212.184, 2187.835, 2208.485, 2183.286, 2192.2, 2181.295, 2183.134, 2193.742]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	24668.4082		619010	12	-1	214.1598629951477	{'train_loss': [630511.25, 397482.281, 308209.5, 273653.125, 262031.547, 255800.359, 250642.219, 248001.875, 245479.547, 242600.969, 240601.453, 239761.781, 237951.234, 236534.25, 235790.797, 234836.562, 232633.719, 231939.422, 231157.078, 230419.641, 229709.344, 228911.906, 228011.25, 227559.078, 226187.938, 225286.844, 225665.547, 224889.531, 223448.594, 223383.188, 221734.75, 221903.828, 221165.203, 220259.734, 219707.953, 219105.578, 219254.969, 218727.219, 218308.734, 217999.094, 217331.703, 216475.156, 216205.062, 214686.0, 215143.047, 214319.062, 214445.688, 214204.203, 213435.172, 213427.391, 212549.828, 212253.578, 211458.938, 211426.672, 211461.922, 211157.141, 210640.641, 209606.281, 209395.766, 209844.875, 209390.906, 209067.875, 208127.766, 208962.984, 208422.953, 207166.375, 206968.109, 207293.391, 205959.281, 207018.188, 206804.281, 205761.328, 206209.938, 205613.516, 206113.094, 205864.812, 205332.0, 203474.938, 204812.453, 204000.0, 204329.906, 204091.922, 202903.969, 203046.781, 203951.109, 201950.516, 202435.469, 202491.859, 201654.156, 201998.797, 201217.875, 201547.641, 201822.062, 200552.297, 201543.688, 201678.875, 201306.422, 200819.344, 200312.234, 199931.656], 'val_loss': [4062.768, 3186.194, 2838.405, 2759.375, 2683.077, 2574.663, 2578.965, 2547.268, 2533.541, 2528.099, 2514.486, 2534.984, 2489.518, 2493.431, 2479.657, 2479.273, 2431.953, 2452.264, 2435.39, 2404.822, 2405.381, 2423.081, 2390.847, 2397.362, 2412.617, 2401.91, 2391.409, 2391.235, 2371.183, 2400.933, 2348.9, 2344.027, 2371.095, 2352.201, 2322.17, 2365.455, 2352.764, 2331.948, 2319.349, 2357.268, 2328.094, 2312.729, 2327.084, 2300.449, 2297.213, 2343.391, 2304.966, 2306.277, 2285.207, 2262.326, 2271.163, 2260.144, 2291.529, 2273.617, 2296.254, 2252.302, 2270.851, 2250.45, 2268.476, 2269.374, 2257.991, 2258.394, 2232.402, 2264.076, 2230.082, 2239.521, 2254.25, 2220.562, 2223.342, 2231.963, 2246.302, 2221.425, 2237.274, 2244.255, 2206.616, 2202.316, 2218.985, 2230.55, 2218.586, 2186.364, 2195.116, 2213.619, 2213.307, 2198.342, 2185.595, 2188.628, 2195.871, 2189.427, 2202.411, 2181.003, 2185.635, 2208.386, 2175.446, 2204.72, 2206.295, 2173.57, 2225.06, 2175.47, 2179.771, 2156.986]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:91 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:4 kernel_size:10 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.19269798826618062 beta1:0.9085669446507918 beta2:0.943140397017592 weight_decay:6.173092698476879e-05 batch_size:32 epochs:100	100	1000	True	131121.45312		586755	11	-1	217.42422342300415	{'train_loss': [1068795.625, 1062567.125, 1063907.0, 1196607.5, 1062567.125, 1062566.875, 1069832.375, 1132240.5, 1063683.25, 1066945.625, 1107950.375, 1115107.25, 1066003.5, 1065314.625, 1107991.125, 1096765.0, 1072773.375, 1073043.25, 1091027.25, 1090964.75, 1075439.625, 1068275.625, 1086416.625, 1091254.5, 1074438.75, 1070534.375, 1077477.125, 1093723.5, 1072263.625, 1071489.875, 1080035.125, 1088290.25, 1073856.75, 1069984.25, 1079546.25, 1086552.625, 1076364.875, 1072319.875, 1071985.75, 1087742.625, 1071445.75, 1074869.375, 1075132.75, 1084067.5, 1075054.0, 1074568.5, 1073914.375, 1078926.625, 1074216.75, 1075526.0, 1076272.5, 1080679.75, 1075660.375, 1075021.125, 1078394.25, 1084473.0, 1071692.875, 1074042.625, 1076846.375, 1080170.0, 1072498.125, 1072504.125, 1074721.875, 1079587.125, 1070434.375, 1072567.125, 1072308.125, 1078532.75, 1075388.125, 1071856.75, 1076841.125, 1074962.5, 1073599.625, 1074375.25, 1076043.0, 1073387.75, 1073698.25, 1075416.625, 1077458.75, 1073783.0, 1070775.125, 1073133.25, 1074119.375, 1069756.625, 1076673.25, 1071736.375, 1077875.375, 1070010.125, 1073316.75, 1070325.375, 1076838.375, 1073648.75, 1070828.375, 1073281.125, 1081754.375, 1072788.25, 1069757.0, 1071012.5, 1076826.75, 1072421.125], 'val_loss': [11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 25440.053, 11110.434, 11110.434, 11110.344, 14298.25, 11110.434, 11110.434, 11103.995, 19642.627, 11388.653, 11110.434, 11110.422, 15770.111, 11180.373, 11110.304, 11110.346, 12408.855, 11110.434, 11110.434, 11110.434, 11109.111, 11075.339, 11110.434, 11627.472, 11712.538, 11709.852, 11110.434, 12034.849, 11565.217, 11109.818, 11110.412, 11110.433, 11110.427, 11110.433, 11110.434, 11097.172, 11110.434, 11137.32, 11110.434, 11303.415, 11110.434, 11110.428, 11316.224, 11242.003, 11108.979, 11108.921, 11659.943, 11110.427, 11239.83, 11074.781, 11110.434, 11110.432, 11092.916, 11110.434, 11110.434, 11110.434, 11109.481, 11110.434, 11078.579, 11110.434, 11175.881, 11110.434, 12361.429, 11110.434, 11110.431, 11110.434, 11133.472, 11107.434, 11110.434, 11110.357, 11110.434, 11110.434, 11110.433, 11110.434, 11110.434, 11110.376, 11103.88, 11110.434, 11366.068, 11200.694, 11110.434, 11113.008, 15566.433, 11110.335, 11239.145, 11110.428, 11110.434, 11406.739, 11110.434, 11110.423]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:24 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	27331.22852		1230154	11	-1	208.76900124549866	{'train_loss': [1135701.875, 1085103.625, 542816.75, 341971.562, 297699.375, 277645.656, 268563.062, 263283.531, 260319.734, 255848.25, 254107.516, 251239.188, 249832.375, 247042.562, 245349.172, 243991.484, 242591.75, 240072.328, 239465.531, 238175.812, 236747.562, 235621.047, 235702.141, 234138.172, 233130.953, 231706.531, 231033.375, 231358.859, 229544.047, 229279.906, 229008.297, 227930.156, 227690.438, 227694.906, 226368.25, 225791.391, 225413.109, 224371.453, 224199.688, 224581.578, 224087.766, 222210.656, 222411.562, 222276.953, 222070.688, 221529.25, 220777.375, 220547.844, 220898.625, 220109.391, 219663.484, 219689.5, 218818.344, 218590.797, 219296.125, 218800.781, 217571.188, 217991.734, 218971.641, 217777.078, 216784.953, 217725.156, 216973.328, 215584.547, 215320.344, 216187.906, 216391.797, 215232.75, 215464.047, 215138.719, 215031.219, 215239.375, 214684.734, 214382.766, 214457.75, 213828.719, 213838.266, 213510.547, 213735.531, 213370.203, 212115.797, 213341.344, 212807.172, 212848.812, 212864.641, 211845.906, 212180.203, 211500.625, 211588.625, 211520.0, 210522.266, 211303.344, 210960.297, 211089.781, 210606.953, 209166.453, 210319.75, 210350.641, 209859.344, 210599.875], 'val_loss': [11079.927, 10570.726, 3732.997, 3029.843, 2826.427, 2736.984, 2686.976, 2660.488, 2625.712, 2615.563, 2578.98, 2612.113, 2550.934, 2525.933, 2547.161, 2517.008, 2508.684, 2530.379, 2554.817, 2483.495, 2481.244, 2452.982, 2461.553, 2479.945, 2472.094, 2484.576, 2462.22, 2469.422, 2445.898, 2453.032, 2410.031, 2442.712, 2443.687, 2440.615, 2452.749, 2404.159, 2404.068, 2399.587, 2370.023, 2381.323, 2401.377, 2400.566, 2385.85, 2451.05, 2368.614, 2402.769, 2401.709, 2415.476, 2362.407, 2376.259, 2375.767, 2379.719, 2389.798, 2366.261, 2350.551, 2349.528, 2353.451, 2385.105, 2332.536, 2382.027, 2361.225, 2343.86, 2379.072, 2362.735, 2359.284, 2360.762, 2332.634, 2431.755, 2358.557, 2392.051, 2343.548, 2392.355, 2338.169, 2368.58, 2341.27, 2318.389, 2343.269, 2374.799, 2317.358, 2342.382, 2326.461, 2346.6, 2361.303, 2349.584, 2357.677, 2344.951, 2353.05, 2342.634, 2340.393, 2359.74, 2363.978, 2329.789, 2306.052, 2301.558, 2314.101, 2388.399, 2331.083, 2278.007, 2325.124, 2363.979]}	100	100	True
