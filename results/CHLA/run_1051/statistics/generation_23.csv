id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:26 epochs:100	100	1000	True	24484.92578		468105	9	-1	209.43341875076294	{'train_loss': [604980.812, 333457.156, 293678.562, 277017.125, 268181.531, 263216.188, 258949.141, 255909.031, 252771.703, 250511.953, 247779.828, 245726.078, 243830.031, 241764.969, 240247.359, 238703.094, 237539.906, 236181.391, 234828.703, 233822.922, 232514.234, 230619.688, 230356.109, 229610.594, 228150.453, 228216.078, 227279.5, 226346.984, 225448.75, 224087.125, 224041.391, 224026.047, 223124.141, 222525.172, 222148.453, 221023.0, 221332.625, 219769.422, 219257.297, 217988.828, 219090.984, 218046.859, 216992.297, 217584.625, 217120.969, 216468.047, 216793.141, 216140.891, 215519.172, 214379.109, 215337.484, 214299.438, 214081.031, 214283.891, 213545.719, 213898.766, 212595.688, 213579.594, 212668.391, 213059.453, 212408.156, 211555.062, 211787.562, 211363.844, 210569.109, 211413.734, 210877.844, 210699.266, 209398.312, 209728.625, 210149.016, 208876.281, 208701.797, 209046.766, 208184.234, 208496.438, 208113.188, 208405.562, 207153.484, 207679.875, 207641.344, 207949.766, 207644.969, 206768.938, 205684.672, 205932.5, 206927.781, 206136.297, 205316.156, 205166.328, 205730.188, 204924.109, 206049.422, 205997.438, 204284.156, 205210.188, 204812.594, 204452.406, 203859.266, 204518.188], 'val_loss': [2832.6, 2425.256, 2242.494, 2177.96, 2154.131, 2108.522, 2075.682, 2049.608, 2034.598, 2030.208, 2016.449, 2005.463, 1993.245, 1978.782, 1972.859, 1944.703, 1954.725, 1934.541, 1922.72, 1929.56, 1912.288, 1919.754, 1902.772, 1897.23, 1902.49, 1886.151, 1872.699, 1876.641, 1866.554, 1884.663, 1869.14, 1858.785, 1854.811, 1861.844, 1850.595, 1861.45, 1856.526, 1843.59, 1846.701, 1832.47, 1829.792, 1830.724, 1834.695, 1825.134, 1824.512, 1814.844, 1817.497, 1820.073, 1805.787, 1825.616, 1813.304, 1817.363, 1801.206, 1794.847, 1801.0, 1804.875, 1790.87, 1799.204, 1798.85, 1787.944, 1791.758, 1793.03, 1794.797, 1781.536, 1785.679, 1790.893, 1778.984, 1805.782, 1777.054, 1782.089, 1764.973, 1779.93, 1764.898, 1769.112, 1752.255, 1765.3, 1757.918, 1765.567, 1765.704, 1772.31, 1774.314, 1749.082, 1765.375, 1763.179, 1750.901, 1765.323, 1757.823, 1759.309, 1765.362, 1747.218, 1750.613, 1746.965, 1766.942, 1736.843, 1740.304, 1739.721, 1736.722, 1755.901, 1729.643, 1737.765]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:63 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:5 layer:fc act:selu out_features:200 bias:True input:6 learning:adam lr:0.22073560692554822 beta1:0.8362590869594899 beta2:0.9833809063223161 weight_decay:7.086238039363724e-05 batch_size:26 epochs:100	100	1000	True	131121.0625		402820	8	-1	209.144788980484	{'train_loss': [1090570.125, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062579.375, 1107062.25, 1178064.875, 1448291.75, 1062567.25, 1062567.25, 1062857.25, 1062776.5, 1062567.25, 1071271.5, 1079165.75, 1093620.75, 1089022.625, 1107085.75, 1083353.125, 1062567.25, 1062567.25, 1062773.875, 1062567.25, 1069265.875, 1077617.625, 1074248.0, 1073450.5, 1081534.125, 1073277.0, 1066131.75, 1062768.25, 1063024.75, 1062724.625, 1064270.5, 1068841.625, 1084513.875, 1069333.5, 1083163.875, 1074651.5, 1107706.5, 1064365.375, 1067072.0, 1064668.0, 1064567.875, 1075228.375, 1073932.5, 1068963.0, 1081702.375, 1093885.5, 1079825.5, 1069858.125, 1072535.375, 1078182.375, 1070352.5, 1068255.625, 1064045.5, 1092439.0, 1067106.375, 1084881.75, 1068980.5, 1074947.125, 1084118.625, 1100148.0, 1066379.625, 1064346.125, 1071413.625, 1073346.375, 1074748.625, 1065532.375, 1080901.125, 1071943.125, 1077060.5, 1090277.5, 1068490.125, 1067036.625, 1071308.25, 1075087.5, 1067692.125, 1072258.25, 1078962.625, 1065430.125, 1089617.625, 1064565.5, 1067655.375, 1076564.875, 1067480.875, 1080739.25, 1085156.875, 1074828.75, 1070304.875, 1078381.625, 1079389.25, 1066761.5, 1080930.25, 1075654.875, 1081159.75, 1068668.625, 1072821.5, 1089168.25], 'val_loss': [8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.347, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.336, 8888.349, 8888.349, 8888.349, 8888.349]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:5 layer:fc act:selu out_features:200 bias:True input:6 learning:adadelta batch_size:9 epochs:100	100	1000	True	25702.61523		686226	8	-1	396.2195484638214	{'train_loss': [471079.719, 293863.469, 281219.844, 274023.344, 267665.125, 263569.969, 259492.844, 256362.891, 252725.312, 249895.797, 246807.25, 243554.375, 241680.5, 239613.359, 238045.734, 235864.078, 235166.438, 233820.719, 232029.859, 231753.094, 229539.547, 228451.516, 228662.0, 227221.641, 226459.812, 226324.734, 225114.734, 224570.625, 223456.703, 222486.969, 222191.641, 221899.234, 221343.625, 220530.594, 219501.25, 218844.016, 218888.453, 218422.5, 217915.797, 217703.172, 216811.672, 216198.234, 216651.359, 215481.781, 215239.234, 214491.125, 214426.688, 214066.359, 213779.078, 213430.094, 213277.297, 212894.938, 212781.766, 211594.531, 212013.531, 211022.703, 210646.328, 210702.656, 209956.969, 210039.422, 209873.0, 209488.844, 208871.109, 209100.438, 209057.453, 207966.734, 207851.25, 207607.016, 207029.141, 207649.828, 207349.844, 207187.25, 206416.703, 207152.453, 206617.062, 205569.0, 205430.516, 205492.078, 205443.609, 205577.453, 205180.391, 204968.516, 204953.484, 205364.109, 204540.203, 204829.891, 204040.469, 204127.031, 203603.781, 204048.922, 203227.328, 203202.516, 202867.844, 203225.203, 202008.281, 202340.391, 202814.391, 202697.109, 202306.109, 201525.312], 'val_loss': [911.61, 837.286, 825.525, 800.928, 782.929, 767.927, 757.256, 747.555, 734.775, 730.37, 725.65, 723.315, 714.887, 715.812, 718.694, 712.563, 705.354, 715.72, 692.054, 705.297, 706.935, 694.686, 691.162, 690.28, 696.518, 686.395, 690.437, 697.225, 685.805, 684.187, 675.099, 682.409, 683.963, 676.875, 677.575, 684.413, 677.734, 676.502, 678.609, 678.365, 678.96, 674.506, 673.071, 674.385, 669.566, 681.823, 679.381, 673.798, 678.33, 686.97, 677.548, 685.692, 679.712, 672.828, 684.375, 671.107, 668.483, 688.217, 683.293, 676.464, 665.221, 678.085, 679.549, 673.528, 674.05, 681.777, 669.379, 678.056, 671.529, 671.3, 679.224, 673.531, 663.59, 666.634, 669.022, 655.125, 665.45, 666.276, 662.241, 664.008, 668.219, 666.48, 664.236, 659.789, 662.075, 657.678, 670.085, 664.773, 665.919, 660.53, 671.166, 666.728, 662.345, 659.674, 666.058, 666.92, 665.076, 667.128, 660.163, 654.31]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:5 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:5 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	27750.75391		2092686	11	-1	238.82522106170654	{'train_loss': [1085264.5, 733830.688, 414308.531, 285107.125, 270303.406, 259730.812, 254507.312, 249364.234, 245603.641, 242310.938, 239761.141, 237604.703, 235310.922, 234043.234, 231889.734, 230778.531, 229570.375, 228128.328, 227567.875, 225858.516, 225220.391, 224814.172, 222655.672, 222346.25, 220895.016, 220203.125, 218986.078, 218496.812, 218018.344, 218258.688, 216220.516, 215087.625, 214760.547, 213786.047, 211532.672, 214058.406, 211836.578, 210212.969, 211598.094, 209089.438, 208722.953, 209932.0, 208550.547, 206847.094, 207734.203, 206361.047, 204683.891, 204974.734, 204014.625, 204346.953, 202799.875, 203365.031, 201843.703, 201579.0, 201844.859, 200194.375, 199531.219, 199655.406, 198535.375, 199297.312, 198107.516, 198832.094, 197125.031, 197549.453, 196763.453, 196892.234, 194112.5, 195671.875, 194491.375, 194138.953, 194013.406, 193680.641, 192755.734, 192438.266, 193182.156, 191510.906, 192600.422, 190647.484, 190815.875, 190742.875, 190300.406, 190051.484, 188774.688, 189282.141, 188771.031, 188979.812, 188259.672, 188918.484, 188221.031, 187167.391, 186900.438, 187964.578, 186824.828, 185659.531, 185587.234, 186454.281, 185631.594, 184737.422, 184698.422, 185353.531], 'val_loss': [5079.056, 4006.95, 2349.454, 2251.824, 2191.249, 2101.483, 2084.256, 2042.402, 1994.797, 1997.871, 1994.306, 2010.541, 1962.485, 2025.38, 1950.58, 1951.53, 1938.962, 1934.526, 1927.029, 1897.0, 1917.25, 1899.1, 1895.217, 1912.561, 1887.336, 1884.194, 1881.153, 1885.555, 1928.323, 1884.649, 1883.467, 1867.327, 1893.901, 1850.144, 1846.22, 1904.095, 1871.919, 1857.495, 1846.064, 1889.425, 1867.676, 1848.031, 1858.863, 1901.438, 1865.512, 1824.51, 1880.243, 1846.044, 1870.758, 1831.348, 1872.043, 1859.895, 1867.264, 1886.286, 1883.833, 1870.612, 1843.133, 1847.777, 1881.911, 1882.72, 1891.04, 1852.203, 1880.318, 1905.672, 1877.5, 1886.835, 1918.536, 1889.565, 1891.341, 1859.039, 1909.541, 1901.528, 1888.881, 1935.844, 1930.402, 1896.327, 1909.229, 1889.044, 1895.948, 1881.084, 1876.095, 1947.098, 1882.704, 1910.522, 1970.798, 1944.094, 1915.851, 1975.178, 1887.208, 1880.719, 1912.989, 1890.594, 1890.404, 1905.87, 1913.151, 1935.198, 1884.4, 1966.495, 1956.479, 1909.428]}	100	100	True
