id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:26 epochs:100	100	1000	True	25101.49414		468105	9	-1	212.8798816204071	{'train_loss': [548570.875, 329793.781, 289623.594, 273859.906, 265856.438, 261004.828, 257453.234, 253426.188, 250077.891, 247144.172, 244522.453, 242667.531, 240562.875, 237700.953, 236712.25, 235147.688, 233393.625, 232291.297, 231722.953, 230230.703, 229605.859, 228481.219, 226825.078, 227597.422, 225756.547, 226044.562, 224900.328, 224070.406, 224019.234, 221942.109, 221010.266, 220757.109, 220934.703, 220842.516, 219816.906, 218906.141, 218986.219, 218383.703, 218634.219, 216693.203, 216917.938, 216263.641, 216592.953, 216055.344, 215799.453, 215224.016, 215184.422, 214220.188, 214198.281, 213118.172, 213708.641, 213773.641, 212909.406, 212587.219, 212640.031, 212288.234, 211656.875, 212131.75, 211202.359, 211504.156, 211502.641, 210770.141, 210370.516, 210206.281, 209107.156, 209796.484, 209325.031, 208944.391, 208681.016, 209392.391, 208933.125, 207984.625, 208385.906, 207947.25, 207260.641, 207397.109, 207149.359, 207620.453, 206005.203, 206573.797, 206197.922, 206197.547, 205461.438, 206241.047, 205301.359, 205675.516, 205624.672, 205099.75, 205118.203, 205327.0, 205100.953, 204837.984, 204322.094, 204541.109, 204218.547, 204176.719, 204051.562, 204159.844, 204370.453, 202928.453], 'val_loss': [2779.924, 2358.637, 2228.94, 2127.152, 2106.65, 2069.846, 2056.105, 2029.686, 2019.051, 2015.887, 1981.27, 1952.461, 1956.402, 1945.652, 1940.777, 1928.496, 1906.337, 1918.955, 1902.584, 1896.574, 1894.576, 1884.663, 1893.613, 1878.388, 1875.831, 1868.008, 1857.014, 1867.0, 1877.037, 1848.061, 1847.182, 1847.862, 1835.168, 1843.464, 1867.124, 1838.392, 1839.244, 1841.666, 1835.861, 1833.328, 1848.667, 1827.004, 1833.596, 1837.814, 1822.463, 1816.76, 1822.223, 1823.717, 1820.279, 1818.464, 1815.071, 1827.11, 1804.004, 1812.373, 1799.32, 1803.29, 1795.247, 1802.89, 1792.694, 1818.53, 1798.308, 1805.976, 1792.846, 1795.433, 1775.311, 1798.006, 1791.319, 1780.781, 1811.56, 1773.748, 1779.008, 1779.518, 1784.152, 1763.84, 1765.131, 1787.911, 1788.717, 1773.078, 1771.909, 1768.05, 1780.057, 1774.285, 1774.087, 1773.43, 1752.276, 1789.141, 1772.192, 1764.887, 1784.64, 1748.698, 1755.229, 1753.408, 1767.874, 1773.264, 1772.299, 1761.867, 1775.464, 1777.274, 1765.01, 1770.628]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:46 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:26 epochs:100	100	1000	True	26303.57812		492372	10	-1	222.809508562088	{'train_loss': [638956.938, 337234.875, 294369.781, 279116.594, 269776.969, 263272.531, 257877.75, 254338.812, 251715.469, 249242.031, 247712.922, 246015.359, 243707.297, 241953.0, 240168.109, 239069.828, 237604.531, 236562.984, 235427.5, 234598.266, 233473.516, 231936.422, 231852.25, 230608.812, 230562.281, 229435.562, 229561.828, 227756.453, 227038.047, 226915.688, 226690.375, 225308.438, 225495.984, 224710.422, 224757.5, 223599.156, 223170.188, 223606.641, 222973.438, 222083.094, 222349.078, 221770.766, 220803.328, 221060.266, 220741.266, 221682.547, 220097.531, 220495.734, 220339.734, 219717.75, 220509.297, 219250.797, 219019.5, 219117.297, 218409.672, 218141.859, 217771.125, 217218.188, 217448.266, 216918.797, 216981.344, 216774.125, 215364.453, 216693.672, 216197.828, 215758.5, 215682.375, 215370.422, 214759.938, 214860.812, 214011.203, 214286.328, 213959.859, 213813.172, 214447.25, 213115.594, 213342.062, 212910.891, 214105.484, 212860.359, 212471.484, 213589.391, 212502.219, 212391.609, 212317.938, 211832.094, 212325.25, 211214.359, 212140.891, 211231.844, 211481.453, 211051.141, 211454.859, 209995.188, 210774.047, 210678.234, 210816.75, 209710.375, 210789.406, 209641.188], 'val_loss': [2962.653, 2443.4, 2287.234, 2204.479, 2169.607, 2133.343, 2089.164, 2094.357, 2076.672, 2051.868, 2042.399, 2074.976, 1993.795, 1995.312, 2003.94, 2053.279, 2029.247, 1980.798, 1979.896, 1980.827, 1985.54, 1961.383, 1971.464, 1928.204, 1914.222, 1939.607, 1935.335, 1916.951, 1883.11, 1937.476, 1902.389, 1889.899, 1918.834, 1942.436, 1932.431, 1919.695, 1878.568, 1900.801, 1910.655, 1866.37, 1895.425, 1897.816, 1881.02, 1921.073, 1871.072, 1873.566, 1861.886, 1867.091, 1916.94, 1862.933, 1893.47, 1843.379, 1880.916, 1888.145, 1872.229, 1859.649, 1891.177, 1881.15, 1845.651, 1847.678, 1863.843, 1907.834, 1846.273, 1864.66, 1871.171, 1836.65, 1851.606, 1839.391, 1839.79, 1828.965, 1862.149, 1833.254, 1857.676, 1820.752, 1814.228, 1833.619, 1829.178, 1844.208, 1820.669, 1832.164, 1839.39, 1850.092, 1848.392, 1838.739, 1834.832, 1829.289, 1823.892, 1826.051, 1863.526, 1822.592, 1820.25, 1814.641, 1845.521, 1852.72, 1816.894, 1823.951, 1807.673, 1806.682, 1826.326, 1836.778]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:fc act:selu out_features:200 bias:True input:6 learning:adadelta batch_size:4 epochs:100	100	1000	True	28236.08398		692307	8	-1	748.0997943878174	{'train_loss': [446234.938, 325649.219, 287547.812, 275257.312, 267247.156, 261035.953, 257519.797, 254885.078, 253395.359, 250699.406, 248642.281, 247112.359, 246875.453, 244765.578, 244143.656, 242574.688, 241924.0, 241905.0, 240414.188, 239295.281, 238984.859, 238202.703, 237679.953, 237354.031, 236942.594, 236520.016, 235471.469, 234797.219, 234881.859, 233369.672, 233642.266, 233306.375, 233099.406, 232565.906, 232432.672, 232234.609, 231462.172, 229768.828, 231183.953, 230472.531, 229434.984, 230076.562, 228564.703, 227890.25, 228194.562, 228293.516, 228008.516, 226535.031, 227332.484, 227005.359, 226244.312, 226043.156, 226198.25, 225796.109, 226389.281, 225641.281, 224964.703, 225553.328, 225696.859, 224833.5, 223438.75, 224972.953, 224309.625, 223425.328, 223965.234, 223091.531, 222873.047, 223928.625, 222229.5, 222463.469, 221781.797, 222147.453, 221355.125, 222378.422, 221868.875, 221072.281, 222066.156, 221314.406, 221022.438, 221781.328, 220820.266, 221122.25, 220467.828, 220090.469, 220427.672, 220198.922, 219752.281, 219542.406, 220464.703, 220338.516, 220212.281, 219651.359, 219498.828, 218807.188, 218956.844, 218552.156, 218531.547, 217900.5, 218311.938, 218946.656], 'val_loss': [449.189, 385.028, 369.396, 350.142, 346.307, 335.16, 333.803, 335.431, 330.097, 325.296, 321.711, 325.49, 320.967, 324.602, 319.021, 321.069, 314.632, 317.816, 317.56, 322.814, 320.417, 318.893, 317.22, 316.061, 323.27, 313.464, 316.933, 318.825, 316.955, 320.752, 319.827, 319.868, 316.414, 324.92, 313.702, 316.291, 308.881, 318.883, 316.551, 310.86, 309.954, 316.56, 309.178, 306.283, 309.2, 307.204, 310.372, 306.287, 319.272, 318.78, 304.972, 307.048, 306.26, 307.716, 308.35, 312.364, 305.298, 303.898, 308.079, 305.815, 300.478, 305.902, 304.372, 312.906, 304.023, 309.797, 307.357, 305.081, 314.891, 308.862, 307.913, 295.645, 318.814, 299.724, 306.959, 311.212, 312.462, 305.159, 320.423, 307.23, 301.854, 302.474, 301.147, 306.572, 307.876, 303.302, 301.14, 305.183, 304.868, 314.504, 315.98, 301.865, 314.251, 304.737, 304.936, 306.566, 313.112, 303.099, 302.531, 310.61]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:34 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	24044.64062		379725	11	-1	234.0725998878479	{'train_loss': [524980.125, 375535.406, 307084.5, 275048.812, 263188.125, 258206.719, 254435.328, 251633.0, 248264.797, 245725.594, 243307.922, 241299.766, 238951.562, 237743.297, 235808.641, 234285.375, 232810.562, 230777.562, 228972.156, 228194.531, 226408.0, 225414.906, 225004.672, 224725.391, 222962.672, 222655.609, 221341.203, 220713.438, 220152.594, 219794.562, 218601.5, 218294.156, 217257.906, 215901.438, 215199.094, 215341.891, 214757.094, 214366.969, 213964.469, 213208.656, 213319.141, 211882.953, 211844.391, 210427.297, 211337.922, 210472.688, 209936.844, 209301.375, 209465.875, 208082.438, 207303.875, 207718.859, 208117.391, 207084.219, 206955.031, 206589.984, 207431.391, 205748.312, 205289.672, 204700.344, 204691.125, 204454.641, 205353.5, 203912.047, 203861.438, 203893.938, 203379.422, 202567.812, 202590.75, 202633.688, 201433.844, 201504.547, 201204.328, 200662.203, 200854.656, 201049.906, 200134.516, 199983.844, 200018.234, 199789.859, 199439.781, 198151.172, 198311.594, 198644.156, 198647.656, 198270.422, 197873.422, 197566.031, 197708.734, 197827.516, 196368.75, 196638.203, 196154.438, 196694.797, 196449.938, 196920.938, 196558.438, 195805.984, 195934.922, 195378.828], 'val_loss': [3446.16, 2721.11, 2317.792, 2229.362, 2152.229, 2128.866, 2087.684, 2052.386, 2023.545, 2011.621, 2028.047, 1964.803, 1952.909, 1963.947, 1936.977, 1911.726, 1896.989, 1892.619, 1870.694, 1882.669, 1872.636, 1853.123, 1849.65, 1854.647, 1848.705, 1845.491, 1835.37, 1826.546, 1830.435, 1823.847, 1810.899, 1812.36, 1814.919, 1798.708, 1820.714, 1790.342, 1796.298, 1803.768, 1796.989, 1802.374, 1807.673, 1778.681, 1782.033, 1779.347, 1781.885, 1775.945, 1785.913, 1762.711, 1765.531, 1789.337, 1752.877, 1773.568, 1761.922, 1771.528, 1741.152, 1752.553, 1741.961, 1752.878, 1735.228, 1762.448, 1748.844, 1749.61, 1758.964, 1736.77, 1737.897, 1742.361, 1750.094, 1741.005, 1747.425, 1736.538, 1730.722, 1732.244, 1735.243, 1746.163, 1734.172, 1705.917, 1726.348, 1729.463, 1726.563, 1723.575, 1741.094, 1720.905, 1709.942, 1727.005, 1710.303, 1708.401, 1712.606, 1700.889, 1716.627, 1739.446, 1743.742, 1705.651, 1691.214, 1685.706, 1689.461, 1712.768, 1705.141, 1703.819, 1731.763, 1697.715]}	100	100	True
