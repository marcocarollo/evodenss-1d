id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	3000	True	24184.21094		416732	11	-1	237.59681057929993	{'train_loss': [527301.125, 327204.562, 287857.125, 275912.594, 268416.375, 262775.938, 257889.984, 254168.266, 251862.969, 248327.781, 244945.781, 243714.203, 240021.047, 238498.344, 236326.406, 234391.969, 232920.922, 230390.844, 228997.906, 227803.531, 225854.047, 224934.0, 223849.422, 223334.234, 221655.703, 221967.812, 219751.609, 219207.906, 217815.172, 216899.969, 216994.109, 216496.484, 217324.109, 216089.781, 215290.406, 214408.609, 213795.234, 212870.469, 213132.188, 211368.906, 211525.391, 210765.594, 210268.031, 210478.031, 209662.172, 208409.25, 209541.641, 208921.359, 208067.297, 208000.609, 207272.203, 207211.844, 205868.969, 205913.547, 205488.078, 204622.0, 204165.859, 204744.766, 204112.75, 202401.719, 202733.703, 203250.453, 202193.812, 201907.266, 202284.969, 202591.75, 201611.281, 200561.672, 202029.391, 201343.0, 200578.25, 200616.062, 199726.688, 198473.406, 198381.656, 199253.969, 199458.172, 198884.641, 199132.609, 199028.203, 199045.188, 196763.625, 196436.609, 197009.516, 197882.859, 196541.406, 196606.531, 197067.672, 195274.406, 194984.109, 196819.609, 195821.062, 195257.359, 195734.906, 196024.859, 194220.375, 195907.766, 194555.047, 194080.281, 194654.234], 'val_loss': [3015.519, 2386.509, 2290.83, 2251.238, 2179.256, 2157.051, 2090.074, 2065.225, 2052.973, 2006.953, 2002.957, 1979.349, 1954.359, 1950.004, 1919.449, 1954.914, 1917.577, 1916.263, 1883.713, 1892.725, 1880.636, 1881.693, 1879.13, 1873.392, 1847.176, 1855.025, 1842.157, 1873.358, 1858.735, 1854.773, 1838.035, 1831.239, 1844.658, 1823.886, 1820.516, 1827.269, 1812.414, 1813.267, 1800.589, 1803.342, 1777.406, 1796.746, 1777.973, 1775.597, 1777.657, 1791.967, 1799.242, 1781.138, 1797.035, 1776.805, 1779.062, 1752.36, 1767.171, 1754.294, 1772.211, 1784.461, 1755.659, 1750.003, 1738.109, 1756.04, 1756.581, 1754.966, 1745.708, 1732.357, 1755.163, 1747.872, 1736.265, 1747.852, 1742.632, 1723.915, 1737.336, 1737.725, 1728.059, 1718.429, 1705.359, 1720.21, 1718.222, 1712.47, 1700.325, 1739.643, 1708.9, 1704.49, 1708.755, 1709.888, 1707.113, 1712.273, 1731.663, 1695.184, 1735.057, 1696.732, 1695.714, 1716.182, 1705.905, 1701.685, 1679.725, 1686.91, 1702.602, 1695.852, 1709.112, 1730.166]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:120 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:91 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:10 epochs:100	100	1000	True	24930.88672		645033	13	-1	501.8183653354645	{'train_loss': [504084.375, 320240.625, 298940.438, 291695.906, 287275.031, 283707.219, 280360.906, 274101.812, 263651.781, 255303.031, 250466.844, 246842.031, 243561.359, 240017.172, 237534.328, 235445.281, 233724.797, 230694.469, 229748.844, 228082.047, 226795.766, 226249.094, 223481.719, 223081.562, 222594.125, 221466.359, 220922.328, 219689.359, 219157.266, 218884.578, 217231.391, 216474.141, 216152.531, 215546.844, 214387.484, 213709.625, 213079.828, 213999.969, 212643.156, 211188.641, 212747.109, 211165.594, 210859.453, 210294.859, 209353.406, 209300.953, 208200.734, 208821.344, 207887.594, 208281.5, 207878.75, 207049.375, 206031.984, 206164.094, 205233.344, 204941.188, 205518.359, 205158.031, 204115.188, 205518.516, 203414.281, 204314.891, 202542.516, 202892.562, 202708.906, 202628.578, 202202.797, 201566.109, 200907.578, 200850.297, 200171.266, 200023.969, 199707.703, 199137.766, 199024.516, 198627.391, 198876.625, 198897.031, 198081.562, 196856.812, 197096.547, 197876.688, 196507.016, 196963.406, 196788.531, 196010.812, 196505.328, 196042.578, 194726.766, 195831.359, 196199.547, 194354.219, 194003.703, 195625.594, 194250.609, 193086.734, 192850.344, 192648.469, 193338.594, 192966.641], 'val_loss': [1093.787, 987.245, 979.151, 961.89, 947.366, 944.749, 937.596, 890.237, 859.765, 835.76, 818.88, 810.851, 806.061, 802.379, 792.252, 776.222, 789.58, 788.127, 777.241, 773.477, 762.833, 763.815, 768.051, 749.461, 759.155, 765.504, 754.442, 752.92, 740.099, 751.207, 746.642, 754.262, 740.541, 747.064, 735.494, 733.261, 736.457, 741.018, 748.893, 727.975, 729.968, 725.832, 736.29, 724.927, 730.229, 731.062, 727.899, 721.395, 728.701, 720.917, 730.962, 712.323, 717.656, 723.462, 720.065, 714.159, 712.698, 714.308, 707.246, 709.078, 716.747, 701.285, 703.649, 705.508, 702.356, 709.617, 700.387, 701.913, 708.921, 710.825, 711.368, 694.362, 707.096, 702.877, 710.835, 697.552, 699.781, 704.316, 700.607, 711.5, 705.92, 708.203, 692.475, 705.323, 699.798, 702.724, 704.354, 708.251, 695.256, 713.994, 701.118, 717.179, 703.49, 692.961, 695.039, 694.157, 703.573, 707.505, 695.249, 704.616]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:54 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.20778749338905939 beta1:0.9815602340471066 beta2:0.8875509963712764 weight_decay:2.072639220485305e-05 batch_size:26 epochs:100	100	1000	True	131122.60938		819740	11	-1	246.62387824058533	{'train_loss': [1094275.375, 3094033.75, 1068643.25, 3066336.75, 1062567.25, 2429483.0, 1678522.75, 7750610.5, 9931059.0, 12833725.0, 5023616.5, 2774779.0, 1079285.875, 1282168.0, 1118158.875, 1195582.625, 2114201.75, 1104337.75, 1610761.0, 1102273.375, 1190609.375, 1122454.625, 1240893.875, 1117272.25, 1072622.5, 1062567.25, 1344635.75, 1107224.875, 1094855.5, 1062567.25, 1062567.25, 1142177.125, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1081914.125, 1062567.25, 1062567.25, 1062567.25, 1138500.375, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062628.0, 1062567.25, 1062567.25, 1078164.375, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1077113.0, 1062567.25, 1062567.25, 1093107.125, 1131516.375, 1077748.75, 1071417.625, 1069396.875, 1136039.375, 1161063.75, 1308892.125, 1393644.375, 1183019.625, 1125115.25, 1120433.75, 1102702.875, 1157076.5, 1095059.25, 1101266.875, 1139286.5], 'val_loss': [8888.349, 8888.349, 5546765824.0, 8888.349, 69643000.0, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	23531.41211		416732	11	-1	238.66815400123596	{'train_loss': [506036.969, 321826.469, 278804.938, 265295.594, 258788.5, 253578.688, 248891.766, 245812.188, 242971.375, 239554.781, 237993.719, 235542.078, 233262.797, 232152.344, 229959.422, 228681.234, 227594.188, 226465.203, 225126.719, 225005.594, 223637.969, 221610.969, 221324.594, 220505.438, 219707.547, 217776.766, 218090.938, 216959.703, 216264.891, 215000.391, 214883.891, 213448.5, 212907.594, 212174.609, 211374.938, 211516.969, 211210.375, 210133.438, 209301.141, 209618.844, 209054.484, 207884.375, 208346.812, 207611.969, 206901.188, 205880.812, 206689.438, 205386.594, 204793.109, 204985.375, 204288.031, 205443.219, 204360.516, 204295.422, 203227.547, 203156.375, 201817.594, 202207.25, 202432.141, 201635.047, 201102.531, 201235.172, 201201.547, 200567.672, 200243.562, 199449.141, 199414.328, 199333.969, 199603.0, 197734.906, 198273.094, 197900.25, 197207.188, 197857.906, 197229.406, 196731.844, 196735.266, 196465.531, 196108.609, 195577.125, 195340.375, 196332.766, 195518.0, 195094.047, 193806.031, 194423.297, 194723.547, 193508.391, 192950.688, 193240.266, 192970.172, 193693.344, 192700.047, 193016.609, 192515.0, 192114.375, 191621.438, 192570.422, 191306.312, 191773.234], 'val_loss': [2980.451, 2434.67, 2264.648, 2147.451, 2135.387, 2083.235, 2006.954, 1996.117, 1971.025, 1946.89, 1959.442, 1928.795, 1931.916, 1908.677, 1903.76, 1888.94, 1881.062, 1875.213, 1861.16, 1866.908, 1856.442, 1863.294, 1827.404, 1846.846, 1833.232, 1825.563, 1825.718, 1805.169, 1809.293, 1802.621, 1807.55, 1783.506, 1797.597, 1799.911, 1801.11, 1794.146, 1802.989, 1803.623, 1777.552, 1782.767, 1782.937, 1773.682, 1769.936, 1773.224, 1752.473, 1767.396, 1750.273, 1756.258, 1744.695, 1764.728, 1753.225, 1731.847, 1776.34, 1767.368, 1738.929, 1749.29, 1725.786, 1744.215, 1726.299, 1728.522, 1735.523, 1725.989, 1711.006, 1746.186, 1714.396, 1707.064, 1713.026, 1702.985, 1702.945, 1734.076, 1689.193, 1711.078, 1691.458, 1698.599, 1708.569, 1710.715, 1712.889, 1715.254, 1701.554, 1694.043, 1687.077, 1711.713, 1709.759, 1690.29, 1699.848, 1697.175, 1688.441, 1700.371, 1698.418, 1690.838, 1705.017, 1691.045, 1704.033, 1668.205, 1673.817, 1690.908, 1682.641, 1694.841, 1664.711, 1680.585]}	100	100	True
