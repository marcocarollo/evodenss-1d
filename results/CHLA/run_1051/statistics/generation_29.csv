id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	2000	True	23636.0332		416732	11	-1	238.0659191608429	{'train_loss': [515649.562, 326932.219, 281333.812, 262309.25, 254005.828, 248718.141, 243661.562, 240558.625, 236999.547, 233538.938, 231892.641, 229995.062, 228565.875, 226684.406, 226128.891, 224799.891, 224107.422, 222089.391, 222211.547, 220477.922, 220502.969, 219525.484, 218303.172, 217520.562, 217348.125, 216545.375, 215918.516, 215732.172, 215236.516, 214944.547, 213884.844, 213749.984, 213068.312, 212736.078, 211892.359, 211540.922, 210774.953, 210057.156, 211126.297, 210142.031, 209142.094, 208291.734, 208570.0, 208216.641, 208097.781, 207230.484, 206664.641, 206499.203, 205611.766, 205864.938, 205674.547, 205766.812, 204548.969, 204202.094, 204207.594, 203727.906, 203955.562, 202425.547, 202332.375, 201944.953, 201322.609, 202230.547, 200286.922, 200686.516, 200755.484, 200140.922, 200025.641, 199201.156, 199957.641, 198535.094, 198165.297, 198379.141, 198731.875, 198424.75, 198159.344, 196791.328, 196780.828, 197552.656, 196535.156, 195482.359, 195643.547, 195771.469, 195893.797, 194384.172, 195195.266, 193682.188, 194642.0, 194650.703, 193342.109, 193783.062, 192593.016, 193193.391, 192586.141, 193357.953, 192273.547, 192170.125, 192342.812, 191521.016, 191774.969, 192055.281], 'val_loss': [2898.815, 2426.478, 2153.137, 2100.489, 2040.791, 2033.363, 2004.0, 1961.781, 1924.289, 1929.048, 1901.909, 1907.18, 1876.637, 1860.906, 1854.73, 1856.383, 1846.647, 1859.018, 1840.677, 1825.784, 1828.502, 1815.325, 1809.75, 1797.986, 1810.08, 1804.535, 1803.209, 1797.557, 1823.923, 1797.275, 1791.051, 1807.383, 1804.485, 1771.413, 1806.383, 1758.437, 1778.39, 1789.059, 1763.983, 1767.195, 1771.475, 1771.236, 1746.128, 1756.576, 1760.644, 1747.297, 1739.875, 1731.642, 1749.949, 1729.125, 1735.956, 1734.176, 1740.318, 1740.224, 1733.974, 1735.178, 1744.326, 1759.571, 1723.705, 1747.781, 1766.321, 1727.525, 1742.306, 1739.167, 1714.46, 1736.091, 1715.962, 1745.061, 1718.904, 1729.85, 1725.949, 1724.676, 1700.38, 1716.895, 1711.773, 1728.097, 1713.42, 1724.231, 1726.184, 1702.749, 1713.732, 1724.653, 1712.939, 1727.062, 1729.046, 1701.634, 1699.909, 1714.378, 1708.098, 1725.378, 1726.699, 1708.036, 1720.123, 1712.437, 1698.522, 1693.091, 1695.762, 1719.222, 1713.407, 1695.45]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:39 epochs:100	100	2000	True	24204.29688		416732	11	-1	192.63242268562317	{'train_loss': [580288.312, 378023.406, 312836.156, 284076.75, 270833.156, 263532.688, 258186.531, 255332.219, 252373.562, 250362.891, 248235.375, 247137.172, 244728.312, 242989.234, 242083.781, 239254.672, 237809.203, 236702.469, 234824.641, 232874.5, 231027.234, 228632.531, 226505.641, 225533.641, 224522.359, 222996.172, 221770.484, 220966.938, 219620.219, 218655.281, 217803.609, 217444.188, 217128.859, 216115.719, 216173.188, 214893.062, 214407.281, 213991.188, 213865.875, 213264.625, 212516.156, 211885.719, 211354.875, 211908.641, 210647.969, 210633.766, 209854.156, 209178.562, 209820.047, 209384.031, 207783.297, 207086.375, 207357.406, 206485.406, 206379.266, 205834.469, 205153.234, 205842.219, 203901.531, 204396.094, 204038.141, 204743.422, 203464.625, 203402.5, 203041.656, 202280.984, 201854.656, 201371.5, 202610.969, 201197.016, 201289.0, 200317.328, 199766.719, 200423.562, 199220.25, 199135.422, 198736.344, 199440.266, 198463.188, 198253.578, 198484.031, 198187.812, 197462.188, 197272.359, 197968.625, 196694.609, 197198.016, 196581.609, 196896.844, 196765.016, 196471.969, 195383.094, 195397.0, 196222.016, 195165.0, 194784.297, 194442.016, 194807.219, 194153.094, 194250.062], 'val_loss': [4512.191, 3798.69, 3532.718, 3409.772, 3230.159, 3229.826, 3148.327, 3166.702, 3113.763, 3084.245, 3053.723, 3073.517, 3014.156, 3020.891, 2986.424, 2972.786, 2980.907, 2986.867, 2972.894, 2929.439, 2889.403, 2917.039, 2884.776, 2854.456, 2822.943, 2832.62, 2814.555, 2835.264, 2803.881, 2788.049, 2798.576, 2780.849, 2768.844, 2756.449, 2763.588, 2784.171, 2739.368, 2741.671, 2753.278, 2726.153, 2713.835, 2796.809, 2730.732, 2735.409, 2710.47, 2735.938, 2733.577, 2658.106, 2676.903, 2669.053, 2646.472, 2643.031, 2702.106, 2680.082, 2647.591, 2636.011, 2615.621, 2646.835, 2645.929, 2639.777, 2616.657, 2622.285, 2641.583, 2638.684, 2637.908, 2630.925, 2627.732, 2647.608, 2605.71, 2589.979, 2589.683, 2570.482, 2590.142, 2615.445, 2575.612, 2579.723, 2565.075, 2558.97, 2593.921, 2569.595, 2554.068, 2549.532, 2601.935, 2585.445, 2545.945, 2539.895, 2521.306, 2574.403, 2532.503, 2550.27, 2544.847, 2570.077, 2558.841, 2509.602, 2528.213, 2543.88, 2529.819, 2509.788, 2529.977, 2520.963]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:15 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:5 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.11819694456731768 beta1:0.8791878250950359 beta2:0.9875110116535035 weight_decay:4.029597134693598e-05 batch_size:26 epochs:100	100	1000	True	131127.53125		2183932	11	-1	246.93326497077942	{'train_loss': [1107836.875, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1149235.75, 1127928.625, 1062803.375, 1210139.25, 1062842.375, 1063727.75, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1141830.875, 1129760.625, 1102785.375, 1093654.25, 1091586.375, 1119182.625, 1089503.125, 1062587.875, 1068726.75, 1066275.125, 1069878.125, 1074085.0, 1086784.375, 1070551.0, 1070889.25, 1080326.0, 1081605.875, 1081431.125, 1079009.625, 1080498.875, 1073479.0, 1064270.75, 1087863.75, 1065116.75, 1066958.375, 1065355.5, 1070036.875, 1078304.5, 1073772.75, 1066774.0, 1083227.875, 1080252.25, 1087123.875, 1079972.75, 1089501.125, 1078348.875, 1067687.75, 1063303.875, 1063634.375, 1062618.75, 1067211.875, 1082325.125, 1076883.875, 1068435.75, 1076281.25, 1074889.5, 1084205.0, 1090766.375, 1079267.75, 1087985.125, 1083142.5, 1072488.0, 1064659.625, 1068149.75, 1076185.625, 1062573.375, 1068676.375, 1062565.125, 1076803.75, 1068895.5, 1080714.625, 1079154.875, 1078599.5, 1082525.375, 1097468.375, 1084977.0, 1074444.375, 1078930.125, 1079837.25, 1067602.875, 1071444.0, 1064331.625, 1072994.75, 1064289.25, 1062651.625, 1081011.375, 1066361.375, 1082739.125, 1088467.5, 1086817.5, 1079621.125, 1086392.25], 'val_loss': [8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 230902.078, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 11163.512, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:3 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:rmsprop lr:0.14229653625215455 alpha:0.9921583701015243 weight_decay:3.0480411701156144e-05 batch_size:26 epochs:100	100	1000	True	131122.375		760328	11	-1	231.5077769756317	{'train_loss': [1510469.5, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1069654.75, 7529474.5, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 26829184.0, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 4957016.5, 1062567.25, 8399095.0, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1072798.625, 2232146.75, 7518874.5, 1072589.125, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062616.875, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1111315.125, 1062567.25, 1062567.25, 1062567.25, 1591387.0, 1200146.25, 1283208.0, 1090977.25, 1423095.5, 1718083.125], 'val_loss': [8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 2135824.5, 22315.438, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 72639232.0, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349]}	100	100	True
