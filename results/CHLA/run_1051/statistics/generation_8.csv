id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	24876.03906		619010	12	-1	207.22161221504211	{'train_loss': [636241.875, 405001.812, 306666.219, 270735.75, 258067.625, 250956.094, 247048.656, 243263.797, 240947.859, 239006.828, 237334.5, 234918.953, 234410.062, 232977.734, 231692.766, 229778.312, 229405.75, 228032.031, 226037.797, 225971.172, 224697.172, 223969.672, 222584.203, 222047.625, 220902.922, 220750.828, 219353.266, 218245.062, 218281.047, 217221.547, 216613.984, 216527.141, 215937.438, 214484.594, 213699.266, 213537.734, 213316.859, 211929.234, 212526.219, 211734.344, 211916.297, 210686.406, 211241.219, 210597.281, 210172.312, 209413.484, 209443.016, 208978.672, 209032.812, 208039.016, 207727.484, 207141.969, 206883.469, 207163.812, 206122.75, 206801.75, 205623.656, 205753.453, 204592.984, 205340.391, 204405.312, 203812.484, 203835.109, 204035.953, 203946.359, 203839.625, 202771.906, 202254.344, 202284.984, 202642.531, 201173.984, 200554.219, 201163.359, 201437.062, 201532.234, 200565.828, 199295.266, 199890.297, 198571.516, 200472.172, 199855.078, 198813.438, 198486.938, 198127.641, 198848.906, 198797.516, 198342.984, 197546.438, 198194.078, 196570.672, 196745.922, 196341.875, 195802.875, 196400.938, 195771.516, 196953.938, 195606.25, 196381.656, 195606.656, 194726.891], 'val_loss': [4370.647, 3374.07, 2833.447, 2813.476, 2682.862, 2589.664, 2566.591, 2600.5, 2546.072, 2541.939, 2497.559, 2474.26, 2483.701, 2461.237, 2469.587, 2515.504, 2440.49, 2489.743, 2456.848, 2431.44, 2386.666, 2362.983, 2394.466, 2382.789, 2355.654, 2361.148, 2365.178, 2354.252, 2345.098, 2343.439, 2371.432, 2360.339, 2322.174, 2347.194, 2344.613, 2328.869, 2308.321, 2331.152, 2305.755, 2298.643, 2329.139, 2273.618, 2307.133, 2301.43, 2320.414, 2292.841, 2291.406, 2308.682, 2274.087, 2291.838, 2313.881, 2298.627, 2275.746, 2285.334, 2286.193, 2265.11, 2309.668, 2299.232, 2276.625, 2269.548, 2272.117, 2258.876, 2239.307, 2264.117, 2263.516, 2250.619, 2259.577, 2224.795, 2241.468, 2247.547, 2276.691, 2223.68, 2283.541, 2228.462, 2250.689, 2209.382, 2246.729, 2239.437, 2213.944, 2195.93, 2190.92, 2180.682, 2198.0, 2225.602, 2207.742, 2192.586, 2213.263, 2204.687, 2198.19, 2206.157, 2215.299, 2184.421, 2195.868, 2160.584, 2170.978, 2205.229, 2156.636, 2188.173, 2190.526, 2181.107]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	25584.50391		829999	13	-1	214.7666254043579	{'train_loss': [861680.188, 466624.969, 332366.219, 270399.906, 259211.938, 253603.25, 248253.562, 244197.766, 241569.141, 239756.234, 236706.578, 235025.078, 234423.719, 232982.766, 231236.016, 230426.75, 230089.484, 228597.672, 227704.969, 227404.609, 226535.906, 224958.547, 224407.547, 223251.688, 222530.172, 221649.625, 221835.422, 219884.125, 219397.234, 218346.125, 218026.094, 218110.266, 217400.031, 216472.125, 215045.438, 215422.031, 213973.719, 213794.766, 213301.594, 211811.266, 211880.578, 211833.672, 211155.797, 210127.906, 209825.781, 209017.266, 209231.016, 207421.266, 207522.5, 207749.484, 206986.531, 205400.266, 205485.062, 204978.094, 205074.516, 204532.234, 203919.719, 204377.109, 203580.0, 203577.281, 202960.203, 202851.938, 202971.516, 201706.375, 201377.062, 201055.125, 202131.875, 200468.281, 200132.094, 200152.906, 200038.984, 200118.188, 199776.219, 199935.766, 199647.891, 199111.641, 199159.016, 198784.141, 198382.266, 198559.859, 198254.031, 197962.984, 198212.453, 197488.984, 197706.828, 196238.531, 197124.766, 196242.078, 196640.266, 196173.812, 195346.562, 195670.234, 195676.062, 195672.469, 194973.969, 194410.656, 194458.281, 194385.859, 193322.828, 194941.812], 'val_loss': [4280.865, 3498.349, 2913.937, 2788.038, 2673.504, 2576.406, 2565.775, 2550.262, 2491.701, 2509.143, 2513.789, 2479.531, 2466.93, 2469.712, 2450.427, 2475.723, 2445.302, 2428.275, 2437.312, 2420.102, 2420.966, 2445.307, 2433.27, 2404.87, 2385.489, 2375.666, 2358.783, 2385.78, 2365.198, 2360.172, 2381.42, 2339.565, 2341.623, 2335.245, 2354.746, 2318.985, 2371.151, 2320.399, 2318.667, 2331.057, 2341.12, 2320.612, 2323.969, 2318.883, 2327.147, 2317.886, 2302.56, 2317.368, 2316.936, 2284.713, 2295.145, 2316.148, 2290.139, 2315.977, 2301.338, 2320.544, 2319.398, 2317.774, 2267.902, 2293.326, 2287.178, 2289.7, 2265.067, 2281.419, 2280.278, 2277.003, 2266.774, 2255.772, 2284.707, 2263.653, 2274.109, 2230.16, 2271.646, 2269.327, 2243.227, 2228.708, 2241.455, 2232.77, 2259.12, 2257.908, 2253.721, 2242.205, 2231.487, 2256.182, 2234.969, 2212.684, 2228.974, 2224.964, 2238.559, 2221.039, 2235.497, 2248.501, 2240.07, 2211.396, 2222.085, 2203.183, 2221.975, 2185.388, 2221.068, 2212.212]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:61 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	131135.64062		5413675	12	-1	209.47645258903503	{'train_loss': [1109677.0, 1161877.75, 1063533.75, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125], 'val_loss': [11110.434, 12596.661, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:40 epochs:100	100	1000	True	25810.14258		703006	11	-1	179.9396951198578	{'train_loss': [997090.812, 496301.812, 341181.219, 319454.938, 305984.188, 295690.406, 290013.25, 286013.844, 280205.656, 274307.75, 267404.062, 260548.188, 257194.859, 253127.078, 248991.984, 248090.016, 247041.688, 244306.141, 243652.719, 242210.766, 241137.531, 240118.375, 238839.391, 238469.469, 236477.312, 235558.141, 234878.516, 234826.984, 233807.547, 233388.406, 233026.75, 231437.016, 231399.469, 230160.219, 230236.375, 228380.281, 228669.281, 227881.438, 227573.891, 226610.844, 226015.141, 225393.609, 225222.438, 224501.531, 223788.453, 223065.578, 223385.75, 221630.781, 222204.562, 221844.078, 220535.047, 220586.156, 220445.656, 220062.047, 218319.953, 218240.594, 218535.25, 217052.969, 216955.141, 217362.156, 215717.297, 216011.172, 216278.625, 214261.344, 214895.609, 215354.812, 213324.25, 213170.562, 212675.438, 214053.906, 213021.984, 212592.812, 211415.828, 211005.484, 211056.906, 211071.781, 210634.141, 210114.906, 210687.0, 210715.422, 208563.594, 210041.281, 209389.656, 208092.047, 209394.688, 208480.359, 207731.047, 207051.031, 208103.203, 207022.297, 207536.562, 207131.016, 207602.891, 206343.969, 206404.438, 205989.719, 204465.469, 205061.156, 205245.688, 205927.438], 'val_loss': [7930.172, 4051.958, 4059.474, 3755.856, 3557.643, 3423.438, 3384.047, 3333.524, 3259.705, 3155.162, 3130.336, 3077.49, 3062.226, 3079.21, 3034.943, 3029.117, 3034.31, 2970.647, 2997.327, 3013.456, 3081.99, 2940.136, 2999.381, 2942.962, 2955.548, 2957.522, 2955.053, 2938.519, 2961.029, 2966.769, 2913.37, 2942.677, 2933.801, 2880.118, 2909.767, 2866.073, 2922.491, 2848.457, 2914.011, 2844.024, 2883.683, 2876.469, 2851.894, 2842.988, 2905.726, 2862.984, 2839.593, 2822.573, 2863.635, 2831.049, 2828.304, 2820.081, 2796.713, 2802.935, 2810.259, 2810.471, 2817.142, 2798.558, 2785.598, 2803.975, 2837.915, 2801.694, 2793.186, 2800.59, 2759.886, 2780.288, 2772.566, 2763.154, 2801.835, 2769.823, 2770.104, 2740.06, 2764.85, 2761.875, 2740.118, 2770.06, 2790.64, 2777.929, 2771.81, 2785.175, 2762.472, 2751.959, 2743.576, 2758.173, 2784.563, 2761.461, 2739.124, 2761.367, 2748.611, 2739.032, 2759.175, 2748.045, 2735.268, 2747.032, 2745.057, 2762.786, 2733.458, 2759.41, 2746.446, 2746.112]}	100	100	True
