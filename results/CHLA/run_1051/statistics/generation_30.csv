id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	3000	True	24296.01562		416732	11	-1	235.83503222465515	{'train_loss': [506087.031, 328529.438, 289279.031, 273226.688, 265102.875, 260718.328, 257703.453, 256066.641, 253359.328, 250580.234, 248337.156, 246336.484, 244925.219, 242700.906, 239989.062, 238731.734, 236483.75, 234259.719, 232405.938, 230897.109, 229406.688, 227056.562, 226130.688, 225080.359, 224521.703, 223411.469, 222674.031, 220654.5, 220796.25, 220186.281, 220140.656, 218829.625, 218487.703, 217544.609, 216016.516, 216069.156, 215555.609, 215175.719, 214811.812, 214690.844, 214436.406, 213470.25, 212819.594, 212700.625, 211926.984, 212027.562, 211149.453, 211436.438, 210649.562, 210235.203, 209583.016, 209724.891, 209077.062, 208754.125, 208397.266, 207881.922, 207539.922, 207131.25, 206490.391, 206635.438, 205429.375, 204878.312, 204372.031, 204779.938, 205045.781, 204260.438, 203534.0, 204267.078, 203645.5, 202915.953, 201682.203, 202382.938, 202296.719, 203300.156, 201010.594, 201402.359, 200605.516, 199804.578, 199813.203, 199598.453, 199975.047, 199049.516, 198693.469, 198239.812, 199659.172, 197826.562, 198187.875, 198167.531, 198988.953, 196489.031, 196918.016, 197231.906, 196406.141, 196563.875, 196128.766, 195841.297, 195770.391, 195163.469, 194304.109, 193682.484], 'val_loss': [2766.787, 2434.845, 2244.612, 2158.305, 2119.771, 2098.159, 2090.347, 2083.092, 2068.43, 2074.418, 2045.756, 2033.8, 2023.356, 2029.897, 2008.511, 1963.315, 1956.022, 1973.844, 1964.414, 1896.707, 1955.352, 1897.258, 1917.621, 1885.238, 1887.674, 1888.901, 1850.066, 1851.752, 1842.893, 1847.07, 1837.612, 1846.528, 1851.284, 1813.654, 1837.687, 1828.66, 1820.288, 1822.246, 1826.324, 1807.434, 1812.156, 1808.319, 1802.789, 1798.746, 1807.571, 1801.624, 1796.143, 1797.389, 1798.547, 1786.859, 1785.17, 1799.22, 1788.136, 1776.472, 1780.366, 1757.694, 1766.043, 1771.536, 1752.977, 1752.994, 1732.169, 1756.105, 1759.186, 1739.244, 1773.804, 1757.566, 1752.343, 1725.187, 1742.976, 1734.434, 1726.596, 1733.926, 1721.158, 1715.365, 1745.982, 1733.332, 1718.57, 1731.316, 1734.811, 1721.316, 1709.417, 1727.141, 1695.497, 1701.294, 1709.188, 1708.374, 1709.548, 1685.114, 1700.757, 1705.342, 1683.751, 1710.073, 1698.396, 1696.7, 1712.066, 1680.668, 1664.944, 1676.217, 1687.362, 1708.343]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	3000	True	23636.0332		416732	11	-1	238.74832391738892	{'train_loss': [515649.562, 326932.219, 281333.812, 262309.25, 254005.828, 248718.141, 243661.562, 240558.625, 236999.547, 233538.938, 231892.641, 229995.062, 228565.875, 226684.406, 226128.891, 224799.891, 224107.422, 222089.391, 222211.547, 220477.922, 220502.969, 219525.484, 218303.172, 217520.562, 217348.125, 216545.375, 215918.516, 215732.172, 215236.516, 214944.547, 213884.844, 213749.984, 213068.312, 212736.078, 211892.359, 211540.922, 210774.953, 210057.156, 211126.297, 210142.031, 209142.094, 208291.734, 208570.0, 208216.641, 208097.781, 207230.484, 206664.641, 206499.203, 205611.766, 205864.938, 205674.547, 205766.812, 204548.969, 204202.094, 204207.594, 203727.906, 203955.562, 202425.547, 202332.375, 201944.953, 201322.609, 202230.547, 200286.922, 200686.516, 200755.484, 200140.922, 200025.641, 199201.156, 199957.641, 198535.094, 198165.297, 198379.141, 198731.875, 198424.75, 198159.344, 196791.328, 196780.828, 197552.656, 196535.156, 195482.359, 195643.547, 195771.469, 195893.797, 194384.172, 195195.266, 193682.188, 194642.0, 194650.703, 193342.109, 193783.062, 192593.016, 193193.391, 192586.141, 193357.953, 192273.547, 192170.125, 192342.812, 191521.016, 191774.969, 192055.281], 'val_loss': [2898.815, 2426.478, 2153.137, 2100.489, 2040.791, 2033.363, 2004.0, 1961.781, 1924.289, 1929.048, 1901.909, 1907.18, 1876.637, 1860.906, 1854.73, 1856.383, 1846.647, 1859.018, 1840.677, 1825.784, 1828.502, 1815.325, 1809.75, 1797.986, 1810.08, 1804.535, 1803.209, 1797.557, 1823.923, 1797.275, 1791.051, 1807.383, 1804.485, 1771.413, 1806.383, 1758.437, 1778.39, 1789.059, 1763.983, 1767.195, 1771.475, 1771.236, 1746.128, 1756.576, 1760.644, 1747.297, 1739.875, 1731.642, 1749.949, 1729.125, 1735.956, 1734.176, 1740.318, 1740.224, 1733.974, 1735.178, 1744.326, 1759.571, 1723.705, 1747.781, 1766.321, 1727.525, 1742.306, 1739.167, 1714.46, 1736.091, 1715.962, 1745.061, 1718.904, 1729.85, 1725.949, 1724.676, 1700.38, 1716.895, 1711.773, 1728.097, 1713.42, 1724.231, 1726.184, 1702.749, 1713.732, 1724.653, 1712.939, 1727.062, 1729.046, 1701.634, 1699.909, 1714.378, 1708.098, 1725.378, 1726.699, 1708.036, 1720.123, 1712.437, 1698.522, 1693.091, 1695.762, 1719.222, 1713.407, 1695.45]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:14 epochs:100	100	1000	True	24350.23438		416732	11	-1	348.3081967830658	{'train_loss': [435300.406, 287677.531, 266145.219, 258583.859, 252911.422, 249470.969, 246340.328, 243918.297, 241226.391, 239388.328, 236597.906, 235158.578, 232941.906, 230737.594, 229564.688, 228024.266, 226491.984, 225482.672, 224803.0, 223855.406, 223655.922, 221921.016, 220276.562, 219883.75, 219023.547, 218016.844, 218147.969, 216871.391, 215810.062, 215735.812, 215056.016, 214684.156, 213960.359, 213118.0, 213024.203, 212481.594, 211720.906, 210865.25, 210858.562, 209857.984, 210706.516, 209216.438, 209468.016, 207871.641, 208131.188, 208168.25, 207432.75, 207617.0, 206739.203, 206544.453, 206763.609, 205763.203, 204803.688, 205048.562, 206025.062, 205055.562, 204323.438, 203316.922, 203517.734, 203214.812, 203303.438, 203693.828, 203620.156, 202906.859, 201660.438, 202187.922, 200988.859, 201451.859, 200655.422, 200722.969, 201016.203, 200830.641, 199933.031, 199593.234, 199812.25, 199884.812, 199461.547, 198731.969, 197717.812, 198010.703, 197540.172, 197852.562, 197167.75, 197593.016, 197403.141, 197090.547, 196516.281, 197445.125, 197011.906, 195816.406, 197348.125, 194854.703, 195023.531, 195142.984, 195171.109, 195162.922, 193469.656, 194812.438, 194354.734, 193737.703], 'val_loss': [1403.025, 1233.748, 1193.439, 1165.307, 1146.44, 1136.425, 1130.951, 1111.485, 1106.274, 1099.644, 1086.973, 1089.975, 1096.805, 1083.562, 1065.583, 1067.454, 1070.998, 1055.036, 1057.828, 1066.176, 1051.025, 1047.336, 1042.987, 1049.487, 1038.466, 1047.752, 1044.148, 1037.636, 1041.552, 1033.231, 1019.91, 1031.41, 1031.515, 1003.785, 1020.092, 1019.19, 1004.333, 1007.82, 1016.729, 992.815, 1005.475, 1001.833, 989.996, 1001.788, 992.68, 979.671, 985.55, 1004.572, 989.654, 981.782, 996.157, 984.247, 993.771, 975.489, 990.043, 989.635, 987.308, 973.528, 981.305, 989.324, 991.674, 978.488, 977.971, 967.709, 988.658, 964.959, 979.944, 984.524, 976.788, 969.784, 963.734, 969.531, 961.104, 966.057, 969.993, 974.027, 964.627, 981.642, 983.385, 967.161, 957.331, 978.698, 973.376, 972.12, 966.276, 971.355, 959.679, 953.127, 954.866, 958.831, 952.219, 966.154, 946.282, 947.935, 953.367, 960.309, 958.196, 971.719, 958.673, 959.027]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:18 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	3000	True	23939.36719		724204	11	-1	237.78507900238037	{'train_loss': [924183.812, 454791.125, 309913.344, 273314.0, 263241.344, 257906.109, 254539.188, 252398.938, 249500.891, 246887.188, 244262.75, 241399.828, 238568.234, 237783.969, 234639.219, 232894.859, 231176.75, 228992.188, 227149.719, 225537.484, 223282.328, 222403.062, 221729.312, 220640.344, 219475.156, 219549.047, 218090.312, 217026.562, 216632.766, 215221.469, 214551.719, 214971.703, 214740.234, 212981.797, 213178.875, 211716.203, 210891.828, 211262.312, 210018.891, 209789.766, 208433.359, 208336.438, 207712.297, 207699.906, 206818.531, 206735.938, 205366.781, 205000.969, 205335.672, 205352.656, 203796.141, 202220.172, 204019.219, 202880.812, 202669.125, 202601.094, 200830.703, 201779.172, 202071.25, 200606.219, 200244.719, 200179.734, 199324.516, 200208.734, 198736.734, 198185.172, 198227.234, 198671.094, 197841.984, 197135.047, 196491.469, 196778.438, 196544.953, 196005.297, 195538.156, 195963.078, 195331.484, 195391.828, 195013.938, 194734.859, 196070.281, 193644.797, 193948.547, 193310.703, 194453.969, 193325.859, 193591.031, 193495.781, 192786.266, 192488.359, 191308.516, 191567.922, 190750.0, 191922.109, 192000.594, 189442.734, 190392.609, 190300.047, 190242.578, 189969.828], 'val_loss': [3567.358, 2717.325, 2284.674, 2169.455, 2130.56, 2090.186, 2065.809, 2034.747, 2030.346, 2030.854, 1984.093, 1956.854, 1954.111, 1947.563, 1972.291, 1914.723, 1899.975, 1891.497, 1901.649, 1872.305, 1835.29, 1850.723, 1831.588, 1833.659, 1844.034, 1803.798, 1812.922, 1804.44, 1805.675, 1816.065, 1792.16, 1820.357, 1803.861, 1834.463, 1811.652, 1807.203, 1781.063, 1796.631, 1787.906, 1804.629, 1779.867, 1772.137, 1763.639, 1782.385, 1770.62, 1772.423, 1779.646, 1768.592, 1745.469, 1771.501, 1739.204, 1769.581, 1743.915, 1733.337, 1746.55, 1748.039, 1731.187, 1747.245, 1732.36, 1734.294, 1731.145, 1747.723, 1734.19, 1739.871, 1720.529, 1709.719, 1722.511, 1703.048, 1734.578, 1747.625, 1723.091, 1732.515, 1719.02, 1760.44, 1725.624, 1716.937, 1720.43, 1714.254, 1716.471, 1724.213, 1706.865, 1706.32, 1723.371, 1726.377, 1694.192, 1715.477, 1705.778, 1709.231, 1711.25, 1703.814, 1714.284, 1707.018, 1700.148, 1738.906, 1701.556, 1703.201, 1700.506, 1731.862, 1723.744, 1703.861]}	0	100	True
