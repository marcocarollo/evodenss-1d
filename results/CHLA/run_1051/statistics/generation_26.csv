id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:34 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	24420.17188		379725	11	-1	266.6485550403595	{'train_loss': [511154.281, 359515.344, 316816.219, 284112.75, 265936.531, 259601.922, 254633.531, 251265.297, 248352.609, 245953.312, 245228.125, 242616.938, 241484.781, 240229.703, 238563.734, 237748.375, 236609.312, 235729.234, 234584.047, 233860.312, 233101.406, 232311.062, 231725.219, 230669.656, 229545.406, 228791.703, 226963.359, 226118.359, 224876.469, 223575.172, 221492.75, 221216.281, 220356.328, 219611.219, 218309.281, 217607.469, 217975.594, 216460.172, 215017.875, 214578.453, 214875.484, 214377.016, 212968.922, 212867.969, 211672.859, 212012.734, 211323.297, 211493.844, 210340.75, 209415.859, 209109.422, 209114.547, 209511.391, 207538.203, 207935.812, 207731.016, 206600.625, 206070.344, 206305.547, 206633.234, 205485.703, 205082.453, 203841.734, 205210.453, 204782.625, 203933.875, 204010.094, 203845.391, 203319.562, 203693.594, 202148.391, 202378.594, 201727.281, 201824.125, 201707.344, 202327.422, 201687.0, 200149.625, 200176.812, 199986.422, 200119.297, 200469.016, 199389.719, 199885.734, 197927.172, 197245.125, 199115.453, 198239.703, 198224.906, 197968.969, 197013.844, 197744.281, 197424.641, 197020.594, 197653.422, 196487.906, 195965.844, 196534.469, 196673.953, 196047.938], 'val_loss': [3108.761, 2657.102, 2456.916, 2236.824, 2161.064, 2145.496, 2112.085, 2060.778, 2057.731, 2044.639, 2031.137, 2011.425, 2019.676, 1980.447, 2003.834, 1989.31, 1979.526, 1993.881, 1977.279, 1958.864, 1957.007, 1962.518, 1965.219, 1945.833, 1943.32, 1921.871, 1917.091, 1919.484, 1906.227, 1900.967, 1877.011, 1913.605, 1859.767, 1861.285, 1885.989, 1843.228, 1852.067, 1856.481, 1836.07, 1824.066, 1831.599, 1818.78, 1801.245, 1800.584, 1804.911, 1810.778, 1791.921, 1793.797, 1777.875, 1790.861, 1770.152, 1776.818, 1769.674, 1816.446, 1787.697, 1755.731, 1764.689, 1780.704, 1778.872, 1771.415, 1775.872, 1739.178, 1742.851, 1753.128, 1743.017, 1766.46, 1747.818, 1753.978, 1739.093, 1743.285, 1718.758, 1745.203, 1738.046, 1760.498, 1731.993, 1710.425, 1730.222, 1717.45, 1747.748, 1700.659, 1725.992, 1712.02, 1708.255, 1725.733, 1707.619, 1685.861, 1725.996, 1724.399, 1720.879, 1710.934, 1706.383, 1702.651, 1732.619, 1686.626, 1704.493, 1724.761, 1722.7, 1725.273, 1712.423, 1699.568]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:116 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:26 epochs:100	100	1000	True	131139.0625		5348816	10	-1	269.43928480148315	{'train_loss': [1135484.125, 1063612.125, 1062567.25, 1062567.25, 1062567.25, 1072118.125, 1063740.625, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25], 'val_loss': [8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:62 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:34 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	23755.98633		382704	11	-1	229.87190008163452	{'train_loss': [499373.625, 320086.969, 279111.406, 266188.062, 260334.078, 256930.75, 252766.703, 248156.25, 245003.25, 240969.062, 238023.188, 236560.156, 232161.594, 231407.703, 229802.953, 228653.203, 226870.797, 226279.891, 226199.922, 224714.594, 222520.656, 222290.859, 221797.188, 220666.844, 219691.359, 218880.219, 217912.766, 217912.891, 216717.812, 216039.75, 216131.016, 215661.312, 213494.625, 213386.094, 212928.531, 212501.969, 211828.594, 210570.719, 211606.25, 209837.297, 210023.609, 209011.156, 208620.297, 208803.812, 208270.922, 206914.141, 207385.859, 207219.188, 206431.75, 205896.641, 205133.906, 204503.641, 204451.969, 204774.672, 204128.0, 203708.406, 203961.344, 203132.25, 202836.016, 203191.078, 202225.859, 202485.406, 202144.766, 201611.188, 200115.531, 200396.688, 201467.578, 200570.938, 200354.562, 199409.938, 199406.25, 199105.609, 198212.203, 198060.266, 197415.031, 198546.047, 198096.922, 197908.781, 198052.359, 197681.719, 196715.203, 196468.672, 196586.484, 196841.406, 196063.688, 195143.141, 195074.844, 195226.828, 195268.781, 195994.953, 193934.828, 194182.953, 193878.5, 194261.875, 193744.531, 194177.344, 193165.438, 193013.5, 193466.594, 192664.953], 'val_loss': [2984.598, 2379.631, 2245.661, 2169.921, 2121.789, 2066.46, 2041.206, 2002.065, 1998.345, 1981.641, 1981.911, 1939.971, 1955.508, 1927.619, 1925.139, 1943.738, 1916.411, 1951.855, 1900.32, 1895.337, 1904.111, 1878.388, 1879.462, 1862.576, 1885.807, 1859.419, 1844.758, 1853.154, 1841.189, 1835.589, 1838.368, 1829.262, 1824.387, 1818.542, 1823.776, 1817.628, 1823.134, 1802.147, 1790.22, 1793.076, 1790.866, 1792.599, 1779.749, 1760.064, 1773.583, 1778.434, 1785.23, 1772.073, 1769.59, 1771.319, 1775.635, 1755.297, 1771.335, 1755.846, 1767.39, 1763.499, 1751.944, 1756.023, 1749.329, 1757.411, 1737.876, 1743.172, 1746.948, 1727.873, 1745.769, 1729.342, 1745.918, 1721.682, 1735.135, 1735.888, 1731.134, 1724.022, 1725.979, 1715.979, 1726.551, 1710.793, 1730.637, 1713.196, 1721.467, 1718.195, 1699.628, 1698.338, 1689.993, 1714.885, 1710.992, 1701.036, 1709.149, 1702.54, 1720.546, 1702.188, 1708.063, 1708.491, 1719.459, 1742.571, 1694.837, 1695.764, 1708.734, 1686.245, 1686.733, 1685.697]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	23583.20312		416732	11	-1	238.03556609153748	{'train_loss': [510125.188, 324876.5, 285739.344, 267789.719, 258384.062, 253269.281, 249423.953, 246874.078, 244144.422, 241525.297, 238929.594, 237136.906, 235576.234, 233720.453, 231460.5, 230554.562, 229303.531, 227911.328, 226228.672, 225249.031, 224661.516, 223274.391, 222816.156, 222303.719, 221471.016, 219957.875, 219221.516, 218437.703, 218480.953, 217870.922, 216765.281, 217057.141, 215203.562, 214860.078, 214195.891, 214101.172, 213262.062, 212621.812, 212237.203, 212436.359, 211506.719, 211712.234, 210792.531, 210294.312, 209850.344, 210014.625, 209403.484, 208185.125, 207960.25, 207016.422, 207810.594, 206548.375, 205832.344, 205839.922, 205386.703, 206082.156, 203832.766, 204737.641, 204134.891, 203442.625, 203639.0, 203415.203, 203078.562, 202449.984, 202719.188, 201322.844, 201306.781, 201106.562, 201101.125, 200741.719, 200972.156, 200781.891, 199524.25, 199684.594, 199646.0, 198536.156, 198438.516, 199218.562, 199170.719, 197973.625, 197421.078, 198164.891, 197317.062, 196643.562, 196680.281, 197095.922, 196189.312, 196322.953, 196589.875, 195171.703, 195887.641, 195600.0, 194699.031, 195165.469, 195108.922, 194188.125, 193584.906, 193668.906, 193415.641, 193385.844], 'val_loss': [3348.703, 2383.443, 2273.306, 2135.918, 2094.101, 2075.367, 2037.183, 2018.26, 1999.883, 1983.685, 2006.616, 1984.47, 1946.004, 1978.657, 1935.555, 1928.181, 1910.463, 1919.909, 1910.694, 1892.806, 1887.149, 1878.726, 1868.851, 1860.882, 1871.484, 1854.099, 1839.492, 1833.373, 1827.412, 1829.474, 1816.536, 1823.983, 1825.628, 1792.258, 1788.197, 1787.618, 1801.856, 1774.448, 1790.537, 1789.351, 1779.352, 1771.639, 1795.948, 1774.387, 1763.2, 1767.386, 1770.751, 1757.072, 1752.479, 1755.694, 1754.912, 1741.83, 1748.381, 1747.673, 1749.064, 1739.421, 1739.868, 1732.017, 1739.031, 1726.828, 1734.249, 1711.202, 1724.757, 1714.812, 1711.258, 1717.883, 1700.852, 1707.327, 1720.403, 1714.907, 1718.947, 1721.462, 1691.548, 1701.791, 1707.086, 1693.957, 1694.63, 1697.617, 1699.452, 1700.843, 1672.876, 1676.142, 1686.892, 1710.115, 1699.534, 1697.333, 1710.026, 1676.089, 1688.401, 1682.436, 1684.815, 1681.626, 1688.201, 1676.96, 1676.494, 1679.093, 1684.797, 1690.531, 1680.473, 1678.338]}	100	100	True
