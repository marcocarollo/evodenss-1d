id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	25512.01758		639490	13	-1	207.71330499649048	{'train_loss': [634817.125, 408838.781, 332820.188, 291416.938, 264651.469, 254674.797, 248335.891, 243724.734, 240398.828, 237114.688, 235921.141, 234036.375, 232636.203, 231427.984, 230516.922, 229518.453, 227528.312, 227429.078, 226271.188, 224938.547, 224941.938, 225159.703, 223777.016, 223473.016, 223058.844, 222584.828, 221564.781, 221893.141, 220113.297, 220653.844, 220225.609, 219343.188, 218945.516, 218537.109, 217811.75, 217792.406, 217649.125, 216685.156, 217365.266, 217195.406, 215977.234, 215727.516, 214855.75, 214687.469, 216611.688, 214450.125, 214988.578, 214180.703, 213570.203, 213541.906, 213685.188, 212826.391, 212868.25, 212881.844, 212817.391, 212961.422, 212671.672, 212082.484, 213267.453, 211737.859, 211616.203, 211190.656, 210164.5, 210706.984, 211019.469, 210192.781, 209360.625, 209281.375, 209433.391, 209686.734, 209052.25, 208599.906, 209669.422, 208854.578, 208435.734, 208580.641, 208421.172, 207838.922, 207257.188, 207526.359, 207374.797, 207658.484, 207470.812, 207401.234, 205815.672, 206066.469, 206863.328, 206260.516, 206535.375, 205422.641, 205210.5, 204451.031, 204462.031, 204498.531, 203534.484, 204559.203, 204205.078, 204515.75, 204353.094, 204358.75], 'val_loss': [3754.46, 3560.356, 3139.332, 2870.392, 2653.635, 2713.828, 2589.381, 2567.443, 2495.98, 2522.812, 2482.255, 2475.417, 2511.198, 2447.829, 2410.135, 2439.024, 2445.08, 2478.596, 2391.271, 2392.167, 2405.826, 2433.181, 2361.051, 2363.418, 2354.955, 2356.505, 2369.52, 2345.398, 2372.43, 2329.357, 2344.571, 2366.011, 2369.404, 2352.908, 2354.916, 2326.985, 2330.191, 2344.951, 2375.923, 2331.245, 2350.015, 2332.135, 2336.626, 2363.222, 2317.308, 2318.214, 2325.125, 2326.79, 2313.459, 2299.969, 2318.753, 2364.991, 2328.436, 2321.831, 2308.095, 2313.234, 2310.306, 2352.72, 2332.62, 2316.495, 2301.645, 2282.302, 2359.713, 2320.744, 2292.988, 2302.418, 2295.925, 2327.386, 2306.026, 2335.028, 2295.642, 2330.281, 2312.937, 2292.492, 2324.027, 2299.162, 2294.857, 2333.216, 2273.142, 2299.015, 2308.413, 2266.915, 2284.016, 2288.308, 2295.938, 2276.529, 2292.232, 2281.303, 2259.219, 2280.628, 2283.694, 2284.622, 2284.279, 2286.919, 2244.911, 2283.885, 2269.027, 2264.169, 2278.09, 2283.855]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:99 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	25625.96484		790123	12	-1	204.82863354682922	{'train_loss': [887754.812, 479240.219, 364694.562, 296321.969, 278686.312, 268026.844, 258869.922, 253380.656, 248928.484, 245642.422, 242823.688, 239847.047, 238342.828, 235369.531, 234285.328, 232727.453, 231219.031, 229648.469, 229068.609, 228437.0, 226860.734, 226657.641, 225072.25, 224428.562, 223642.641, 222714.953, 222095.078, 221720.859, 220852.531, 220875.312, 220115.906, 221405.516, 219450.266, 218693.453, 218314.484, 217834.547, 216663.656, 216742.266, 216955.156, 215637.75, 215777.328, 215907.156, 215766.547, 215193.375, 214420.969, 213902.125, 213994.328, 213230.656, 212569.141, 213138.109, 211920.766, 211257.984, 211550.141, 210603.422, 209716.891, 209412.469, 210373.562, 208780.031, 208336.125, 209596.094, 208173.547, 208134.406, 207686.25, 206775.672, 207396.516, 206805.891, 206430.859, 206693.031, 206721.516, 205330.906, 205851.406, 205112.859, 205064.406, 205675.156, 204791.406, 204887.297, 204929.5, 204123.109, 204370.875, 203717.703, 203602.391, 203648.469, 203240.734, 203188.719, 202851.562, 203639.094, 203185.297, 202467.781, 201801.031, 201855.0, 202526.969, 201520.172, 200965.734, 201329.062, 201735.641, 200854.719, 200989.219, 200303.969, 200257.969, 201670.172], 'val_loss': [4382.112, 3717.586, 3085.208, 2881.682, 2784.012, 2723.148, 2605.55, 2540.941, 2513.995, 2510.292, 2481.349, 2472.846, 2449.226, 2450.557, 2433.417, 2438.682, 2423.022, 2410.385, 2433.021, 2401.239, 2429.973, 2418.044, 2396.41, 2408.716, 2393.334, 2382.512, 2377.483, 2374.637, 2360.861, 2356.415, 2368.718, 2397.123, 2375.138, 2349.323, 2398.324, 2357.176, 2355.084, 2360.816, 2339.85, 2332.971, 2328.091, 2317.264, 2334.889, 2343.615, 2330.096, 2355.003, 2328.58, 2329.692, 2323.669, 2316.712, 2298.442, 2296.449, 2323.765, 2318.417, 2329.076, 2315.682, 2321.083, 2320.566, 2308.466, 2311.259, 2303.969, 2317.396, 2286.134, 2288.077, 2291.965, 2301.803, 2298.302, 2305.843, 2312.315, 2332.615, 2285.73, 2312.598, 2299.753, 2297.994, 2316.857, 2312.028, 2273.629, 2262.357, 2282.001, 2280.179, 2294.26, 2308.246, 2262.088, 2267.545, 2263.892, 2268.972, 2276.203, 2272.965, 2276.327, 2270.19, 2271.098, 2263.633, 2239.673, 2253.093, 2247.938, 2250.592, 2234.193, 2249.115, 2230.078, 2242.244]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:18 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:50 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.14250422205805927 beta1:0.8943436434194686 beta2:0.8514307236520071 weight_decay:8.59669512261346e-05 batch_size:32 epochs:100	100	1000	True	131132.59375		4358020	13	-1	221.1457076072693	{'train_loss': [1132211.0, 1779842.0, 1198194.875, 1408940.875, 1808539.0, 1921759.0, 1894718.625, 1581911.75, 1646701.125, 1176663.125, 1243613.875, 1103088.5, 1090053.875, 1099577.0, 1107951.125, 1092098.5, 1080045.875, 1084340.75, 1089059.625, 1087652.375, 1088821.875, 1095765.125, 1085498.125, 1121819.125, 1088642.125, 1100596.25, 1092496.25, 1091360.75, 1101002.0, 1104247.375, 1088044.125, 1091665.875, 1073597.0, 1090259.25, 1090714.875, 1090442.625, 1113488.625, 1090378.25, 1091835.75, 1093698.875, 1079748.875, 1098790.625, 1090595.75, 1112838.125, 1094058.25, 1097688.375, 1082881.875, 1082461.625, 1095172.75, 1092257.25, 1094934.125, 1091494.5, 1104088.875, 1078092.75, 1095681.625, 1152550.125, 1090414.125, 1098098.125, 1106023.5, 1097919.125, 1100485.5, 1100067.25, 1084358.25, 1100992.625, 1085386.375, 1097339.75, 1076041.25, 1097266.0, 1080641.125, 1085882.375, 1082436.25, 1109879.0, 1078088.5, 1087123.625, 1085873.5, 1098738.375, 1101607.5, 1084874.0, 1087610.0, 1092947.5, 1101921.5, 1084340.25, 1121587.125, 1080604.125, 1086157.375, 1087213.875, 1100262.875, 1078961.875, 1102080.125, 1087205.625, 1092758.75, 1082506.0, 1121208.875, 1130217.25, 1092719.875, 1109236.625, 1100833.5, 1106561.0, 1094270.875, 1108900.875], 'val_loss': [11110.434, 649412.625, 16413.473, 11110.434, 11110.434, 12961.428, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 12319.617, 11110.434, 13214.216, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 22258.504, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.428, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:78 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:86 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.12265168927724195 beta1:0.968764267460535 beta2:0.8316686985141352 weight_decay:9.731876384574155e-05 batch_size:32 epochs:100	100	1000	True	131122.75		979016	14	-1	227.38367629051208	{'train_loss': [1076051.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1397574.0, 2205191.0, 1738682.5, 1068803.625, 1063840.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1099350.375, 1161843.0, 1108241.625, 1067509.875, 1062567.125, 1125549.875, 1062567.125, 1134890.0, 1337490.0, 1153921.375, 1113689.25, 1077569.375, 1062756.75, 1064527.375, 2649723.5, 6724201.0, 3328690.25, 1184684.625, 1337612.125, 1097105.375, 1073507.75, 1218881.25, 1190652.5, 1076838.875, 1152282.125, 1277154.375, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1075763.125, 1071207.5, 1062567.125, 1125594.25, 1112236.375, 1089163.375, 1163746.625, 1097162.0, 1105999.0, 1146027.5, 1130657.5, 1138578.875, 1085223.0, 1062567.125, 1062567.125, 1070087.5, 1090129.75, 1105221.25, 1062567.125, 1063017.875, 1065744.5, 1092342.0, 1062567.125, 1090938.25, 1070250.25, 1078664.25, 1069824.25, 1102223.125, 1116046.875, 1069210.125, 1063936.125, 1067990.75, 1069019.5, 1104237.75, 1077988.875, 1615972.125, 1076652.25, 1068492.625, 1062567.125, 1077128.625, 1063794.0], 'val_loss': [443332768.0, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 2514027008.0, 112616816.0, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 147496.844, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 3019655936.0, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434]}	100	100	True
