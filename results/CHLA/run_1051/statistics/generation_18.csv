id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	25375.86133		520442	11	-1	205.52752995491028	{'train_loss': [620194.812, 350980.594, 299974.75, 283504.281, 272815.469, 264473.625, 258245.469, 255378.984, 251799.516, 249010.016, 247047.203, 243884.016, 242536.203, 240388.297, 239085.125, 237629.406, 236624.156, 234791.297, 234967.766, 232681.328, 232035.578, 230878.406, 230239.078, 229496.625, 228567.094, 227530.5, 228686.453, 227275.547, 226946.422, 225551.016, 225736.25, 225120.656, 224794.75, 223139.094, 223509.812, 223699.625, 222015.047, 221951.422, 221425.141, 220967.453, 221268.078, 220625.547, 219703.672, 220366.5, 218818.469, 219251.5, 219209.734, 217849.219, 218169.0, 217392.156, 217765.875, 217359.875, 216605.828, 217122.344, 215426.016, 216225.578, 215335.531, 215231.016, 215113.453, 214496.125, 214949.422, 214323.75, 214231.312, 213693.516, 213933.641, 213207.016, 213587.719, 213915.25, 212897.125, 212940.812, 211035.219, 212373.922, 211591.906, 212796.906, 212286.781, 211186.688, 211516.875, 210714.172, 211933.469, 212151.781, 211006.203, 211104.875, 210934.609, 210352.156, 209757.156, 210330.078, 209878.328, 209620.531, 209506.031, 208174.531, 209553.625, 208793.828, 208535.672, 208802.016, 208273.859, 209142.188, 208358.125, 208560.578, 207789.266, 207501.125], 'val_loss': [3661.312, 3091.829, 2915.811, 2832.51, 2702.923, 2635.736, 2677.407, 2643.069, 2587.603, 2592.403, 2547.56, 2511.342, 2543.726, 2519.368, 2502.195, 2453.931, 2467.868, 2506.905, 2430.295, 2430.081, 2411.79, 2425.59, 2413.797, 2448.303, 2411.714, 2373.361, 2435.745, 2368.732, 2366.897, 2365.229, 2376.261, 2336.486, 2320.302, 2350.011, 2321.891, 2315.223, 2314.611, 2308.713, 2314.855, 2317.142, 2289.619, 2305.263, 2301.356, 2290.053, 2290.513, 2300.519, 2314.98, 2278.544, 2274.577, 2283.419, 2321.678, 2260.88, 2259.282, 2277.711, 2254.908, 2293.047, 2286.399, 2271.298, 2258.158, 2264.631, 2279.246, 2279.659, 2278.284, 2253.186, 2262.491, 2263.298, 2248.262, 2247.42, 2252.208, 2262.115, 2259.636, 2273.858, 2270.22, 2219.938, 2271.701, 2240.043, 2261.581, 2244.393, 2266.286, 2226.703, 2266.896, 2249.542, 2246.833, 2257.924, 2242.699, 2250.743, 2221.001, 2261.495, 2222.565, 2236.132, 2230.311, 2225.877, 2238.462, 2236.56, 2221.602, 2225.936, 2228.609, 2221.146, 2220.103, 2211.622]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:96 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.16615748065598326 alpha:0.804735700294884 weight_decay:9.064536644979257e-05 batch_size:32 epochs:100	100	1000	True	131122.20312		796011	12	-1	219.53837704658508	{'train_loss': [1121989.375, 1208261.375, 1557985.5, 1283179.0, 1707160.75, 1524134.5, 1541900.125, 1546977.625, 1416914.625, 1689557.875, 1589813.875, 1341413.5, 1660983.75, 1581205.625, 1448099.125, 1424679.125, 1561306.75, 1483180.625, 1316796.125, 1240185.25, 1207634.125, 1212845.75, 1147407.5, 1163454.875, 1168960.375, 1156230.125, 1158635.625, 1204673.125, 1179274.875, 1205340.25, 1173733.875, 1181503.0, 1183320.0, 1145634.25, 1153371.625, 1180440.0, 1179717.0, 1201360.0, 1153911.5, 1203236.375, 1196677.75, 1132639.25, 1216750.75, 1221255.875, 1142801.625, 1189055.0, 1190450.25, 1141163.875, 1148071.375, 1175724.25, 1202041.25, 1145006.0, 1181829.75, 1158988.5, 1180835.625, 1184317.375, 1200457.25, 1142604.125, 1167012.25, 1197662.75, 1184037.25, 1203478.25, 1194105.125, 1216196.0, 1161786.25, 1152943.75, 1154082.0, 1240333.625, 1208187.5, 1162249.5, 1154452.375, 1244123.125, 1176215.125, 1170573.125, 1182244.25, 1226713.75, 1152149.875, 1178541.625, 1184731.125, 1209267.25, 1164997.5, 1199580.875, 1192836.125, 1231461.375, 1219125.125, 1168841.375, 1174950.75, 1201316.625, 1154133.125, 1214132.25, 1165363.75, 1187370.25, 1209677.5, 1176286.875, 1224855.125, 1186559.0, 1180687.75, 1152201.25, 1206895.375, 1196818.5], 'val_loss': [11110.434, 11110.414, 11110.434, 12734.775, 13531.023, 12619.611, 12929.705, 14255.38, 11110.433, 11110.434, 14662.79, 35858.238, 11109.781, 13209.32, 11629.855, 14582.202, 11110.434, 11982.506, 12598.487, 13307.298, 11110.434, 11110.433, 21485.635, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11177.537, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 13180.859, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 12126.546, 11110.434, 11110.434, 11110.434, 11110.432, 11110.434, 11110.434, 12720.977, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 12207.799, 11110.434, 17732.09, 11107.293, 11110.434, 11110.434, 11110.434, 18760.799, 11110.434, 11110.434, 14399.993, 11110.434, 16089.421, 11110.434, 11110.434, 11489.094, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11624.046, 11103.035, 11110.434, 11108.209, 12984.291, 11110.434, 11110.434, 12573.76, 11110.434]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	25856.26172		701370	10	-1	199.95798540115356	{'train_loss': [767046.938, 358925.375, 313447.156, 292609.844, 279826.062, 270178.594, 264423.781, 259167.516, 254484.953, 251898.875, 248367.438, 245676.734, 243745.25, 242079.016, 239880.219, 237648.031, 238113.109, 236052.672, 235298.078, 233417.156, 234212.375, 232396.781, 232620.062, 231321.391, 229996.578, 229597.266, 229737.844, 229983.016, 228215.375, 227708.969, 226607.688, 227062.859, 226182.047, 226203.094, 225756.812, 224116.703, 225076.734, 223436.062, 224357.844, 223756.734, 223481.562, 223431.047, 223934.625, 222453.141, 222069.359, 222261.078, 222081.547, 221729.891, 221493.516, 221019.344, 221638.844, 220786.594, 220622.453, 220445.781, 220097.078, 219407.0, 219774.031, 219311.281, 219246.031, 220163.125, 218612.844, 218289.188, 218198.047, 219352.938, 218638.047, 217378.984, 217734.844, 218066.516, 217648.766, 217685.219, 217163.844, 217429.766, 216594.031, 216852.594, 216312.609, 217228.766, 215664.328, 216295.469, 216069.656, 215566.281, 215484.703, 215179.016, 215355.656, 214912.734, 215402.734, 214323.641, 215711.609, 215742.344, 214745.734, 213926.969, 214429.641, 215265.422, 214037.078, 213493.25, 213214.234, 214312.172, 213571.656, 213256.734, 213237.188, 213201.812], 'val_loss': [3622.144, 3200.197, 3082.078, 2883.733, 2806.448, 2724.083, 2750.238, 2663.14, 2638.935, 2597.426, 2571.354, 2552.869, 2545.273, 2521.597, 2485.217, 2513.1, 2458.325, 2478.517, 2464.035, 2466.827, 2449.013, 2463.117, 2436.146, 2412.111, 2433.145, 2442.002, 2393.718, 2409.025, 2409.62, 2375.598, 2405.303, 2392.471, 2406.323, 2389.586, 2383.192, 2372.583, 2376.535, 2374.344, 2371.668, 2390.29, 2368.66, 2369.606, 2360.73, 2365.841, 2342.211, 2345.044, 2335.963, 2353.289, 2326.604, 2369.316, 2340.737, 2332.614, 2356.362, 2330.095, 2322.129, 2324.673, 2337.393, 2338.386, 2324.597, 2328.236, 2325.733, 2311.272, 2329.86, 2331.05, 2314.833, 2309.687, 2335.128, 2324.683, 2321.374, 2317.153, 2328.244, 2303.01, 2298.241, 2318.192, 2351.191, 2305.839, 2302.997, 2303.578, 2325.294, 2333.6, 2314.914, 2301.173, 2302.647, 2303.406, 2317.155, 2313.823, 2328.159, 2301.094, 2289.318, 2305.16, 2285.125, 2293.053, 2292.166, 2294.657, 2321.164, 2302.229, 2296.237, 2311.524, 2300.832, 2302.227]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	24836.80273		520442	11	-1	205.4639105796814	{'train_loss': [587131.938, 358275.281, 301196.25, 276500.562, 263270.812, 255944.719, 251724.812, 248788.172, 244888.766, 242778.188, 241044.109, 239492.531, 237391.203, 236520.297, 235523.438, 234329.188, 232602.688, 231562.375, 230827.078, 231057.969, 229267.297, 228679.859, 228131.5, 227065.859, 226421.109, 226095.781, 225393.328, 224485.391, 224695.719, 223668.203, 223371.938, 222711.547, 222241.719, 221291.844, 220646.984, 220361.547, 219822.125, 218961.859, 219805.234, 219494.266, 218222.031, 218346.562, 217728.453, 218250.625, 216598.609, 216094.172, 216477.344, 215869.516, 214964.578, 215372.781, 215057.469, 215314.406, 213994.25, 214555.438, 212879.156, 214397.203, 212472.688, 213179.609, 211460.875, 213277.844, 212861.047, 212121.375, 211928.312, 210870.828, 212165.734, 211049.594, 211157.609, 210544.312, 210149.625, 209714.656, 209991.031, 209493.141, 209541.438, 208508.969, 208921.328, 208310.859, 208877.844, 208080.594, 208286.391, 207467.578, 207938.828, 207619.266, 207277.281, 207713.016, 206086.484, 207637.047, 206699.906, 205787.406, 206027.75, 207085.547, 206607.594, 205731.438, 205540.891, 205338.438, 205677.172, 205628.219, 204740.703, 204265.938, 204637.453, 204721.188], 'val_loss': [3910.86, 3109.2, 2893.745, 2677.371, 2591.965, 2587.099, 2562.297, 2544.975, 2513.931, 2524.611, 2501.085, 2532.462, 2478.465, 2472.953, 2462.309, 2451.912, 2470.356, 2452.972, 2443.365, 2438.485, 2444.372, 2421.009, 2435.694, 2410.792, 2420.688, 2408.713, 2395.593, 2390.53, 2410.734, 2397.27, 2381.924, 2378.797, 2382.222, 2367.039, 2360.049, 2353.409, 2365.527, 2330.399, 2342.714, 2350.763, 2355.478, 2317.906, 2334.398, 2331.292, 2349.708, 2343.449, 2320.911, 2339.885, 2323.419, 2334.073, 2310.323, 2289.048, 2313.038, 2320.829, 2312.903, 2303.439, 2304.036, 2297.48, 2297.233, 2293.185, 2296.331, 2299.135, 2293.893, 2277.101, 2277.692, 2286.803, 2297.63, 2273.296, 2297.069, 2271.882, 2273.379, 2258.702, 2267.729, 2278.385, 2264.146, 2265.352, 2251.636, 2261.421, 2248.031, 2253.748, 2255.351, 2248.534, 2247.095, 2256.742, 2257.394, 2235.648, 2238.639, 2238.911, 2240.803, 2229.936, 2240.017, 2226.915, 2223.912, 2246.038, 2256.308, 2266.96, 2224.144, 2221.379, 2219.529, 2218.843]}	0	100	True
