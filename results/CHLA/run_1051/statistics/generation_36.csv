id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	24060.52344		416732	11	-1	240.00232005119324	{'train_loss': [544777.312, 369945.594, 299245.188, 276803.938, 262261.031, 253137.25, 248469.344, 244155.578, 240639.734, 238191.281, 234882.078, 233291.484, 232277.438, 230719.031, 229191.062, 228530.109, 226189.141, 224140.438, 223783.641, 223410.516, 221010.641, 221556.703, 219485.094, 219032.469, 217881.797, 217231.234, 216230.312, 216266.016, 215892.75, 214753.469, 214232.125, 213200.172, 212929.312, 211727.781, 211601.953, 211356.062, 211344.484, 210109.625, 209141.672, 209513.312, 209559.594, 208184.078, 207732.891, 207195.562, 206347.344, 206600.938, 206607.328, 206172.062, 206007.438, 204855.891, 204819.062, 204634.078, 204165.531, 203569.047, 203836.969, 203362.469, 202865.844, 201816.75, 201649.625, 201930.078, 202351.719, 200631.734, 200633.328, 200866.219, 200802.5, 199641.828, 199133.516, 199651.297, 200220.344, 199372.609, 198720.172, 198460.281, 197706.188, 197616.125, 198113.344, 197260.328, 197283.938, 195944.516, 197124.75, 196719.531, 195745.984, 195520.688, 195874.188, 194648.125, 195498.641, 194552.688, 194054.375, 194525.938, 194417.859, 192389.75, 193076.531, 193421.234, 192368.734, 192283.375, 192478.312, 192833.875, 193250.781, 192572.438, 191807.281, 191267.672], 'val_loss': [3238.446, 2681.282, 2318.927, 2207.324, 2108.744, 2073.766, 2076.311, 2040.434, 2018.094, 1984.799, 1996.125, 1990.76, 1953.673, 1952.83, 1926.842, 1940.931, 1904.061, 1909.98, 1917.563, 1912.871, 1887.261, 1884.94, 1897.319, 1866.794, 1872.777, 1866.53, 1854.366, 1859.149, 1843.965, 1843.194, 1836.713, 1837.514, 1838.365, 1819.72, 1811.449, 1821.436, 1827.271, 1802.505, 1811.674, 1812.138, 1799.919, 1806.448, 1794.189, 1787.645, 1806.188, 1782.756, 1807.109, 1792.067, 1783.687, 1761.027, 1785.316, 1773.563, 1771.739, 1771.292, 1770.171, 1776.503, 1769.792, 1753.59, 1770.937, 1771.397, 1766.364, 1754.518, 1771.066, 1758.543, 1748.214, 1738.52, 1748.223, 1739.952, 1725.479, 1756.716, 1735.871, 1758.767, 1727.47, 1718.281, 1714.016, 1735.882, 1738.821, 1713.289, 1722.845, 1736.593, 1709.928, 1707.48, 1688.134, 1712.078, 1718.307, 1705.105, 1708.544, 1702.335, 1679.35, 1693.15, 1711.344, 1700.275, 1694.93, 1693.764, 1700.34, 1684.942, 1668.671, 1695.14, 1686.462, 1694.35]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:102 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:51 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	24942.33789		791433	11	-1	254.52669143676758	{'train_loss': [935230.188, 429985.188, 318463.281, 285402.75, 271554.406, 264166.281, 261098.766, 258612.719, 255497.172, 253683.547, 251050.938, 249525.219, 248590.562, 246137.719, 244235.875, 242735.312, 241397.703, 239115.375, 237431.391, 236625.578, 235415.484, 233843.125, 233259.766, 232419.469, 231550.109, 230976.188, 229794.547, 228872.234, 228059.531, 227875.844, 227270.922, 226091.609, 224824.422, 224848.953, 224510.516, 223027.438, 224011.562, 223258.047, 221780.984, 220812.125, 221277.516, 220123.953, 220386.766, 220238.047, 218806.828, 218750.5, 218481.047, 217534.734, 217767.016, 217695.469, 217191.125, 217523.844, 217402.688, 216036.953, 215543.719, 214629.547, 215162.688, 214719.859, 214293.844, 213363.844, 213888.641, 212649.234, 212441.828, 213075.766, 212375.891, 212068.875, 211135.047, 211727.109, 211382.859, 210713.188, 210690.188, 210088.734, 210546.672, 210135.266, 208991.625, 208844.484, 208925.359, 207953.828, 208168.484, 207788.438, 207681.281, 207612.656, 207486.531, 206688.828, 206375.297, 206294.438, 205651.375, 205370.859, 205333.109, 205690.922, 204748.484, 204551.031, 204837.5, 203072.828, 203760.75, 204006.953, 203536.031, 202400.609, 202821.078, 203131.281], 'val_loss': [4736.119, 2677.639, 2399.213, 2251.279, 2136.95, 2197.725, 2137.084, 2114.943, 2066.248, 2046.578, 2035.805, 2016.605, 2034.061, 2005.883, 1994.572, 1971.229, 1966.017, 1949.626, 1941.425, 1952.885, 1927.082, 1932.563, 1907.174, 1935.129, 1902.839, 1894.6, 1886.996, 1881.286, 1894.079, 1900.579, 1873.732, 1877.681, 1869.289, 1866.607, 1853.244, 1860.012, 1869.833, 1859.875, 1852.231, 1864.707, 1842.831, 1847.214, 1857.903, 1848.385, 1852.839, 1861.436, 1857.934, 1860.022, 1839.01, 1846.622, 1838.796, 1842.995, 1835.525, 1867.864, 1859.123, 1846.915, 1840.922, 1814.935, 1831.785, 1815.019, 1845.514, 1820.49, 1817.809, 1828.591, 1811.028, 1813.346, 1833.85, 1817.709, 1826.881, 1820.01, 1815.275, 1807.417, 1816.621, 1820.684, 1811.441, 1810.298, 1805.3, 1821.992, 1787.166, 1797.344, 1804.083, 1804.994, 1801.19, 1790.77, 1785.086, 1787.797, 1793.546, 1791.187, 1799.237, 1793.143, 1792.961, 1785.901, 1796.841, 1789.773, 1784.182, 1775.29, 1771.004, 1782.138, 1778.744, 1786.361]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:81 kernel_size:5 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:116 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.19939276777747716 alpha:0.9921877581363177 weight_decay:9.132901854645363e-05 batch_size:26 epochs:100	100	1000	True	148500.26562		439619	12	-1	247.77496004104614	{'train_loss': [1436269.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062568.125, 1062567.25, 1062567.25, 1062567.25, 1117297.0, 1062567.25, 1065802.25, 3072869.0, 1156100.5, 1070804.25, 1109199.125, 1062567.25, 1063917.0, 1106150.25, 1108989.125, 1062567.25, 1062566.875, 1062567.25, 1062567.25, 1062567.25, 1062628.375, 1536070.25, 1062567.25, 1062567.25, 1062567.25, 3389545.25, 1221683.125, 1119208.5, 1074249.875, 1062567.25, 1130423.125, 1062567.25, 1062567.25, 1062567.25, 1062632.125, 1063747.75, 1372179.5, 1062567.25, 1062567.25, 1075547.5, 1248499.0, 1062603.125, 1124537.625, 1062567.25, 1085154.125, 1305436.75, 1062567.25, 1064575.75, 1076047.0, 1125693.5, 1099705.875, 1107210.75, 1398699.25, 1062563.625, 1067550.0, 1249410.0, 1062783.75, 1062567.25, 2035830.875, 1062567.25, 1062600.125, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1065934.125, 1129723.375, 1062567.25, 1062567.25, 1154830.125, 1062567.25, 1063295.75, 1063753.875, 1075833.625, 2372948.25, 1143442.625, 1174048.0, 1062567.25, 1062601.625, 1062567.25, 1062567.25, 1062567.25, 1088485.875, 1087094.375, 1062567.25, 1071304.375, 1742972.25, 1064950.5, 1103083.75, 1068462.375, 1075268.375], 'val_loss': [8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8928.734, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 465473.75, 11484.36, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 10042.111]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:79 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:110 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:26 epochs:100	100	1000	True	24723.5332		465570	12	-1	257.78427171707153	{'train_loss': [521878.781, 387674.219, 322958.938, 301589.906, 294272.625, 288035.531, 271699.031, 263148.5, 259153.578, 256727.984, 254678.438, 252389.094, 251397.156, 249485.688, 247796.5, 245877.0, 244310.797, 242917.953, 241591.594, 240164.375, 239241.5, 238207.938, 236548.391, 236163.109, 235240.828, 233663.016, 233074.0, 232402.531, 232354.281, 231564.25, 230126.297, 230578.641, 229066.906, 228111.016, 227592.875, 227894.25, 227219.234, 226351.938, 225509.031, 225330.062, 224374.469, 223976.172, 223099.297, 223233.234, 222408.156, 221421.344, 220069.625, 219747.969, 219398.203, 218196.859, 217815.141, 217495.219, 217780.828, 216222.312, 215308.656, 214083.875, 214684.859, 213608.047, 212780.453, 212880.969, 211844.328, 210648.328, 210435.719, 209897.391, 209707.781, 209524.031, 208766.0, 208539.359, 208603.266, 208482.938, 207791.484, 206654.578, 206813.938, 205623.375, 205514.266, 204727.766, 203877.984, 204085.438, 203448.891, 204774.188, 203714.75, 203453.188, 202750.422, 202695.625, 202472.703, 201490.281, 201424.344, 201407.266, 201199.078, 200676.375, 200871.547, 200571.312, 200524.172, 200819.406, 199271.234, 200429.875, 199673.734, 198807.047, 198266.812, 198717.234], 'val_loss': [3388.343, 2774.204, 2597.731, 2523.282, 2484.383, 2317.059, 2263.836, 2167.122, 2153.789, 2116.856, 2113.161, 2090.139, 2076.401, 2056.14, 2080.471, 2054.232, 2026.547, 2015.599, 1986.684, 2002.544, 1970.772, 1970.706, 1961.162, 1966.901, 1955.2, 1944.784, 1956.67, 1935.951, 1975.739, 1934.277, 1938.856, 1928.807, 1923.24, 1918.928, 1939.814, 1922.124, 1891.56, 1897.043, 1896.835, 1884.874, 1892.773, 1909.6, 1861.296, 1896.268, 1903.385, 1873.535, 1862.786, 1841.358, 1899.721, 1836.969, 1836.704, 1841.564, 1827.181, 1818.281, 1836.94, 1841.134, 1782.869, 1800.76, 1827.613, 1773.91, 1760.58, 1830.911, 1769.945, 1777.69, 1817.977, 1804.568, 1823.017, 1752.877, 1787.381, 1761.73, 1787.556, 1755.797, 1766.722, 1757.329, 1765.11, 1727.693, 1878.62, 1765.608, 1783.033, 1718.218, 1781.943, 1792.695, 1796.554, 1741.489, 1727.799, 1744.621, 1741.859, 1771.35, 1717.63, 1787.151, 1717.589, 1753.269, 1738.945, 1736.27, 1780.55, 1834.982, 1809.56, 1773.56, 1750.312, 1761.558]}	100	100	True
