id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	2000	True	23818.17383		416732	11	-1	240.97630167007446	{'train_loss': [484864.812, 323467.031, 280940.625, 265004.469, 256034.625, 250660.859, 246261.781, 243589.234, 240656.531, 238259.734, 236794.234, 235393.672, 233490.297, 232539.016, 230954.75, 229678.516, 228327.703, 226917.094, 226324.484, 225393.234, 224851.125, 224448.703, 223149.406, 222289.234, 222655.562, 220162.688, 220104.656, 220047.703, 218258.266, 218426.438, 217016.047, 217463.734, 216487.625, 214902.578, 215699.906, 214557.641, 214884.391, 213595.953, 213490.875, 213513.969, 213524.031, 212574.266, 211861.25, 210806.859, 211365.281, 209372.344, 210383.875, 209440.422, 209108.812, 208237.891, 208209.047, 208015.047, 207621.469, 207864.891, 207282.391, 206768.016, 206138.562, 205853.188, 206564.031, 205766.438, 204501.266, 205008.688, 205232.766, 204110.953, 203654.344, 204222.844, 203788.391, 203866.688, 203138.75, 202207.594, 203014.516, 202383.828, 201944.469, 201811.422, 201079.797, 201356.078, 200747.156, 200866.234, 200257.75, 200024.516, 199510.375, 199455.688, 200121.891, 199273.141, 198783.641, 198647.094, 199031.844, 198430.688, 197692.875, 198205.266, 198014.484, 196613.438, 197050.438, 196786.516, 197091.031, 196561.094, 196337.062, 196320.047, 195968.938, 195633.016], 'val_loss': [2868.734, 2311.157, 2222.113, 2103.792, 2058.394, 2016.446, 1983.951, 1975.886, 1952.94, 1959.132, 1945.835, 1928.837, 1926.684, 1906.991, 1915.157, 1884.026, 1884.686, 1872.845, 1876.807, 1849.246, 1856.952, 1850.081, 1847.485, 1856.392, 1813.825, 1826.312, 1824.066, 1810.899, 1816.564, 1806.801, 1802.533, 1800.568, 1814.33, 1789.924, 1785.446, 1793.422, 1787.636, 1792.759, 1784.518, 1771.781, 1788.443, 1801.572, 1766.052, 1771.976, 1772.85, 1764.993, 1757.679, 1757.356, 1767.944, 1745.045, 1762.657, 1760.621, 1773.568, 1773.418, 1763.696, 1751.712, 1752.028, 1767.78, 1747.422, 1742.331, 1750.609, 1754.196, 1728.754, 1739.847, 1746.099, 1735.363, 1726.215, 1726.597, 1729.94, 1730.108, 1734.468, 1719.736, 1723.308, 1738.623, 1721.308, 1724.386, 1727.368, 1729.129, 1725.545, 1708.502, 1714.544, 1715.891, 1715.12, 1726.203, 1709.729, 1705.391, 1722.417, 1714.786, 1717.73, 1700.49, 1708.016, 1711.614, 1709.618, 1710.93, 1715.484, 1722.446, 1698.777, 1690.979, 1699.225, 1696.476]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:52 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:23 epochs:100	100	2000	True	24650.5293		571800	12	-1	276.98501420021057	{'train_loss': [623453.25, 334362.531, 277599.719, 265613.5, 260689.188, 257051.141, 254352.844, 251084.484, 249432.938, 246739.547, 245220.484, 243423.812, 240881.422, 239349.359, 237145.859, 235991.344, 234452.531, 231544.719, 230167.688, 230274.094, 228534.125, 227488.391, 226843.562, 227191.984, 224739.766, 224246.297, 223719.203, 222830.016, 221805.562, 220855.219, 219943.875, 219080.047, 219208.844, 218491.281, 217700.344, 217254.734, 217010.844, 216541.891, 215360.078, 215958.359, 214961.453, 214183.516, 213183.328, 213494.547, 213367.297, 212348.156, 212090.859, 212052.906, 212163.516, 211518.172, 210693.562, 210189.156, 210200.812, 210060.375, 209430.578, 208702.625, 208184.391, 208451.016, 206761.266, 207564.484, 208176.75, 207535.516, 208135.938, 206433.234, 206221.016, 205457.828, 205376.344, 205093.984, 204964.0, 205026.734, 204087.312, 203584.078, 203800.484, 202702.422, 202920.828, 203346.188, 202338.047, 201270.172, 201573.031, 201752.281, 201247.328, 201016.375, 200080.5, 201328.938, 200809.125, 200055.406, 199921.844, 200244.984, 198829.562, 198945.281, 198268.203, 198123.094, 198249.797, 197647.391, 197951.75, 196980.594, 197779.797, 197181.5, 196039.547, 196834.766], 'val_loss': [2620.011, 2083.433, 1957.108, 1954.7, 1924.447, 1906.617, 1860.271, 1870.746, 1836.266, 1819.15, 1860.872, 1819.851, 1798.728, 1805.826, 1766.057, 1775.474, 1748.681, 1736.795, 1739.757, 1729.908, 1726.299, 1727.468, 1711.55, 1709.909, 1690.531, 1707.045, 1690.456, 1707.138, 1682.582, 1710.996, 1685.205, 1683.043, 1652.611, 1685.405, 1665.164, 1664.953, 1655.769, 1662.494, 1655.524, 1660.915, 1654.378, 1632.271, 1647.253, 1628.458, 1640.551, 1635.316, 1661.263, 1627.992, 1623.79, 1638.088, 1645.905, 1616.159, 1649.432, 1613.088, 1624.314, 1618.136, 1622.261, 1588.948, 1637.45, 1623.491, 1587.045, 1614.23, 1595.205, 1598.188, 1603.904, 1591.474, 1606.705, 1589.492, 1578.397, 1623.95, 1575.461, 1582.396, 1585.071, 1574.008, 1584.586, 1592.878, 1585.241, 1564.949, 1568.902, 1565.393, 1593.62, 1578.552, 1561.617, 1536.002, 1551.808, 1564.56, 1550.23, 1542.214, 1542.785, 1556.366, 1542.137, 1577.646, 1546.074, 1551.807, 1534.384, 1526.802, 1532.78, 1544.927, 1536.243, 1533.02]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	3000	True	24040.39844		416732	11	-1	241.66180992126465	{'train_loss': [521653.5, 353874.938, 300281.0, 271830.781, 261594.188, 255273.172, 251817.703, 248642.0, 245842.641, 243854.094, 242816.297, 241049.719, 239196.578, 238281.375, 237291.547, 236353.203, 234897.719, 233835.141, 232887.469, 232774.953, 230938.406, 229480.0, 228139.844, 227361.297, 225899.719, 223743.859, 223403.75, 221193.156, 220115.656, 219057.172, 217387.672, 217028.156, 216287.75, 214892.672, 214360.422, 212770.047, 212827.547, 212333.328, 211064.094, 209567.875, 209568.188, 208889.391, 209114.422, 207784.016, 207525.844, 206413.672, 205936.078, 206248.469, 206259.609, 204012.359, 204896.312, 204028.656, 203293.516, 202857.75, 202219.766, 201832.953, 201444.219, 201662.75, 200471.25, 200509.984, 200187.297, 199811.953, 199573.984, 200002.297, 198323.266, 198945.469, 197674.203, 197615.219, 197981.453, 197546.562, 196800.078, 198023.922, 195817.453, 195510.828, 197202.266, 195688.078, 195946.266, 195558.172, 194837.625, 194843.672, 194045.734, 195078.469, 192779.922, 193631.172, 193236.484, 192474.281, 193318.516, 192579.406, 192964.141, 191224.969, 191920.375, 192626.703, 191846.219, 190182.234, 190655.875, 190896.031, 191095.703, 189958.578, 190174.0, 190676.188], 'val_loss': [3199.771, 2619.505, 2263.882, 2165.787, 2128.115, 2105.477, 2052.524, 2044.235, 2026.04, 2022.086, 1999.771, 2000.3, 1985.556, 1976.946, 1974.371, 1979.711, 1952.932, 1948.284, 1933.733, 1941.393, 1926.297, 1901.703, 1900.296, 1897.864, 1865.667, 1877.618, 1848.972, 1827.797, 1843.807, 1838.137, 1832.391, 1811.775, 1811.597, 1800.357, 1779.143, 1789.724, 1794.047, 1759.235, 1761.214, 1775.512, 1758.114, 1794.421, 1778.882, 1781.621, 1760.335, 1748.369, 1750.709, 1747.314, 1752.165, 1752.656, 1737.122, 1755.697, 1737.681, 1729.869, 1726.177, 1749.342, 1749.457, 1744.816, 1731.059, 1739.629, 1708.28, 1718.906, 1718.451, 1723.217, 1719.873, 1711.722, 1692.553, 1712.465, 1705.89, 1718.592, 1724.722, 1714.493, 1693.929, 1709.235, 1707.309, 1695.084, 1711.25, 1693.025, 1703.68, 1717.581, 1692.651, 1667.397, 1673.636, 1675.47, 1722.505, 1683.882, 1682.558, 1686.054, 1669.701, 1674.873, 1686.887, 1697.844, 1673.591, 1668.891, 1704.609, 1711.847, 1739.746, 1672.181, 1706.452, 1682.657]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	3000	True	24040.39844		416732	11	-1	241.66787266731262	{'train_loss': [521653.5, 353874.938, 300281.0, 271830.781, 261594.188, 255273.172, 251817.703, 248642.0, 245842.641, 243854.094, 242816.297, 241049.719, 239196.578, 238281.375, 237291.547, 236353.203, 234897.719, 233835.141, 232887.469, 232774.953, 230938.406, 229480.0, 228139.844, 227361.297, 225899.719, 223743.859, 223403.75, 221193.156, 220115.656, 219057.172, 217387.672, 217028.156, 216287.75, 214892.672, 214360.422, 212770.047, 212827.547, 212333.328, 211064.094, 209567.875, 209568.188, 208889.391, 209114.422, 207784.016, 207525.844, 206413.672, 205936.078, 206248.469, 206259.609, 204012.359, 204896.312, 204028.656, 203293.516, 202857.75, 202219.766, 201832.953, 201444.219, 201662.75, 200471.25, 200509.984, 200187.297, 199811.953, 199573.984, 200002.297, 198323.266, 198945.469, 197674.203, 197615.219, 197981.453, 197546.562, 196800.078, 198023.922, 195817.453, 195510.828, 197202.266, 195688.078, 195946.266, 195558.172, 194837.625, 194843.672, 194045.734, 195078.469, 192779.922, 193631.172, 193236.484, 192474.281, 193318.516, 192579.406, 192964.141, 191224.969, 191920.375, 192626.703, 191846.219, 190182.234, 190655.875, 190896.031, 191095.703, 189958.578, 190174.0, 190676.188], 'val_loss': [3199.771, 2619.505, 2263.882, 2165.787, 2128.115, 2105.477, 2052.524, 2044.235, 2026.04, 2022.086, 1999.771, 2000.3, 1985.556, 1976.946, 1974.371, 1979.711, 1952.932, 1948.284, 1933.733, 1941.393, 1926.297, 1901.703, 1900.296, 1897.864, 1865.667, 1877.618, 1848.972, 1827.797, 1843.807, 1838.137, 1832.391, 1811.775, 1811.597, 1800.357, 1779.143, 1789.724, 1794.047, 1759.235, 1761.214, 1775.512, 1758.114, 1794.421, 1778.882, 1781.621, 1760.335, 1748.369, 1750.709, 1747.314, 1752.165, 1752.656, 1737.122, 1755.697, 1737.681, 1729.869, 1726.177, 1749.342, 1749.457, 1744.816, 1731.059, 1739.629, 1708.28, 1718.906, 1718.451, 1723.217, 1719.873, 1711.722, 1692.553, 1712.465, 1705.89, 1718.592, 1724.722, 1714.493, 1693.929, 1709.235, 1707.309, 1695.084, 1711.25, 1693.025, 1703.68, 1717.581, 1692.651, 1667.397, 1673.636, 1675.47, 1722.505, 1683.882, 1682.558, 1686.054, 1669.701, 1674.873, 1686.887, 1697.844, 1673.591, 1668.891, 1704.609, 1711.847, 1739.746, 1672.181, 1706.452, 1682.657]}	0	100	True
