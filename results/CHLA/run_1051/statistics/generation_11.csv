id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	2000	True	25429.43555		619010	12	-1	210.94621682167053	{'train_loss': [581659.875, 412699.344, 346308.375, 314205.562, 300370.938, 285180.25, 264546.25, 255794.031, 249737.484, 245963.609, 243562.312, 241084.562, 238772.375, 238071.219, 236394.344, 235285.984, 234529.531, 233031.641, 232367.578, 231480.5, 230574.719, 229287.906, 230110.75, 227969.719, 227235.422, 226212.016, 225406.422, 225190.547, 224919.406, 224293.688, 223871.781, 223410.047, 222793.562, 222709.312, 222549.75, 222416.359, 221797.016, 220658.609, 220493.562, 220173.953, 219818.062, 220001.766, 219073.453, 219496.547, 218014.562, 217049.828, 217910.016, 217580.031, 216880.016, 216374.281, 216514.328, 214722.797, 215309.047, 214998.25, 215424.094, 214493.734, 213519.344, 214237.969, 213306.859, 213382.859, 212271.297, 211921.25, 211532.625, 211549.406, 211638.859, 210663.438, 210967.188, 209363.312, 210221.938, 209289.719, 209124.453, 208981.156, 208974.172, 208713.688, 207186.484, 208195.219, 207488.391, 206085.891, 206438.406, 206466.188, 206344.172, 205608.516, 205199.969, 205380.016, 204960.297, 204693.656, 204287.641, 203807.766, 203269.922, 203328.719, 202942.125, 204139.703, 202477.297, 202601.875, 202445.828, 202421.766, 201865.078, 201526.375, 201347.75, 199695.562], 'val_loss': [4028.082, 3524.743, 3337.96, 3140.77, 3100.868, 2798.56, 2728.198, 2618.282, 2630.923, 2570.708, 2525.534, 2535.429, 2517.984, 2495.822, 2532.394, 2497.676, 2503.095, 2493.172, 2505.662, 2460.515, 2431.165, 2416.292, 2450.709, 2433.477, 2436.694, 2416.948, 2450.358, 2439.935, 2438.095, 2421.941, 2400.297, 2438.949, 2405.774, 2412.489, 2403.645, 2438.836, 2401.506, 2439.415, 2410.757, 2361.184, 2384.594, 2382.437, 2378.227, 2405.152, 2373.205, 2354.827, 2361.084, 2365.096, 2344.615, 2344.134, 2346.372, 2342.933, 2356.952, 2349.764, 2378.273, 2325.18, 2334.91, 2312.957, 2314.74, 2322.234, 2311.246, 2313.65, 2311.281, 2327.779, 2301.443, 2293.325, 2291.911, 2313.145, 2301.457, 2306.489, 2316.032, 2277.131, 2292.136, 2298.708, 2297.186, 2323.112, 2308.86, 2287.28, 2291.092, 2290.257, 2299.586, 2299.23, 2323.021, 2288.971, 2287.124, 2292.072, 2278.004, 2297.645, 2284.615, 2268.466, 2302.735, 2275.172, 2271.453, 2275.853, 2264.883, 2276.827, 2255.965, 2265.737, 2249.972, 2268.134]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:80 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	27072.12109		1118478	10	-1	192.91144347190857	{'train_loss': [1100237.625, 1058235.75, 601944.5, 374620.719, 315587.156, 291417.969, 280308.625, 273373.906, 267333.156, 261187.656, 255696.203, 252514.125, 248100.422, 246079.938, 242999.234, 241164.922, 239788.109, 237445.312, 235574.766, 234457.469, 233318.094, 232666.266, 231719.906, 229310.375, 229612.328, 228338.641, 228234.141, 226778.562, 226201.734, 225585.203, 225022.438, 224472.625, 222959.109, 223075.422, 222433.125, 221652.656, 220882.359, 220201.328, 219494.641, 219264.188, 218462.859, 218811.344, 218416.219, 217802.906, 217487.594, 216842.891, 216760.156, 216536.172, 215617.266, 215473.875, 215039.344, 214949.812, 214269.219, 214518.219, 213557.297, 213771.484, 212628.109, 212583.5, 211773.328, 211769.438, 211973.891, 210839.172, 211274.594, 210595.422, 211078.719, 209988.672, 209984.688, 209544.453, 210064.766, 210244.031, 208966.781, 208086.969, 208784.453, 208301.781, 208027.422, 207422.484, 206730.453, 207249.719, 206616.969, 205996.312, 206496.141, 204958.156, 205267.391, 205258.484, 205613.938, 205203.438, 204786.625, 205423.781, 203573.516, 203978.141, 204272.797, 203407.141, 203534.938, 202733.75, 202668.047, 203270.359, 202111.531, 202198.547, 202743.641, 201103.859], 'val_loss': [11393.851, 7421.364, 3640.641, 3251.063, 2979.325, 2875.023, 2798.475, 2750.603, 2702.106, 2640.259, 2605.98, 2565.522, 2547.452, 2528.954, 2546.686, 2483.19, 2486.274, 2485.151, 2454.955, 2466.532, 2436.734, 2455.492, 2424.217, 2406.678, 2421.042, 2437.066, 2416.888, 2395.442, 2402.394, 2394.909, 2386.226, 2389.925, 2372.582, 2392.683, 2382.696, 2373.052, 2364.889, 2359.207, 2370.402, 2351.784, 2352.777, 2354.374, 2374.221, 2337.796, 2342.708, 2362.501, 2336.897, 2337.956, 2322.137, 2336.377, 2359.098, 2357.283, 2350.898, 2342.046, 2330.348, 2349.437, 2334.487, 2323.941, 2347.159, 2342.943, 2337.682, 2303.231, 2324.767, 2335.619, 2333.179, 2323.907, 2334.837, 2321.525, 2347.24, 2322.871, 2350.24, 2336.07, 2320.028, 2359.874, 2337.909, 2334.651, 2336.564, 2338.617, 2301.978, 2302.859, 2303.379, 2319.187, 2329.835, 2298.838, 2339.991, 2303.903, 2359.845, 2321.88, 2330.436, 2325.14, 2308.915, 2298.979, 2312.768, 2303.342, 2278.106, 2314.166, 2294.909, 2283.021, 2317.929, 2332.939]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:9 epochs:100	100	2000	True	26045.53125		487170	10	-1	452.1746015548706	{'train_loss': [424627.406, 285740.188, 270173.344, 262961.719, 257233.891, 253877.891, 249584.031, 247505.047, 245298.547, 243723.344, 242565.5, 241049.703, 238903.797, 238484.234, 237825.984, 236321.797, 235766.141, 234417.938, 233899.047, 233265.719, 231768.719, 231363.109, 230519.141, 230067.969, 228877.703, 229236.328, 227199.172, 228363.156, 226393.891, 226750.469, 225237.109, 224676.203, 225228.641, 224248.891, 224126.391, 224220.531, 223291.5, 222936.672, 222125.25, 221382.422, 222181.875, 220344.031, 220890.766, 220065.812, 220552.953, 219600.641, 219789.109, 219547.281, 218811.984, 219413.469, 217913.719, 218505.797, 217313.469, 217640.375, 216782.516, 215863.141, 217304.75, 215813.109, 216186.25, 215951.344, 215713.734, 215115.281, 215334.938, 213734.266, 214662.703, 213326.234, 214503.562, 213565.609, 213344.656, 214216.969, 212647.406, 211672.828, 211637.297, 212410.359, 212705.125, 212457.484, 212518.719, 211582.203, 211765.578, 211978.906, 211425.516, 211255.375, 211827.828, 211424.391, 209963.078, 209721.5, 210169.625, 210933.594, 210479.047, 209707.844, 209811.297, 209585.594, 209029.203, 209235.047, 208272.297, 209193.469, 209090.812, 208806.656, 209085.0, 207691.828], 'val_loss': [862.484, 796.989, 772.21, 752.525, 751.764, 730.326, 729.438, 734.171, 721.28, 720.894, 725.978, 715.481, 732.471, 727.174, 717.576, 712.259, 713.756, 707.174, 697.384, 701.973, 701.301, 712.933, 703.418, 701.257, 698.083, 700.958, 702.126, 700.172, 709.488, 696.98, 690.694, 686.446, 694.69, 696.954, 689.774, 688.439, 682.593, 695.788, 691.003, 682.725, 687.888, 699.202, 688.022, 676.806, 683.464, 678.737, 678.718, 683.488, 682.74, 679.015, 681.937, 677.025, 679.803, 687.741, 679.37, 685.872, 678.927, 680.803, 688.348, 681.114, 674.915, 680.347, 683.311, 680.199, 680.598, 682.006, 674.842, 674.866, 687.855, 686.924, 696.533, 677.6, 679.287, 683.207, 674.69, 676.742, 672.625, 672.934, 672.341, 668.361, 676.545, 678.892, 670.827, 672.046, 669.934, 676.928, 670.442, 663.676, 665.295, 664.004, 668.631, 671.203, 669.8, 668.212, 673.725, 676.87, 669.059, 663.642, 666.517, 669.356]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	27296.01562		1287874	12	-1	213.24064660072327	{'train_loss': [1217733.25, 1014874.938, 535170.562, 385819.875, 299973.438, 273945.969, 263070.219, 256279.172, 251801.672, 247968.922, 244107.594, 242042.266, 240269.312, 238451.938, 237394.062, 236048.812, 235076.078, 233968.5, 232595.938, 232166.125, 231346.375, 229999.453, 228976.156, 229013.156, 228423.984, 227529.641, 226895.609, 226330.469, 226210.75, 226035.703, 224652.688, 224162.453, 223833.953, 223253.516, 223119.922, 223652.25, 221381.266, 222639.359, 220813.625, 221536.359, 220829.641, 220328.516, 219425.219, 219620.359, 219889.656, 218002.0, 219158.625, 218811.594, 217980.922, 217407.094, 217907.719, 217626.203, 216030.375, 216216.953, 216632.656, 215071.031, 215602.078, 215925.891, 215427.906, 215522.906, 214369.688, 214418.766, 214507.25, 214438.391, 213225.828, 214137.797, 213134.609, 213004.688, 213028.156, 212743.875, 211600.203, 211894.188, 212234.766, 211669.047, 211760.453, 210761.422, 210919.281, 211402.844, 210449.672, 210519.656, 209109.844, 209639.016, 210427.672, 210665.828, 208928.344, 209569.484, 209262.266, 209459.703, 208868.688, 208608.703, 209409.406, 207657.656, 208211.703, 208181.016, 208270.969, 208851.891, 207773.922, 207482.688, 207304.562, 207986.062], 'val_loss': [10609.985, 5157.751, 3604.666, 3259.931, 2869.457, 2765.497, 2654.389, 2620.838, 2581.625, 2571.033, 2515.284, 2488.96, 2507.527, 2484.42, 2493.163, 2477.455, 2479.86, 2500.533, 2463.614, 2472.851, 2465.114, 2433.808, 2436.309, 2419.913, 2410.978, 2453.17, 2409.13, 2428.541, 2418.22, 2442.556, 2426.687, 2426.601, 2441.136, 2412.893, 2413.079, 2414.109, 2404.091, 2415.437, 2407.638, 2421.851, 2409.078, 2415.569, 2428.022, 2464.079, 2435.905, 2423.643, 2429.466, 2429.059, 2433.529, 2425.615, 2428.75, 2406.411, 2405.071, 2428.831, 2414.844, 2434.795, 2423.507, 2450.396, 2447.179, 2416.221, 2412.0, 2417.145, 2418.604, 2425.289, 2436.31, 2411.827, 2404.203, 2418.725, 2420.542, 2430.448, 2420.556, 2408.552, 2415.948, 2430.353, 2428.351, 2407.359, 2405.306, 2443.053, 2419.385, 2444.231, 2435.632, 2389.707, 2437.248, 2412.375, 2407.688, 2410.702, 2405.632, 2404.991, 2418.612, 2429.946, 2405.558, 2410.363, 2395.715, 2394.773, 2403.395, 2398.55, 2397.049, 2394.37, 2418.674, 2408.293]}	100	100	True
