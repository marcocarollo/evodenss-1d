id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	25182.16016		520442	11	-1	205.00747179985046	{'train_loss': [557650.438, 353597.312, 296312.0, 272589.094, 260947.469, 253606.734, 250296.25, 246112.688, 242979.75, 240213.656, 236499.391, 234905.391, 233919.984, 231793.375, 229888.781, 229101.938, 227702.312, 226805.406, 225816.031, 225003.562, 223526.125, 222926.719, 222177.422, 221332.984, 220957.25, 220410.016, 219060.312, 219999.938, 218950.25, 217068.625, 217380.172, 216646.234, 217178.219, 216192.672, 216084.344, 214994.5, 214327.094, 214284.859, 214316.344, 213685.297, 213697.516, 212641.516, 213144.656, 212323.297, 212188.922, 211886.438, 211518.453, 211470.906, 211601.812, 210505.781, 210461.75, 210501.438, 209685.844, 209642.016, 209164.75, 209052.031, 209022.969, 208450.516, 209091.75, 208321.422, 207992.531, 208611.609, 208099.172, 206813.781, 206425.062, 207326.172, 206679.141, 206199.828, 206836.688, 206881.734, 206038.453, 205442.625, 205316.109, 206096.953, 204742.797, 205682.516, 204828.047, 204579.266, 205010.109, 204325.344, 203303.719, 204880.0, 204493.359, 203463.875, 204193.016, 203131.609, 203214.891, 203207.0, 203208.516, 202919.703, 202550.109, 202199.5, 201553.328, 201559.656, 201839.031, 202015.984, 200791.203, 201302.422, 201014.562, 201063.109], 'val_loss': [3638.572, 3046.167, 2771.47, 2705.207, 2638.663, 2599.349, 2566.133, 2560.647, 2540.406, 2499.481, 2486.549, 2452.705, 2448.129, 2429.843, 2419.278, 2408.348, 2405.162, 2380.169, 2361.152, 2389.298, 2379.297, 2389.151, 2374.675, 2357.668, 2335.701, 2331.726, 2332.197, 2339.767, 2338.794, 2321.243, 2311.907, 2319.933, 2321.335, 2308.101, 2301.36, 2314.617, 2293.806, 2292.363, 2312.14, 2293.924, 2275.59, 2307.418, 2263.341, 2290.751, 2260.802, 2280.213, 2266.759, 2255.122, 2288.16, 2278.185, 2251.734, 2259.052, 2258.121, 2271.864, 2240.183, 2245.97, 2242.482, 2250.64, 2250.234, 2246.211, 2246.875, 2260.772, 2255.115, 2254.315, 2229.96, 2243.912, 2233.002, 2263.506, 2242.368, 2236.575, 2254.875, 2263.691, 2251.292, 2250.824, 2229.335, 2247.984, 2234.961, 2227.5, 2240.817, 2221.677, 2232.795, 2257.178, 2227.018, 2234.957, 2206.957, 2209.799, 2220.144, 2228.961, 2233.39, 2252.578, 2220.749, 2221.735, 2224.598, 2223.592, 2241.077, 2202.132, 2225.599, 2226.441, 2234.627, 2213.969]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:39 epochs:100	100	1000	True	25113.0293		505857	11	-1	186.44686007499695	{'train_loss': [661613.562, 393633.875, 325032.469, 290234.625, 267730.156, 260313.141, 256284.672, 252302.766, 249864.594, 247886.875, 244615.656, 242746.5, 241807.766, 239231.344, 237244.656, 235499.875, 234543.344, 232432.609, 232566.938, 231386.719, 229817.344, 228175.781, 227266.391, 226498.984, 225883.297, 223939.031, 223975.438, 224246.125, 221776.984, 221331.109, 220151.203, 219606.297, 219775.719, 218182.125, 217756.812, 217152.188, 217146.688, 216164.953, 215465.172, 214822.781, 215349.391, 213690.344, 213722.781, 212797.422, 212562.781, 212564.234, 211217.562, 211486.469, 210188.219, 209430.328, 210179.609, 209900.719, 208507.453, 209400.844, 209505.719, 207754.047, 208622.922, 207886.359, 207227.094, 207242.703, 207777.359, 206476.812, 206093.266, 206611.562, 206059.562, 205139.969, 205284.062, 204472.422, 204840.438, 204728.75, 205234.766, 204448.906, 203503.391, 204390.281, 203913.0, 202965.891, 203724.734, 203071.938, 203296.297, 203293.266, 202786.641, 202486.422, 201437.891, 202243.641, 201527.0, 201080.5, 201297.625, 201534.109, 200203.75, 200828.672, 200869.422, 200505.344, 200521.812, 200277.203, 199943.828, 199674.328, 200537.406, 200051.844, 199343.719, 199974.922], 'val_loss': [4971.581, 3926.865, 3622.924, 3323.137, 3197.324, 3145.931, 3104.606, 3095.141, 3068.601, 3032.916, 3038.847, 3011.903, 2981.061, 3013.691, 2966.396, 2955.237, 2927.062, 2911.618, 2914.132, 2911.706, 2903.325, 2881.372, 2888.542, 2863.537, 2858.605, 2836.699, 2841.938, 2859.296, 2829.663, 2820.681, 2819.766, 2824.897, 2805.333, 2792.407, 2787.758, 2806.666, 2792.801, 2762.905, 2775.743, 2766.028, 2748.051, 2799.033, 2733.609, 2763.453, 2708.839, 2755.669, 2727.235, 2697.77, 2737.098, 2716.97, 2721.641, 2750.224, 2703.083, 2696.173, 2719.679, 2731.993, 2690.836, 2683.379, 2708.363, 2675.682, 2658.443, 2660.442, 2674.902, 2673.493, 2676.742, 2685.898, 2688.624, 2653.276, 2638.026, 2663.09, 2666.268, 2646.301, 2663.368, 2642.569, 2659.828, 2658.375, 2657.005, 2668.653, 2652.503, 2638.501, 2624.436, 2674.218, 2646.652, 2619.75, 2613.461, 2625.413, 2656.973, 2638.272, 2647.728, 2619.965, 2628.661, 2628.766, 2677.515, 2621.367, 2618.16, 2631.193, 2639.672, 2619.562, 2627.908, 2630.84]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:35 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:53 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	25168.56055		727399	12	-1	213.45975399017334	{'train_loss': [911280.5, 486294.25, 382563.969, 317608.094, 280245.5, 264262.906, 254367.656, 247535.312, 242025.922, 238958.797, 236394.172, 234593.406, 232552.938, 230348.891, 229411.953, 227522.297, 225846.391, 225401.578, 224229.203, 223273.281, 223084.672, 221965.406, 220355.875, 219458.359, 219234.812, 218586.203, 217845.344, 217268.812, 217361.297, 215967.688, 215265.406, 214861.578, 214124.641, 214041.891, 212487.312, 212807.75, 212525.578, 211875.328, 210943.469, 211384.344, 210548.031, 209764.688, 208918.734, 208615.672, 208651.531, 207783.938, 207723.656, 206180.219, 206481.984, 206108.125, 206102.172, 204959.719, 204827.688, 204660.047, 204786.188, 204261.234, 204032.125, 204041.391, 202745.578, 203535.078, 203460.406, 202262.203, 201114.047, 201734.906, 201385.641, 200909.609, 200303.422, 200656.828, 200288.422, 200639.016, 198438.078, 199858.281, 199249.344, 198117.406, 197780.156, 198660.141, 198382.703, 197912.969, 197937.406, 197430.297, 197038.703, 197422.656, 196228.203, 196792.062, 195822.484, 196346.266, 195829.875, 196379.906, 195715.156, 195755.391, 195390.5, 194678.406, 194163.594, 193666.422, 194695.344, 193850.094, 194491.641, 194322.875, 193601.859, 193854.156], 'val_loss': [4559.081, 4110.144, 3472.033, 3098.161, 2794.131, 2665.157, 2629.649, 2578.929, 2550.077, 2525.338, 2507.246, 2470.526, 2437.672, 2428.281, 2420.764, 2399.425, 2398.386, 2416.322, 2378.678, 2380.448, 2389.137, 2375.827, 2344.977, 2369.333, 2380.523, 2388.766, 2392.685, 2365.862, 2366.022, 2359.459, 2334.563, 2394.279, 2342.303, 2356.289, 2300.744, 2321.21, 2338.71, 2348.764, 2309.509, 2334.513, 2308.903, 2297.314, 2322.926, 2288.61, 2315.295, 2309.897, 2283.722, 2310.86, 2298.092, 2278.876, 2278.063, 2276.063, 2280.037, 2299.958, 2275.815, 2273.401, 2270.462, 2270.661, 2279.413, 2290.662, 2272.226, 2260.754, 2274.934, 2269.647, 2255.561, 2262.603, 2260.539, 2277.314, 2275.389, 2243.278, 2251.688, 2267.273, 2246.115, 2263.285, 2257.012, 2267.931, 2266.03, 2247.416, 2253.167, 2246.33, 2242.988, 2245.259, 2250.273, 2249.936, 2254.481, 2267.487, 2249.617, 2261.09, 2247.923, 2233.848, 2244.873, 2253.089, 2258.214, 2245.783, 2230.998, 2276.242, 2253.532, 2273.198, 2263.387, 2249.646]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:74 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	131137.65625		6103296	11	-1	212.6207025051117	{'train_loss': [1167424.875, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125], 'val_loss': [11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434]}	100	100	True
