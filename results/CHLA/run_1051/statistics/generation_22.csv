id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:39 epochs:100	100	1000	True	25240.12695		505857	11	-1	183.15787839889526	{'train_loss': [683360.75, 373360.781, 318424.375, 286102.688, 268644.75, 261731.078, 257664.219, 253964.859, 251624.625, 249138.703, 247170.797, 245927.0, 243624.062, 241445.562, 238819.141, 237647.766, 235676.562, 233364.422, 232341.172, 230803.266, 229409.547, 228184.25, 227511.766, 226548.016, 225349.469, 224566.141, 224058.422, 222629.594, 222248.75, 221292.062, 221068.375, 219610.406, 220784.656, 219073.688, 218621.031, 217401.359, 217242.812, 216764.656, 215567.375, 215354.328, 214560.172, 214055.203, 214373.453, 213980.234, 212779.203, 212987.188, 212381.641, 212355.547, 212260.734, 211995.031, 211700.859, 210738.625, 210929.797, 209001.078, 210199.016, 209517.766, 209350.141, 209062.062, 208064.047, 207858.281, 207072.906, 207331.797, 207009.469, 206335.453, 206699.453, 206456.812, 206202.328, 205541.312, 205489.516, 205708.609, 204815.547, 204314.297, 204943.172, 203665.344, 203940.359, 202692.219, 204467.781, 204241.062, 204129.188, 203783.922, 203074.438, 202552.453, 202222.766, 202146.578, 201689.125, 201883.344, 202364.109, 200477.656, 201941.562, 201117.484, 200339.25, 201373.469, 201233.234, 199910.938, 200475.703, 199383.188, 200029.422, 199454.609, 199243.953, 199617.984], 'val_loss': [4796.097, 4030.958, 3616.344, 3311.869, 3217.042, 3172.665, 3125.764, 3121.109, 3126.564, 3090.095, 3062.394, 3047.213, 3023.278, 3012.742, 3006.554, 2997.182, 2957.539, 2945.36, 2925.989, 2910.409, 2919.447, 2867.883, 2877.26, 2867.24, 2852.34, 2870.408, 2860.677, 2857.221, 2835.822, 2825.222, 2832.014, 2839.892, 2824.487, 2813.615, 2803.297, 2791.261, 2792.688, 2802.882, 2781.314, 2761.198, 2795.293, 2799.133, 2762.833, 2748.832, 2780.516, 2779.473, 2736.231, 2754.642, 2749.752, 2757.648, 2767.717, 2784.791, 2732.987, 2765.682, 2742.089, 2722.104, 2756.491, 2728.007, 2747.301, 2717.153, 2738.305, 2753.992, 2718.275, 2713.379, 2701.06, 2695.305, 2704.52, 2680.883, 2657.906, 2703.809, 2718.925, 2710.341, 2684.835, 2699.692, 2696.901, 2676.391, 2719.693, 2678.887, 2698.461, 2712.127, 2749.776, 2690.823, 2685.776, 2702.514, 2718.092, 2694.515, 2709.147, 2688.842, 2713.913, 2696.792, 2657.583, 2677.879, 2689.36, 2652.104, 2691.121, 2690.515, 2674.782, 2679.232, 2666.344, 2672.587]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:26 epochs:100	100	1000	True	24798.18555		468105	9	-1	217.15545868873596	{'train_loss': [567616.25, 334393.812, 291358.969, 276083.562, 267819.562, 263364.625, 259746.891, 256393.969, 254120.859, 252163.453, 250190.328, 247878.906, 245589.172, 244390.141, 242062.203, 240323.922, 239448.922, 237830.5, 235890.078, 236096.938, 234061.0, 233769.531, 232321.875, 230856.609, 230504.5, 229544.766, 228390.969, 227279.188, 227400.156, 226672.016, 225311.406, 225528.297, 224495.766, 223472.906, 222299.969, 222185.422, 221280.375, 221192.047, 220842.422, 220053.625, 220015.5, 220214.844, 218885.484, 218192.844, 217822.391, 217462.828, 217096.109, 216800.891, 217203.516, 215646.266, 216260.641, 215618.625, 215121.625, 215197.781, 214039.203, 214570.203, 214585.922, 213675.562, 213389.578, 213333.516, 213535.609, 211921.062, 211889.672, 211460.859, 212502.609, 210876.156, 210574.0, 211220.969, 211046.766, 210982.281, 209969.312, 209995.953, 208675.594, 209689.906, 208535.969, 209397.828, 209800.625, 208595.141, 209513.656, 208454.297, 208728.406, 208329.438, 207417.734, 208099.594, 206822.391, 206466.719, 208176.703, 206385.297, 206618.375, 206296.797, 206555.844, 204850.562, 205652.625, 205535.688, 204780.047, 205340.562, 204081.266, 204730.812, 204386.031, 204274.047], 'val_loss': [2872.011, 2417.202, 2227.963, 2203.047, 2162.707, 2114.014, 2093.738, 2091.478, 2086.72, 2066.12, 2046.446, 2023.091, 1997.066, 1998.653, 2002.58, 1977.394, 1956.632, 1948.252, 1934.351, 1933.431, 1914.45, 1916.895, 1916.166, 1923.301, 1909.044, 1901.862, 1911.309, 1874.319, 1884.39, 1874.786, 1875.0, 1860.564, 1865.309, 1855.975, 1849.977, 1854.76, 1855.227, 1849.531, 1861.502, 1835.322, 1836.588, 1845.778, 1830.364, 1829.001, 1825.635, 1819.969, 1827.995, 1835.691, 1815.258, 1825.066, 1820.011, 1812.263, 1813.165, 1819.558, 1811.917, 1806.256, 1806.311, 1800.027, 1821.449, 1802.371, 1795.219, 1793.849, 1798.798, 1796.234, 1805.097, 1794.58, 1784.488, 1795.921, 1804.355, 1787.099, 1785.744, 1786.389, 1788.08, 1791.997, 1782.027, 1793.944, 1786.262, 1781.522, 1772.44, 1786.197, 1783.513, 1792.315, 1775.448, 1769.016, 1769.811, 1767.778, 1785.866, 1779.382, 1771.285, 1769.102, 1770.119, 1774.028, 1787.262, 1763.078, 1766.621, 1784.935, 1780.717, 1768.156, 1775.381, 1771.948]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:124 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:11 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.12213496383713322 beta1:0.9961019125117089 beta2:0.946266688488379 weight_decay:0.0007013149861702169 batch_size:39 epochs:100	100	1000	True	131121.76562		676692	13	-1	203.02411484718323	{'train_loss': [1163039.125, 1062567.25, 1062567.25, 1131424.625, 1062567.25, 1062567.25, 1062567.25, 1902884.25, 1157564.875, 3134577.0, 6189483.5, 19422520.0, 84386472.0, 132747648.0, 7856304.5, 1126276.75, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 3404169.75, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 6449516.0, 7661268.0, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 3053407.0, 36238132.0, 2657456.5, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 3202665.25, 6348117.0, 13504357.0, 42499716.0, 57957812.0, 36181692.0, 8070111.5, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 15165074.0, 10453996.0, 28459362.0, 23855584.0, 18315332.0, 2382983.75], 'val_loss': [13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:93 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:15 epochs:100	100	1000	True	27099.67969		1235191	11	-1	331.51622247695923	{'train_loss': [992673.875, 365387.75, 301744.531, 279751.594, 265055.844, 256009.812, 250847.0, 246780.297, 244128.344, 240542.938, 238834.344, 237272.422, 235038.188, 233336.672, 232050.875, 231344.469, 230566.75, 228660.438, 227744.422, 226669.016, 225731.156, 224487.188, 224839.125, 223967.797, 223045.047, 222211.094, 220974.656, 220162.5, 219022.594, 219220.797, 218819.344, 218148.156, 217064.734, 217109.469, 215719.547, 216407.359, 215463.922, 214767.344, 214825.469, 214353.016, 213090.938, 213948.312, 211648.844, 212074.781, 212314.047, 210671.812, 210625.5, 210577.953, 210221.578, 209465.047, 209735.578, 207740.609, 207454.062, 208748.469, 207684.297, 207681.047, 206892.5, 206266.125, 206391.141, 206102.109, 205149.719, 206213.141, 204818.281, 204543.188, 204264.125, 204001.016, 203634.641, 203467.047, 204169.141, 202537.641, 203073.172, 202740.953, 202739.391, 202704.531, 202085.0, 201748.531, 201316.375, 201035.516, 201347.406, 200141.906, 200679.359, 200800.312, 199854.828, 200324.406, 199586.094, 199339.0, 198729.266, 198492.594, 198482.391, 197934.766, 197662.438, 198074.672, 198353.172, 197765.125, 197070.0, 197549.391, 197623.375, 196869.281, 197342.172, 196755.516], 'val_loss': [1739.677, 1426.015, 1327.292, 1288.869, 1212.419, 1208.433, 1201.971, 1174.028, 1168.305, 1156.997, 1147.267, 1141.177, 1148.991, 1149.317, 1131.555, 1124.267, 1131.487, 1122.388, 1124.061, 1122.756, 1112.896, 1103.231, 1090.767, 1107.06, 1095.873, 1098.942, 1093.42, 1103.642, 1110.588, 1097.924, 1104.468, 1103.707, 1107.421, 1097.551, 1093.29, 1100.064, 1088.162, 1094.699, 1076.413, 1108.9, 1083.708, 1092.104, 1093.705, 1092.082, 1073.954, 1097.502, 1078.187, 1082.831, 1104.268, 1075.589, 1087.926, 1068.607, 1079.636, 1067.79, 1096.547, 1082.481, 1086.219, 1083.049, 1072.062, 1067.229, 1079.461, 1090.108, 1087.798, 1080.513, 1091.904, 1084.314, 1081.095, 1072.074, 1069.106, 1095.203, 1085.198, 1082.897, 1088.488, 1082.136, 1099.094, 1093.074, 1069.99, 1063.576, 1068.505, 1080.307, 1066.76, 1072.034, 1073.805, 1069.303, 1074.887, 1076.251, 1071.928, 1082.885, 1073.28, 1064.931, 1060.519, 1064.023, 1085.234, 1074.298, 1074.417, 1067.12, 1090.46, 1078.093, 1085.312, 1090.926]}	100	100	True
