id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	26000.0918		452251	14	-1	222.45428919792175	{'train_loss': [423542.938, 307182.438, 272582.094, 260143.75, 253515.562, 249635.625, 246518.953, 243969.203, 241738.297, 240484.875, 238391.547, 237298.141, 235703.859, 234018.734, 232918.859, 231558.297, 230407.016, 229429.938, 228363.031, 227359.406, 226867.516, 226080.516, 224501.0, 224653.844, 223274.0, 222639.359, 222173.547, 221643.531, 220616.328, 219595.734, 219054.422, 218327.391, 217810.75, 217402.016, 216641.609, 216549.109, 215360.031, 215262.359, 215067.219, 214028.781, 214199.359, 213105.594, 212798.453, 212354.812, 212101.406, 211586.875, 210989.484, 210253.688, 210077.219, 209579.594, 209634.812, 209339.375, 208589.859, 208970.484, 208099.984, 207665.828, 207011.547, 206462.125, 206355.562, 205962.359, 206077.891, 205718.594, 205259.484, 205262.141, 205417.484, 204958.156, 204406.266, 203815.297, 203697.938, 203844.188, 203621.312, 203585.125, 202988.953, 203288.766, 202839.312, 202837.297, 202359.469, 202379.609, 201660.234, 201237.859, 201668.641, 201922.828, 202034.719, 200780.109, 201080.844, 200381.203, 200844.719, 200209.547, 200215.438, 200463.438, 199582.609, 199544.562, 199591.969, 199640.891, 199152.781, 198757.109, 199080.562, 199088.875, 198265.156, 198341.719], 'val_loss': [3588.818, 2926.128, 2856.648, 2749.535, 2667.469, 2646.11, 2600.153, 2596.202, 2595.136, 2561.67, 2565.649, 2531.613, 2511.19, 2507.733, 2504.251, 2485.997, 2473.544, 2481.044, 2447.504, 2467.233, 2440.614, 2457.903, 2452.647, 2435.0, 2457.092, 2456.818, 2443.129, 2427.639, 2413.494, 2405.912, 2402.567, 2392.663, 2388.796, 2388.126, 2383.415, 2393.179, 2360.382, 2374.594, 2364.613, 2365.133, 2353.567, 2362.066, 2351.924, 2357.436, 2330.915, 2330.66, 2316.197, 2361.219, 2332.543, 2344.81, 2317.951, 2339.128, 2326.521, 2323.4, 2318.093, 2327.037, 2320.434, 2303.333, 2321.102, 2333.636, 2307.918, 2314.815, 2325.646, 2324.99, 2323.523, 2314.844, 2306.059, 2300.182, 2299.798, 2314.628, 2320.577, 2297.436, 2306.646, 2335.792, 2303.728, 2317.962, 2308.464, 2313.702, 2299.778, 2323.73, 2327.113, 2323.276, 2295.123, 2312.433, 2313.963, 2297.889, 2291.238, 2312.297, 2309.766, 2312.725, 2304.507, 2301.361, 2305.606, 2281.06, 2311.848, 2291.399, 2286.615, 2281.189, 2297.496, 2289.571]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:deconv1d out_channels:84 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	131131.04688		3827156	15	-1	223.62236380577087	{'train_loss': [1113967.625, 1435495.875, 1506840.0, 1070563.75, 1120550.875, 1102340.5, 1084251.125, 1062933.75, 1104348.25, 1064344.625, 1062567.125, 1071705.625, 1062567.125, 1112841.875, 1064738.25, 1070101.375, 1079261.0, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1066975.0, 1062567.125, 1064198.5, 1062558.125, 1064003.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1064100.25, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1063153.5, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125], 'val_loss': [11110.434, 11110.434, 12037.854, 11110.422, 11110.433, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.409, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	25213.33594		639490	13	-1	201.09827709197998	{'train_loss': [626668.188, 391337.5, 305244.969, 270913.469, 261177.688, 256450.297, 253435.016, 250543.5, 248059.641, 245616.062, 243255.688, 241320.234, 240083.094, 237578.656, 237006.641, 234893.938, 233999.594, 232336.359, 230896.344, 230080.312, 227568.906, 226605.234, 225681.875, 225190.156, 223858.328, 223256.203, 221893.938, 222060.75, 220656.281, 220136.141, 219140.969, 219079.031, 218104.594, 217395.688, 217184.797, 216246.75, 215881.891, 215198.391, 215292.344, 214913.938, 214732.953, 214154.281, 214311.594, 213664.312, 212416.094, 211277.828, 212323.234, 211658.141, 210870.578, 211135.906, 211652.953, 210161.859, 210568.547, 209878.953, 209513.812, 210349.078, 207998.141, 209002.781, 208625.312, 208708.375, 207621.125, 206799.938, 207390.969, 206548.109, 206117.203, 206727.141, 206968.906, 206617.719, 204866.891, 205499.297, 204954.625, 204291.938, 205824.109, 204714.297, 204459.828, 204647.344, 203692.781, 203801.828, 203429.469, 203470.797, 203015.875, 204584.453, 203117.922, 202652.984, 202994.688, 202348.609, 202620.594, 201830.859, 201515.328, 201847.641, 201273.547, 200191.516, 201196.125, 201502.281, 201088.922, 201187.719, 200400.062, 198967.125, 200766.719, 200251.719], 'val_loss': [3995.679, 3159.414, 3033.345, 2692.621, 2682.259, 2616.566, 2613.94, 2624.07, 2577.355, 2560.056, 2590.631, 2500.933, 2498.645, 2496.656, 2468.329, 2462.178, 2426.891, 2421.42, 2424.158, 2443.827, 2396.564, 2418.415, 2399.927, 2380.354, 2350.165, 2365.372, 2354.448, 2375.384, 2372.863, 2366.142, 2365.494, 2350.614, 2330.649, 2353.147, 2355.017, 2383.683, 2335.236, 2337.357, 2342.865, 2330.094, 2345.013, 2316.823, 2318.805, 2311.885, 2327.792, 2312.001, 2321.794, 2324.785, 2308.158, 2285.853, 2309.459, 2316.069, 2284.052, 2301.535, 2276.609, 2276.876, 2286.87, 2272.825, 2271.288, 2249.13, 2262.229, 2262.702, 2265.556, 2250.856, 2259.252, 2238.186, 2229.014, 2272.268, 2256.539, 2236.692, 2241.286, 2273.533, 2223.381, 2247.84, 2247.312, 2236.832, 2234.072, 2220.851, 2285.493, 2220.661, 2238.1, 2239.213, 2240.705, 2237.538, 2234.475, 2201.085, 2194.788, 2213.294, 2193.194, 2223.048, 2218.955, 2227.489, 2191.758, 2204.548, 2162.75, 2207.379, 2239.682, 2195.192, 2220.981, 2183.572]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:96 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.20877010841772514 beta1:0.920373898137654 beta2:0.8588312239587809 weight_decay:5.8192803963409654e-06 batch_size:32 epochs:100	100	1000	True	131121.29688		486603	13	-1	241.26687240600586	{'train_loss': [1066545.625, 1082472.25, 1063897.125, 1098119.5, 1068695.125, 1083742.5, 1074782.125, 1065055.875, 1071116.375, 1068617.375, 1073679.875, 1066356.75, 1064058.25, 1067511.75, 1067356.875, 1072540.375, 1071025.75, 1071211.625, 1065008.25, 1066314.25, 1070598.125, 1066944.125, 1069914.0, 1070682.375, 1075573.5, 1070118.0, 1070318.125, 1075339.375, 1071194.0, 1069565.0, 1066741.5, 1066447.375, 1078010.75, 1067706.75, 1070706.25, 1065741.0, 1066485.75, 1066785.75, 1066170.0, 1067662.75, 1064539.375, 1070713.375, 1068339.625, 1070514.375, 1068933.0, 1063959.875, 1066000.625, 1066445.125, 1064731.125, 1066017.875, 1066528.75, 1066838.5, 1067684.5, 1064258.0, 1066322.875, 1064544.5, 1066783.25, 1066392.5, 1066282.75, 1075819.625, 1065151.0, 1068378.75, 1065724.25, 1068232.625, 1068143.375, 1069565.125, 1070796.375, 1066448.625, 1070358.75, 1066806.25, 1068945.75, 1069144.5, 1064541.25, 1079466.875, 1066131.75, 1075235.375, 1067488.0, 1067204.125, 1070314.25, 1064407.0, 1077796.625, 1064839.875, 1067426.5, 1072544.25, 1064483.625, 1077694.375, 1063735.375, 1068136.625, 1067706.25, 1070025.75, 1072663.125, 1063793.625, 1070335.625, 1064769.125, 1073854.5, 1068182.875, 1066019.25, 1074032.75, 1069459.125, 1074431.0], 'val_loss': [11110.434, 11163.055, 11110.434, 11110.434, 11110.434, 12241.123, 11110.434, 11110.434, 11110.434, 11598.852, 11110.434, 11110.434, 11421.148, 11110.434, 11620.439, 11110.434, 11110.434, 11076.607, 11110.434, 11108.753, 11110.434, 11110.434, 11583.269, 11110.434, 11109.202, 11110.434, 11380.846, 11110.434, 11110.434, 11110.434, 11110.434, 11585.803, 11110.434, 11110.434, 33301.27, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11091.568, 13231.319, 11110.434, 11110.404, 11110.434, 11110.434, 11549.076, 11110.434, 11080.829, 11110.434, 11110.434, 11109.757, 11617.924, 11110.434, 11110.434, 11090.598, 11110.434, 11110.434, 11108.33, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11082.592, 11110.434, 12240.517, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11382.121, 11110.434, 11498.844, 11110.433, 11110.434, 11110.434, 11110.434, 12016.81, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434]}	100	100	True
