id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	2000	True	24949.08789		619010	12	-1	212.36537146568298	{'train_loss': [622885.875, 411050.75, 346246.969, 309810.75, 296759.281, 285967.375, 266799.812, 257009.625, 251151.906, 247238.203, 244498.969, 241592.297, 240000.578, 237554.297, 236463.844, 234256.0, 233684.906, 232084.016, 231243.031, 230331.375, 229990.688, 228899.125, 227050.656, 226196.516, 225226.812, 225133.438, 224432.062, 223259.984, 223317.25, 222838.828, 222124.109, 221009.281, 220712.844, 220423.828, 219525.938, 218845.469, 218300.578, 217333.172, 218195.797, 217920.516, 216544.031, 215649.625, 216446.531, 215566.25, 215253.719, 214626.859, 214230.203, 214104.812, 213906.078, 213028.953, 212348.625, 212798.953, 212425.453, 211403.844, 211679.438, 210319.609, 210515.266, 210051.422, 209379.422, 210159.094, 209686.828, 208580.688, 208836.625, 208782.047, 207812.0, 207626.078, 207774.578, 206798.0, 206882.609, 206468.969, 206943.203, 206520.969, 207082.953, 205962.5, 204330.375, 205995.703, 204708.641, 205426.406, 205049.172, 203900.031, 204701.109, 203357.656, 203874.859, 204004.594, 203158.734, 204104.953, 204071.469, 202600.078, 201971.75, 202609.594, 201654.547, 202141.969, 201366.953, 201223.203, 201681.188, 201834.469, 201259.359, 200744.938, 200849.938, 201195.328], 'val_loss': [3726.359, 3645.006, 3258.302, 3079.547, 3055.728, 2838.98, 2699.772, 2623.314, 2574.601, 2562.939, 2531.048, 2500.143, 2491.116, 2488.764, 2475.978, 2492.571, 2446.772, 2473.931, 2422.932, 2426.942, 2425.588, 2428.189, 2460.233, 2405.706, 2390.23, 2381.223, 2382.795, 2381.448, 2369.91, 2350.889, 2370.985, 2339.178, 2334.862, 2347.163, 2336.465, 2322.763, 2343.002, 2338.351, 2303.27, 2311.293, 2311.303, 2291.729, 2307.474, 2279.135, 2287.753, 2286.545, 2303.563, 2287.485, 2267.244, 2284.146, 2285.069, 2272.525, 2285.801, 2290.385, 2280.137, 2269.286, 2281.788, 2285.713, 2276.972, 2276.696, 2252.022, 2264.431, 2287.569, 2291.718, 2276.013, 2287.646, 2264.085, 2265.081, 2267.64, 2265.683, 2259.525, 2256.296, 2301.74, 2252.381, 2292.376, 2271.531, 2267.504, 2256.811, 2275.208, 2251.28, 2262.213, 2304.073, 2233.529, 2252.221, 2283.639, 2236.231, 2271.587, 2227.218, 2211.272, 2229.729, 2252.855, 2220.67, 2240.327, 2225.186, 2216.799, 2233.323, 2220.188, 2203.279, 2202.192, 2193.335]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:78 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:106 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.12923034939370717 beta1:0.9939053750763434 beta2:0.8619327659229998 weight_decay:5.968935150498885e-05 batch_size:32 epochs:100	100	1000	True	131126.70312		1278520	11	-1	215.86251640319824	{'train_loss': [1233048.875, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 9311432.0, 1062567.125, 30110430.0, 18766156.0, 72143504.0, 87973112.0, 65674580.0, 20323124.0, 43372920.0, 38427428.0, 33677732.0, 15061104.0, 24126284.0, 20593962.0, 16040603.0, 12760649.0, 15741580.0, 19413942.0, 14986900.0, 2609295.5, 7341741.5, 20479112.0, 6631583.0, 28898006.0, 42473788.0, 52422568.0, 58841736.0, 56295464.0, 33382372.0, 1989138.375, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125], 'val_loss': [441547808.0, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:95 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	25657.52539		816526	11	-1	207.34702491760254	{'train_loss': [1010143.25, 516307.344, 363388.625, 294748.125, 270909.438, 261009.125, 255209.594, 250815.891, 246899.297, 244094.703, 241439.484, 239785.812, 238154.531, 236681.453, 233669.172, 232438.219, 232844.031, 230851.719, 229911.109, 229184.594, 227912.109, 226704.719, 226483.266, 225427.828, 223479.391, 223427.297, 223341.641, 221487.094, 221246.828, 220133.484, 219747.203, 218665.578, 218620.984, 217395.625, 217753.156, 217149.484, 215470.562, 215401.641, 214705.406, 214832.719, 214467.469, 214064.266, 212946.172, 212317.359, 211568.859, 212021.922, 211558.016, 211042.516, 210448.469, 209723.766, 208806.188, 207484.719, 208413.891, 208351.625, 207807.422, 207784.438, 207353.25, 206178.812, 206905.562, 206144.469, 204792.016, 205166.406, 204294.25, 204997.25, 205553.781, 204121.109, 203509.453, 203286.422, 203454.391, 202470.359, 202935.078, 203043.609, 201689.281, 200922.641, 201396.938, 200601.688, 201808.484, 200038.219, 200531.016, 199642.188, 199611.203, 199651.875, 199499.359, 198023.219, 198816.0, 198737.125, 197986.953, 197981.453, 197782.734, 198105.359, 197468.219, 197801.297, 195921.344, 196538.156, 196501.0, 196125.188, 196563.359, 195950.797, 195393.688, 196089.031], 'val_loss': [5796.529, 3634.016, 2986.088, 2807.453, 2715.442, 2649.972, 2644.871, 2548.413, 2533.588, 2511.513, 2479.085, 2485.661, 2450.151, 2466.992, 2446.311, 2451.104, 2425.571, 2431.078, 2418.755, 2421.131, 2375.191, 2378.385, 2383.428, 2389.462, 2369.621, 2348.473, 2361.318, 2414.948, 2364.835, 2342.265, 2336.232, 2342.381, 2326.723, 2337.865, 2329.616, 2326.851, 2334.675, 2321.025, 2312.042, 2320.555, 2304.221, 2306.073, 2310.625, 2300.992, 2272.133, 2299.253, 2287.879, 2273.866, 2268.448, 2288.061, 2280.511, 2271.793, 2256.67, 2254.148, 2265.294, 2263.819, 2278.598, 2255.16, 2264.325, 2230.785, 2258.643, 2238.302, 2252.533, 2239.366, 2252.216, 2232.568, 2251.634, 2243.941, 2263.983, 2231.24, 2246.044, 2240.818, 2248.752, 2270.696, 2240.777, 2232.669, 2246.336, 2220.786, 2248.738, 2234.695, 2238.79, 2249.65, 2239.203, 2223.345, 2236.281, 2242.931, 2242.149, 2261.108, 2236.954, 2240.967, 2247.576, 2216.658, 2242.447, 2251.364, 2237.591, 2247.75, 2246.073, 2268.68, 2256.06, 2234.984]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	24688.64062		619010	12	-1	211.38886189460754	{'train_loss': [617015.438, 385951.031, 308155.312, 276856.344, 263937.844, 257665.672, 253707.688, 251225.047, 247747.703, 245912.562, 243670.422, 242145.016, 240742.109, 238752.406, 237358.0, 235745.094, 234861.859, 232685.484, 231721.969, 230459.141, 229023.078, 227927.047, 226420.562, 225624.734, 224274.016, 223040.812, 223062.891, 222249.922, 221332.641, 220161.453, 219232.438, 219206.625, 218098.391, 217823.0, 217123.641, 216418.453, 216373.859, 215325.594, 214595.047, 214410.875, 214243.375, 212729.906, 213320.75, 212397.438, 212100.281, 211267.141, 211130.062, 210046.359, 210543.281, 209953.906, 209175.031, 209930.062, 209716.391, 207965.906, 207896.375, 208252.953, 208056.281, 207407.453, 207505.422, 206500.047, 207508.781, 206123.109, 204860.141, 205875.688, 205251.0, 204877.781, 205273.516, 204972.328, 205111.203, 204062.062, 204455.25, 204379.156, 203679.344, 203249.328, 203879.297, 202791.438, 202034.359, 203424.781, 201829.984, 202427.375, 202353.219, 201182.531, 201931.141, 200759.125, 201151.562, 201080.109, 200079.75, 200929.609, 200538.875, 199056.297, 200601.031, 199424.094, 199403.656, 200866.297, 198393.422, 198019.75, 199324.5, 198599.656, 198647.672, 198911.953], 'val_loss': [4036.785, 3191.049, 2997.383, 2760.399, 2666.449, 2652.964, 2639.23, 2623.656, 2547.024, 2542.499, 2517.681, 2506.105, 2501.254, 2468.247, 2509.712, 2457.178, 2466.087, 2453.032, 2438.242, 2424.999, 2401.969, 2409.757, 2404.334, 2401.501, 2395.709, 2406.874, 2405.616, 2423.147, 2401.026, 2370.51, 2368.629, 2352.708, 2384.267, 2368.651, 2357.167, 2352.881, 2353.598, 2334.072, 2355.06, 2348.479, 2341.293, 2322.192, 2332.435, 2331.198, 2342.542, 2327.256, 2313.721, 2316.222, 2348.027, 2293.244, 2293.599, 2314.509, 2307.658, 2297.515, 2307.861, 2282.839, 2273.036, 2316.342, 2282.54, 2284.438, 2268.789, 2277.729, 2291.428, 2262.39, 2259.117, 2269.754, 2251.131, 2264.678, 2267.669, 2251.791, 2283.368, 2284.596, 2259.971, 2234.46, 2268.387, 2244.485, 2266.922, 2244.816, 2254.579, 2222.357, 2233.133, 2227.795, 2222.877, 2249.968, 2243.026, 2226.06, 2242.12, 2232.277, 2220.972, 2245.265, 2229.74, 2229.6, 2238.452, 2217.465, 2259.632, 2248.202, 2217.803, 2235.969, 2239.257, 2205.849]}	100	100	True
