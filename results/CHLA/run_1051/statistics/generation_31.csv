id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	3000	True	23835.25781		416732	11	-1	234.79385137557983	{'train_loss': [494314.969, 328183.844, 281001.5, 266234.344, 259828.453, 255246.891, 252085.547, 248782.0, 245506.359, 242027.391, 239871.547, 238021.547, 235683.125, 234124.0, 232697.469, 231461.766, 230662.266, 230110.766, 228303.359, 227474.844, 226532.672, 225133.891, 225354.734, 223369.625, 222972.047, 222221.031, 221785.016, 221726.906, 219347.875, 219798.141, 218301.859, 218480.797, 217670.094, 216238.5, 216370.469, 215362.531, 214792.359, 214436.5, 213905.656, 213942.109, 212722.141, 212999.703, 212371.922, 212021.531, 211657.141, 211244.797, 210147.594, 210004.828, 209570.719, 209157.531, 209757.562, 207444.844, 208047.734, 207253.641, 207180.453, 207611.578, 206703.75, 206508.219, 205957.375, 204973.0, 204860.203, 204786.844, 204298.906, 202723.438, 204004.547, 203763.188, 203820.5, 203180.109, 202217.594, 201464.578, 201772.375, 201092.312, 199917.812, 200454.484, 200584.125, 199756.922, 199551.531, 199939.141, 199634.812, 199501.312, 198363.547, 198487.531, 197883.297, 197831.047, 197440.141, 197845.375, 198013.688, 196534.891, 197728.578, 196750.859, 196955.516, 196065.797, 195183.594, 196223.406, 195550.391, 194885.25, 194675.547, 194228.0, 194763.328, 194785.172], 'val_loss': [3494.07, 2416.108, 2194.794, 2135.95, 2126.214, 2093.844, 2067.359, 2037.007, 2035.002, 1980.9, 1981.919, 2001.832, 1927.764, 1922.643, 1931.222, 1889.861, 1920.574, 1902.742, 1879.589, 1884.492, 1871.529, 1865.482, 1868.284, 1873.617, 1845.426, 1852.246, 1854.758, 1845.721, 1868.836, 1827.583, 1837.552, 1842.053, 1817.136, 1822.295, 1796.548, 1817.192, 1828.242, 1821.515, 1824.914, 1796.439, 1838.026, 1816.11, 1816.783, 1819.84, 1795.767, 1798.624, 1772.953, 1796.423, 1779.444, 1783.746, 1781.716, 1772.271, 1779.77, 1760.528, 1797.973, 1775.233, 1750.173, 1775.646, 1768.382, 1765.078, 1761.275, 1745.872, 1774.118, 1747.035, 1743.043, 1813.899, 1728.438, 1736.541, 1741.618, 1743.549, 1753.285, 1757.873, 1749.115, 1741.724, 1753.116, 1727.447, 1728.769, 1720.668, 1729.453, 1737.841, 1721.412, 1729.636, 1720.535, 1719.222, 1723.6, 1722.401, 1726.074, 1705.708, 1718.434, 1727.936, 1702.457, 1717.236, 1733.994, 1696.856, 1717.228, 1703.146, 1730.343, 1706.076, 1708.017, 1685.258]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:26 epochs:100	100	3000	True	24764.51172		834170	10	-1	228.23972964286804	{'train_loss': [1122820.0, 684934.75, 462447.594, 415882.281, 370270.688, 314414.844, 283075.375, 272678.969, 265843.562, 261808.125, 257845.484, 253980.188, 250491.609, 247036.172, 244826.391, 242168.562, 239596.656, 238123.719, 235972.125, 234192.766, 233311.359, 231929.766, 229792.625, 228027.25, 226306.938, 225655.234, 224552.453, 224057.844, 222351.453, 221225.672, 219771.047, 220125.219, 219263.922, 218668.406, 217085.797, 217769.891, 216537.844, 216206.672, 214225.625, 214659.25, 214093.719, 213372.188, 212990.641, 211186.922, 212313.719, 210900.688, 211554.531, 210513.609, 209934.578, 208848.25, 209524.516, 209110.109, 209088.062, 208521.625, 208360.766, 206492.109, 206867.578, 206457.922, 206429.766, 206443.375, 206153.484, 205385.641, 204540.109, 205090.578, 203678.938, 204045.516, 204041.734, 203029.188, 202507.953, 202944.875, 202584.203, 201178.359, 201028.453, 201394.5, 202104.375, 202045.156, 199934.078, 200218.938, 199801.219, 199541.484, 199009.359, 198906.016, 199726.938, 198330.141, 198159.156, 197570.453, 198626.188, 196498.094, 197943.469, 196673.562, 197398.047, 196397.719, 196790.469, 196609.984, 195813.094, 195641.188, 195376.031, 195173.641, 194781.953, 195441.5], 'val_loss': [8671.672, 3793.457, 3115.148, 2721.182, 2501.766, 2428.952, 2301.309, 2212.854, 2185.084, 2152.138, 2153.563, 2114.262, 2085.246, 2064.481, 2021.783, 2037.817, 2019.488, 1996.83, 2012.319, 1960.776, 1947.473, 1949.802, 1940.183, 1936.053, 1898.938, 1882.348, 1880.501, 1904.22, 1900.601, 1885.244, 1871.835, 1901.916, 1837.374, 1893.312, 1856.362, 1836.616, 1843.792, 1864.447, 1840.756, 1842.502, 1848.667, 1833.907, 1846.491, 1875.645, 1839.535, 1831.631, 1826.752, 1838.213, 1817.765, 1811.615, 1800.635, 1840.626, 1822.653, 1837.667, 1801.043, 1782.56, 1796.807, 1783.643, 1771.892, 1780.314, 1792.378, 1785.233, 1768.076, 1773.433, 1780.381, 1770.119, 1760.524, 1762.689, 1777.622, 1769.31, 1793.41, 1770.817, 1793.132, 1746.377, 1751.783, 1746.714, 1770.261, 1785.192, 1770.224, 1761.642, 1762.978, 1754.795, 1770.124, 1736.085, 1729.271, 1729.495, 1731.937, 1747.358, 1722.687, 1720.824, 1743.083, 1728.647, 1724.793, 1728.952, 1734.996, 1716.596, 1739.414, 1703.844, 1716.364, 1734.68]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:88 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:21 epochs:100	100	1000	True	25703.5332		509460	10	-1	255.67453813552856	{'train_loss': [600746.25, 323994.344, 286072.094, 272327.812, 263724.438, 257849.125, 253361.094, 250823.562, 247650.0, 245667.703, 243201.172, 241473.969, 239518.797, 237691.547, 235987.844, 235168.734, 233329.266, 232578.266, 231647.109, 229641.812, 229648.391, 228101.984, 227506.188, 225824.25, 225913.219, 224911.578, 224320.719, 223514.25, 223201.703, 222075.312, 221180.828, 220763.781, 220452.406, 220042.391, 219122.375, 219835.938, 218472.438, 218328.891, 218147.188, 216888.125, 216647.219, 216490.828, 216533.672, 215984.484, 214987.828, 214576.094, 214450.062, 213803.641, 213526.188, 213538.594, 213365.141, 212952.0, 212114.375, 211569.812, 211550.453, 210735.719, 210961.328, 210801.328, 210798.0, 210906.188, 211057.172, 209354.078, 209898.297, 209632.766, 209767.828, 209591.156, 208882.578, 208188.828, 208837.766, 208410.828, 207876.531, 207580.703, 207425.344, 206885.656, 207318.312, 206067.938, 205805.938, 205885.312, 204938.719, 206022.656, 205088.516, 205980.312, 206247.344, 204502.844, 204825.125, 205150.062, 204193.734, 204393.328, 205090.562, 203179.281, 203674.172, 203460.375, 204612.094, 203129.766, 202597.641, 203108.094, 203079.797, 202006.406, 202388.312, 201879.141], 'val_loss': [2324.478, 1975.094, 1884.376, 1800.227, 1780.776, 1758.872, 1746.611, 1771.566, 1697.093, 1683.58, 1689.084, 1692.287, 1675.345, 1652.627, 1645.787, 1646.582, 1641.546, 1639.583, 1641.247, 1657.476, 1622.555, 1602.623, 1614.642, 1600.399, 1602.028, 1602.436, 1587.534, 1578.39, 1570.973, 1598.409, 1587.073, 1587.978, 1585.754, 1565.962, 1566.393, 1574.395, 1568.815, 1574.091, 1561.942, 1558.199, 1564.049, 1566.709, 1543.632, 1562.331, 1564.31, 1560.178, 1556.544, 1565.566, 1567.16, 1554.586, 1552.753, 1543.2, 1547.938, 1553.739, 1539.556, 1535.151, 1540.353, 1540.838, 1539.075, 1544.345, 1544.722, 1514.776, 1520.179, 1536.54, 1536.243, 1531.004, 1528.805, 1533.977, 1547.4, 1509.12, 1514.65, 1529.972, 1525.218, 1500.333, 1527.553, 1522.053, 1526.548, 1512.183, 1524.418, 1528.617, 1528.113, 1523.245, 1518.036, 1510.623, 1512.159, 1511.779, 1527.655, 1507.869, 1516.059, 1505.332, 1529.422, 1529.755, 1521.609, 1544.305, 1505.048, 1511.094, 1513.15, 1506.466, 1509.606, 1517.698]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:rmsprop lr:0.12246590120188686 alpha:0.8090747269967717 weight_decay:0.0003910872800064342 batch_size:26 epochs:100	100	1000	True	131121.10938		422365	11	-1	227.18560075759888	{'train_loss': [1225332.375, 1163011.25, 1136479.75, 1144206.625, 1134234.0, 1130714.375, 1114780.25, 1104173.125, 1106780.75, 1099666.5, 1094142.125, 1101954.25, 1102953.625, 1100625.125, 1100688.0, 1101767.625, 1100718.375, 1095392.375, 1101872.25, 1102443.75, 1095436.0, 1101249.375, 1104839.5, 1092918.75, 1101162.125, 1101177.0, 1104282.375, 1099819.5, 1098382.125, 1096602.625, 1097169.375, 1095737.0, 1100825.25, 1097528.25, 1091601.875, 1101608.125, 1094011.5, 1100469.875, 1099205.375, 1097929.625, 1089949.625, 1099451.75, 1095846.0, 1093910.375, 1097084.625, 1097716.875, 1094598.375, 1097332.625, 1095519.25, 1103118.875, 1096678.5, 1094818.875, 1094698.75, 1097646.5, 1106333.125, 1096438.375, 1097422.75, 1091767.75, 1097983.875, 1095653.75, 1097621.25, 1097156.5, 1105946.75, 1091768.875, 1094386.875, 1096685.75, 1089993.125, 1094040.5, 1095736.75, 1097959.0, 1099473.0, 1096324.75, 1092751.125, 1095032.0, 1097996.625, 1102044.625, 1092075.5, 1098581.5, 1101464.75, 1100401.625, 1092105.625, 1093054.625, 1095749.25, 1097894.875, 1101974.75, 1100332.0, 1099616.875, 1094472.125, 1094022.125, 1096566.375, 1098782.625, 1096253.625, 1093066.5, 1098303.5, 1097809.375, 1092211.625, 1093590.75, 1091726.0, 1093648.75, 1095540.25], 'val_loss': [8888.341, 8865.098, 8930.033, 9949.883, 10479.61, 9484.169, 8856.686, 8880.462, 9105.714, 8888.22, 9205.652, 8885.125, 8881.752, 8885.211, 8888.349, 8834.645, 9258.374, 9404.057, 8860.332, 8888.34, 9577.847, 8887.961, 8959.33, 9198.7, 8881.091, 9036.606, 9694.239, 8887.909, 8880.773, 8888.244, 9292.782, 8993.711, 9248.474, 8859.528, 10206.731, 8874.862, 8879.553, 9011.269, 8879.781, 8995.621, 9258.172, 9356.436, 9011.096, 8904.422, 8877.625, 9745.018, 9881.794, 8967.846, 9488.782, 8994.456, 8978.189, 8973.571, 9217.213, 9846.349, 9955.642, 8887.542, 9335.823, 9395.818, 8882.118, 8857.149, 8886.327, 9822.878, 8850.849, 9284.089, 9065.83, 8888.346, 9546.055, 8887.609, 9382.983, 8972.444, 8888.333, 9000.125, 9079.352, 9238.8, 9342.409, 9144.941, 8935.251, 8884.486, 8893.17, 8888.164, 9787.18, 9219.386, 8885.498, 8888.328, 9389.534, 8872.944, 8994.148, 8863.218, 8899.882, 8971.298, 9424.912, 10042.936, 8888.172, 9217.245, 8940.642, 8898.862, 9521.604, 9649.044, 9692.759, 8888.347]}	100	100	True
