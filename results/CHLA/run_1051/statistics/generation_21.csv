id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:39 epochs:100	100	1000	True	24416.36328		505857	11	-1	182.3672513961792	{'train_loss': [680240.0, 371680.094, 322245.719, 286308.594, 264198.062, 254504.312, 248178.906, 243261.906, 240126.734, 236485.625, 234182.891, 232499.812, 230634.422, 228981.266, 227870.75, 226409.438, 224704.766, 223916.828, 223430.734, 221098.688, 221221.188, 220986.297, 219661.328, 218317.734, 218121.828, 216371.203, 216181.516, 215698.906, 215130.047, 214433.172, 214185.547, 213317.969, 212388.5, 212840.297, 212555.406, 212754.531, 210910.391, 211288.969, 210582.906, 210290.062, 210301.609, 208981.016, 209266.391, 209467.312, 208097.969, 208367.531, 207698.953, 207881.438, 206964.703, 207721.047, 207267.938, 206293.922, 207011.516, 205796.156, 206707.625, 205932.391, 205526.297, 205307.875, 205393.375, 204980.344, 204760.047, 204332.281, 203457.203, 203340.859, 203426.484, 204318.875, 203833.219, 202608.219, 202086.234, 202582.016, 203495.75, 203082.797, 202105.0, 202171.453, 202158.406, 200879.516, 201515.328, 200365.625, 201059.719, 200771.531, 200562.016, 200806.875, 201097.594, 200623.672, 199806.656, 199291.203, 199864.516, 199396.188, 199459.609, 198926.188, 199120.031, 198062.641, 199403.547, 198961.172, 198755.844, 198013.547, 198088.078, 197694.031, 198356.578, 197965.391], 'val_loss': [4779.195, 4320.353, 3641.453, 3304.179, 3153.123, 3054.285, 3012.397, 2974.47, 2961.783, 2922.369, 2916.426, 2903.134, 2872.137, 2843.813, 2862.474, 2813.06, 2837.412, 2797.578, 2824.644, 2792.797, 2774.053, 2802.325, 2764.082, 2825.375, 2790.084, 2773.684, 2748.723, 2738.646, 2751.949, 2756.485, 2729.63, 2721.727, 2714.523, 2714.165, 2730.04, 2721.262, 2733.098, 2718.735, 2713.185, 2726.38, 2708.666, 2705.182, 2693.451, 2683.237, 2705.552, 2698.279, 2706.775, 2690.605, 2702.889, 2688.232, 2687.339, 2715.267, 2683.285, 2682.76, 2686.637, 2683.171, 2689.217, 2691.371, 2684.878, 2660.065, 2673.14, 2685.99, 2674.581, 2682.974, 2654.89, 2677.672, 2659.217, 2672.081, 2636.988, 2653.914, 2637.77, 2653.947, 2663.516, 2613.71, 2617.42, 2636.219, 2611.299, 2631.725, 2647.02, 2626.793, 2647.921, 2632.925, 2605.552, 2626.156, 2608.372, 2639.566, 2623.948, 2606.206, 2617.36, 2596.952, 2613.158, 2606.75, 2603.838, 2589.005, 2578.363, 2609.742, 2589.5, 2609.459, 2586.525, 2587.164]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.11457348003389384 beta1:0.9149314348280542 beta2:0.8694769389056843 weight_decay:6.859114491221921e-05 batch_size:39 epochs:100	100	1000	True	131121.34375		518402	11	-1	188.1120250225067	{'train_loss': [1070560.0, 1062657.875, 1089482.375, 1066792.0, 1097999.25, 1064574.625, 1082649.625, 1073795.625, 1072222.625, 1077732.0, 1067040.375, 1100822.125, 1065298.0, 1072563.75, 1070771.125, 1066693.25, 1068996.75, 1064270.125, 1067584.0, 1064549.0, 1066346.125, 1065063.5, 1065131.875, 1065747.5, 1064769.5, 1065914.0, 1064968.0, 1065257.5, 1065305.75, 1065410.125, 1065527.375, 1065148.75, 1065271.625, 1065166.375, 1065800.125, 1064664.375, 1065003.875, 1065626.125, 1065440.125, 1065244.625, 1065078.125, 1065428.625, 1065258.125, 1064927.0, 1065257.625, 1065424.625, 1065656.25, 1065054.25, 1064801.0, 1066117.25, 1065444.75, 1065965.25, 1065738.25, 1066397.5, 1065099.375, 1065366.5, 1065588.25, 1065307.625, 1065974.875, 1065303.5, 1064821.625, 1065641.125, 1064688.375, 1064722.25, 1065602.5, 1065273.125, 1066720.5, 1064659.5, 1065851.25, 1065123.375, 1065281.625, 1065361.375, 1065371.375, 1065602.0, 1065773.0, 1065568.0, 1065014.875, 1065491.375, 1066340.0, 1065105.0, 1066111.75, 1065135.875, 1065174.125, 1065712.375, 1065910.25, 1065444.25, 1065850.75, 1065155.0, 1065099.0, 1066550.875, 1065570.375, 1065555.75, 1066063.375, 1065421.125, 1065943.875, 1065404.0, 1064969.375, 1065799.625, 1064649.625, 1065662.75], 'val_loss': [13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 14055.239, 13332.521, 13332.521, 14116.123, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.52, 13332.521, 13332.521, 13332.515, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13324.628, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13606.926, 13332.521, 13332.521, 13332.521, 13332.52, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.505, 13332.521, 13332.521, 13332.521, 13332.521, 13332.506, 13332.521, 13332.521, 13341.355, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.347, 13332.521, 13332.521, 13380.276, 13367.887, 13332.521, 13332.521, 13332.521, 13332.506, 13332.521, 13332.52, 13332.521, 13332.521, 13332.521]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:39 epochs:100	100	1000	True	25143.78711		505857	11	-1	182.05496501922607	{'train_loss': [683467.125, 384116.938, 323527.656, 290928.344, 271820.406, 260545.812, 251528.859, 247948.375, 244437.25, 241823.469, 238838.906, 236818.078, 235083.594, 233330.812, 231860.438, 230541.812, 229088.312, 229058.703, 226315.703, 226437.969, 226224.984, 224766.078, 224088.125, 223716.516, 221405.375, 220771.531, 219945.547, 220554.516, 219371.203, 219033.922, 218545.531, 217822.516, 217596.219, 216201.953, 216087.688, 216316.656, 215446.047, 214786.953, 214521.781, 214048.391, 213266.094, 213496.844, 212868.672, 212869.406, 211609.047, 211268.547, 211350.5, 210804.156, 209470.016, 210222.281, 209129.875, 209850.703, 208778.969, 208776.547, 208383.094, 208563.297, 207655.359, 207099.031, 206142.734, 207002.781, 206292.828, 205603.391, 205770.234, 205670.766, 204832.031, 206385.594, 205055.734, 204312.016, 204819.016, 204247.922, 203314.906, 202996.156, 202869.219, 201877.75, 202859.438, 202439.047, 202396.75, 201742.391, 202629.125, 200940.641, 201222.297, 201157.062, 201046.859, 201169.281, 200113.969, 200940.125, 200097.625, 199458.297, 200290.609, 198813.516, 198931.172, 199919.969, 198629.734, 200145.141, 199899.656, 198775.906, 199143.328, 199382.703, 198954.781, 199016.562], 'val_loss': [5075.424, 4229.413, 3708.125, 3337.859, 3249.23, 3108.446, 3090.925, 3023.828, 2990.765, 2976.628, 2975.81, 2929.312, 2908.231, 2908.523, 2898.183, 2904.545, 2862.802, 2873.974, 2856.437, 2851.696, 2844.198, 2839.964, 2822.994, 2824.707, 2860.546, 2817.272, 2808.451, 2814.522, 2789.071, 2816.178, 2812.983, 2799.425, 2795.433, 2783.522, 2769.297, 2803.733, 2776.406, 2829.752, 2785.185, 2792.856, 2791.517, 2786.954, 2747.199, 2739.369, 2736.72, 2768.379, 2777.515, 2741.042, 2744.675, 2732.32, 2734.57, 2755.324, 2722.824, 2730.492, 2760.831, 2724.462, 2727.065, 2727.947, 2714.132, 2733.066, 2706.578, 2706.979, 2743.216, 2706.509, 2733.79, 2698.913, 2682.659, 2723.82, 2689.444, 2716.13, 2714.648, 2689.321, 2697.845, 2699.34, 2697.736, 2699.654, 2704.085, 2680.121, 2703.195, 2692.739, 2663.987, 2668.791, 2674.798, 2674.129, 2694.927, 2661.012, 2661.817, 2672.547, 2637.233, 2606.319, 2642.928, 2636.864, 2626.108, 2625.156, 2676.214, 2642.006, 2673.141, 2627.077, 2639.881, 2627.781]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:60 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.18007086107570966 alpha:0.821768381658242 weight_decay:1.4020691846827748e-05 batch_size:39 epochs:100	100	1000	True	131121.375		527069	12	-1	190.52025961875916	{'train_loss': [1217432.125, 1062566.875, 1484169.0, 1242959.25, 1070389.375, 1251028.875, 1276030.375, 1177568.25, 1299795.25, 1407319.125, 1137086.625, 1260546.625, 1145085.375, 1421920.375, 1189124.625, 1314541.0, 1314792.25, 1213690.25, 1257055.25, 1177691.375, 1136639.125, 1119625.5, 1157261.375, 1145832.75, 1109214.375, 1135109.25, 1111743.25, 1117354.375, 1139391.875, 1123047.625, 1122388.375, 1130406.875, 1131128.0, 1137071.0, 1181359.0, 1121702.125, 1138852.25, 1129750.5, 1159315.25, 1147394.0, 1129713.25, 1134555.625, 1114803.375, 1118704.625, 1119624.75, 1110317.375, 1146358.125, 1101307.0, 1149544.75, 1119111.0, 1110379.375, 1130510.0, 1136821.5, 1119307.625, 1160147.125, 1128100.625, 1134083.0, 1123058.875, 1119655.625, 1129890.375, 1116961.0, 1124357.875, 1108007.0, 1151125.125, 1112594.25, 1131359.0, 1104574.625, 1142316.25, 1122310.0, 1125183.5, 1116230.625, 1131346.5, 1115103.0, 1125838.125, 1111847.875, 1177392.375, 1103677.0, 1127328.75, 1123668.0, 1117400.75, 1142783.25, 1141083.5, 1129159.125, 1135857.375, 1163342.5, 1111885.25, 1158800.125, 1114048.875, 1120038.625, 1120268.25, 1133529.5, 1138276.25, 1126601.25, 1130834.875, 1131456.375, 1101786.25, 1128482.75, 1115826.25, 1163012.125, 1110099.125], 'val_loss': [13332.516, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.52, 234860.75, 13332.521, 13327.351, 13332.521, 13332.521, 13967.812, 13332.521, 19876.756, 25963.59, 18068.055, 14132.385, 14150.269, 13964.573, 13332.521, 17776.979, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 16830.406, 14020.403, 13332.52, 13332.521, 16525.586, 13332.521, 13625.848, 13332.521, 13332.521, 13318.006, 13332.521, 13627.721, 15802.924, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13437.0, 15477.896, 13332.521, 14349.224, 13332.517, 14351.738, 13332.521, 13330.05, 19016.043, 13332.521, 13332.521, 13332.521, 13481.73, 13428.604, 13332.521, 13332.521, 13602.176, 13332.521, 15446.202, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13332.442, 13332.521, 13332.521, 13332.521, 13634.076, 13332.521, 13332.521, 13332.521, 13332.521, 17679.357, 17610.162, 15057.404, 21318.545, 13332.521, 14338.718, 13332.521, 13332.521, 15873.013, 13332.521, 13332.521, 14040.626, 16956.746, 13332.521, 13332.521, 13332.521, 13332.521, 13332.521, 13328.035, 13332.521]}	100	100	True
