id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	3000	True	23613.63867		416732	11	-1	236.12293815612793	{'train_loss': [524117.75, 328899.312, 285631.219, 269787.062, 262753.406, 258227.438, 254224.641, 251083.422, 248918.156, 246670.469, 243613.047, 242186.766, 240431.234, 237580.984, 236676.266, 234687.422, 233564.953, 231864.125, 230457.391, 229511.594, 228225.703, 226580.438, 224723.75, 224176.906, 223100.766, 222847.828, 221044.0, 220743.234, 219016.469, 218781.438, 217907.312, 217710.703, 216244.969, 216233.547, 214967.031, 214362.969, 214219.047, 213308.859, 212159.531, 211552.391, 210793.094, 209571.172, 210477.234, 209869.109, 209567.922, 209263.0, 207827.812, 207561.141, 207820.094, 207517.453, 206484.297, 206355.469, 205121.109, 205050.422, 205180.375, 205197.938, 203461.797, 203945.531, 202802.297, 202274.812, 202268.312, 201311.0, 202396.609, 201243.75, 200733.797, 200064.0, 200295.234, 199877.75, 198614.984, 199106.031, 199108.719, 198689.594, 197567.75, 198926.391, 197710.859, 196699.938, 196873.375, 197593.219, 196473.438, 196311.562, 195881.031, 195472.266, 195762.641, 194930.266, 194798.141, 194702.516, 194754.609, 193175.828, 192863.438, 193441.016, 194179.047, 192178.594, 192435.078, 191975.547, 191997.172, 191064.953, 191624.906, 191110.234, 191212.703, 190964.688], 'val_loss': [3961.701, 2483.696, 2247.48, 2183.385, 2120.048, 2088.144, 2073.693, 2035.188, 2032.285, 2025.593, 2000.642, 1983.959, 1980.98, 1964.289, 1984.728, 1949.624, 1933.873, 1916.939, 1901.758, 1901.109, 1907.415, 1903.552, 1880.25, 1869.478, 1885.409, 1869.042, 1861.136, 1842.388, 1841.378, 1834.231, 1826.37, 1839.84, 1811.356, 1833.904, 1806.365, 1813.176, 1806.615, 1801.928, 1796.638, 1808.525, 1775.356, 1764.383, 1784.864, 1784.132, 1765.113, 1765.054, 1781.385, 1764.78, 1745.421, 1746.643, 1779.82, 1758.422, 1749.303, 1755.343, 1744.777, 1758.054, 1742.526, 1732.974, 1738.546, 1726.37, 1752.18, 1743.87, 1712.962, 1739.452, 1734.846, 1718.99, 1756.597, 1749.933, 1697.796, 1714.277, 1700.936, 1717.864, 1710.598, 1714.72, 1712.561, 1704.157, 1700.814, 1715.071, 1690.514, 1717.661, 1693.501, 1673.416, 1714.912, 1689.491, 1680.081, 1693.8, 1698.612, 1683.069, 1690.651, 1705.513, 1693.368, 1662.173, 1691.026, 1701.678, 1690.549, 1693.607, 1714.555, 1696.038, 1676.808, 1670.677]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:39 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:98 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	3000	True	24466.83984		793633	11	-1	240.5724277496338	{'train_loss': [1060389.25, 738204.688, 379414.344, 314580.438, 292736.156, 275401.062, 265079.938, 259003.891, 253810.172, 250305.5, 247479.656, 245353.766, 242589.797, 240625.734, 238711.688, 236674.859, 235290.453, 234256.141, 231711.984, 230812.859, 229759.016, 228597.031, 227040.672, 225615.703, 224957.656, 224535.234, 223889.203, 223567.266, 221736.953, 221486.438, 219732.0, 220781.734, 219633.141, 219100.578, 218348.438, 218141.141, 217004.766, 216217.859, 215491.922, 214439.5, 214783.969, 214727.922, 214113.172, 214098.094, 212517.406, 213383.5, 211136.156, 211043.141, 211106.281, 210742.766, 211456.797, 210944.078, 209768.438, 209492.969, 209747.219, 208626.641, 208619.734, 208734.281, 207640.516, 207763.062, 207635.188, 206548.984, 205721.656, 206087.094, 206653.484, 206148.062, 205242.047, 205358.672, 204278.984, 205362.688, 203553.484, 205329.547, 203818.5, 203427.719, 203055.641, 203037.281, 202197.406, 202469.469, 202148.812, 202661.031, 201087.969, 201975.656, 201838.953, 201386.078, 201212.312, 200226.594, 200425.812, 199628.516, 199801.328, 199650.406, 200089.969, 199300.0, 198647.578, 198367.422, 198573.109, 198101.641, 198101.938, 198002.094, 198528.234, 197763.453], 'val_loss': [8681.951, 3712.046, 2667.411, 2456.808, 2368.516, 2246.6, 2118.728, 2090.919, 2048.038, 2041.644, 1988.769, 2009.92, 1983.908, 1976.061, 1956.186, 1960.171, 1951.228, 1922.136, 1956.466, 1913.281, 1908.184, 1924.164, 1910.159, 1896.425, 1917.471, 1912.567, 1881.067, 1907.958, 1875.773, 1875.919, 1875.688, 1856.305, 1850.794, 1853.127, 1876.847, 1855.365, 1844.074, 1861.67, 1836.751, 1865.135, 1848.924, 1843.781, 1853.482, 1819.207, 1829.923, 1829.789, 1831.268, 1810.347, 1837.018, 1816.476, 1813.296, 1797.372, 1823.982, 1807.464, 1811.634, 1792.51, 1785.077, 1802.964, 1805.408, 1807.932, 1826.274, 1807.241, 1770.403, 1808.44, 1763.849, 1796.942, 1814.299, 1769.683, 1812.595, 1767.102, 1784.349, 1789.697, 1757.836, 1792.646, 1749.763, 1791.417, 1787.013, 1800.371, 1747.211, 1762.569, 1804.993, 1780.729, 1768.522, 1777.93, 1739.392, 1763.311, 1757.269, 1807.405, 1750.723, 1762.743, 1772.046, 1755.849, 1728.634, 1747.391, 1754.347, 1748.378, 1756.603, 1729.619, 1742.639, 1732.806]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	25018.87305		531932	11	-1	238.2053303718567	{'train_loss': [573918.375, 341377.031, 291677.219, 272873.312, 267158.875, 262416.531, 259084.156, 256016.766, 252254.938, 249561.125, 246267.375, 243996.25, 241280.109, 238826.344, 236873.312, 235522.672, 234333.172, 232090.0, 230441.859, 229454.516, 228198.984, 226295.125, 227113.781, 225959.453, 224781.641, 224672.578, 223122.984, 222818.016, 221707.516, 220544.812, 220850.906, 219274.594, 218456.75, 218528.453, 218030.812, 218273.391, 216580.188, 216672.125, 215952.172, 215594.547, 214618.922, 215091.344, 214815.281, 214111.859, 214162.156, 213081.281, 213483.812, 213163.406, 212577.234, 211163.922, 211736.766, 211802.844, 210032.781, 210510.172, 210307.672, 209310.641, 208975.781, 209573.078, 208125.359, 209652.734, 209072.312, 208972.672, 208393.812, 208345.656, 207593.438, 207098.391, 206625.344, 206390.625, 206318.625, 206179.047, 205178.359, 205641.188, 205521.328, 205482.812, 204785.391, 203697.562, 204719.297, 204645.0, 204479.328, 203842.219, 203340.656, 202788.438, 203377.016, 202994.969, 202990.016, 202942.297, 202652.406, 202439.25, 202265.938, 201014.297, 201192.609, 200966.703, 200246.531, 200783.578, 201150.484, 201781.031, 199545.969, 200897.641, 200870.359, 199826.266], 'val_loss': [2745.164, 2397.108, 2221.937, 2224.309, 2159.418, 2117.969, 2111.394, 2088.564, 2096.931, 2057.351, 2045.328, 2030.398, 1969.367, 1958.018, 1948.196, 1917.355, 1924.002, 1925.642, 1916.549, 1890.77, 1914.862, 1921.169, 1871.463, 1870.067, 1872.875, 1866.62, 1854.737, 1866.37, 1848.554, 1852.666, 1855.184, 1836.495, 1843.284, 1831.066, 1847.654, 1837.226, 1834.678, 1834.632, 1834.916, 1835.309, 1822.936, 1813.742, 1810.999, 1819.459, 1801.542, 1807.062, 1797.182, 1834.087, 1809.874, 1815.645, 1816.298, 1828.813, 1827.547, 1803.598, 1811.283, 1818.953, 1798.993, 1802.855, 1790.94, 1796.69, 1802.572, 1794.39, 1793.233, 1811.103, 1807.451, 1796.027, 1784.93, 1810.354, 1791.721, 1799.03, 1772.455, 1785.554, 1780.433, 1804.525, 1767.675, 1771.254, 1766.75, 1771.025, 1774.093, 1774.623, 1757.804, 1760.192, 1770.075, 1769.013, 1756.211, 1767.973, 1736.649, 1759.895, 1753.572, 1774.719, 1742.568, 1747.564, 1765.098, 1763.474, 1744.872, 1740.356, 1734.085, 1739.205, 1728.472, 1729.307]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:69 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.2130712591900339 alpha:0.9021276164261341 weight_decay:7.544275424605812e-05 batch_size:26 epochs:100	100	1000	True	131121.65625		565080	12	-1	244.19990301132202	{'train_loss': [1102266.125, 1620326.125, 1813027.75, 1303184.25, 1438910.375, 1313947.75, 1491491.875, 1236395.5, 1480187.125, 1262377.75, 1448280.25, 1213306.125, 1175636.5, 1151689.25, 1172547.25, 1123719.25, 1195955.5, 1162312.375, 1116023.375, 1179867.375, 1117208.625, 1176194.25, 1158621.5, 1195143.375, 1137505.25, 1151645.0, 1153357.75, 1150875.625, 1185571.0, 1183143.625, 1235815.625, 1107983.375, 1167116.625, 1156832.625, 1173926.875, 1137635.5, 1186240.875, 1156509.5, 1171215.375, 1206078.875, 1162214.125, 1194145.125, 1121320.75, 1171062.25, 1192186.5, 1177300.875, 1195936.0, 1121808.625, 1178875.125, 1150704.0, 1234366.25, 1134642.0, 1207004.125, 1138101.375, 1184853.75, 1117685.375, 1183920.0, 1178965.75, 1177692.125, 1141592.75, 1200259.75, 1216275.375, 1170594.5, 1187952.25, 1225881.25, 1160737.5, 1162349.25, 1182383.375, 1125520.25, 1158170.25, 1167882.25, 1170693.0, 1211645.875, 1134289.75, 1193409.0, 1166031.375, 1183571.0, 1221658.25, 1149129.625, 1210635.5, 1161019.75, 1176479.875, 1162647.25, 1149643.25, 1169587.875, 1131316.5, 1139547.25, 1177303.0, 1180585.25, 1205283.25, 1220577.875, 1170367.125, 1134226.375, 1143131.875, 1136667.75, 1163150.0, 1207153.0, 1165408.875, 1157963.625, 1194129.375], 'val_loss': [8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8884.707, 8888.349, 10543.371, 8888.349, 9155.527, 9333.002, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8866.544, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 13875.82, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 9093.704, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 12208.533, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 17595.066, 8888.349, 8888.349, 8888.349, 8888.349, 11034.792, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 9177.814, 9510.217, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 9420.932, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349]}	100	100	True
