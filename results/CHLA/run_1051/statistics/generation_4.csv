id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	24885.98047		639490	13	-1	207.49277782440186	{'train_loss': [638064.562, 380412.438, 302575.531, 272119.75, 263813.75, 258350.516, 254002.141, 250171.422, 246311.109, 244005.75, 241783.094, 240342.141, 237627.125, 237132.297, 235827.969, 234327.844, 233587.844, 232479.375, 231053.281, 230500.906, 230280.828, 228619.406, 228581.375, 227176.703, 227224.156, 226792.234, 225834.609, 225767.969, 224465.547, 223886.594, 223506.438, 223422.891, 223153.422, 222039.156, 221558.391, 220427.234, 220964.031, 220737.266, 219984.156, 219897.328, 219900.359, 219408.312, 218465.125, 217741.562, 217975.812, 217884.609, 217478.141, 216200.703, 215782.641, 215583.672, 215756.594, 215494.109, 214512.875, 215234.625, 214342.234, 214349.453, 213590.922, 212607.031, 211400.062, 212524.656, 210932.906, 211644.625, 210484.047, 210645.078, 209720.719, 209461.172, 208634.75, 208579.297, 208087.016, 208356.547, 208377.0, 208443.297, 207690.031, 206915.375, 206374.656, 206392.438, 206419.953, 205245.688, 207059.578, 206564.453, 204656.328, 205201.125, 203949.484, 204310.734, 204800.828, 204191.172, 202968.094, 202387.891, 203197.312, 203066.672, 203364.547, 202768.469, 202969.844, 201350.484, 202251.312, 201677.453, 201480.516, 201400.25, 201799.969, 200839.141], 'val_loss': [4108.481, 3463.631, 3015.579, 2867.149, 2829.701, 2712.782, 2696.719, 2594.994, 2598.973, 2606.697, 2574.1, 2513.105, 2543.977, 2484.383, 2499.224, 2444.229, 2473.46, 2494.255, 2480.576, 2476.232, 2457.991, 2455.396, 2449.392, 2496.938, 2415.118, 2415.354, 2406.332, 2453.828, 2401.35, 2432.012, 2392.397, 2383.337, 2418.853, 2393.636, 2364.446, 2374.051, 2382.325, 2380.852, 2352.293, 2370.736, 2388.344, 2327.479, 2338.553, 2330.536, 2305.964, 2310.017, 2302.39, 2303.129, 2298.568, 2277.861, 2329.314, 2308.802, 2280.575, 2274.966, 2269.368, 2291.33, 2301.126, 2270.017, 2275.766, 2296.146, 2267.027, 2283.857, 2273.658, 2283.514, 2279.819, 2264.504, 2281.603, 2261.719, 2309.042, 2279.666, 2288.351, 2279.09, 2287.354, 2258.653, 2290.686, 2291.306, 2251.398, 2286.648, 2246.141, 2289.242, 2271.116, 2282.224, 2283.159, 2286.895, 2250.116, 2225.544, 2281.677, 2226.883, 2265.58, 2240.111, 2222.955, 2244.876, 2236.755, 2229.747, 2209.917, 2212.576, 2228.418, 2244.299, 2195.639, 2229.839]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:110 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.24077202071787634 beta1:0.8035959257859422 beta2:0.9531544370435999 weight_decay:0.0003042261999736751 batch_size:32 epochs:100	100	1000	True	131121.82812		670304	14	-1	223.59814524650574	{'train_loss': [1073923.0, 1062567.125, 1072404.0, 1542658.125, 1062567.125, 1080892.125, 1278927.5, 1225984.5, 1066517.375, 1111400.5, 1192309.125, 1195763.625, 1064465.625, 1112048.375, 1194251.625, 1120571.75, 1089108.625, 1107323.75, 1141756.125, 1154517.375, 1069909.0, 1082403.625, 1105198.375, 1104257.125, 1088974.625, 1082237.375, 1084527.125, 1140086.75, 1074173.0, 1082087.625, 1088499.875, 1134868.5, 1071116.75, 1079848.125, 1089780.875, 1116465.125, 1077584.75, 1080267.125, 1092286.375, 1132630.0, 1099355.375, 1078602.25, 1090336.375, 1102359.625, 1100357.125, 1097317.0, 1086280.0, 1122445.75, 1108215.625, 1069891.25, 1098335.625, 1089796.125, 1090601.25, 1091515.125, 1104233.875, 1109433.0, 1093252.0, 1073578.25, 1114563.625, 1122933.5, 1083469.75, 1072124.75, 1094665.0, 1112048.5, 1092697.375, 1067106.625, 1091866.375, 1123370.25, 1078691.625, 1073061.0, 1099597.875, 1120974.125, 1079038.0, 1077020.0, 1138744.0, 1113498.375, 1078482.125, 1084521.0, 1083964.25, 1130917.875, 1076518.0, 1091270.375, 1078993.5, 1155007.0, 1080904.0, 1090375.375, 1088490.25, 1121216.5, 1079510.875, 1080165.75, 1089563.625, 1096485.75, 1102181.25, 1078615.125, 1118857.125, 1111766.875, 1104104.875, 1081016.875, 1080226.125, 1095461.625], 'val_loss': [11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.429, 11110.434, 11110.434, 14274.063, 11110.434, 11110.434, 11110.434, 11246.383, 11110.434, 11675.892, 11110.434, 11110.434, 11110.434, 11110.068, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11484.005, 11110.434, 11110.434, 11110.434, 11110.434, 11138.979, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11574.842, 11641.451, 11110.434, 69868.891, 11110.434, 12824.302, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.352, 11110.434, 13336.901, 11110.434, 11110.434, 12161.4, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11107.619, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11206.329, 11110.434, 11110.434, 11110.434, 11110.434, 12636.754, 11110.434, 12205.563, 13002.314, 11110.424, 11110.434, 11110.434, 11178.797, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.433, 11110.434, 12791.361, 11110.434, 11110.434, 11303.027, 11110.434]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:111 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:64 kernel_size:5 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	28757.68359		2446113	15	-1	266.8601143360138	{'train_loss': [1794924.25, 1163733.75, 1121984.25, 1068000.625, 1127424.0, 1069133.5, 880999.625, 476319.75, 282600.031, 269174.312, 258115.609, 251736.406, 246712.062, 243245.391, 239565.047, 237092.125, 233996.906, 232925.406, 231025.656, 229872.344, 227360.234, 226299.297, 224197.781, 222654.688, 220894.266, 220384.766, 219326.969, 218499.203, 216543.016, 215397.531, 214288.406, 213091.609, 212417.688, 211753.797, 210537.422, 209391.125, 208075.812, 207933.766, 207252.625, 206059.031, 205486.609, 205089.859, 203748.734, 201908.859, 201258.734, 201134.891, 200427.281, 200387.141, 198710.734, 198441.859, 198430.969, 195774.844, 197097.547, 195884.562, 194357.078, 194228.875, 192595.906, 193772.109, 192442.266, 191772.344, 191819.875, 191267.438, 190762.203, 188691.391, 189597.609, 187854.172, 186839.219, 186491.75, 187425.656, 185460.453, 185566.734, 184427.75, 184633.469, 183196.641, 183619.969, 181929.891, 182736.594, 181920.641, 184520.672, 183213.312, 181908.031, 180759.266, 180184.25, 181866.297, 179024.547, 179452.266, 180114.953, 177510.406, 177362.047, 178265.812, 177049.984, 176341.266, 178909.234, 175697.703, 177015.828, 177090.422, 175946.094, 177336.516, 174209.422, 173974.422], 'val_loss': [11110.429, 11110.386, 11110.352, 11110.428, 11109.429, 11826.818, 4091.73, 2980.318, 2801.568, 2718.938, 2651.095, 2638.082, 2581.23, 2536.188, 2490.786, 2480.402, 2504.629, 2473.16, 2429.008, 2396.572, 2423.149, 2406.976, 2411.974, 2401.033, 2393.383, 2388.343, 2418.975, 2394.35, 2377.448, 2345.256, 2394.372, 2358.931, 2391.185, 2439.729, 2403.255, 2426.41, 2385.63, 2376.657, 2405.998, 2376.814, 2364.286, 2369.223, 2378.319, 2369.004, 2390.936, 2403.102, 2464.103, 2443.652, 2436.873, 2463.632, 2420.582, 2501.908, 2465.218, 2439.626, 2445.839, 2445.71, 2435.194, 2487.821, 2448.643, 2435.488, 2475.447, 2439.736, 2437.1, 2513.385, 2440.535, 2495.735, 2496.719, 2500.563, 2439.214, 2463.298, 2474.295, 2483.026, 2458.248, 2476.449, 2470.062, 2476.432, 2491.988, 2473.456, 2441.514, 2459.315, 2418.194, 2496.348, 2472.421, 2483.736, 2510.271, 2501.865, 2511.805, 2516.024, 2490.863, 2501.638, 2492.763, 2565.285, 2459.814, 2494.688, 2573.115, 2541.566, 2517.785, 2540.09, 2461.251, 2527.25]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:87 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:120 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:55 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:98 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	26180.76562		1000693	14	-1	217.39595007896423	{'train_loss': [1102558.625, 807292.75, 469139.688, 366510.469, 310429.812, 272384.031, 256836.516, 249413.562, 245012.812, 243031.312, 240640.516, 239006.234, 237595.016, 235711.047, 234800.422, 233345.375, 232687.875, 231850.641, 230449.219, 229314.891, 228335.125, 228295.578, 227342.859, 227083.891, 226458.531, 225164.391, 223717.547, 224006.719, 223248.969, 223063.484, 221247.953, 221495.422, 221295.969, 221120.781, 219916.125, 219598.609, 219819.25, 219915.406, 218084.75, 218254.031, 217680.641, 217030.844, 216566.984, 216821.938, 216327.812, 216613.375, 216712.516, 215637.656, 214815.906, 214108.266, 214487.125, 213750.719, 213578.234, 212733.203, 213113.812, 213147.969, 211826.266, 212758.812, 211965.281, 212191.047, 210977.797, 210471.281, 211366.125, 211038.953, 209287.047, 209944.234, 209905.484, 209182.484, 210383.578, 209695.094, 209183.172, 209307.125, 208813.578, 207250.109, 207614.875, 207077.719, 207251.609, 206899.547, 206876.516, 206994.797, 206036.953, 206131.766, 205734.422, 206172.375, 205077.297, 204613.031, 204645.609, 204378.062, 204229.484, 205048.891, 204216.219, 203874.453, 204122.406, 203717.609, 203343.531, 203197.0, 203826.906, 203102.062, 202365.797, 201790.297], 'val_loss': [9714.408, 4095.354, 3682.201, 3366.402, 2885.873, 2689.356, 2610.393, 2553.412, 2526.215, 2507.921, 2501.776, 2475.021, 2450.092, 2475.851, 2453.369, 2454.615, 2464.749, 2438.833, 2437.943, 2466.723, 2441.589, 2415.196, 2429.574, 2431.007, 2418.804, 2413.322, 2438.459, 2422.062, 2412.348, 2390.565, 2402.686, 2388.529, 2396.365, 2367.552, 2381.214, 2434.809, 2369.836, 2393.063, 2378.781, 2400.283, 2391.445, 2379.067, 2356.415, 2381.457, 2363.413, 2398.625, 2351.653, 2354.042, 2396.589, 2365.528, 2357.471, 2358.195, 2349.646, 2338.463, 2360.593, 2346.115, 2356.847, 2348.146, 2368.067, 2348.303, 2359.721, 2358.623, 2365.795, 2366.39, 2351.084, 2375.545, 2367.985, 2370.378, 2367.5, 2359.822, 2352.424, 2369.305, 2363.565, 2393.945, 2381.259, 2329.147, 2374.544, 2342.352, 2328.382, 2359.667, 2350.569, 2351.796, 2345.439, 2346.206, 2337.374, 2349.365, 2321.021, 2326.85, 2335.844, 2367.968, 2335.357, 2326.973, 2333.978, 2330.66, 2328.343, 2329.522, 2352.864, 2315.649, 2311.121, 2319.617]}	100	100	True
