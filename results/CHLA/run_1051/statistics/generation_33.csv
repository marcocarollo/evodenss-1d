id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	3000	True	23973.48242		416732	11	-1	236.54776406288147	{'train_loss': [508747.25, 327441.969, 286786.812, 271195.562, 263747.531, 259648.172, 255835.812, 253417.516, 250585.062, 248523.406, 246366.984, 244475.281, 243493.031, 241360.547, 239381.812, 238160.688, 237455.531, 236018.578, 234259.281, 233735.438, 231485.016, 230741.703, 229390.5, 228264.781, 227618.047, 225665.719, 224473.859, 224838.547, 223662.141, 223265.5, 221544.516, 221144.438, 220056.594, 218071.047, 219275.703, 217689.312, 217527.875, 216910.969, 216792.094, 216372.109, 215420.406, 215711.219, 214722.469, 213410.328, 213536.953, 212567.312, 211747.828, 212347.125, 211287.422, 211081.125, 209794.719, 209965.188, 211613.188, 209604.828, 209385.344, 208560.969, 208135.734, 208161.0, 207507.875, 208084.078, 207255.047, 205966.438, 206125.219, 206224.078, 205566.391, 205888.688, 205363.078, 204818.703, 204281.391, 204047.188, 203362.359, 203775.547, 202563.047, 201406.516, 203269.438, 201683.547, 201376.562, 201516.484, 200648.859, 200971.344, 199692.672, 200776.172, 200868.938, 199567.391, 200667.609, 199149.359, 199492.609, 199496.141, 199802.0, 198345.625, 199062.656, 198370.797, 199124.094, 197363.016, 197582.828, 197169.266, 196815.109, 196719.656, 196494.906, 196403.641], 'val_loss': [2897.159, 2417.501, 2223.709, 2182.592, 2141.682, 2117.857, 2087.275, 2061.85, 2044.423, 2041.212, 2025.902, 2017.073, 1991.001, 1981.02, 1989.007, 1982.37, 1987.619, 1948.296, 1930.996, 1930.778, 1904.44, 1926.322, 1904.7, 1868.476, 1873.235, 1870.862, 1865.766, 1858.897, 1869.106, 1850.363, 1825.158, 1823.938, 1835.066, 1828.838, 1817.863, 1812.777, 1807.997, 1791.529, 1804.797, 1803.931, 1790.631, 1790.12, 1785.895, 1775.362, 1773.358, 1794.606, 1768.923, 1764.559, 1752.35, 1776.031, 1764.582, 1755.713, 1754.395, 1761.325, 1747.25, 1743.928, 1753.348, 1747.83, 1739.339, 1726.989, 1733.062, 1741.144, 1730.37, 1724.482, 1737.865, 1738.871, 1736.605, 1731.742, 1716.039, 1713.242, 1721.815, 1718.079, 1723.374, 1713.624, 1714.799, 1704.955, 1713.878, 1703.531, 1706.598, 1704.828, 1712.888, 1703.02, 1691.565, 1717.091, 1697.234, 1700.914, 1709.647, 1710.036, 1693.938, 1724.307, 1683.701, 1690.545, 1699.392, 1683.494, 1708.554, 1702.857, 1702.841, 1696.055, 1685.199, 1692.347]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:77 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:rmsprop lr:0.23037559401962626 alpha:0.8525538644665507 weight_decay:2.0227001184187434e-05 batch_size:26 epochs:100	100	1000	True	131121.14062		427192	11	-1	230.64734983444214	{'train_loss': [1211390.875, 1116091.0, 1185597.25, 1283744.625, 1144975.5, 1130640.0, 1188925.25, 1108250.375, 1142579.625, 1141318.125, 1127276.75, 1155754.125, 1158810.5, 1154379.0, 1143780.5, 1141678.0, 1157189.625, 1155779.5, 1135222.0, 1148240.5, 1132019.75, 1106442.0, 1121050.0, 1208097.125, 1087706.25, 1155510.375, 1152522.625, 1123845.0, 1122809.75, 1149763.625, 1130889.5, 1107971.625, 1139306.375, 1110881.0, 1130045.375, 1140543.5, 1130950.5, 1147836.75, 1129676.75, 1144983.25, 1120757.75, 1136163.25, 1129538.875, 1138954.0, 1121895.25, 1147681.375, 1145168.125, 1127909.75, 1110708.25, 1131724.875, 1138189.75, 1120123.625, 1120835.5, 1121894.75, 1144983.125, 1122386.625, 1143700.25, 1123923.25, 1114576.375, 1131802.625, 1117513.75, 1176291.0, 1132007.125, 1122031.5, 1122762.75, 1113711.0, 1130236.75, 1126150.875, 1124266.75, 1135216.125, 1131757.875, 1127701.375, 1124079.5, 1145700.875, 1130352.875, 1132456.5, 1108475.75, 1115240.0, 1135461.625, 1127685.5, 1133721.25, 1132014.875, 1135716.625, 1130824.625, 1130154.875, 1130108.0, 1133672.5, 1105621.875, 1119493.75, 1149850.25, 1137220.125, 1113159.25, 1249702.25, 1098237.875, 1149567.875, 1103175.0, 1108480.5, 1143521.375, 1127276.0, 1123048.375], 'val_loss': [8888.257, 8888.344, 13868.386, 11632.692, 8888.349, 8888.349, 9632.423, 8888.349, 8888.349, 11411.264, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 12070.884, 8888.349, 8888.349, 8888.349, 8888.349, 8888.281, 8888.349, 11565.33, 8888.349, 8888.349, 8888.349, 10973.64, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.346, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.347, 10475.782, 8888.349, 9806.463, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 9443.49, 8888.349, 8888.349, 8888.349, 8888.349, 11201.224, 8888.349, 9739.967, 8888.349, 8888.349, 10850.461, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 9789.906, 9582.355, 8888.349, 8888.349, 13490.06, 8888.349, 11789.456, 8888.349, 8888.349, 10637.932, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:126 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:rmsprop lr:0.13787621970141672 alpha:0.8419046067694225 weight_decay:0.0004898477979095639 batch_size:26 epochs:100	100	1000	True	131121.57812		546721	11	-1	237.4751307964325	{'train_loss': [1223356.25, 1403909.25, 1233207.125, 1326414.0, 1268170.75, 1242144.0, 1253236.625, 1248148.625, 1252153.375, 1258930.0, 1251911.625, 1262460.0, 1266074.875, 1243346.875, 1261626.0, 1264722.75, 1243037.0, 1262934.5, 1257473.125, 1236507.25, 1276026.125, 1244926.75, 1241232.0, 1252415.75, 1238310.75, 1240936.75, 1276298.125, 1239960.75, 1252589.25, 1228119.125, 1245403.375, 1260527.5, 1241516.875, 1251832.75, 1235048.75, 1239226.875, 1227947.0, 1243587.5, 1186920.125, 1138701.875, 1120610.625, 1123947.375, 1109762.625, 1132616.5, 1123995.25, 1125335.75, 1116058.375, 1126136.0, 1131970.5, 1132015.0, 1104568.75, 1136209.25, 1117846.125, 1118486.0, 1118995.375, 1099286.375, 1100300.625, 1120458.625, 1116032.5, 1112647.375, 1122564.375, 1112099.125, 1116559.625, 1113262.0, 1129673.625, 1122044.875, 1117347.625, 1117739.375, 1123353.125, 1120183.5, 1113756.5, 1124837.25, 1126911.375, 1112063.25, 1114436.875, 1133242.25, 1115663.5, 1117080.125, 1109775.375, 1118742.625, 1113035.25, 1127043.125, 1118612.375, 1107250.75, 1120374.75, 1155121.25, 1124900.875, 1119487.75, 1114345.125, 1116366.75, 1120189.125, 1131622.0, 1120240.625, 1125938.75, 1121697.75, 1115907.125, 1126743.0, 1117586.75, 1119645.0, 1113861.125], 'val_loss': [8888.349, 8888.349, 10599.37, 9222.219, 11934.592, 8877.031, 8887.097, 8879.298, 16053.904, 8944.283, 8888.328, 9842.602, 8874.156, 8944.541, 8997.255, 8888.313, 10223.062, 12850.251, 8888.348, 9656.133, 10147.076, 10188.814, 8881.56, 8888.171, 10589.282, 8862.17, 8877.895, 8880.407, 8888.085, 9813.954, 9864.608, 8857.652, 9056.046, 9111.889, 8885.257, 8948.331, 10953.312, 9364.898, 10416.389, 10429.368, 9051.027, 8998.146, 8888.349, 9306.091, 8888.349, 8888.349, 8888.349, 8888.238, 8888.271, 8888.289, 8887.268, 8888.349, 8888.349, 8888.349, 9829.688, 8888.346, 10622.049, 8888.349, 8888.349, 12111.904, 8888.349, 8888.349, 8888.346, 10699.913, 8888.349, 8888.349, 8886.204, 10461.06, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.344, 8888.347, 8888.349, 8888.349, 11144.658, 8880.953, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8981.951, 8899.005, 8888.349, 8888.349, 9657.3, 11541.648, 14852.202, 8888.348, 8887.002, 8888.345, 8888.349]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:19 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:25 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:13 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:27 epochs:100	100	3000	True	24306.89258		296688	13	-1	243.89963173866272	{'train_loss': [410638.531, 308160.719, 282269.25, 273416.719, 268121.531, 264187.156, 260096.984, 256692.797, 253798.172, 251755.891, 249949.453, 247575.812, 246958.406, 245852.672, 243413.094, 243064.312, 241772.375, 239747.938, 239303.641, 237518.375, 236088.938, 235196.953, 232963.391, 233923.688, 230386.406, 230541.375, 228826.672, 228087.578, 227210.891, 225586.172, 225246.625, 223763.812, 223830.281, 223526.016, 222344.969, 223217.344, 221273.641, 220410.062, 220694.797, 220112.734, 220488.844, 219241.359, 218541.234, 218619.516, 217361.203, 216473.672, 217535.469, 217698.375, 215672.016, 216021.719, 215056.531, 214502.625, 214283.594, 214225.547, 214377.281, 215072.984, 212999.203, 212161.172, 211911.016, 211086.953, 210797.125, 211734.156, 211593.141, 209963.641, 209236.359, 210234.594, 209819.75, 209267.609, 209334.938, 207683.016, 207844.578, 207679.766, 208093.469, 207730.984, 207368.781, 206785.422, 206535.75, 205773.391, 206144.734, 205753.656, 204718.703, 206637.516, 204186.219, 205174.203, 203857.125, 204665.984, 204401.359, 204222.375, 203491.734, 203341.672, 202617.984, 203379.312, 202508.203, 202449.797, 201982.375, 200449.016, 200743.281, 203455.594, 201813.547, 200999.625], 'val_loss': [2742.899, 2520.25, 2436.176, 2386.084, 2347.651, 2329.095, 2318.752, 2258.258, 2285.153, 2236.351, 2222.336, 2234.55, 2246.02, 2205.129, 2199.658, 2190.816, 2149.637, 2169.686, 2122.74, 2165.151, 2127.219, 2110.041, 2095.499, 2104.926, 2055.232, 2068.476, 2071.861, 2036.19, 2017.111, 2031.485, 1992.929, 2038.077, 2029.267, 2014.281, 2003.278, 2022.308, 1983.037, 1959.967, 1972.061, 1961.668, 2000.062, 1971.298, 1974.813, 1956.512, 1947.714, 1953.538, 1948.087, 1955.54, 1927.857, 1957.33, 1922.155, 1939.793, 1934.751, 1918.851, 1913.173, 1958.627, 1918.246, 1902.727, 1927.345, 1924.433, 1911.685, 1912.588, 1897.524, 1902.006, 1868.926, 1879.83, 1866.397, 1882.595, 1885.513, 1868.482, 1896.255, 1871.29, 1877.349, 1860.041, 1851.251, 1863.735, 1841.561, 1873.751, 1872.423, 1869.195, 1855.879, 1846.688, 1836.541, 1836.788, 1857.537, 1837.173, 1877.174, 1857.135, 1849.219, 1849.047, 1854.664, 1885.219, 1855.159, 1837.956, 1805.975, 1850.371, 1831.0, 1817.231, 1834.061, 1821.699]}	0	100	True
