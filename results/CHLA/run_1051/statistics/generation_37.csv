id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	24040.39844		416732	11	-1	240.24608278274536	{'train_loss': [521653.5, 353874.938, 300281.0, 271830.781, 261594.188, 255273.172, 251817.703, 248642.0, 245842.641, 243854.094, 242816.297, 241049.719, 239196.578, 238281.375, 237291.547, 236353.203, 234897.719, 233835.141, 232887.469, 232774.953, 230938.406, 229480.0, 228139.844, 227361.297, 225899.719, 223743.859, 223403.75, 221193.156, 220115.656, 219057.172, 217387.672, 217028.156, 216287.75, 214892.672, 214360.422, 212770.047, 212827.547, 212333.328, 211064.094, 209567.875, 209568.188, 208889.391, 209114.422, 207784.016, 207525.844, 206413.672, 205936.078, 206248.469, 206259.609, 204012.359, 204896.312, 204028.656, 203293.516, 202857.75, 202219.766, 201832.953, 201444.219, 201662.75, 200471.25, 200509.984, 200187.297, 199811.953, 199573.984, 200002.297, 198323.266, 198945.469, 197674.203, 197615.219, 197981.453, 197546.562, 196800.078, 198023.922, 195817.453, 195510.828, 197202.266, 195688.078, 195946.266, 195558.172, 194837.625, 194843.672, 194045.734, 195078.469, 192779.922, 193631.172, 193236.484, 192474.281, 193318.516, 192579.406, 192964.141, 191224.969, 191920.375, 192626.703, 191846.219, 190182.234, 190655.875, 190896.031, 191095.703, 189958.578, 190174.0, 190676.188], 'val_loss': [3199.771, 2619.505, 2263.882, 2165.787, 2128.115, 2105.477, 2052.524, 2044.235, 2026.04, 2022.086, 1999.771, 2000.3, 1985.556, 1976.946, 1974.371, 1979.711, 1952.932, 1948.284, 1933.733, 1941.393, 1926.297, 1901.703, 1900.296, 1897.864, 1865.667, 1877.618, 1848.972, 1827.797, 1843.807, 1838.137, 1832.391, 1811.775, 1811.597, 1800.357, 1779.143, 1789.724, 1794.047, 1759.235, 1761.214, 1775.512, 1758.114, 1794.421, 1778.882, 1781.621, 1760.335, 1748.369, 1750.709, 1747.314, 1752.165, 1752.656, 1737.122, 1755.697, 1737.681, 1729.869, 1726.177, 1749.342, 1749.457, 1744.816, 1731.059, 1739.629, 1708.28, 1718.906, 1718.451, 1723.217, 1719.873, 1711.722, 1692.553, 1712.465, 1705.89, 1718.592, 1724.722, 1714.493, 1693.929, 1709.235, 1707.309, 1695.084, 1711.25, 1693.025, 1703.68, 1717.581, 1692.651, 1667.397, 1673.636, 1675.47, 1722.505, 1683.882, 1682.558, 1686.054, 1669.701, 1674.873, 1686.887, 1697.844, 1673.591, 1668.891, 1704.609, 1711.847, 1739.746, 1672.181, 1706.452, 1682.657]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:108 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:rmsprop lr:0.1160573594666132 alpha:0.8386733625801588 weight_decay:0.0006325857867344395 batch_size:26 epochs:100	100	1000	True	131122.28125		738243	10	-1	227.9884581565857	{'train_loss': [1464480.875, 1424676.125, 1232099.25, 1302739.25, 1183996.875, 1142309.125, 1119882.625, 1116998.25, 1123010.375, 1101826.0, 1114011.75, 1125403.0, 1112363.0, 1118594.875, 1118003.0, 1111145.75, 1110913.375, 1103025.625, 1125564.625, 1121568.5, 1119825.0, 1114787.875, 1109721.375, 1106951.625, 1109404.0, 1111500.375, 1122792.625, 1103647.5, 1120237.75, 1125526.625, 1104786.25, 1097611.625, 1120615.0, 1108166.5, 1100478.375, 1119063.375, 1108904.875, 1109071.375, 1105680.75, 1111104.5, 1114380.625, 1113976.75, 1114410.25, 1115879.625, 1113818.625, 1122300.375, 1115682.125, 1108309.5, 1113766.5, 1106102.125, 1120710.25, 1119657.0, 1127681.375, 1109406.0, 1110352.625, 1130852.5, 1123098.125, 1112782.625, 1113105.625, 1117760.25, 1105329.25, 1120334.375, 1120017.875, 1110248.375, 1108904.375, 1119890.25, 1119102.0, 1103035.875, 1117078.25, 1126502.25, 1106969.625, 1116491.375, 1108847.0, 1115053.5, 1113547.625, 1115751.5, 1104095.125, 1106935.375, 1109029.25, 1121780.5, 1120640.75, 1114429.5, 1117590.125, 1101907.75, 1102840.875, 1117756.125, 1109858.75, 1115488.375, 1113135.0, 1122667.5, 1107681.25, 1104280.5, 1116540.25, 1118362.875, 1115430.25, 1128660.5, 1099596.0, 1112828.625, 1102960.875, 1117794.75], 'val_loss': [15973.351, 8888.347, 8888.349, 9598.159, 9635.465, 8888.349, 11372.203, 9633.927, 8888.349, 8888.342, 8888.349, 8888.349, 8888.349, 11520.271, 9914.492, 8888.349, 8888.349, 8888.344, 9212.687, 8911.111, 8888.349, 8888.349, 9348.787, 8888.349, 8888.349, 10798.904, 8888.349, 8888.349, 10262.174, 20601.496, 8888.349, 9045.928, 8888.349, 9504.268, 9564.19, 9678.883, 9276.323, 8866.582, 8888.349, 9916.415, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 10316.527, 8885.8, 8888.349, 10364.434, 11215.383, 8888.349, 8888.349, 8888.349, 9523.129, 8888.349, 9065.013, 9572.521, 8888.349, 10110.103, 9899.221, 11421.398, 9196.672, 8888.337, 9001.136, 9878.866, 10363.314, 8888.349, 8888.349, 8888.349, 9763.774, 8888.349, 8888.349, 8888.349, 8888.349, 8908.871, 8888.289, 8888.349, 8888.349, 9251.249, 9276.618, 8888.349, 8888.348, 8888.338, 8888.349, 8869.5, 8888.349, 8888.349, 8888.349, 9929.296, 8888.349, 8888.349, 8888.349, 8888.349, 9819.58, 10464.309, 8888.349, 8888.349, 8885.779, 10337.901, 8888.349]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:75 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:4 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:98 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:26 epochs:100	100	1000	True	26754.23047		1355200	12	-1	271.40602684020996	{'train_loss': [1193589.0, 1098729.75, 1090433.125, 605649.062, 328488.531, 282003.875, 269507.094, 263283.406, 259015.984, 254738.312, 250703.266, 247810.219, 245922.594, 243726.547, 241960.344, 239789.219, 238048.891, 235988.5, 235384.109, 233348.672, 232416.234, 231481.359, 229849.453, 228243.297, 225960.672, 225463.562, 224230.922, 222300.844, 221863.547, 220295.828, 218614.312, 218480.953, 217106.5, 216768.75, 215257.156, 215372.359, 214769.297, 213374.094, 213685.75, 213019.953, 211290.281, 211279.984, 210599.438, 208971.062, 210017.453, 209172.156, 207474.078, 207153.531, 206540.344, 206650.844, 205601.562, 204772.172, 203917.781, 204843.5, 203084.656, 202291.922, 202135.984, 202535.109, 201999.906, 201072.938, 200571.531, 200056.672, 199821.0, 199451.562, 198805.391, 197134.375, 197794.453, 196910.125, 197419.016, 195711.469, 196367.234, 194880.828, 195268.609, 194280.984, 194227.406, 194334.359, 194236.844, 193239.031, 192279.625, 192654.422, 191213.031, 192621.5, 191981.562, 191237.188, 190902.719, 190703.219, 190087.484, 190192.656, 189652.625, 189799.188, 188939.547, 188336.75, 187925.562, 188419.25, 188371.812, 187254.0, 187868.844, 187853.359, 186879.016, 186640.469], 'val_loss': [8912.803, 8990.978, 8854.767, 2770.471, 2348.197, 2221.986, 2223.164, 2121.369, 2096.811, 2071.261, 2056.136, 2105.137, 2040.399, 2054.642, 2008.422, 2034.51, 1991.887, 2024.488, 2020.985, 1937.402, 1965.488, 2000.063, 1964.951, 1914.968, 1930.005, 1908.665, 1924.797, 1905.669, 1905.416, 1879.385, 1898.706, 1910.194, 1911.029, 1915.246, 1904.872, 1894.257, 1926.625, 1930.461, 1931.801, 1867.753, 1869.616, 1914.653, 1877.963, 1886.718, 1896.24, 1891.213, 1899.985, 1884.453, 1878.506, 1878.538, 1876.611, 1898.28, 1915.313, 1873.304, 1886.278, 1925.902, 1891.045, 1867.609, 1869.039, 1868.921, 1877.031, 1854.008, 1851.909, 1886.16, 1878.697, 1845.147, 1876.955, 1854.123, 1860.47, 1865.637, 1872.31, 1848.29, 1844.667, 1871.106, 1832.734, 1856.967, 1863.833, 1836.446, 1854.296, 1847.716, 1849.559, 1851.573, 1876.927, 1821.518, 1839.348, 1857.057, 1821.07, 1816.905, 1822.79, 1856.423, 1832.031, 1810.757, 1833.421, 1831.817, 1834.808, 1829.683, 1850.584, 1846.949, 1839.008, 1839.61]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	2000	True	24060.52344		416732	11	-1	240.70146799087524	{'train_loss': [544777.312, 369945.594, 299245.188, 276803.938, 262261.031, 253137.25, 248469.344, 244155.578, 240639.734, 238191.281, 234882.078, 233291.484, 232277.438, 230719.031, 229191.062, 228530.109, 226189.141, 224140.438, 223783.641, 223410.516, 221010.641, 221556.703, 219485.094, 219032.469, 217881.797, 217231.234, 216230.312, 216266.016, 215892.75, 214753.469, 214232.125, 213200.172, 212929.312, 211727.781, 211601.953, 211356.062, 211344.484, 210109.625, 209141.672, 209513.312, 209559.594, 208184.078, 207732.891, 207195.562, 206347.344, 206600.938, 206607.328, 206172.062, 206007.438, 204855.891, 204819.062, 204634.078, 204165.531, 203569.047, 203836.969, 203362.469, 202865.844, 201816.75, 201649.625, 201930.078, 202351.719, 200631.734, 200633.328, 200866.219, 200802.5, 199641.828, 199133.516, 199651.297, 200220.344, 199372.609, 198720.172, 198460.281, 197706.188, 197616.125, 198113.344, 197260.328, 197283.938, 195944.516, 197124.75, 196719.531, 195745.984, 195520.688, 195874.188, 194648.125, 195498.641, 194552.688, 194054.375, 194525.938, 194417.859, 192389.75, 193076.531, 193421.234, 192368.734, 192283.375, 192478.312, 192833.875, 193250.781, 192572.438, 191807.281, 191267.672], 'val_loss': [3238.446, 2681.282, 2318.927, 2207.324, 2108.744, 2073.766, 2076.311, 2040.434, 2018.094, 1984.799, 1996.125, 1990.76, 1953.673, 1952.83, 1926.842, 1940.931, 1904.061, 1909.98, 1917.563, 1912.871, 1887.261, 1884.94, 1897.319, 1866.794, 1872.777, 1866.53, 1854.366, 1859.149, 1843.965, 1843.194, 1836.713, 1837.514, 1838.365, 1819.72, 1811.449, 1821.436, 1827.271, 1802.505, 1811.674, 1812.138, 1799.919, 1806.448, 1794.189, 1787.645, 1806.188, 1782.756, 1807.109, 1792.067, 1783.687, 1761.027, 1785.316, 1773.563, 1771.739, 1771.292, 1770.171, 1776.503, 1769.792, 1753.59, 1770.937, 1771.397, 1766.364, 1754.518, 1771.066, 1758.543, 1748.214, 1738.52, 1748.223, 1739.952, 1725.479, 1756.716, 1735.871, 1758.767, 1727.47, 1718.281, 1714.016, 1735.882, 1738.821, 1713.289, 1722.845, 1736.593, 1709.928, 1707.48, 1688.134, 1712.078, 1718.307, 1705.105, 1708.544, 1702.335, 1679.35, 1693.15, 1711.344, 1700.275, 1694.93, 1693.764, 1700.34, 1684.942, 1668.671, 1695.14, 1686.462, 1694.35]}	0	100	True
