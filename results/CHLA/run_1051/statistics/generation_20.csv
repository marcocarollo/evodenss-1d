id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:39 epochs:100	100	1000	True	24424.80469		505857	11	-1	180.5349452495575	{'train_loss': [635881.562, 372004.0, 323548.531, 286821.0, 268801.844, 261160.219, 255744.156, 251057.781, 246784.078, 244515.031, 241019.078, 239668.0, 237181.344, 235689.109, 234018.359, 233044.797, 231440.281, 229675.797, 228796.625, 227446.312, 227174.406, 225417.984, 225167.906, 223417.438, 223502.344, 222835.828, 221768.484, 221118.031, 219561.844, 219496.625, 218294.547, 218578.234, 216998.031, 217152.688, 216710.203, 215483.422, 216139.109, 215919.922, 214512.484, 213871.328, 214146.562, 214216.578, 212811.047, 211396.391, 211436.188, 212249.859, 211265.109, 211441.5, 211042.281, 210769.891, 210211.766, 209221.609, 209022.047, 209134.766, 209338.109, 208463.125, 208210.078, 208773.438, 207955.734, 207861.312, 207189.141, 206980.156, 206602.0, 206680.781, 206753.078, 206350.281, 206109.344, 205138.781, 205738.844, 204972.234, 204330.828, 204887.125, 204667.219, 203615.281, 203888.938, 203094.516, 203422.266, 203855.281, 203271.719, 203182.219, 202383.641, 201764.656, 202784.344, 202383.078, 202316.047, 201427.375, 201325.547, 202321.297, 202244.297, 201141.531, 200902.406, 200208.234, 200093.641, 200407.281, 200048.375, 199662.922, 200167.062, 199012.078, 200129.828, 199282.281], 'val_loss': [4465.426, 3981.601, 3524.381, 3291.453, 3214.282, 3141.705, 3088.709, 3065.724, 3019.663, 3013.319, 2967.669, 2968.454, 2983.219, 2928.643, 2962.469, 2914.294, 2897.404, 2886.849, 2892.367, 2845.366, 2861.157, 2843.497, 2855.826, 2845.677, 2836.716, 2852.859, 2799.948, 2797.226, 2798.623, 2793.902, 2781.307, 2757.878, 2784.063, 2772.759, 2758.802, 2750.048, 2739.559, 2769.642, 2764.373, 2745.219, 2734.106, 2696.076, 2704.966, 2710.922, 2706.378, 2715.562, 2698.293, 2693.045, 2710.821, 2702.538, 2695.104, 2723.253, 2691.847, 2693.891, 2706.515, 2679.374, 2701.654, 2673.193, 2702.56, 2698.043, 2674.024, 2667.159, 2677.05, 2687.249, 2685.92, 2661.042, 2677.972, 2658.194, 2697.283, 2664.122, 2694.39, 2654.655, 2652.022, 2643.571, 2644.689, 2647.28, 2642.892, 2665.081, 2657.547, 2640.39, 2640.488, 2636.324, 2621.275, 2662.907, 2632.361, 2643.228, 2647.751, 2619.237, 2648.371, 2646.414, 2625.046, 2625.027, 2649.387, 2631.309, 2635.028, 2660.874, 2663.752, 2663.325, 2610.789, 2614.551]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:118 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:39 epochs:100	100	1000	True	25215.4707		772323	12	-1	196.13234186172485	{'train_loss': [988574.25, 484768.875, 405342.969, 325633.219, 281132.125, 263285.406, 257253.062, 252829.547, 247553.141, 244418.578, 239935.219, 236805.406, 234737.562, 231782.5, 230774.766, 228808.094, 226686.266, 225797.531, 224858.094, 223908.656, 222874.109, 221710.953, 220317.469, 220132.453, 218670.469, 217579.172, 217144.359, 216358.953, 215896.344, 215301.672, 214033.703, 213017.375, 213044.641, 212886.0, 211840.422, 210977.25, 211085.203, 209939.141, 209576.938, 209764.344, 207993.797, 208389.562, 207326.75, 208102.094, 207013.609, 206410.188, 205629.125, 206082.594, 205630.797, 205148.156, 204245.391, 204483.344, 202975.094, 203634.703, 203406.078, 202719.453, 202534.859, 202730.953, 201650.0, 201289.344, 201574.625, 201440.453, 200543.984, 201217.203, 200311.516, 200306.281, 200610.422, 199967.047, 199799.953, 199458.547, 199218.609, 198926.062, 199346.609, 198662.688, 197306.234, 198496.641, 196673.547, 197546.0, 196525.969, 196981.203, 195956.438, 196371.625, 196622.906, 195891.016, 195984.531, 195898.781, 195084.375, 195739.953, 195005.781, 194456.797, 194587.484, 194099.344, 195884.094, 193998.422, 194235.719, 193529.938, 193839.0, 193466.969, 193870.375, 193773.719], 'val_loss': [6617.622, 12631.279, 4048.163, 3649.411, 3330.171, 3235.208, 3206.459, 3131.274, 3206.999, 3119.753, 3084.482, 2982.037, 2933.933, 2952.602, 2888.355, 2944.531, 3024.458, 2948.13, 2972.886, 2936.507, 2835.556, 2858.477, 2935.267, 2822.399, 2908.402, 2830.747, 2786.328, 2812.68, 2849.513, 2767.462, 2808.171, 2742.255, 2794.199, 2839.139, 2745.517, 2744.512, 2780.742, 2758.26, 2727.779, 2852.972, 2803.435, 2775.806, 2762.194, 2764.218, 2772.896, 2722.891, 2709.389, 2721.334, 2685.935, 2747.018, 2702.62, 2728.517, 2729.011, 2722.284, 2688.336, 2772.495, 2725.198, 2721.573, 2704.326, 2711.243, 2684.523, 2702.013, 2671.84, 2691.71, 2705.772, 2674.385, 2713.227, 2686.348, 2679.815, 2693.502, 2680.696, 2665.792, 2695.862, 2695.235, 2645.89, 2749.103, 2687.251, 2676.232, 2690.947, 2689.966, 2668.997, 2644.091, 2678.632, 2682.057, 2674.842, 2646.282, 2656.072, 2645.188, 2655.652, 2687.516, 2644.744, 2652.386, 2665.393, 2664.855, 2657.486, 2657.856, 2625.696, 2640.709, 2670.252, 2669.11]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:4 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:39 epochs:100	100	1000	True	26442.95312		2209029	12	-1	245.1706919670105	{'train_loss': [1102479.5, 1147129.875, 1073965.25, 1184850.875, 1071782.125, 1053962.75, 681391.5, 390053.219, 314750.281, 285831.312, 268013.188, 257438.609, 251406.281, 245811.531, 243427.953, 239916.656, 235181.5, 234883.891, 232881.0, 231138.781, 228289.156, 227210.0, 224874.016, 224184.891, 222886.891, 222581.984, 221516.688, 221464.672, 218641.078, 218928.281, 216618.0, 217459.781, 217059.078, 215279.578, 215656.047, 216024.547, 213711.031, 213513.719, 213425.234, 212110.703, 211774.578, 212106.953, 211715.625, 210792.469, 211428.938, 209508.922, 208836.812, 209005.266, 208212.531, 207766.281, 207717.703, 207109.531, 207388.719, 206918.469, 206800.609, 206099.781, 204927.688, 206366.359, 204128.875, 203991.266, 204441.547, 203917.078, 204687.094, 202471.281, 202875.594, 203509.188, 201748.094, 201415.438, 202548.141, 201312.234, 200928.375, 200374.5, 199661.516, 200434.578, 200497.375, 198309.109, 200095.234, 198361.266, 198141.469, 197776.734, 197891.5, 197697.891, 197096.75, 197790.047, 197073.781, 195612.844, 196464.969, 195884.25, 196705.219, 195720.688, 195090.859, 194532.578, 195253.781, 193710.984, 194130.219, 193338.406, 193901.844, 193725.016, 194721.172, 192153.641], 'val_loss': [13332.52, 13332.505, 13332.506, 13406.356, 13410.025, 9233.541, 5422.711, 4191.004, 3463.023, 3308.508, 3140.717, 3088.846, 3021.683, 3035.113, 2960.59, 2974.187, 2941.664, 2916.061, 2881.607, 2865.555, 2864.401, 2839.669, 2932.26, 2907.35, 2935.817, 2864.824, 2864.51, 2857.918, 2806.973, 2820.605, 2928.272, 2827.003, 2829.057, 2807.259, 2819.786, 2806.513, 2801.923, 2836.663, 2872.145, 2848.025, 2769.591, 2850.657, 2814.009, 2817.562, 2816.884, 2781.29, 2819.511, 2856.95, 2810.299, 2809.559, 2796.418, 2761.382, 2823.2, 2813.654, 2820.229, 2813.559, 2812.15, 2807.284, 2818.574, 2759.53, 2785.946, 2779.278, 2827.157, 2809.506, 2805.84, 2800.219, 2751.574, 2809.968, 2821.392, 2802.189, 2794.335, 2793.49, 2781.114, 2804.209, 2792.398, 2842.411, 2788.781, 2771.767, 2830.072, 2757.19, 2785.044, 2817.381, 2790.885, 2781.509, 2828.442, 2778.189, 2799.85, 2767.92, 2772.927, 2822.523, 2772.947, 2807.81, 2772.454, 2800.352, 2804.632, 2808.529, 2808.881, 2795.484, 2775.756, 2787.476]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:39 epochs:100	100	2000	True	25113.00195		505857	11	-1	187.0458002090454	{'train_loss': [661613.562, 393633.875, 325032.469, 290234.625, 267730.156, 260313.141, 256284.672, 252302.766, 249864.594, 247886.875, 244615.656, 242746.5, 241807.766, 239231.344, 237244.656, 235499.875, 234543.344, 232432.609, 232566.938, 231386.719, 229817.344, 228175.781, 227266.391, 226498.984, 225883.297, 223939.031, 223975.438, 224246.125, 221776.984, 221331.109, 220151.203, 219606.297, 219775.719, 218182.125, 217756.812, 217152.188, 217146.688, 216164.953, 215465.172, 214822.781, 215349.391, 213690.344, 213722.781, 212797.422, 212562.781, 212564.234, 211217.562, 211486.469, 210188.219, 209430.328, 210179.609, 209900.719, 208507.453, 209400.844, 209505.719, 207754.047, 208622.922, 207886.359, 207227.094, 207242.703, 207777.359, 206476.812, 206093.266, 206611.562, 206059.562, 205139.969, 205284.062, 204472.422, 204840.438, 204728.75, 205234.766, 204448.906, 203503.391, 204390.281, 203913.0, 202965.891, 203724.734, 203071.938, 203296.297, 203293.266, 202786.641, 202486.422, 201437.891, 202243.641, 201527.0, 201080.5, 201297.625, 201534.109, 200203.75, 200828.672, 200869.422, 200505.344, 200521.812, 200277.203, 199943.828, 199674.328, 200537.406, 200051.844, 199343.719, 199974.922], 'val_loss': [4971.581, 3926.865, 3622.924, 3323.137, 3197.324, 3145.931, 3104.606, 3095.141, 3068.601, 3032.916, 3038.847, 3011.903, 2981.061, 3013.691, 2966.396, 2955.237, 2927.062, 2911.618, 2914.132, 2911.706, 2903.325, 2881.372, 2888.542, 2863.537, 2858.605, 2836.699, 2841.938, 2859.296, 2829.663, 2820.681, 2819.766, 2824.897, 2805.333, 2792.407, 2787.758, 2806.666, 2792.801, 2762.905, 2775.743, 2766.028, 2748.051, 2799.033, 2733.609, 2763.453, 2708.839, 2755.669, 2727.235, 2697.77, 2737.098, 2716.97, 2721.641, 2750.224, 2703.083, 2696.173, 2719.679, 2731.993, 2690.836, 2683.379, 2708.363, 2675.682, 2658.443, 2660.442, 2674.902, 2673.493, 2676.742, 2685.898, 2688.624, 2653.276, 2638.026, 2663.09, 2666.268, 2646.301, 2663.368, 2642.569, 2659.828, 2658.375, 2657.005, 2668.653, 2652.503, 2638.501, 2624.436, 2674.218, 2646.652, 2619.75, 2613.461, 2625.413, 2656.973, 2638.272, 2647.728, 2619.965, 2628.661, 2628.766, 2677.515, 2621.367, 2618.16, 2631.193, 2639.672, 2619.562, 2627.908, 2630.84]}	0	100	True
