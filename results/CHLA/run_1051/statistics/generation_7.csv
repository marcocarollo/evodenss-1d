id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	25229.45312		639490	13	-1	210.31761264801025	{'train_loss': [662522.938, 423840.688, 320459.375, 275715.281, 262327.781, 257341.469, 252525.172, 249186.781, 246167.016, 243671.516, 240347.453, 238651.062, 237524.562, 235019.516, 234174.5, 232994.234, 232192.125, 231382.203, 230697.047, 228933.328, 228414.594, 227577.891, 227259.125, 226671.766, 225948.703, 225361.594, 224194.094, 223970.391, 222633.609, 222373.859, 221631.516, 220896.125, 220249.109, 219792.047, 218823.344, 218702.719, 217853.766, 217751.469, 216688.812, 216353.031, 216761.781, 216271.547, 214943.906, 214911.297, 215706.094, 213997.516, 213289.078, 212613.172, 212438.312, 212599.062, 212415.578, 211585.297, 211538.078, 211217.484, 210904.344, 210640.219, 210138.781, 210937.906, 209847.922, 209472.219, 210070.25, 209684.641, 208567.062, 207925.141, 208121.547, 207187.344, 207556.641, 207794.703, 207751.953, 206896.781, 206488.438, 206843.438, 206741.703, 206540.656, 205399.203, 205707.438, 205390.531, 205217.469, 205348.281, 203905.391, 204790.828, 204680.453, 204880.062, 203753.281, 203935.109, 204193.312, 203249.625, 202937.047, 203418.328, 202644.094, 201449.609, 202441.203, 202094.078, 201386.328, 200669.078, 201677.906, 202361.516, 201857.109, 200404.25, 201048.141], 'val_loss': [4206.705, 3247.353, 2881.297, 2750.427, 2692.489, 2643.935, 2622.2, 2592.246, 2563.403, 2561.45, 2525.886, 2532.765, 2533.25, 2521.82, 2551.122, 2516.917, 2515.423, 2486.088, 2429.57, 2443.05, 2452.038, 2423.395, 2427.115, 2428.234, 2421.943, 2404.086, 2415.671, 2403.847, 2412.331, 2364.039, 2355.121, 2375.878, 2356.409, 2352.829, 2371.837, 2362.535, 2334.4, 2391.935, 2327.008, 2370.64, 2312.45, 2334.219, 2322.354, 2328.857, 2300.515, 2290.69, 2282.665, 2308.313, 2267.564, 2288.04, 2278.205, 2291.428, 2295.933, 2265.265, 2283.216, 2296.988, 2275.829, 2280.155, 2252.703, 2249.433, 2295.186, 2277.014, 2276.177, 2252.029, 2276.01, 2287.047, 2269.394, 2248.019, 2248.58, 2260.826, 2256.033, 2252.689, 2254.012, 2236.072, 2244.983, 2247.156, 2265.716, 2286.097, 2250.831, 2237.161, 2233.503, 2239.225, 2226.04, 2231.499, 2256.862, 2255.807, 2237.554, 2234.339, 2273.009, 2226.462, 2258.055, 2237.664, 2242.852, 2229.392, 2203.46, 2210.482, 2234.706, 2229.373, 2208.287, 2237.849]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:21 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	26563.38477		1248411	12	-1	209.17726802825928	{'train_loss': [1104193.125, 906514.438, 549701.938, 361162.906, 304535.219, 284708.719, 272504.75, 267709.344, 261887.516, 258026.578, 254511.312, 252022.203, 248702.234, 246921.859, 244266.5, 243458.719, 240652.328, 239350.156, 238390.828, 236802.797, 235079.0, 233491.641, 232892.062, 231164.5, 230721.391, 230613.703, 229155.844, 228630.297, 227284.906, 226696.875, 225600.609, 226402.812, 224838.0, 223598.719, 223819.109, 222421.344, 223168.453, 221841.781, 221859.031, 221179.812, 220565.188, 219550.609, 219746.688, 219282.484, 218446.672, 217950.125, 218607.828, 217355.75, 217622.375, 217724.438, 216459.438, 217058.609, 216688.844, 214725.625, 216475.703, 215550.109, 215331.891, 215858.578, 215936.438, 214327.125, 214192.203, 214251.312, 213830.391, 214304.328, 214044.484, 212169.781, 213335.516, 213413.547, 212090.375, 212629.734, 212193.484, 212019.516, 211010.562, 211149.312, 211974.609, 211054.016, 211526.094, 212000.797, 210577.094, 209955.375, 210824.75, 209894.109, 210348.156, 209591.234, 210015.906, 209460.344, 209032.203, 209403.25, 209685.359, 208412.672, 207239.047, 209068.531, 209092.172, 208280.453, 207390.812, 208260.094, 207579.344, 207654.031, 207983.766, 207224.766], 'val_loss': [11022.693, 4471.298, 3702.171, 3166.105, 2884.854, 2828.046, 2719.81, 2697.155, 2677.651, 2633.021, 2563.602, 2564.123, 2564.519, 2565.235, 2515.892, 2522.946, 2490.603, 2496.871, 2495.571, 2466.937, 2433.879, 2465.583, 2440.496, 2423.354, 2414.31, 2432.154, 2418.834, 2405.321, 2407.481, 2464.239, 2389.86, 2426.526, 2406.4, 2442.896, 2373.563, 2419.132, 2358.695, 2372.809, 2368.509, 2429.352, 2380.567, 2387.189, 2336.913, 2362.894, 2375.01, 2366.545, 2369.737, 2373.286, 2375.799, 2348.929, 2339.084, 2342.731, 2408.374, 2375.942, 2378.469, 2331.23, 2365.774, 2379.91, 2355.779, 2356.733, 2318.621, 2326.483, 2351.205, 2348.655, 2363.318, 2345.363, 2329.948, 2358.718, 2336.342, 2376.442, 2349.541, 2391.208, 2324.978, 2318.388, 2346.651, 2378.034, 2359.41, 2360.502, 2353.384, 2323.297, 2347.776, 2343.986, 2367.729, 2348.622, 2369.876, 2350.879, 2321.101, 2336.817, 2354.872, 2319.557, 2295.516, 2345.761, 2316.292, 2336.429, 2365.466, 2312.297, 2301.831, 2340.634, 2312.448, 2324.101]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:95 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:108 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:98 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.08871423223681665 alpha:0.9077912666691462 weight_decay:0.00019994778205307937 batch_size:32 epochs:100	100	1000	True	131122.73438		976367	14	-1	224.58965229988098	{'train_loss': [1083024.375, 1651766.875, 1062567.125, 1094616.5, 1062567.125, 1116344.375, 1062567.125, 1398311.875, 1062731.125, 1952237.375, 1217578.25, 1313579.875, 1141398.0, 1192554.375, 1260440.5, 1146249.375, 1257850.875, 1197956.25, 1186047.125, 1156773.5, 1276966.0, 1162097.125, 1162243.875, 1215275.25, 1135013.125, 1166654.25, 1160887.25, 1107263.875, 1118052.375, 1103519.25, 1086183.375, 1093886.375, 1075012.875, 1089705.75, 1099480.375, 1080041.125, 1087447.5, 1099328.125, 1091162.625, 1095539.5, 1078924.625, 1102123.625, 1083360.375, 1101935.0, 1089349.375, 1075928.875, 1120605.0, 1085548.625, 1094043.0, 1111326.375, 1077478.5, 1103756.875, 1084386.625, 1074216.375, 1120028.75, 1080098.125, 1086511.75, 1114276.125, 1084757.125, 1092256.25, 1094550.0, 1089816.875, 1083087.375, 1090649.375, 1084176.875, 1108245.125, 1091958.25, 1083045.25, 1091603.125, 1090711.75, 1104693.125, 1091094.875, 1092249.0, 1103202.625, 1089013.25, 1094329.0, 1100474.5, 1096034.25, 1090190.0, 1086579.0, 1099106.0, 1094803.625, 1081883.75, 1100124.5, 1079662.875, 1093124.5, 1103834.625, 1087241.375, 1101197.0, 1075756.125, 1102778.875, 1084893.0, 1083265.5, 1090570.125, 1098812.5, 1084799.375, 1089895.375, 1106558.0, 1097817.625, 1100178.125], 'val_loss': [11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11108.478, 11110.434, 11110.434, 11110.434, 77639.664, 11110.267, 11102.588, 12363.955, 12166.8, 15901.704, 11110.434, 11110.434, 11110.428, 11110.424, 11110.434, 11110.434, 11110.105, 11110.429, 11052.344, 11350.148, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 13522.33, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11109.078, 11649.691, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 12763.641, 11106.27, 11110.434, 11159.251, 11110.434, 11108.402, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.427, 13940.958, 11110.434, 11110.434, 11110.434, 12181.451, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 12715.504, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11528.688, 11110.434, 13238.69, 11110.434, 11282.019, 12593.123, 11110.434]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	25060.49805		619010	12	-1	211.80228996276855	{'train_loss': [586599.25, 415674.5, 352481.094, 315080.0, 301183.219, 295316.188, 290591.25, 278751.156, 264809.875, 256723.75, 251868.078, 247835.547, 245063.156, 242589.188, 241337.531, 239773.438, 238130.188, 236810.406, 235434.266, 234233.906, 233041.969, 232717.234, 231538.016, 230798.469, 229801.031, 228612.797, 228969.266, 227766.156, 228002.906, 226295.969, 226750.688, 226062.141, 225624.5, 224801.469, 224613.734, 223578.062, 223111.984, 223115.797, 221245.312, 221673.906, 221085.516, 219908.422, 220110.938, 218931.25, 218539.078, 217978.906, 217644.578, 217661.109, 217397.266, 216755.703, 215416.625, 214322.781, 214575.047, 213963.5, 213424.0, 213648.203, 212827.625, 213293.188, 212086.953, 211762.547, 211162.25, 210551.125, 210784.297, 210621.359, 209706.766, 209063.375, 209181.219, 209285.828, 208917.891, 208695.547, 207669.203, 208267.406, 206867.547, 207537.109, 206820.891, 206741.422, 206791.828, 205790.516, 205539.609, 204936.109, 204853.859, 204688.031, 203737.516, 204349.312, 203723.266, 203276.266, 204054.094, 204223.578, 202657.828, 203483.422, 203158.344, 202575.109, 202715.344, 203026.016, 201785.516, 201968.703, 201200.516, 201630.547, 200953.328, 200790.75], 'val_loss': [4194.312, 3524.964, 3291.103, 3121.319, 3091.333, 3062.333, 3064.256, 2782.335, 2700.251, 2631.812, 2599.456, 2559.839, 2521.319, 2527.908, 2504.797, 2503.805, 2493.976, 2468.522, 2474.124, 2477.325, 2484.554, 2446.492, 2443.875, 2428.657, 2423.65, 2441.12, 2439.465, 2428.658, 2433.739, 2435.525, 2409.239, 2420.202, 2432.106, 2415.306, 2419.984, 2399.152, 2402.063, 2390.7, 2417.989, 2381.823, 2374.881, 2362.488, 2361.172, 2354.99, 2339.12, 2360.124, 2338.344, 2316.574, 2335.68, 2316.382, 2317.644, 2289.702, 2319.333, 2315.478, 2345.703, 2289.133, 2293.634, 2292.673, 2292.193, 2305.141, 2289.434, 2262.874, 2296.651, 2267.981, 2284.638, 2274.804, 2249.835, 2261.881, 2277.752, 2252.87, 2270.646, 2273.917, 2280.198, 2262.917, 2253.473, 2270.456, 2286.099, 2240.544, 2247.551, 2251.125, 2243.082, 2245.343, 2243.023, 2260.19, 2238.284, 2229.438, 2255.387, 2235.425, 2235.133, 2253.411, 2240.267, 2257.01, 2254.775, 2249.979, 2252.827, 2235.838, 2262.305, 2230.458, 2223.411, 2247.254]}	100	100	True
