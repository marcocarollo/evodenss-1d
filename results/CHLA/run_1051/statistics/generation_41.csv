id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:73 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:80 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	23500.50391		345804	11	-1	239.625910282135	{'train_loss': [408673.188, 302892.562, 281813.531, 273059.906, 267749.031, 264255.5, 258525.797, 254114.969, 251044.594, 248468.062, 244619.234, 242250.109, 240195.75, 237650.156, 235593.594, 233919.844, 232397.156, 230585.734, 228630.312, 226958.625, 226502.203, 225346.812, 223372.031, 223426.266, 221819.031, 221748.844, 220177.422, 218445.594, 219002.656, 217195.609, 217063.531, 215620.25, 214645.203, 214748.156, 214244.422, 212841.703, 212022.516, 212175.297, 210922.766, 210183.406, 209671.094, 210156.859, 208415.703, 208193.859, 207635.844, 207115.812, 206583.828, 205773.438, 205434.672, 205785.094, 205095.656, 205185.484, 204895.484, 203555.859, 203754.859, 201909.703, 201446.297, 202106.188, 202320.203, 202141.656, 200770.859, 200885.203, 201617.188, 200260.891, 199123.109, 199293.406, 199133.516, 199684.453, 197923.172, 198446.109, 198352.578, 197461.797, 196934.547, 197961.062, 197568.906, 197732.312, 196501.344, 195841.984, 194819.125, 194653.781, 194944.984, 194839.328, 195342.312, 194274.156, 192673.172, 193326.156, 192902.891, 193571.672, 193117.391, 192452.391, 192550.438, 191694.828, 191900.969, 191917.547, 191600.516, 191099.672, 190710.734, 189689.859, 190799.797, 189836.516], 'val_loss': [2497.263, 2294.641, 2244.693, 2207.94, 2162.219, 2133.349, 2103.69, 2088.53, 2020.348, 2007.809, 2021.686, 2000.295, 1965.392, 1954.679, 1934.52, 1917.445, 1913.018, 1891.14, 1854.286, 1882.083, 1849.311, 1856.714, 1834.938, 1844.465, 1826.006, 1840.139, 1816.917, 1822.686, 1801.276, 1808.836, 1798.966, 1806.888, 1778.01, 1789.971, 1781.943, 1775.204, 1779.227, 1769.142, 1776.595, 1784.129, 1761.677, 1768.278, 1765.867, 1763.924, 1764.339, 1761.304, 1738.955, 1743.978, 1769.643, 1736.888, 1735.229, 1736.179, 1722.483, 1747.075, 1743.24, 1735.718, 1723.403, 1717.975, 1741.917, 1747.282, 1737.287, 1746.614, 1731.833, 1706.629, 1735.163, 1729.543, 1733.908, 1731.002, 1727.811, 1712.109, 1706.478, 1710.298, 1708.956, 1726.438, 1697.442, 1703.857, 1729.903, 1718.657, 1726.138, 1709.542, 1703.21, 1713.797, 1730.693, 1706.034, 1722.094, 1713.314, 1687.653, 1702.276, 1705.666, 1708.193, 1690.281, 1676.37, 1723.605, 1708.995, 1701.549, 1684.453, 1693.733, 1694.996, 1688.795, 1709.808]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:73 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:80 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:11 epochs:100	100	1000	True	25333.10156		467048	10	-1	454.2403984069824	{'train_loss': [476810.625, 311552.156, 285309.531, 270627.719, 261970.688, 256032.766, 250855.766, 246756.766, 244173.969, 241698.938, 239542.656, 237537.203, 236346.172, 235041.719, 233327.344, 232435.016, 230826.406, 229912.156, 229409.281, 227967.906, 227665.375, 226029.219, 225300.578, 224657.391, 223932.344, 223972.297, 222186.172, 221427.594, 221426.203, 221125.906, 219364.219, 219303.219, 218653.734, 218221.562, 217918.312, 216084.766, 216615.641, 215947.406, 215031.0, 214642.625, 214572.453, 214468.562, 213736.0, 212666.781, 213441.812, 212578.219, 211620.469, 211935.562, 211926.125, 212206.656, 210859.484, 210481.453, 209988.672, 209985.531, 209967.406, 208311.703, 208097.344, 208241.781, 207883.547, 207309.719, 207537.75, 206099.609, 205792.391, 206476.484, 204520.484, 205187.688, 204197.578, 204641.609, 204704.25, 204586.062, 204107.953, 203766.625, 203940.156, 203043.344, 203998.484, 202226.016, 202664.656, 202119.141, 201637.844, 202028.266, 201766.344, 201060.25, 201098.125, 200912.719, 201078.078, 199685.422, 199277.344, 199801.531, 199214.828, 200012.594, 199561.172, 199564.562, 197985.734, 198562.828, 198046.031, 198149.922, 197409.891, 197087.031, 197861.109, 196151.75], 'val_loss': [1232.472, 1026.771, 963.027, 932.088, 907.442, 903.686, 877.001, 877.357, 872.147, 856.107, 863.308, 849.983, 858.259, 842.198, 850.371, 855.397, 847.191, 851.948, 826.814, 835.41, 830.095, 828.438, 823.379, 825.587, 818.308, 837.625, 826.324, 832.633, 826.347, 818.6, 799.634, 809.62, 828.87, 795.282, 818.016, 804.977, 793.998, 804.88, 802.279, 821.627, 810.597, 812.758, 784.691, 801.561, 802.269, 796.88, 791.521, 796.215, 782.624, 788.158, 792.324, 784.44, 798.08, 770.387, 800.326, 792.856, 794.127, 788.063, 776.921, 794.03, 787.467, 792.237, 774.585, 765.534, 799.794, 801.442, 782.325, 788.805, 794.317, 792.855, 780.406, 790.416, 775.676, 792.166, 778.453, 777.721, 780.579, 772.76, 786.328, 785.947, 788.806, 757.212, 789.234, 772.847, 777.352, 766.555, 801.697, 764.643, 763.423, 778.19, 789.975, 790.249, 771.418, 758.958, 762.478, 775.952, 768.263, 775.941, 782.989, 757.558]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:85 kernel_size:10 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:84 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:80 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.148030937551874 beta1:0.8521569914142962 beta2:0.8073229002695487 weight_decay:4.518889604492122e-05 batch_size:26 epochs:100	100	1000	True	131121.51562		526975	11	-1	249.44490504264832	{'train_loss': [1147910.0, 1109196.0, 1156055.125, 1081915.25, 1075884.5, 1071765.25, 1072574.25, 1071980.125, 1071530.0, 1073463.625, 1075710.125, 1072304.125, 1070410.375, 1073007.5, 1071530.25, 1073427.375, 1074741.125, 1073934.875, 1072537.375, 1071435.125, 1071679.125, 1072905.75, 1071918.0, 1070085.375, 1072748.5, 1073898.25, 1073338.5, 1073353.625, 1072210.125, 1072945.375, 1072713.125, 1072617.0, 1072644.125, 1071754.0, 1071592.875, 1070657.375, 1071879.5, 1070525.5, 1071389.0, 1070705.75, 1071793.0, 1071233.5, 1071519.875, 1073079.0, 1070222.625, 1071531.125, 1071328.125, 1073292.75, 1071647.75, 1071605.5, 1072125.75, 1070673.125, 1070200.125, 1070264.5, 1071387.25, 1070993.0, 1071700.125, 1072513.875, 1073755.75, 1071763.125, 1071280.75, 1071612.125, 1070695.0, 1072062.0, 1072888.625, 1072207.375, 1070933.75, 1071130.75, 1073402.375, 1071663.875, 1069641.125, 1070087.875, 1070737.375, 1071742.5, 1072701.25, 1070903.625, 1069713.0, 1069092.125, 1070345.5, 1070129.0, 1071207.75, 1072057.375, 1070957.125, 1071229.25, 1069637.125, 1069588.375, 1069070.5, 1070444.625, 1069397.875, 1069984.75, 1073667.625, 1071208.75, 1072128.0, 1070898.5, 1072191.125, 1071472.125, 1070902.125, 1070121.25, 1071998.5, 1070253.625], 'val_loss': [11249.375, 9000.865, 8888.349, 8888.349, 8888.349, 8888.349, 9243.955, 8888.349, 8881.68, 9433.712, 8888.338, 8888.349, 9618.623, 8886.215, 8888.349, 8888.349, 10020.983, 9739.882, 9152.758, 8997.363, 8888.186, 8888.349, 8888.349, 8888.349, 8921.986, 8888.349, 8888.349, 8888.349, 9019.467, 9480.132, 8888.349, 8888.348, 8888.349, 8888.349, 8888.349, 8888.349, 9442.373, 8888.348, 8888.349, 9442.053, 8866.271, 8888.347, 8888.349, 8888.349, 8888.349, 8888.349, 9109.722, 8888.349, 9244.971, 9723.419, 8888.349, 8888.349, 8896.109, 8888.349, 8888.349, 9464.55, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8879.646, 8883.278, 8888.347, 8888.349, 9015.378, 8888.349, 8888.348, 8888.349, 8888.349, 9441.517, 9173.117, 8888.349, 8888.199, 8888.349, 8888.349, 9146.062, 8885.953, 8888.349, 9110.601, 8888.346, 8888.349, 9266.915, 8888.346, 9037.299, 8888.132, 8888.282, 8888.349, 9293.707, 8888.349, 8888.349, 8888.349, 8888.349, 9267.928, 8888.327, 9076.33, 9294.654, 8885.051, 8888.346, 8888.349]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:12 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:47 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:73 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:104 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:26 epochs:100	100	1000	True	27760.37305		2154792	12	-1	258.014527797699	{'train_loss': [1184247.0, 1081129.0, 1085661.0, 1081084.375, 629372.688, 332447.562, 303290.094, 271971.844, 258415.359, 251390.828, 245231.75, 242006.016, 240244.844, 237907.25, 237106.078, 234922.25, 233132.5, 231313.875, 231343.094, 229173.953, 227558.422, 226251.484, 226169.688, 225537.234, 224057.359, 222784.891, 222393.094, 221631.906, 219957.922, 218717.359, 219033.922, 216938.016, 217022.094, 215535.812, 215796.438, 215546.281, 214158.828, 214245.578, 212808.625, 211345.406, 211683.172, 211330.109, 210911.625, 210625.438, 209227.578, 208138.5, 207550.406, 208142.188, 206508.969, 206855.281, 205522.062, 206557.797, 205137.109, 204695.266, 203300.734, 203557.094, 202205.781, 202755.516, 202159.312, 201775.266, 201319.797, 200170.625, 200878.078, 199106.203, 199585.672, 199596.891, 198502.062, 198251.844, 197129.234, 197384.031, 196081.375, 197119.016, 195999.812, 195459.953, 194285.984, 194740.875, 193571.469, 195451.828, 193750.422, 192468.75, 193457.422, 192918.703, 190749.781, 190973.375, 192060.844, 191626.172, 190912.469, 191568.531, 189947.625, 190021.578, 190150.266, 189749.375, 189675.469, 188613.375, 189019.062, 186629.266, 187718.828, 187474.375, 187819.219, 187273.953], 'val_loss': [8888.349, 8888.344, 8874.654, 8416.01, 2842.281, 2623.211, 2319.739, 2222.013, 2084.302, 2054.696, 2034.05, 2034.922, 2015.6, 1979.565, 1968.536, 1992.411, 1966.969, 1957.265, 1978.075, 1941.143, 1946.132, 1938.384, 1953.069, 1937.921, 1945.592, 1961.547, 1958.088, 1930.515, 1933.292, 1925.981, 1927.029, 1946.425, 1943.996, 1926.542, 1951.452, 1945.187, 1921.157, 2019.566, 1945.434, 1961.096, 1976.528, 1972.153, 2004.792, 1983.763, 1947.916, 1943.816, 1992.491, 1919.7, 1969.526, 1981.689, 1960.672, 1957.501, 1926.42, 1976.639, 1956.149, 1964.241, 1980.76, 1940.32, 1954.223, 1957.664, 1929.19, 1915.252, 1941.241, 1942.882, 1940.611, 1947.229, 1950.986, 1924.372, 1962.915, 1912.101, 1945.665, 1910.282, 1971.53, 1934.132, 1941.915, 1909.897, 1953.813, 1946.81, 1956.386, 1923.424, 1930.908, 1932.896, 1911.998, 1901.374, 1919.057, 1923.298, 1930.921, 1914.667, 1958.238, 1967.942, 1932.906, 1961.948, 1911.465, 1937.378, 1912.436, 1943.355, 1955.189, 1940.105, 1895.627, 1936.575]}	100	100	True
