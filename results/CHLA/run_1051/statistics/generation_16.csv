id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	24501.30469		619010	12	-1	213.5953414440155	{'train_loss': [643098.125, 436616.438, 328893.219, 276549.062, 260193.812, 254956.172, 251059.609, 247549.156, 244266.266, 242490.391, 240331.641, 238806.391, 237273.734, 235714.688, 234641.047, 232824.109, 231377.484, 229853.531, 229235.172, 228984.125, 226888.891, 226319.703, 225627.922, 224828.641, 224437.359, 223234.438, 221736.609, 221592.844, 221298.281, 220472.062, 220013.281, 219213.203, 218346.328, 217519.578, 216860.547, 216515.359, 215395.641, 215373.781, 213771.156, 214339.734, 213550.0, 212257.062, 212768.625, 211595.469, 211708.5, 210862.5, 211067.969, 210895.5, 210944.734, 210098.953, 210384.234, 208966.688, 208581.594, 208771.609, 207582.328, 207581.875, 208200.5, 207520.656, 207260.359, 206168.203, 206217.766, 205370.156, 205808.953, 205451.672, 204804.156, 205482.5, 204028.453, 203529.859, 204611.641, 204536.672, 203940.031, 203884.016, 203005.172, 203113.703, 202429.281, 203125.594, 202364.438, 201609.578, 202406.266, 201892.797, 202140.75, 202108.562, 200592.859, 200423.172, 200526.828, 200484.016, 200849.734, 199758.734, 199099.234, 199828.828, 198858.562, 199792.438, 200142.109, 200233.25, 198422.031, 199227.156, 198749.828, 198844.344, 198526.828, 197580.391], 'val_loss': [4076.958, 3423.199, 2921.017, 2829.824, 2641.697, 2673.63, 2589.152, 2543.847, 2530.457, 2501.774, 2495.847, 2490.978, 2480.403, 2470.395, 2475.894, 2435.486, 2451.58, 2420.664, 2409.45, 2412.351, 2408.371, 2403.162, 2385.356, 2360.842, 2355.963, 2355.87, 2339.853, 2330.027, 2343.23, 2316.083, 2327.896, 2338.762, 2335.998, 2317.7, 2300.877, 2321.89, 2309.095, 2316.698, 2315.178, 2306.812, 2315.91, 2283.789, 2284.202, 2293.871, 2299.427, 2310.586, 2269.328, 2310.059, 2291.245, 2278.001, 2276.26, 2260.102, 2285.729, 2254.403, 2256.707, 2252.222, 2262.577, 2258.125, 2254.539, 2263.254, 2227.632, 2239.423, 2227.486, 2276.906, 2230.453, 2248.292, 2228.241, 2233.669, 2211.083, 2240.641, 2229.814, 2235.657, 2213.37, 2212.106, 2231.079, 2208.737, 2206.405, 2199.219, 2210.667, 2196.294, 2213.29, 2185.892, 2209.856, 2185.375, 2195.182, 2190.316, 2167.078, 2157.044, 2174.793, 2173.516, 2172.523, 2181.217, 2184.351, 2166.969, 2201.776, 2206.441, 2179.687, 2166.673, 2178.371, 2183.664]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:124 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:16 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	25917.55664		766018	13	-1	229.51414918899536	{'train_loss': [914856.562, 528704.312, 385144.375, 308383.125, 291660.594, 278531.125, 269834.125, 264256.625, 259580.75, 257806.531, 254280.266, 252826.328, 249952.891, 248601.375, 246883.578, 245790.953, 244290.891, 243362.562, 241951.609, 240343.531, 239453.5, 238971.0, 236941.312, 236386.156, 235536.938, 235846.359, 235014.578, 233975.938, 233398.672, 232182.875, 232814.609, 231526.125, 231917.766, 230509.766, 230393.891, 229765.719, 229236.312, 228388.031, 228818.766, 228049.578, 227167.0, 227503.672, 227249.078, 226760.844, 225293.5, 225534.188, 224594.891, 224511.484, 225553.5, 224434.922, 223575.531, 224556.703, 222582.266, 221996.312, 221680.984, 221378.875, 222402.125, 220971.719, 221799.0, 220788.953, 220661.219, 220262.109, 219811.422, 220198.125, 219395.062, 218606.625, 217926.25, 217635.547, 217979.438, 217279.672, 217527.359, 216655.406, 215368.125, 216460.453, 215256.844, 215682.797, 214806.797, 214345.312, 215279.516, 214958.547, 213095.469, 214503.688, 213604.688, 212324.047, 213346.297, 211612.406, 212325.828, 211402.922, 212032.984, 211894.609, 211379.406, 210418.75, 210495.344, 209485.0, 210109.438, 209978.547, 209237.203, 208779.219, 208252.094, 209376.953], 'val_loss': [5238.313, 4120.262, 3266.727, 3070.122, 2914.816, 2822.677, 2713.04, 2633.028, 2634.294, 2588.353, 2609.446, 2593.343, 2557.814, 2564.975, 2548.606, 2528.285, 2552.642, 2537.145, 2539.092, 2548.767, 2522.906, 2494.558, 2531.02, 2512.273, 2495.544, 2530.91, 2533.982, 2548.32, 2521.658, 2487.435, 2478.949, 2476.395, 2524.787, 2497.177, 2482.458, 2469.981, 2473.284, 2465.524, 2474.129, 2478.067, 2468.818, 2481.95, 2491.281, 2444.25, 2461.634, 2450.094, 2458.927, 2451.371, 2439.2, 2420.413, 2475.331, 2437.487, 2416.31, 2434.852, 2444.141, 2445.004, 2454.697, 2430.555, 2414.938, 2424.827, 2403.922, 2416.058, 2413.664, 2405.466, 2407.786, 2419.645, 2451.92, 2393.603, 2386.542, 2402.3, 2380.349, 2380.156, 2397.949, 2362.563, 2367.457, 2366.969, 2333.51, 2348.896, 2364.052, 2344.129, 2333.68, 2380.56, 2341.97, 2315.943, 2328.266, 2362.165, 2345.655, 2309.429, 2312.728, 2327.516, 2299.947, 2331.13, 2309.307, 2310.988, 2298.095, 2304.281, 2297.341, 2310.771, 2279.566, 2271.185]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:94 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:24 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.22477674190700597 beta1:0.939072706923791 beta2:0.9082384089541663 weight_decay:4.5700576750332754e-05 batch_size:32 epochs:100	100	1000	True	131122.45312		878276	14	-1	229.3764181137085	{'train_loss': [1079188.625, 1062568.5, 1122856.875, 1062615.125, 1292926.25, 1079042.875, 1071961.25, 1107646.25, 1068311.875, 1089545.5, 1075601.0, 1071566.75, 1088732.625, 1071110.625, 1075005.5, 1090293.125, 1074490.375, 1100861.75, 1078716.375, 1070453.875, 1084235.625, 1076831.375, 1070530.0, 1083039.0, 1073584.875, 1071076.625, 1080358.0, 1068172.125, 1092997.875, 1074358.125, 1069447.625, 1076810.25, 1076001.25, 1067150.625, 1080659.875, 1078244.75, 1068355.375, 1078176.125, 1074102.25, 1071588.875, 1075850.375, 1070297.125, 1083750.5, 1083053.5, 1073855.0, 1079288.25, 1082371.0, 1074281.375, 1075171.875, 1085189.25, 1071728.875, 1074239.875, 1073781.75, 1074925.875, 1075831.75, 1074598.375, 1074973.5, 1075259.375, 1073755.375, 1073669.0, 1075625.5, 1071083.0, 1076845.375, 1077104.125, 1072801.75, 1075651.25, 1077346.375, 1080087.875, 1076717.75, 1071954.5, 1081807.25, 1083903.875, 1072776.5, 1083281.125, 1082718.75, 1074052.25, 1073218.625, 1094722.25, 1071171.0, 1078543.0, 1073628.25, 1078398.875, 1081076.75, 1074804.0, 1081203.75, 1081131.0, 1070302.75, 1085074.5, 1081723.375, 1071172.5, 1087604.625, 1070339.875, 1072045.375, 1077167.625, 1070701.75, 1074889.375, 1078463.375, 1073630.75, 1073841.5, 1079813.75], 'val_loss': [11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11942.57, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	2000	True	24992.22656		619010	12	-1	213.89685678482056	{'train_loss': [604747.188, 400666.688, 307421.969, 272502.656, 260935.625, 254981.234, 249944.328, 246425.578, 242935.125, 240824.359, 238109.953, 237257.188, 235736.516, 234297.672, 232249.625, 231865.172, 230754.328, 229065.641, 228669.047, 227777.812, 226220.078, 225741.312, 225076.953, 225153.344, 224746.656, 223607.234, 223670.5, 222432.625, 221268.953, 221419.141, 220765.344, 220910.078, 219466.922, 220096.844, 218677.547, 217331.938, 217959.812, 218125.547, 217126.688, 216193.5, 215599.484, 215279.016, 215349.562, 214477.094, 213658.859, 213795.297, 214010.828, 212500.531, 212449.984, 212236.609, 211354.266, 211465.766, 210525.141, 210923.141, 210000.812, 209798.578, 209052.031, 208458.547, 209121.938, 208641.672, 207134.844, 207636.0, 207202.734, 206796.797, 207607.031, 207109.844, 206260.734, 205871.359, 205408.938, 204931.906, 206421.031, 204803.109, 205626.922, 205300.281, 204635.922, 204258.266, 204114.375, 203789.406, 203210.391, 203375.656, 203374.234, 202152.172, 201797.797, 202148.203, 201991.328, 201633.078, 201301.062, 201340.906, 201490.094, 201854.156, 200125.438, 200126.422, 199488.125, 200344.781, 199953.359, 199902.391, 198929.609, 199015.578, 200224.609, 198832.344], 'val_loss': [4024.521, 3152.079, 2863.911, 2713.947, 2641.285, 2626.089, 2575.22, 2530.623, 2558.987, 2525.18, 2489.848, 2455.903, 2446.43, 2442.964, 2436.175, 2414.151, 2400.473, 2390.534, 2395.6, 2385.928, 2379.091, 2372.793, 2369.29, 2345.432, 2353.502, 2353.197, 2347.866, 2366.251, 2339.138, 2344.483, 2344.258, 2346.481, 2336.371, 2330.082, 2334.216, 2320.17, 2304.848, 2306.084, 2313.811, 2351.75, 2303.315, 2316.046, 2299.83, 2293.076, 2297.832, 2292.229, 2312.682, 2275.551, 2270.793, 2273.041, 2289.01, 2302.196, 2276.876, 2270.981, 2266.82, 2258.092, 2279.668, 2243.602, 2243.603, 2249.497, 2250.542, 2236.616, 2230.311, 2241.543, 2244.321, 2214.343, 2217.74, 2218.544, 2214.914, 2218.465, 2218.948, 2212.621, 2220.905, 2213.112, 2236.792, 2220.414, 2235.076, 2211.821, 2206.649, 2200.194, 2209.47, 2202.915, 2203.904, 2193.42, 2200.058, 2209.315, 2192.659, 2187.432, 2205.863, 2202.851, 2184.944, 2185.77, 2184.621, 2181.986, 2194.239, 2184.517, 2189.21, 2221.201, 2200.165, 2179.616]}	0	100	True
