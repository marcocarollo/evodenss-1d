id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	23509.82617		416732	11	-1	235.7061583995819	{'train_loss': [501202.469, 325791.188, 281389.688, 265919.375, 258759.375, 255032.094, 251365.234, 248920.328, 245638.281, 243428.109, 241348.828, 239801.969, 238805.75, 237503.094, 235304.906, 235196.094, 232387.266, 231785.438, 229490.688, 228319.062, 227147.328, 224965.0, 223885.359, 222770.656, 221644.109, 220395.469, 219793.984, 219190.781, 217257.031, 217214.484, 216108.625, 215528.375, 214628.734, 214881.578, 214612.953, 213468.844, 212233.891, 211866.234, 212111.125, 211924.156, 210058.547, 209858.406, 209035.672, 209250.938, 207661.906, 208459.766, 207409.375, 206427.391, 207193.016, 206553.547, 206002.391, 205693.906, 204968.641, 204512.906, 203856.891, 204079.859, 203139.984, 203497.656, 202974.547, 201094.797, 201512.344, 201526.781, 200857.203, 201147.844, 201057.812, 200641.047, 200160.516, 199249.547, 199706.047, 199020.828, 198835.328, 198324.328, 198059.359, 198118.125, 197612.609, 197143.734, 197657.078, 196413.406, 197032.328, 196732.297, 195622.719, 195425.047, 194912.406, 194950.188, 195027.266, 194513.391, 194348.891, 193642.594, 194036.844, 193026.078, 192344.688, 193412.641, 193177.812, 193085.359, 191632.531, 192395.734, 192461.172, 191238.312, 192440.047, 192154.891], 'val_loss': [2964.148, 2308.523, 2208.176, 2178.009, 2165.721, 2101.403, 2108.898, 2044.04, 2025.59, 2014.872, 2040.309, 1988.505, 2007.44, 2009.914, 1978.983, 1967.241, 1937.871, 1912.278, 1900.941, 1907.816, 1878.199, 1872.85, 1855.068, 1836.907, 1839.268, 1833.103, 1830.91, 1830.496, 1816.969, 1804.045, 1799.901, 1789.454, 1796.757, 1798.627, 1792.86, 1789.289, 1781.639, 1774.369, 1760.811, 1771.404, 1757.295, 1775.659, 1771.418, 1768.039, 1764.33, 1766.078, 1756.71, 1757.195, 1740.499, 1752.505, 1758.009, 1775.793, 1749.95, 1745.486, 1742.639, 1739.329, 1730.755, 1739.621, 1725.679, 1730.145, 1739.987, 1722.221, 1740.074, 1737.208, 1736.297, 1713.531, 1719.779, 1710.266, 1730.14, 1709.731, 1714.147, 1709.542, 1717.875, 1705.7, 1725.752, 1719.892, 1703.303, 1695.765, 1717.45, 1696.304, 1713.264, 1703.61, 1718.884, 1704.497, 1694.355, 1707.571, 1707.666, 1694.223, 1676.597, 1693.33, 1692.003, 1675.073, 1692.992, 1697.222, 1688.824, 1709.513, 1687.254, 1701.481, 1674.608, 1692.134]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:100 kernel_size:10 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:rmsprop lr:0.1226634369972047 alpha:0.8280701434864465 weight_decay:3.627492174333187e-05 batch_size:26 epochs:100	100	1000	True	131122.78125		878610	11	-1	225.88530921936035	{'train_loss': [1228764.5, 1141551.625, 1382121.75, 1402600.625, 1279290.375, 1326031.625, 1333689.5, 1351694.375, 1245543.25, 1188424.625, 1150658.625, 1131287.75, 1111229.25, 1126447.625, 1130784.625, 1142457.75, 1154849.625, 1130484.125, 1124657.125, 1164406.875, 1126022.125, 1125677.875, 1155902.5, 1130583.5, 1139034.375, 1151038.625, 1140258.625, 1141546.0, 1148484.625, 1155955.75, 1150468.5, 1139699.875, 1148362.125, 1124568.125, 1161598.5, 1154990.0, 1144448.25, 1142613.875, 1171215.5, 1154471.75, 1130347.25, 1123768.875, 1157773.0, 1158696.25, 1120980.75, 1149195.875, 1159607.5, 1131971.375, 1152411.0, 1141591.625, 1146657.125, 1140411.5, 1129059.5, 1155391.125, 1164913.375, 1136383.5, 1151518.625, 1150534.75, 1127964.625, 1167129.375, 1131477.625, 1164821.625, 1149114.5, 1142113.125, 1126935.75, 1145960.25, 1131048.0, 1145418.875, 1182835.125, 1155573.125, 1137720.875, 1155484.25, 1139290.75, 1149350.625, 1120000.125, 1141307.25, 1152515.375, 1157201.5, 1160794.25, 1125582.0, 1172491.25, 1138957.625, 1143884.25, 1144373.875, 1150501.75, 1152883.5, 1151654.5, 1158072.375, 1142735.125, 1129180.375, 1203564.875, 1154817.625, 1163384.0, 1142851.5, 1165715.0, 1119983.375, 1183277.75, 1159254.5, 1156358.75, 1138262.625], 'val_loss': [8888.347, 10892.34, 8888.17, 8888.336, 12565.735, 10704.85, 8939.08, 8888.349, 9036.235, 8888.349, 9569.826, 8888.349, 13445.867, 8888.349, 8888.349, 8888.349, 9216.37, 8888.349, 8888.323, 8888.347, 8888.341, 8888.349, 8947.448, 8861.871, 9775.51, 8888.349, 8888.349, 8888.349, 8888.349, 9925.962, 8882.012, 8888.349, 8888.349, 10721.638, 8888.349, 11989.866, 13045.671, 8894.416, 8888.349, 8888.349, 12198.991, 8888.349, 14669.623, 8888.349, 10610.971, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 11302.645, 8888.349, 8888.349, 10547.709, 10521.33, 11595.318, 8888.349, 8888.349, 8888.204, 8888.349, 8888.349, 8888.349, 8888.349, 8884.264, 12572.204, 8888.349, 9949.402, 8888.349, 8888.349, 10639.87, 11119.976, 9438.632, 8888.349, 8888.349, 11020.658, 8888.349, 10543.077, 8888.348, 12771.926, 8888.349, 8888.349, 8887.681, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 10286.405, 8888.349, 8888.349, 11463.756, 8888.349, 8888.349, 9258.911, 9726.094, 8888.349, 8888.349, 8888.349, 8888.349]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:26 epochs:100	100	1000	True	23588.76953		398072	10	-1	224.53621244430542	{'train_loss': [484824.312, 321774.312, 284032.594, 270229.906, 262377.812, 256899.609, 252767.0, 249090.312, 246466.25, 244349.109, 242231.312, 239426.625, 237562.078, 235958.156, 234147.625, 232655.109, 231101.172, 229725.109, 228599.719, 227506.359, 226916.828, 225310.344, 225476.656, 224048.766, 222904.109, 221633.75, 221791.0, 221265.875, 219442.969, 219968.828, 219075.203, 217919.844, 217770.547, 216169.609, 215785.0, 215063.016, 215102.719, 214001.062, 213827.156, 213819.25, 213005.109, 212467.203, 212412.078, 211237.828, 210807.156, 210496.531, 209378.625, 210447.422, 209241.969, 208207.328, 208223.219, 208509.344, 207191.766, 207377.875, 206898.812, 206717.453, 206641.906, 206059.969, 205348.812, 205243.422, 205630.312, 203919.047, 203837.562, 203962.672, 203193.359, 203252.844, 203103.062, 203041.781, 201927.109, 202436.562, 201488.328, 201306.734, 201425.141, 201061.734, 201077.312, 200238.969, 199360.156, 200345.5, 199318.312, 200048.547, 199775.906, 199897.125, 199523.609, 198668.375, 199208.5, 197575.516, 198354.188, 197710.234, 197596.281, 197147.219, 196302.141, 196820.812, 196454.375, 196194.734, 196023.656, 196343.234, 196173.062, 195598.328, 195376.734, 194951.766], 'val_loss': [2873.903, 2435.289, 2263.279, 2180.073, 2176.285, 2149.908, 2082.005, 2034.092, 2024.961, 1993.114, 1968.134, 1950.932, 1930.342, 1917.696, 1903.951, 1905.024, 1904.762, 1883.434, 1880.244, 1905.23, 1856.39, 1884.318, 1879.719, 1866.874, 1852.543, 1853.908, 1871.393, 1863.01, 1839.161, 1848.32, 1829.583, 1845.606, 1830.266, 1821.628, 1816.035, 1806.848, 1815.425, 1806.439, 1798.787, 1811.775, 1811.113, 1838.004, 1788.086, 1807.766, 1804.205, 1774.716, 1784.126, 1760.89, 1787.448, 1791.281, 1752.416, 1779.302, 1763.202, 1743.984, 1764.416, 1735.941, 1736.028, 1742.647, 1758.709, 1751.18, 1751.272, 1746.834, 1733.777, 1734.168, 1744.597, 1715.043, 1748.859, 1748.423, 1744.11, 1738.803, 1752.954, 1738.58, 1729.829, 1748.862, 1727.839, 1724.456, 1727.583, 1730.189, 1718.5, 1719.138, 1726.849, 1713.667, 1744.714, 1703.908, 1729.073, 1736.794, 1704.242, 1709.287, 1708.136, 1713.811, 1693.541, 1716.775, 1694.438, 1699.424, 1693.054, 1711.46, 1687.757, 1695.153, 1704.402, 1694.637]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:114 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:26 epochs:100	100	1000	True	25015.38086		758072	10	-1	223.27951455116272	{'train_loss': [930949.562, 439824.406, 317776.781, 286023.5, 273499.094, 267569.938, 263554.281, 259446.859, 257368.078, 255110.688, 252864.859, 250951.438, 249011.391, 248135.281, 245812.922, 244441.312, 243800.547, 242447.312, 242551.453, 241338.094, 240310.969, 239016.016, 238336.938, 237092.969, 236424.453, 236092.891, 235454.812, 235194.781, 233885.812, 233737.859, 232300.094, 233287.594, 231613.188, 231213.672, 229750.734, 229141.172, 229407.719, 229221.953, 228078.172, 227040.656, 228000.281, 227167.562, 226396.031, 225580.516, 225331.422, 224629.312, 224105.156, 223330.375, 222717.109, 224238.422, 222260.344, 221079.953, 221727.25, 221315.0, 221255.531, 220887.234, 219784.234, 220356.75, 219647.891, 220379.172, 219086.141, 219125.406, 217423.266, 218687.594, 217771.953, 216987.672, 216362.531, 215783.484, 215536.297, 215134.812, 215549.203, 214599.562, 214271.922, 213633.109, 214181.922, 214390.719, 213938.359, 211950.062, 212959.859, 212554.453, 212267.375, 210786.703, 211968.406, 212833.109, 210874.578, 210124.609, 210517.828, 209123.578, 211087.406, 208758.125, 209368.109, 208965.391, 208645.062, 208860.125, 208159.812, 207856.703, 208126.625, 207032.109, 206522.047, 207244.781], 'val_loss': [5478.191, 2654.288, 2365.427, 2260.136, 2234.36, 2150.958, 2144.271, 2121.43, 2104.372, 2088.216, 2072.142, 2050.509, 2040.501, 2027.534, 2065.357, 2051.464, 2019.877, 2007.246, 2022.377, 2055.1, 1991.516, 1986.933, 1987.991, 2041.128, 1992.055, 1974.057, 2013.54, 1979.508, 1968.909, 1996.023, 1981.446, 2017.339, 1952.798, 1947.527, 1964.067, 1913.833, 1921.534, 1914.925, 1920.845, 1931.781, 1933.385, 1896.441, 1902.196, 1912.946, 1912.372, 1932.896, 1874.487, 1897.979, 1890.11, 1881.776, 1878.973, 1853.026, 1860.398, 1871.146, 1845.97, 1845.852, 1859.566, 1874.571, 1866.614, 1828.278, 1870.95, 1850.626, 1835.149, 1844.496, 1807.018, 1850.564, 1808.937, 1833.94, 1846.176, 1831.707, 1804.892, 1818.173, 1823.942, 1821.155, 1823.61, 1807.077, 1805.505, 1808.584, 1796.989, 1804.459, 1801.11, 1816.239, 1813.124, 1797.188, 1796.51, 1807.76, 1779.79, 1797.244, 1784.438, 1778.513, 1786.237, 1778.394, 1806.998, 1777.136, 1773.48, 1776.006, 1767.75, 1780.609, 1766.317, 1760.972]}	100	100	True
