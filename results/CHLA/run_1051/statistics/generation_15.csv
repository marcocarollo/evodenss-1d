id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	24992.22656		619010	12	-1	213.2227280139923	{'train_loss': [604747.188, 400666.688, 307421.969, 272502.656, 260935.625, 254981.234, 249944.328, 246425.578, 242935.125, 240824.359, 238109.953, 237257.188, 235736.516, 234297.672, 232249.625, 231865.172, 230754.328, 229065.641, 228669.047, 227777.812, 226220.078, 225741.312, 225076.953, 225153.344, 224746.656, 223607.234, 223670.5, 222432.625, 221268.953, 221419.141, 220765.344, 220910.078, 219466.922, 220096.844, 218677.547, 217331.938, 217959.812, 218125.547, 217126.688, 216193.5, 215599.484, 215279.016, 215349.562, 214477.094, 213658.859, 213795.297, 214010.828, 212500.531, 212449.984, 212236.609, 211354.266, 211465.766, 210525.141, 210923.141, 210000.812, 209798.578, 209052.031, 208458.547, 209121.938, 208641.672, 207134.844, 207636.0, 207202.734, 206796.797, 207607.031, 207109.844, 206260.734, 205871.359, 205408.938, 204931.906, 206421.031, 204803.109, 205626.922, 205300.281, 204635.922, 204258.266, 204114.375, 203789.406, 203210.391, 203375.656, 203374.234, 202152.172, 201797.797, 202148.203, 201991.328, 201633.078, 201301.062, 201340.906, 201490.094, 201854.156, 200125.438, 200126.422, 199488.125, 200344.781, 199953.359, 199902.391, 198929.609, 199015.578, 200224.609, 198832.344], 'val_loss': [4024.521, 3152.079, 2863.911, 2713.947, 2641.285, 2626.089, 2575.22, 2530.623, 2558.987, 2525.18, 2489.848, 2455.903, 2446.43, 2442.964, 2436.175, 2414.151, 2400.473, 2390.534, 2395.6, 2385.928, 2379.091, 2372.793, 2369.29, 2345.432, 2353.502, 2353.197, 2347.866, 2366.251, 2339.138, 2344.483, 2344.258, 2346.481, 2336.371, 2330.082, 2334.216, 2320.17, 2304.848, 2306.084, 2313.811, 2351.75, 2303.315, 2316.046, 2299.83, 2293.076, 2297.832, 2292.229, 2312.682, 2275.551, 2270.793, 2273.041, 2289.01, 2302.196, 2276.876, 2270.981, 2266.82, 2258.092, 2279.668, 2243.602, 2243.603, 2249.497, 2250.542, 2236.616, 2230.311, 2241.543, 2244.321, 2214.343, 2217.74, 2218.544, 2214.914, 2218.465, 2218.948, 2212.621, 2220.905, 2213.112, 2236.792, 2220.414, 2235.076, 2211.821, 2206.649, 2200.194, 2209.47, 2202.915, 2203.904, 2193.42, 2200.058, 2209.315, 2192.659, 2187.432, 2205.863, 2202.851, 2184.944, 2185.77, 2184.621, 2181.986, 2194.239, 2184.517, 2189.21, 2221.201, 2200.165, 2179.616]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:48 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:rmsprop lr:0.14697202586694097 alpha:0.8753856660627009 weight_decay:1.4318824515307362e-05 batch_size:32 epochs:100	100	1000	True	132649.3125		555564	11	-1	200.7701051235199	{'train_loss': [1091578.375, 1271485.875, 1062567.125, 1600729.125, 1140432.25, 1314437.5, 1162069.75, 1118466.75, 1095196.375, 1110482.375, 1091247.75, 1129423.625, 1108325.0, 1106686.625, 1115678.375, 1107198.0, 1118149.0, 1116324.75, 1099782.375, 1130083.0, 1082911.875, 1110027.125, 1097441.25, 1107342.875, 1111543.0, 1102945.5, 1121786.75, 1089582.375, 1123582.875, 1098864.375, 1125171.75, 1086845.75, 1120270.5, 1119586.5, 1096325.25, 1120558.75, 1112809.0, 1108980.375, 1114195.75, 1118781.125, 1106522.375, 1102016.375, 1106457.0, 1102120.0, 1107216.375, 1118469.5, 1100870.625, 1115705.0, 1111155.625, 1092383.875, 1121503.875, 1109796.125, 1103355.125, 1112668.125, 1104363.75, 1100826.75, 1099874.875, 1093158.875, 1100263.5, 1103197.625, 1100372.375, 1124761.875, 1111098.0, 1099184.25, 1129947.5, 1109397.875, 1107433.875, 1099545.875, 1112677.5, 1105390.25, 1103882.875, 1097147.625, 1093365.875, 1132287.375, 1132094.25, 1098723.125, 1100962.875, 1126416.5, 1085924.125, 1127887.375, 1097040.875, 1123155.875, 1113961.375, 1107133.375, 1137897.25, 1125817.5, 1108525.625, 1122396.75, 1100225.25, 1112909.625, 1092338.625, 1111756.375, 1119852.0, 1128763.875, 1109532.25, 1089186.5, 1174672.25, 1099507.125, 1123181.5, 1091819.625], 'val_loss': [11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 16118.727, 11110.434, 11110.434, 11110.434, 11110.434, 11140.928, 11110.434, 11110.434, 11110.434, 11110.434, 13983.365, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.429, 13588.336, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.359, 13615.444, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11783.871, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11638.156, 11110.434, 11186.357, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11531.523, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11827.263, 11110.434, 11110.434, 11110.434, 11110.434, 13093.104, 13823.104, 11110.434, 11110.434, 11322.731, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11233.381]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:25 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:118 kernel_size:6 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:28 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:120 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:27 epochs:100	100	1000	True	131188.125		20140961	13	-1	281.1640377044678	{'train_loss': [1573789.0, 1074907.875, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25], 'val_loss': [9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229, 9523.229]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	25039.00977		662335	13	-1	215.7838523387909	{'train_loss': [651211.625, 413563.906, 319147.25, 276195.062, 263351.719, 256892.5, 251700.344, 248035.922, 245476.109, 242521.094, 240660.594, 238691.328, 237517.062, 235668.938, 235042.922, 234177.375, 232048.438, 231549.234, 230033.031, 229541.953, 227556.719, 227122.938, 226147.734, 225424.516, 223955.812, 223612.031, 222757.406, 222007.781, 220036.891, 219132.531, 219317.984, 218974.797, 218026.844, 216857.859, 216995.656, 216279.953, 215438.219, 214968.328, 214375.797, 214245.438, 213913.281, 212902.156, 212975.688, 212860.453, 210741.984, 210780.344, 210992.578, 210000.938, 210554.203, 209433.5, 209225.719, 209365.078, 207840.547, 207801.859, 207781.141, 207513.438, 206950.047, 206768.453, 207367.109, 206333.547, 206434.234, 206056.359, 205003.062, 205565.641, 204451.078, 204168.031, 204223.0, 203453.375, 203998.344, 202996.797, 202557.453, 201547.781, 202783.297, 203143.062, 202330.031, 202394.969, 200656.984, 201131.375, 202344.125, 202084.812, 200447.875, 200142.359, 200339.812, 200109.859, 200642.969, 199710.75, 200441.453, 200607.516, 198830.219, 198979.625, 197966.922, 198906.594, 197183.547, 197673.953, 197938.125, 197022.062, 197190.328, 196566.797, 196874.422, 196982.625], 'val_loss': [3816.026, 3583.818, 3035.186, 2762.924, 2711.838, 2695.085, 2689.572, 2591.954, 2639.74, 2588.645, 2558.701, 2549.123, 2568.537, 2562.343, 2512.252, 2488.679, 2538.056, 2463.59, 2458.156, 2490.412, 2462.283, 2432.569, 2399.617, 2436.356, 2416.84, 2403.157, 2390.57, 2370.258, 2350.96, 2382.194, 2384.408, 2356.581, 2358.018, 2358.171, 2331.678, 2345.017, 2304.61, 2338.562, 2341.583, 2315.778, 2264.006, 2291.661, 2299.921, 2316.791, 2275.559, 2326.849, 2274.721, 2277.616, 2262.899, 2246.544, 2271.555, 2243.844, 2295.078, 2242.408, 2279.536, 2232.748, 2242.066, 2234.588, 2219.383, 2222.647, 2236.916, 2252.647, 2218.075, 2214.626, 2204.961, 2195.143, 2231.539, 2192.133, 2203.58, 2177.894, 2189.012, 2227.616, 2198.916, 2181.964, 2211.964, 2175.997, 2192.939, 2202.919, 2200.968, 2194.358, 2206.141, 2180.945, 2201.909, 2175.373, 2190.581, 2193.86, 2194.59, 2198.081, 2174.538, 2201.428, 2202.395, 2197.608, 2169.094, 2182.364, 2190.898, 2175.446, 2181.762, 2187.555, 2168.236, 2185.963]}	100	100	True
