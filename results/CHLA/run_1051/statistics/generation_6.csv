id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	25197.36719		639490	13	-1	209.86263060569763	{'train_loss': [655625.125, 409333.938, 314217.375, 282069.406, 267960.156, 260892.375, 255259.75, 252094.531, 249207.938, 246497.422, 244851.625, 242323.516, 241057.453, 239780.969, 238951.281, 237821.938, 237359.438, 235932.75, 235529.406, 234373.234, 233190.969, 233290.078, 231876.703, 231599.734, 231002.562, 230072.203, 230004.75, 227908.812, 228575.109, 227116.312, 227077.844, 226057.984, 225797.562, 225129.609, 223789.969, 223557.0, 222555.328, 222314.266, 221319.125, 221112.531, 220565.438, 220273.766, 219613.781, 218963.172, 219048.125, 218489.828, 217791.719, 217559.641, 217504.344, 216373.844, 216244.828, 216413.109, 216240.0, 215561.766, 215487.812, 214836.703, 214828.656, 213635.141, 213305.453, 213422.281, 213528.734, 213744.453, 212558.062, 211971.547, 212621.625, 211400.828, 211698.75, 212056.469, 210796.422, 210621.234, 211125.406, 211055.656, 210999.078, 210073.109, 209486.531, 209041.062, 209087.156, 209323.172, 208727.453, 208757.734, 208805.906, 207766.609, 208098.953, 208232.344, 207894.953, 206913.875, 207181.625, 207192.156, 206629.297, 206784.75, 206556.688, 207503.0, 206629.016, 206336.328, 205720.5, 205479.281, 204362.203, 204445.953, 204803.219, 204570.719], 'val_loss': [4089.428, 3270.58, 2970.661, 2779.782, 2790.929, 2629.445, 2599.258, 2570.558, 2552.478, 2526.485, 2528.836, 2492.798, 2537.154, 2495.125, 2476.877, 2457.989, 2485.434, 2453.446, 2469.508, 2432.966, 2434.712, 2459.598, 2449.785, 2437.268, 2402.712, 2419.119, 2412.919, 2413.729, 2402.719, 2424.883, 2398.853, 2376.419, 2366.232, 2369.901, 2366.098, 2354.037, 2351.668, 2348.189, 2337.917, 2350.039, 2320.181, 2331.342, 2328.056, 2327.797, 2318.292, 2310.852, 2341.218, 2304.38, 2330.967, 2335.599, 2316.446, 2299.522, 2292.89, 2296.41, 2311.381, 2298.519, 2299.486, 2301.839, 2308.248, 2276.45, 2289.035, 2310.337, 2285.836, 2282.195, 2288.147, 2279.729, 2286.953, 2273.123, 2289.744, 2325.622, 2270.172, 2285.853, 2245.117, 2288.034, 2256.012, 2279.377, 2248.068, 2286.782, 2248.093, 2248.85, 2276.303, 2261.005, 2276.408, 2245.098, 2276.995, 2225.828, 2254.065, 2250.701, 2255.83, 2248.592, 2259.679, 2232.427, 2225.99, 2212.409, 2209.831, 2266.734, 2275.342, 2233.869, 2240.801, 2238.925]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:47 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.2288249569603235 beta1:0.9892999049133242 beta2:0.8416730008734703 weight_decay:0.0007899505944749209 batch_size:32 epochs:100	100	1000	True	131122.51562		894103	14	-1	227.88920497894287	{'train_loss': [1137847.5, 1072042.125, 1062567.125, 30582262.0, 286111232.0, 4775168.5, 1062567.125, 2444994.25, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 2549063.25, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 2266919.5, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1272640.25, 1955810.25, 3674775.5, 2431407.25, 1062567.125, 1062567.125, 1648551.125, 1062567.125, 1062567.125, 1062567.125, 1677747.75, 6171244.5, 3224857.75, 1716697.0, 1062567.125, 1062567.125, 1795287.875, 1773511.25, 3058447.25, 1246991.625, 1872975.375, 1480144.375, 1100745.75, 1169596.625, 1790845.0, 2216909.75, 3352635.25, 2273367.5, 1311923.75, 1344221.875, 1454761.75, 2518886.0, 1563788.25, 2255398.25, 1212991.75, 1683438.375, 1810686.375, 1934630.5, 1292205.375, 1399024.125, 1618895.875, 1166832.125, 1138905.125, 1291038.125, 1718703.875, 4550418.5, 2249824.0, 1329603.875, 1496901.5, 1133592.0, 1606520.875, 1360425.0, 1375210.125, 1726512.0, 1438313.125], 'val_loss': [11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 209961858629632.0, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:116 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:77 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:116 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	26751.37109		1397631	14	-1	237.35499358177185	{'train_loss': [1324702.375, 1063009.75, 1071989.0, 1062682.625, 1067956.375, 1111216.875, 1070419.25, 991314.562, 490698.812, 332454.781, 277574.844, 265160.812, 258590.031, 253625.047, 249601.797, 247152.922, 244723.625, 243008.375, 240364.688, 239088.953, 237125.672, 236297.531, 234730.203, 233586.547, 232028.516, 230917.766, 230255.359, 228883.5, 228132.344, 226731.75, 226484.562, 225751.406, 224052.516, 223799.109, 223543.375, 222802.797, 222199.078, 221368.891, 220344.875, 218912.891, 219217.531, 218554.75, 217499.844, 216524.422, 215932.422, 215781.219, 214995.578, 214085.0, 213116.672, 213604.797, 212891.938, 212658.906, 211470.125, 210386.609, 210232.734, 210495.031, 209806.625, 208784.141, 207596.609, 206992.766, 206786.859, 206158.594, 206852.141, 205954.969, 205609.547, 205697.406, 205278.625, 203826.406, 204671.531, 203737.781, 203994.922, 203471.953, 203672.375, 202265.234, 201485.516, 201700.656, 201350.719, 201126.656, 201063.172, 200181.484, 199988.703, 200102.516, 200499.328, 198356.406, 197057.922, 198799.719, 197381.562, 198812.234, 197402.047, 197463.656, 197740.266, 196394.359, 196454.641, 195538.266, 196628.766, 195729.312, 195757.422, 196062.016, 193648.016, 194528.453], 'val_loss': [11110.351, 11110.434, 11110.433, 11108.494, 11110.38, 11108.646, 10987.408, 4628.341, 3615.438, 2891.788, 2898.132, 2770.807, 2759.387, 2702.103, 2654.561, 2653.92, 2641.48, 2576.741, 2561.743, 2576.114, 2585.478, 2569.939, 2530.089, 2540.381, 2516.139, 2527.061, 2494.192, 2556.185, 2523.82, 2485.1, 2530.758, 2462.029, 2511.343, 2474.046, 2427.067, 2492.677, 2503.009, 2428.819, 2452.477, 2460.089, 2456.463, 2422.522, 2415.75, 2372.201, 2401.402, 2434.507, 2408.699, 2390.748, 2402.546, 2409.115, 2375.541, 2366.233, 2391.941, 2384.327, 2343.553, 2338.753, 2352.246, 2342.294, 2338.075, 2330.129, 2345.841, 2350.049, 2348.674, 2335.333, 2331.047, 2305.861, 2327.189, 2368.646, 2303.922, 2318.573, 2306.927, 2310.909, 2312.081, 2339.805, 2313.884, 2315.716, 2332.731, 2335.246, 2304.632, 2322.182, 2316.623, 2337.206, 2300.221, 2328.255, 2323.097, 2301.811, 2307.316, 2306.623, 2332.481, 2294.809, 2308.336, 2330.63, 2318.845, 2358.074, 2328.378, 2318.45, 2337.695, 2320.336, 2303.123, 2324.379]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:66 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:100 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:123 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:rmsprop lr:0.14805919671737366 alpha:0.8183774955350832 weight_decay:7.298173549852566e-05 batch_size:32 epochs:100	100	1000	True	59374.04297		504237	13	-1	207.66090321540833	{'train_loss': [820613.375, 738844.625, 752919.375, 723803.188, 715219.188, 701934.875, 682024.0, 677040.938, 665733.75, 685081.375, 669921.75, 676106.875, 672556.625, 675546.562, 674351.875, 666221.25, 687458.75, 675245.938, 668352.938, 677871.188, 677748.938, 676317.938, 670227.938, 670236.125, 671072.75, 668195.5, 675999.812, 667294.5, 681545.438, 668705.062, 681923.062, 661380.375, 676937.0, 674849.312, 672062.062, 680071.562, 672263.312, 690971.25, 678428.938, 672111.938, 683801.562, 674414.75, 688084.0, 667672.062, 675132.125, 680044.062, 687964.625, 668650.75, 679828.875, 668481.812, 672500.25, 676843.062, 676697.938, 677705.375, 677977.625, 675290.812, 678274.312, 673277.0, 672624.688, 675267.625, 677472.75, 665771.75, 679860.625, 677057.75, 685297.562, 685357.25, 698637.125, 672020.938, 667837.375, 661809.062, 679067.125, 672153.938, 677014.062, 682779.625, 669180.438, 673251.938, 678666.938, 673659.188, 685405.0, 674859.688, 684378.688, 670469.938, 693017.688, 677863.0, 673432.812, 675382.938, 680239.562, 670500.938, 673591.375, 681864.625, 679186.562, 678146.062, 679998.688, 677289.812, 677539.438, 680961.188, 666627.5, 683597.812, 675374.25, 678177.5], 'val_loss': [5272.931, 6716.618, 4945.794, 5773.262, 7995.739, 5567.789, 4820.561, 4999.121, 4893.221, 5222.372, 4612.882, 5152.043, 5355.468, 5161.346, 5285.756, 4454.483, 5097.991, 4713.9, 4873.333, 4705.883, 5254.513, 5260.412, 4512.354, 4542.63, 5122.608, 4940.813, 7507.298, 4588.688, 4817.805, 5464.195, 4554.996, 4669.709, 5508.209, 5591.341, 4457.44, 4771.311, 4656.131, 5435.46, 6106.414, 4685.644, 5017.666, 4951.539, 4919.702, 4654.582, 4949.488, 5306.093, 4584.003, 4789.955, 5821.349, 4524.444, 4498.677, 4977.479, 5101.442, 4976.912, 5647.833, 4875.249, 4673.304, 4769.679, 5104.16, 4776.948, 4454.756, 5601.431, 4649.031, 4957.4, 5399.833, 5227.189, 4783.217, 5559.942, 4615.3, 5070.76, 5285.84, 5344.372, 4474.732, 5677.646, 5303.596, 4687.561, 5080.64, 4737.672, 5542.485, 4574.809, 4495.563, 5409.549, 5327.424, 5044.554, 4756.038, 5045.731, 4833.783, 5258.331, 5071.352, 5248.145, 4796.531, 5099.25, 5933.275, 4984.425, 4418.966, 4800.366, 5129.21, 5542.532, 4625.691, 5046.73]}	100	100	True
