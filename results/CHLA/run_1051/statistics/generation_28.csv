id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	2000	True	23947.94336		416732	11	-1	236.84431672096252	{'train_loss': [541530.75, 338477.125, 283396.156, 266629.094, 260407.484, 255923.641, 252077.969, 249272.484, 246773.281, 244882.469, 242682.719, 240166.719, 238114.25, 236496.797, 234420.141, 231809.453, 231431.906, 229795.719, 227611.719, 226620.281, 225617.109, 224573.547, 223512.906, 222230.672, 221171.969, 220681.844, 219475.266, 219393.281, 218083.828, 217851.328, 217095.078, 216035.906, 215144.0, 215095.391, 214474.812, 214140.672, 212193.75, 212304.984, 211900.047, 211318.578, 210744.172, 210613.656, 209839.406, 209480.734, 208436.25, 209045.688, 207644.047, 206899.922, 206754.234, 207770.203, 205736.422, 205231.875, 205884.672, 205324.703, 204620.562, 203439.219, 203127.938, 203870.719, 203690.719, 202409.391, 202059.562, 201993.828, 201948.0, 200479.969, 201258.547, 200687.0, 200329.391, 199350.938, 198892.984, 199506.125, 198208.297, 198242.969, 198625.094, 197652.406, 197266.438, 197238.125, 196343.984, 197212.016, 197344.328, 195922.422, 196014.953, 195883.641, 195282.875, 195041.094, 195125.938, 194374.188, 194701.859, 194842.609, 194046.844, 194346.75, 193832.078, 193493.594, 193134.578, 193567.109, 193080.031, 192683.797, 191960.766, 192508.406, 191322.188, 191526.641], 'val_loss': [3131.266, 2399.779, 2239.708, 2172.396, 2120.919, 2073.774, 2064.084, 2017.137, 2021.527, 2001.224, 2001.848, 1968.184, 1955.866, 1947.733, 1934.511, 1931.022, 1912.941, 1875.46, 1880.753, 1893.305, 1879.242, 1841.882, 1867.846, 1864.577, 1844.726, 1847.65, 1825.682, 1828.563, 1824.18, 1822.197, 1809.618, 1837.804, 1814.561, 1830.485, 1813.721, 1810.461, 1821.822, 1778.453, 1792.686, 1785.435, 1762.044, 1773.401, 1771.16, 1769.047, 1762.997, 1757.328, 1768.1, 1766.883, 1759.742, 1749.868, 1751.413, 1743.395, 1745.086, 1758.494, 1736.145, 1744.818, 1732.729, 1748.617, 1740.571, 1762.755, 1721.205, 1749.077, 1734.611, 1725.548, 1726.525, 1723.631, 1721.14, 1722.509, 1725.18, 1736.557, 1717.662, 1720.891, 1702.05, 1712.584, 1712.11, 1709.293, 1702.185, 1698.425, 1697.313, 1709.29, 1702.04, 1738.181, 1699.649, 1684.114, 1709.64, 1704.589, 1709.856, 1710.502, 1698.448, 1693.168, 1718.623, 1680.088, 1702.034, 1702.317, 1704.985, 1677.877, 1699.221, 1680.645, 1729.349, 1696.905]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:84 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:26 epochs:100	100	1000	True	24135.33203		497025	10	-1	225.2565884590149	{'train_loss': [691189.375, 369948.0, 293534.906, 268384.375, 256762.516, 249553.406, 244648.234, 240776.062, 238151.719, 236384.422, 234238.0, 232054.125, 230224.969, 229452.812, 228641.828, 226792.312, 224747.234, 224777.594, 223840.312, 222660.281, 222159.453, 221319.406, 221476.781, 220502.891, 219537.109, 218575.969, 217510.062, 218378.234, 217487.5, 216996.703, 215895.172, 216010.953, 215403.953, 214936.125, 213759.516, 213446.953, 212594.109, 211639.344, 211885.859, 211606.953, 210509.078, 210806.5, 210875.047, 209928.984, 208739.766, 208478.812, 208061.094, 208687.875, 207843.875, 208345.188, 207439.234, 206693.859, 206412.516, 205797.625, 205747.844, 205950.031, 204936.562, 204840.969, 205069.688, 204608.312, 204071.609, 202748.812, 203831.172, 203498.469, 203604.078, 203128.312, 201541.328, 201890.969, 201270.391, 201111.703, 201039.531, 200239.312, 200070.531, 200224.844, 200193.375, 200433.078, 200057.766, 198606.109, 199114.328, 198926.75, 199154.109, 199074.312, 198847.875, 197785.688, 198647.031, 197415.656, 197927.625, 197638.641, 197528.312, 197520.625, 196150.297, 196397.766, 195776.172, 195927.922, 194130.078, 195809.422, 195427.281, 195568.891, 194622.625, 194069.578], 'val_loss': [3306.075, 2489.041, 2281.478, 2187.283, 2059.91, 2038.369, 2026.978, 2001.171, 1985.533, 2013.007, 1921.406, 1935.603, 1963.023, 1920.281, 1948.619, 1895.375, 1901.246, 1889.52, 1909.007, 1895.616, 1914.679, 1869.942, 1866.94, 1890.823, 1853.719, 1876.197, 1872.284, 1862.23, 1871.459, 1846.393, 1831.525, 1875.898, 1830.276, 1854.297, 1801.04, 1813.386, 1810.043, 1795.126, 1800.186, 1800.409, 1786.127, 1816.345, 1802.58, 1798.983, 1801.398, 1779.784, 1788.585, 1787.994, 1773.991, 1795.058, 1794.155, 1759.063, 1773.896, 1773.915, 1805.769, 1775.993, 1760.675, 1771.264, 1764.643, 1774.175, 1764.3, 1750.122, 1758.436, 1767.302, 1745.373, 1729.786, 1751.324, 1746.964, 1753.164, 1754.338, 1742.453, 1753.257, 1745.19, 1739.932, 1738.085, 1753.203, 1732.717, 1737.081, 1753.746, 1743.91, 1725.713, 1737.707, 1727.27, 1726.934, 1732.023, 1715.825, 1723.122, 1721.032, 1731.666, 1729.21, 1723.454, 1729.073, 1732.882, 1711.462, 1736.667, 1730.617, 1724.973, 1711.966, 1715.413, 1715.583]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:14 epochs:100	100	1000	True	26036.17969		722182	10	-1	331.45483803749084	{'train_loss': [622574.0, 311872.906, 281910.375, 271216.219, 264854.0, 260111.594, 256777.625, 253989.844, 251717.0, 249289.75, 247975.781, 247240.234, 245504.547, 244921.859, 243549.141, 242818.531, 241998.594, 240633.484, 240922.438, 238593.172, 237437.141, 237031.609, 236060.25, 235436.078, 234565.391, 233218.891, 233411.375, 231386.375, 231027.859, 230117.828, 229784.797, 229483.094, 228982.359, 229161.469, 227884.109, 227561.594, 225999.969, 225203.641, 225611.312, 225568.109, 224517.25, 223866.312, 223666.25, 223184.75, 223497.0, 222907.859, 221501.891, 222040.297, 221122.422, 220940.812, 220118.062, 220309.984, 220120.359, 219340.438, 219886.469, 219376.453, 217939.5, 218351.016, 217642.281, 217969.016, 217952.078, 217495.547, 216871.406, 216441.344, 215985.281, 216801.406, 215093.516, 216358.938, 215468.484, 214441.656, 214568.391, 215537.922, 213666.625, 213212.906, 214192.531, 213562.75, 213978.0, 213357.859, 212908.906, 213113.078, 212226.203, 212324.047, 212216.391, 210613.219, 210432.156, 211010.375, 210886.984, 211086.906, 211263.047, 211020.062, 209660.328, 209686.234, 209995.406, 210164.781, 209861.781, 208917.328, 208060.391, 207997.547, 208550.344, 208555.203], 'val_loss': [1497.622, 1300.954, 1228.345, 1181.341, 1174.493, 1151.528, 1154.833, 1159.848, 1149.111, 1128.226, 1130.601, 1131.484, 1119.674, 1143.233, 1123.417, 1110.366, 1124.256, 1106.177, 1112.174, 1096.799, 1100.835, 1099.815, 1090.44, 1100.643, 1088.085, 1084.932, 1089.513, 1088.619, 1082.763, 1077.231, 1075.74, 1087.375, 1087.471, 1062.652, 1077.346, 1073.119, 1068.644, 1067.683, 1073.412, 1053.017, 1056.278, 1074.479, 1060.553, 1068.054, 1060.451, 1045.189, 1063.654, 1061.802, 1056.089, 1043.512, 1042.398, 1040.706, 1052.714, 1059.751, 1051.814, 1054.592, 1029.836, 1050.202, 1049.632, 1052.956, 1029.681, 1038.183, 1037.341, 1038.014, 1036.777, 1042.8, 1047.72, 1048.603, 1031.12, 1034.512, 1050.341, 1026.472, 1033.067, 1037.393, 1017.476, 1039.93, 1024.691, 1031.314, 1041.97, 1030.935, 1034.619, 1018.136, 1022.346, 1015.46, 1024.786, 1009.198, 1007.761, 1016.329, 1009.01, 1021.413, 1015.025, 1030.954, 1023.368, 1006.413, 1015.729, 1024.438, 1017.084, 1013.438, 1019.962, 1018.826]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	2000	True	23509.82617		416732	11	-1	236.40568137168884	{'train_loss': [501202.469, 325791.188, 281389.688, 265919.375, 258759.375, 255032.094, 251365.234, 248920.328, 245638.281, 243428.109, 241348.828, 239801.969, 238805.75, 237503.094, 235304.906, 235196.094, 232387.266, 231785.438, 229490.688, 228319.062, 227147.328, 224965.0, 223885.359, 222770.656, 221644.109, 220395.469, 219793.984, 219190.781, 217257.031, 217214.484, 216108.625, 215528.375, 214628.734, 214881.578, 214612.953, 213468.844, 212233.891, 211866.234, 212111.125, 211924.156, 210058.547, 209858.406, 209035.672, 209250.938, 207661.906, 208459.766, 207409.375, 206427.391, 207193.016, 206553.547, 206002.391, 205693.906, 204968.641, 204512.906, 203856.891, 204079.859, 203139.984, 203497.656, 202974.547, 201094.797, 201512.344, 201526.781, 200857.203, 201147.844, 201057.812, 200641.047, 200160.516, 199249.547, 199706.047, 199020.828, 198835.328, 198324.328, 198059.359, 198118.125, 197612.609, 197143.734, 197657.078, 196413.406, 197032.328, 196732.297, 195622.719, 195425.047, 194912.406, 194950.188, 195027.266, 194513.391, 194348.891, 193642.594, 194036.844, 193026.078, 192344.688, 193412.641, 193177.812, 193085.359, 191632.531, 192395.734, 192461.172, 191238.312, 192440.047, 192154.891], 'val_loss': [2964.148, 2308.523, 2208.176, 2178.009, 2165.721, 2101.403, 2108.898, 2044.04, 2025.59, 2014.872, 2040.309, 1988.505, 2007.44, 2009.914, 1978.983, 1967.241, 1937.871, 1912.278, 1900.941, 1907.816, 1878.199, 1872.85, 1855.068, 1836.907, 1839.268, 1833.103, 1830.91, 1830.496, 1816.969, 1804.045, 1799.901, 1789.454, 1796.757, 1798.627, 1792.86, 1789.289, 1781.639, 1774.369, 1760.811, 1771.404, 1757.295, 1775.659, 1771.418, 1768.039, 1764.33, 1766.078, 1756.71, 1757.195, 1740.499, 1752.505, 1758.009, 1775.793, 1749.95, 1745.486, 1742.639, 1739.329, 1730.755, 1739.621, 1725.679, 1730.145, 1739.987, 1722.221, 1740.074, 1737.208, 1736.297, 1713.531, 1719.779, 1710.266, 1730.14, 1709.731, 1714.147, 1709.542, 1717.875, 1705.7, 1725.752, 1719.892, 1703.303, 1695.765, 1717.45, 1696.304, 1713.264, 1703.61, 1718.884, 1704.497, 1694.355, 1707.571, 1707.666, 1694.223, 1676.597, 1693.33, 1692.003, 1675.073, 1692.992, 1697.222, 1688.824, 1709.513, 1687.254, 1701.481, 1674.608, 1692.134]}	0	100	True
