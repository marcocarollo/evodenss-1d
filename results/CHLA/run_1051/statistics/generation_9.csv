id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	25751.66992		619010	12	-1	208.97770071029663	{'train_loss': [646036.438, 394215.031, 306439.062, 271474.219, 260480.281, 254163.891, 250015.438, 247699.266, 244762.953, 243093.031, 242566.297, 240230.016, 239134.156, 237908.188, 236519.734, 235722.859, 234411.125, 233422.406, 233587.609, 233125.25, 232209.359, 231840.234, 231144.016, 229997.062, 229935.453, 229026.344, 228260.797, 227756.0, 227396.578, 227328.281, 226366.719, 226098.109, 225892.391, 225600.203, 224741.0, 224724.297, 224361.625, 223841.25, 222377.328, 222531.828, 221094.703, 221106.234, 220566.281, 219248.906, 219775.641, 218710.297, 218406.031, 217505.75, 217033.625, 216394.281, 216482.75, 216943.703, 215556.109, 215003.641, 214952.328, 213744.969, 213493.0, 212594.891, 212817.438, 212136.5, 212260.812, 211959.812, 211586.391, 211331.328, 211072.203, 210114.141, 210305.906, 210545.391, 210172.672, 209659.578, 209395.453, 209487.375, 208681.828, 208331.125, 209233.938, 207031.781, 207575.938, 207315.328, 207605.578, 207965.703, 206012.156, 206717.438, 207398.484, 206062.359, 206065.5, 206345.047, 206469.578, 206014.875, 205505.0, 204858.797, 205977.406, 204346.141, 204165.812, 204183.953, 203568.094, 203732.234, 203113.25, 202966.641, 203195.75, 203365.234], 'val_loss': [4266.995, 3323.419, 3002.804, 2915.263, 2725.235, 2628.613, 2673.822, 2596.179, 2585.991, 2612.965, 2538.306, 2550.561, 2529.149, 2530.954, 2515.529, 2510.109, 2487.795, 2531.959, 2496.577, 2493.725, 2488.173, 2476.584, 2484.163, 2465.143, 2469.971, 2431.042, 2433.541, 2438.739, 2414.851, 2428.542, 2442.046, 2423.894, 2413.872, 2438.464, 2423.622, 2414.426, 2385.982, 2386.288, 2403.023, 2378.106, 2383.971, 2362.368, 2351.336, 2352.319, 2343.245, 2352.537, 2341.54, 2330.064, 2341.663, 2350.462, 2341.802, 2344.063, 2326.041, 2325.69, 2314.369, 2312.438, 2319.856, 2339.802, 2343.417, 2312.177, 2320.683, 2304.898, 2336.739, 2304.825, 2302.442, 2330.69, 2308.277, 2344.68, 2289.416, 2341.985, 2325.92, 2288.447, 2293.364, 2319.353, 2308.852, 2293.599, 2304.026, 2281.134, 2273.765, 2279.021, 2315.926, 2270.43, 2316.673, 2270.234, 2291.348, 2290.025, 2259.832, 2264.998, 2283.98, 2276.248, 2274.555, 2234.414, 2270.477, 2298.147, 2249.101, 2309.465, 2247.896, 2301.53, 2256.709, 2283.587]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:18 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:51 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:39 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:rmsprop lr:0.24432763718478306 alpha:0.8164712434232342 weight_decay:3.190829218217423e-05 batch_size:32 epochs:100	100	1000	True	215280.0		1275648	13	-1	246.13219261169434	{'train_loss': [1150958.0, 1755877.625, 3915703.25, 2874456.0, 3241294.5, 3471164.75, 1162115.75, 2917472.25, 2580983.5, 2128286.75, 2638095.75, 2139137.0, 2661333.75, 2157463.75, 3224767.0, 1846647.375, 2330266.0, 2230043.25, 2042215.25, 2360364.25, 2601202.5, 2176142.25, 2080136.75, 2285388.25, 2200370.0, 2343008.25, 2181336.0, 2156784.75, 2248895.0, 2481842.5, 1908259.25, 2894175.5, 2452228.25, 1832449.0, 2203690.75, 2478488.75, 2267001.25, 2212517.0, 2472451.5, 1848421.0, 2688003.5, 2324704.75, 2282857.0, 2169440.25, 2102566.0, 2678084.75, 2914413.0, 1394189.25, 2303480.0, 2414444.0, 2538392.5, 1909489.5, 2048510.75, 2546912.5, 2030724.5, 1887981.75, 2161386.5, 1940650.375, 2394886.25, 1963286.375, 1910931.25, 2337973.75, 2549584.0, 1777526.75, 3742340.0, 1507676.125, 3627361.25, 2671999.5, 2254637.5, 2277615.5, 2600742.75, 2648548.0, 1687917.5, 2570572.0, 2884287.25, 2026129.0, 2573061.5, 2005110.5, 2095556.25, 1647233.125, 1465052.0, 1325694.5, 1466783.875, 1344609.0, 1376414.75, 1364244.875, 1268471.0, 1471573.25, 1471053.875, 1412541.25, 1592226.875, 1428397.125, 1356614.625, 1586498.75, 1430244.0, 1414398.5, 1657617.0, 1397707.375, 1774983.125, 1359875.875], 'val_loss': [11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 15049.336, 428397.844, 14248.068, 56430.758, 11110.434, 23332.59, 11110.434, 51622.613, 61262.484, 214937.188, 54189.453, 181218.938, 26952.031, 11110.434, 352154.125, 13154.795, 227783.578, 11110.434, 119766.859, 388724.5, 28839.148, 145790.875, 77272.789, 139585.469, 37735.938, 69375.336, 211076.594, 123493.359, 58405.359, 599712.812, 491189.062, 14758.079, 228258.984, 275479.562, 26133.109, 455956.062, 31461.412, 142415.094, 11428.785, 122050.703, 17087.207, 28043.127, 34188.395, 19631.117, 86875.898, 11110.433, 41994.57, 14034.397, 42034.281, 25458.328, 250950.875, 23410.439, 289206.344, 30060.689, 146732.969, 223650.547, 71399.922, 160994.297, 232106.375, 232595.297, 50314.855, 323242.188, 95342.383, 108986.445, 70810.211, 11292.334, 65748.984, 14433.117, 21287.91, 68509.047, 51960.156, 223615.078, 13963.257, 14945.896, 12724.654, 16936.352, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 13686.058, 23680.492, 19612.799, 11110.434, 11110.434, 16030.796, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 18122.691]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	2000	True	24876.03906		619010	12	-1	207.87233424186707	{'train_loss': [636241.875, 405001.812, 306666.219, 270735.75, 258067.625, 250956.094, 247048.656, 243263.797, 240947.859, 239006.828, 237334.5, 234918.953, 234410.062, 232977.734, 231692.766, 229778.312, 229405.75, 228032.031, 226037.797, 225971.172, 224697.172, 223969.672, 222584.203, 222047.625, 220902.922, 220750.828, 219353.266, 218245.062, 218281.047, 217221.547, 216613.984, 216527.141, 215937.438, 214484.594, 213699.266, 213537.734, 213316.859, 211929.234, 212526.219, 211734.344, 211916.297, 210686.406, 211241.219, 210597.281, 210172.312, 209413.484, 209443.016, 208978.672, 209032.812, 208039.016, 207727.484, 207141.969, 206883.469, 207163.812, 206122.75, 206801.75, 205623.656, 205753.453, 204592.984, 205340.391, 204405.312, 203812.484, 203835.109, 204035.953, 203946.359, 203839.625, 202771.906, 202254.344, 202284.984, 202642.531, 201173.984, 200554.219, 201163.359, 201437.062, 201532.234, 200565.828, 199295.266, 199890.297, 198571.516, 200472.172, 199855.078, 198813.438, 198486.938, 198127.641, 198848.906, 198797.516, 198342.984, 197546.438, 198194.078, 196570.672, 196745.922, 196341.875, 195802.875, 196400.938, 195771.516, 196953.938, 195606.25, 196381.656, 195606.656, 194726.891], 'val_loss': [4370.647, 3374.07, 2833.447, 2813.476, 2682.862, 2589.664, 2566.591, 2600.5, 2546.072, 2541.939, 2497.559, 2474.26, 2483.701, 2461.237, 2469.587, 2515.504, 2440.49, 2489.743, 2456.848, 2431.44, 2386.666, 2362.983, 2394.466, 2382.789, 2355.654, 2361.148, 2365.178, 2354.252, 2345.098, 2343.439, 2371.432, 2360.339, 2322.174, 2347.194, 2344.613, 2328.869, 2308.321, 2331.152, 2305.755, 2298.643, 2329.139, 2273.618, 2307.133, 2301.43, 2320.414, 2292.841, 2291.406, 2308.682, 2274.087, 2291.838, 2313.881, 2298.627, 2275.746, 2285.334, 2286.193, 2265.11, 2309.668, 2299.232, 2276.625, 2269.548, 2272.117, 2258.876, 2239.307, 2264.117, 2263.516, 2250.619, 2259.577, 2224.795, 2241.468, 2247.547, 2276.691, 2223.68, 2283.541, 2228.462, 2250.689, 2209.382, 2246.729, 2239.437, 2213.944, 2195.93, 2190.92, 2180.682, 2198.0, 2225.602, 2207.742, 2192.586, 2213.263, 2204.687, 2198.19, 2206.157, 2215.299, 2184.421, 2195.868, 2160.584, 2170.978, 2205.229, 2156.636, 2188.173, 2190.526, 2181.107]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:9 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	2000	True	25005.18555		574625	12	-1	207.08492612838745	{'train_loss': [644407.812, 383890.531, 304387.875, 284397.188, 270990.906, 264365.906, 259621.859, 255429.953, 252600.562, 249757.031, 247413.453, 245738.047, 244398.266, 241898.906, 242083.266, 239124.0, 238227.906, 236891.078, 234727.578, 233184.109, 232233.547, 232066.188, 229729.062, 229671.5, 227900.188, 227027.859, 226039.453, 225550.062, 223860.891, 223300.781, 223168.625, 222505.875, 221680.688, 219754.984, 219983.484, 219154.984, 218728.859, 218754.812, 217311.484, 217963.359, 216883.234, 216993.812, 216238.5, 215739.719, 214278.547, 214154.438, 214457.5, 212396.828, 213242.281, 213153.906, 212477.125, 212764.703, 212465.25, 212315.109, 211239.75, 210096.578, 211226.281, 210782.859, 209385.875, 209526.219, 210592.266, 209490.531, 209082.281, 208628.219, 207702.312, 208026.578, 208254.5, 207159.719, 206467.562, 207575.156, 206442.0, 206237.234, 206654.156, 206266.609, 205391.25, 205249.922, 204898.438, 204813.062, 204361.547, 204572.281, 204569.984, 203159.297, 203623.875, 203540.516, 203160.5, 202963.125, 203336.234, 203839.328, 203353.766, 202099.422, 201465.453, 202085.453, 200548.641, 201245.875, 201395.234, 200254.844, 200918.609, 201410.891, 200990.062, 201068.312], 'val_loss': [4246.461, 3283.965, 2966.277, 2868.605, 2813.669, 2822.498, 2707.978, 2682.756, 2670.802, 2637.701, 2690.248, 2593.717, 2607.394, 2554.618, 2519.576, 2533.884, 2468.072, 2505.428, 2488.843, 2465.495, 2453.861, 2415.106, 2419.333, 2405.282, 2384.737, 2366.242, 2354.217, 2378.634, 2370.167, 2352.684, 2347.507, 2354.776, 2350.886, 2330.821, 2362.98, 2310.191, 2287.813, 2316.888, 2277.212, 2325.714, 2292.339, 2253.164, 2242.375, 2279.868, 2269.648, 2274.553, 2252.145, 2281.806, 2295.974, 2283.5, 2237.186, 2234.404, 2255.571, 2245.156, 2222.779, 2277.686, 2271.896, 2231.508, 2236.337, 2234.145, 2234.237, 2231.467, 2240.786, 2201.888, 2226.602, 2192.807, 2207.254, 2216.661, 2197.972, 2207.857, 2224.112, 2214.512, 2213.879, 2211.083, 2223.961, 2206.772, 2216.405, 2241.0, 2202.384, 2219.347, 2202.344, 2204.475, 2231.953, 2190.308, 2203.518, 2215.527, 2183.235, 2209.878, 2196.654, 2173.079, 2186.26, 2171.074, 2166.53, 2173.49, 2174.254, 2178.421, 2180.662, 2187.192, 2172.379, 2188.819]}	0	100	True
