id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	24952.21875		619010	12	-1	212.7877902984619	{'train_loss': [603670.375, 392263.156, 302460.781, 268100.688, 258997.641, 253809.312, 250691.297, 246769.625, 243745.062, 240728.531, 239235.922, 237175.203, 235198.547, 234176.312, 232720.281, 232078.781, 229134.828, 227992.781, 227383.156, 225603.844, 224306.875, 224902.234, 223167.047, 221921.922, 221556.172, 220267.766, 219515.312, 218544.766, 217557.594, 217274.047, 217101.297, 216469.234, 215169.469, 215115.891, 215307.406, 214613.562, 213663.688, 213284.484, 212733.109, 212850.172, 211940.203, 213302.359, 211918.781, 211213.875, 211352.328, 210213.484, 210397.047, 210812.812, 209813.344, 209348.578, 209117.984, 208722.922, 208316.094, 208040.156, 207395.516, 207957.031, 207454.953, 207387.625, 206921.344, 206884.547, 206573.188, 206670.812, 206893.312, 205210.953, 205617.141, 206407.125, 205086.078, 204691.297, 205179.859, 204898.578, 204276.188, 203210.453, 203391.828, 203730.062, 203320.797, 203104.25, 204593.219, 203193.859, 203099.875, 204025.438, 202327.109, 202589.953, 202673.594, 201752.25, 202613.469, 202166.062, 202524.297, 200992.266, 202288.672, 200293.062, 200830.75, 201412.781, 200778.875, 200621.219, 200430.312, 200253.406, 199934.172, 199690.438, 199231.0, 199012.391], 'val_loss': [3954.188, 3171.255, 2803.667, 2697.936, 2663.168, 2642.764, 2616.383, 2591.016, 2548.873, 2532.216, 2512.987, 2487.657, 2487.408, 2456.237, 2444.488, 2455.847, 2445.18, 2419.747, 2405.375, 2398.438, 2401.405, 2378.205, 2377.568, 2374.422, 2356.255, 2364.662, 2365.033, 2352.882, 2365.127, 2367.732, 2342.868, 2318.393, 2303.083, 2363.615, 2322.571, 2308.101, 2324.958, 2300.609, 2294.631, 2322.642, 2279.908, 2275.239, 2291.557, 2300.861, 2263.187, 2270.132, 2288.671, 2263.541, 2283.308, 2264.046, 2251.066, 2248.916, 2264.173, 2245.644, 2249.421, 2245.4, 2240.442, 2244.588, 2238.036, 2227.49, 2235.883, 2206.551, 2226.468, 2235.475, 2257.697, 2227.781, 2209.877, 2227.792, 2214.288, 2214.631, 2215.416, 2273.748, 2189.572, 2203.123, 2234.131, 2218.224, 2198.975, 2207.021, 2212.748, 2201.692, 2207.767, 2196.686, 2179.486, 2205.507, 2189.929, 2191.519, 2192.867, 2201.851, 2196.159, 2197.501, 2206.387, 2175.209, 2183.373, 2193.312, 2157.289, 2195.696, 2172.588, 2190.618, 2163.754, 2181.883]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:8 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	24836.80273		520442	11	-1	204.81625032424927	{'train_loss': [587131.938, 358275.281, 301196.25, 276500.562, 263270.812, 255944.719, 251724.812, 248788.172, 244888.766, 242778.188, 241044.109, 239492.531, 237391.203, 236520.297, 235523.438, 234329.188, 232602.688, 231562.375, 230827.078, 231057.969, 229267.297, 228679.859, 228131.5, 227065.859, 226421.109, 226095.781, 225393.328, 224485.391, 224695.719, 223668.203, 223371.938, 222711.547, 222241.719, 221291.844, 220646.984, 220361.547, 219822.125, 218961.859, 219805.234, 219494.266, 218222.031, 218346.562, 217728.453, 218250.625, 216598.609, 216094.172, 216477.344, 215869.516, 214964.578, 215372.781, 215057.469, 215314.406, 213994.25, 214555.438, 212879.156, 214397.203, 212472.688, 213179.609, 211460.875, 213277.844, 212861.047, 212121.375, 211928.312, 210870.828, 212165.734, 211049.594, 211157.609, 210544.312, 210149.625, 209714.656, 209991.031, 209493.141, 209541.438, 208508.969, 208921.328, 208310.859, 208877.844, 208080.594, 208286.391, 207467.578, 207938.828, 207619.266, 207277.281, 207713.016, 206086.484, 207637.047, 206699.906, 205787.406, 206027.75, 207085.547, 206607.594, 205731.438, 205540.891, 205338.438, 205677.172, 205628.219, 204740.703, 204265.938, 204637.453, 204721.188], 'val_loss': [3910.86, 3109.2, 2893.745, 2677.371, 2591.965, 2587.099, 2562.297, 2544.975, 2513.931, 2524.611, 2501.085, 2532.462, 2478.465, 2472.953, 2462.309, 2451.912, 2470.356, 2452.972, 2443.365, 2438.485, 2444.372, 2421.009, 2435.694, 2410.792, 2420.688, 2408.713, 2395.593, 2390.53, 2410.734, 2397.27, 2381.924, 2378.797, 2382.222, 2367.039, 2360.049, 2353.409, 2365.527, 2330.399, 2342.714, 2350.763, 2355.478, 2317.906, 2334.398, 2331.292, 2349.708, 2343.449, 2320.911, 2339.885, 2323.419, 2334.073, 2310.323, 2289.048, 2313.038, 2320.829, 2312.903, 2303.439, 2304.036, 2297.48, 2297.233, 2293.185, 2296.331, 2299.135, 2293.893, 2277.101, 2277.692, 2286.803, 2297.63, 2273.296, 2297.069, 2271.882, 2273.379, 2258.702, 2267.729, 2278.385, 2264.146, 2265.352, 2251.636, 2261.421, 2248.031, 2253.748, 2255.351, 2248.534, 2247.095, 2256.742, 2257.394, 2235.648, 2238.639, 2238.911, 2240.803, 2229.936, 2240.017, 2226.915, 2223.912, 2246.038, 2256.308, 2266.96, 2224.144, 2221.379, 2219.529, 2218.843]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:rmsprop lr:0.14689150223001157 alpha:0.8646789476350075 weight_decay:0.0003371751655730333 batch_size:32 epochs:100	100	1000	True	131121.51562		557890	11	-1	209.87161087989807	{'train_loss': [1270923.0, 1707915.0, 1138628.875, 1280699.875, 1259369.875, 1274712.875, 1316065.75, 1225418.625, 1275256.125, 1197054.375, 1224369.625, 1214371.875, 1107244.0, 1126690.625, 1117238.625, 1100315.625, 1121834.875, 1093952.375, 1104987.625, 1114240.625, 1099256.875, 1104502.5, 1130757.5, 1098006.875, 1118013.5, 1109322.25, 1121858.75, 1118759.25, 1133645.625, 1119746.5, 1111689.25, 1105350.75, 1124413.75, 1130943.25, 1105844.0, 1126630.5, 1114072.5, 1119991.125, 1096961.0, 1116886.125, 1104158.75, 1160045.875, 1107509.375, 1109469.0, 1118343.375, 1094189.5, 1144202.875, 1094975.75, 1122434.875, 1096736.875, 1153191.5, 1110159.5, 1143834.0, 1123076.5, 1126835.375, 1111476.75, 1103150.75, 1136996.75, 1096632.125, 1145780.125, 1105051.5, 1099085.625, 1131322.875, 1127307.25, 1119182.0, 1146375.375, 1106223.0, 1104507.625, 1126234.75, 1121677.0, 1102672.5, 1120633.875, 1116616.75, 1122987.25, 1118218.75, 1112205.375, 1115656.75, 1114082.75, 1126080.875, 1127595.125, 1149457.75, 1113074.25, 1119288.25, 1101297.75, 1124232.75, 1106132.375, 1121305.5, 1119894.75, 1133165.375, 1121310.375, 1133131.5, 1103963.625, 1107749.75, 1105398.625, 1118802.875, 1119190.375, 1144037.25, 1096628.875, 1143246.625, 1102753.375], 'val_loss': [11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11667.521, 12974.289, 11887.357, 14541.309, 17551.533, 12844.296, 11084.51, 11167.346, 11110.434, 11110.434, 11110.434, 12821.003, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 17492.506, 12598.511, 11110.434, 11110.434, 11110.434, 13810.512, 11110.434, 11110.434, 11110.114, 11752.463, 11110.434, 11190.594, 11110.434, 11105.366, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11845.117, 11110.434, 11110.434, 11586.361, 17078.574, 13100.144, 11110.434, 11110.434, 11110.434, 11110.434, 11110.429, 11110.434, 12722.516, 13976.02, 11110.434, 11110.434, 11110.434, 11110.434, 11391.572, 11110.434, 11078.809, 11211.352, 11110.434, 11110.429, 11109.938, 11110.434, 11110.434, 11110.434, 11107.604, 11110.434, 11110.434, 11110.434, 11848.418, 11110.434, 11110.434, 11110.434, 11110.434, 12819.599, 12222.087, 11110.434, 11110.434, 11110.434, 11110.434, 11279.764, 11110.434, 11110.434, 13124.352, 11110.434]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:7 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:110 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:15 epochs:100	100	1000	True	28216.56055		2322047	12	-1	367.2153058052063	{'train_loss': [1135044.375, 723402.062, 290078.062, 263985.219, 253824.797, 248439.156, 242949.688, 239460.656, 236993.406, 234552.906, 231144.031, 230046.688, 226957.062, 224830.531, 222666.344, 219664.25, 218802.969, 217076.172, 216478.328, 214196.797, 212333.406, 211093.984, 209904.922, 208305.688, 207196.312, 205691.266, 205078.562, 204415.734, 202432.031, 201816.672, 201061.547, 200979.859, 200095.422, 198447.062, 196832.328, 196779.375, 196017.141, 194396.188, 195487.453, 193326.453, 193546.156, 192258.203, 191287.797, 191999.578, 189755.531, 189738.469, 189582.094, 188338.984, 187196.828, 187694.812, 187044.375, 186852.812, 186726.219, 186180.266, 185995.328, 183779.438, 184459.922, 184409.5, 183699.828, 181142.062, 183009.25, 182377.703, 182361.516, 181721.422, 181697.297, 181644.891, 180657.344, 180679.438, 179495.781, 179480.281, 179263.547, 178851.75, 179452.0, 178204.734, 178928.672, 177277.312, 178937.281, 177818.578, 179420.578, 178593.312, 176874.234, 175951.938, 175939.641, 177214.438, 174827.484, 175570.75, 173198.328, 174849.766, 174992.344, 175331.484, 174866.203, 175013.609, 174164.859, 174992.594, 173351.219, 174363.312, 173133.016, 173873.812, 171987.734, 172322.484], 'val_loss': [3903.567, 1495.068, 1334.07, 1241.693, 1197.026, 1181.187, 1174.221, 1154.599, 1147.475, 1154.568, 1128.967, 1137.566, 1128.057, 1117.693, 1110.192, 1103.868, 1131.077, 1104.121, 1105.563, 1105.754, 1099.319, 1118.066, 1133.014, 1080.9, 1124.772, 1128.421, 1117.278, 1113.045, 1105.474, 1101.802, 1119.09, 1085.583, 1103.607, 1115.994, 1132.942, 1106.557, 1107.328, 1115.418, 1123.823, 1108.594, 1084.269, 1106.047, 1107.414, 1115.788, 1137.629, 1108.45, 1105.18, 1104.026, 1144.613, 1126.093, 1136.147, 1113.078, 1128.472, 1108.012, 1132.096, 1126.167, 1120.268, 1109.646, 1110.745, 1106.039, 1116.583, 1114.729, 1130.939, 1141.372, 1117.683, 1116.015, 1098.58, 1110.784, 1104.719, 1101.007, 1128.351, 1101.867, 1149.938, 1101.466, 1101.693, 1128.561, 1113.962, 1126.102, 1141.83, 1114.225, 1091.235, 1103.802, 1142.788, 1110.982, 1137.237, 1113.485, 1110.143, 1137.938, 1086.289, 1101.298, 1099.436, 1122.563, 1148.698, 1123.872, 1138.178, 1117.057, 1125.983, 1096.185, 1120.403, 1107.449]}	100	100	True
