id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	26436.11133		619010	12	-1	210.2851219177246	{'train_loss': [596785.438, 378592.75, 306284.219, 283267.688, 271400.344, 264515.562, 258499.156, 255291.062, 253898.219, 251489.547, 248773.609, 247355.859, 245361.453, 243775.766, 242610.141, 241209.797, 240045.812, 239371.906, 237538.984, 237144.547, 236521.203, 236081.656, 235250.797, 234364.297, 233146.375, 233343.531, 232544.047, 232120.484, 231543.406, 231012.094, 230253.094, 230331.781, 229864.422, 229153.734, 228857.078, 229107.719, 228773.016, 227863.359, 228173.578, 226830.391, 226796.984, 226143.438, 226442.703, 226165.422, 225236.984, 224777.359, 225674.516, 225141.453, 224481.172, 224798.922, 224629.922, 223825.375, 223430.219, 223513.047, 223006.891, 223266.922, 222098.922, 221698.609, 221205.625, 222193.953, 220936.125, 220496.25, 221120.5, 220669.844, 220168.297, 220232.562, 220645.266, 219400.922, 219130.906, 218251.719, 218928.984, 219218.25, 218811.344, 218302.703, 217645.828, 216350.875, 217447.875, 217264.375, 217450.5, 216589.062, 216744.875, 216290.438, 215548.891, 216867.125, 215236.531, 215825.062, 214919.781, 214467.0, 214519.547, 215514.797, 213902.156, 214259.328, 214329.562, 213941.516, 213237.969, 213439.75, 213205.25, 212571.953, 212859.766, 212711.328], 'val_loss': [4011.145, 3109.764, 3018.471, 2779.355, 2759.849, 2665.897, 2606.323, 2578.43, 2558.173, 2552.562, 2533.181, 2529.763, 2517.448, 2501.812, 2511.471, 2509.989, 2497.034, 2476.283, 2492.68, 2491.764, 2467.122, 2463.53, 2462.489, 2464.548, 2467.792, 2499.139, 2467.982, 2454.393, 2438.994, 2424.035, 2414.036, 2435.466, 2440.975, 2425.989, 2438.022, 2434.143, 2399.547, 2406.135, 2415.866, 2445.81, 2405.565, 2413.611, 2420.808, 2415.498, 2416.376, 2399.22, 2404.723, 2398.909, 2393.485, 2402.486, 2382.649, 2415.355, 2388.484, 2410.67, 2401.666, 2420.463, 2383.018, 2404.354, 2366.539, 2378.816, 2422.118, 2375.294, 2371.244, 2385.166, 2390.14, 2420.689, 2362.865, 2376.751, 2388.991, 2353.403, 2347.557, 2355.547, 2363.998, 2350.736, 2345.884, 2343.648, 2349.064, 2345.263, 2331.999, 2338.891, 2322.918, 2333.16, 2328.208, 2353.034, 2342.014, 2330.62, 2330.841, 2323.239, 2323.181, 2316.453, 2318.754, 2305.019, 2305.753, 2300.213, 2315.005, 2320.247, 2301.691, 2303.952, 2291.558, 2302.392]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:117 kernel_size:10 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:27 epochs:100	100	1000	True	27460.59961		1000801	12	-1	231.0542151927948	{'train_loss': [945572.625, 436706.125, 367492.312, 339793.906, 319310.906, 299650.5, 285715.938, 274539.125, 267672.938, 261915.984, 259513.016, 256463.141, 254246.219, 251960.656, 250638.109, 248940.344, 249113.734, 246324.562, 245268.734, 244209.016, 242833.859, 242836.953, 241510.625, 240653.312, 239729.672, 238917.641, 238333.297, 237443.188, 236817.172, 236504.922, 234931.719, 235182.219, 235327.469, 234380.781, 232561.141, 233310.625, 233202.422, 231507.469, 231441.812, 231806.109, 231518.094, 230961.922, 231272.141, 230519.344, 229605.75, 230225.25, 229494.781, 228041.234, 228682.281, 227644.438, 227725.141, 227066.219, 226336.109, 226486.75, 225960.906, 225646.172, 226014.891, 225597.125, 224499.547, 224421.312, 224178.344, 224959.172, 223922.844, 223057.625, 222710.359, 223154.047, 223466.062, 222066.188, 221814.828, 221478.609, 221738.25, 219908.562, 219737.734, 219954.422, 219486.734, 220154.844, 219648.75, 219434.438, 218287.906, 218572.359, 218329.859, 219055.078, 216624.375, 217048.062, 216930.922, 217589.609, 216398.422, 216235.281, 215126.312, 215884.938, 214473.562, 215602.938, 214660.422, 214375.734, 214387.047, 214821.859, 213988.859, 213184.188, 213570.219, 213590.094], 'val_loss': [4359.292, 3375.071, 3008.2, 2712.388, 2680.158, 2580.133, 2368.089, 2318.7, 2320.084, 2276.57, 2265.256, 2295.695, 2303.329, 2235.976, 2193.106, 2192.573, 2185.87, 2222.103, 2183.608, 2150.908, 2140.508, 2143.856, 2160.12, 2143.034, 2123.546, 2128.414, 2145.118, 2188.059, 2118.963, 2152.116, 2139.783, 2135.871, 2140.191, 2151.378, 2090.462, 2122.2, 2096.333, 2099.182, 2117.741, 2106.651, 2092.7, 2109.943, 2075.354, 2104.05, 2114.911, 2109.58, 2113.509, 2117.501, 2108.026, 2087.551, 2083.343, 2110.812, 2065.736, 2114.37, 2082.782, 2067.835, 2085.793, 2045.471, 2094.98, 2076.716, 2058.727, 2079.738, 2055.739, 2055.235, 2112.296, 2075.669, 2056.637, 2060.783, 2078.818, 2052.314, 2070.262, 2095.229, 2070.797, 2091.038, 2081.822, 2119.125, 2093.506, 2043.887, 2071.619, 2024.89, 2065.761, 2075.057, 2058.225, 2067.009, 2043.727, 2040.677, 2076.488, 2060.032, 2036.666, 2071.762, 2027.059, 2033.112, 2028.148, 2021.404, 2043.154, 2019.602, 2038.985, 2047.561, 2074.34, 2045.299]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:53 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:4 epochs:100	87	1000	True	26133.57422		498337	13	-1	1003.7634124755859	{'train_loss': [376809.062, 286531.969, 267869.156, 260729.703, 256016.062, 253328.422, 251114.188, 248132.797, 246547.734, 244657.922, 243895.328, 242376.188, 240559.594, 240038.344, 238667.984, 238981.422, 237563.203, 237572.156, 235984.469, 235214.672, 234405.156, 233527.172, 233423.234, 232979.594, 232311.906, 231795.922, 230466.062, 230794.469, 229201.281, 229852.328, 228576.125, 228038.984, 228710.156, 227934.188, 226605.188, 227660.531, 225599.344, 226015.5, 225118.031, 225553.5, 223706.297, 224998.781, 225575.734, 223959.312, 223394.078, 223350.906, 222848.703, 222472.094, 222804.391, 221234.125, 221678.812, 220323.047, 220944.766, 221905.578, 221244.172, 220045.312, 220104.969, 218574.953, 220387.969, 218865.75, 218590.547, 219846.609, 218527.984, 219358.25, 217884.953, 217958.969, 216343.875, 217227.078, 218037.938, 215353.25, 216980.938, 216394.75, 216340.391, 216194.719, 216381.359, 215889.219, 214696.75, 215285.906, 215363.938, 215045.188, 214512.281, 215066.875, 214137.234, 213731.547, 213390.5, 213597.594, 213304.953], 'val_loss': [400.552, 345.281, 338.289, 332.411, 328.611, 326.068, 325.31, 325.963, 320.27, 325.475, 320.762, 318.427, 318.629, 314.946, 319.213, 316.034, 317.956, 315.316, 313.977, 313.722, 313.737, 314.632, 317.885, 312.735, 311.717, 312.965, 309.781, 312.325, 306.664, 308.82, 308.415, 307.824, 307.596, 307.302, 309.606, 307.997, 309.858, 307.766, 306.024, 307.974, 302.425, 305.204, 304.22, 305.473, 309.166, 305.363, 305.462, 310.82, 306.147, 312.044, 306.671, 306.595, 305.618, 311.18, 306.614, 302.474, 302.353, 302.865, 303.333, 312.078, 305.049, 311.697, 314.95, 304.657, 308.876, 307.743, 306.751, 310.649, 305.148, 304.212, 301.145, 304.215, 303.65, 302.219, 301.358, 300.293, 301.637, 302.568, 303.632, 296.114, 296.215, 299.23, 297.617, 296.345, 309.189, 296.388, 293.756]}	87	87	False
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	24740.20508		619010	12	-1	211.87845945358276	{'train_loss': [636153.562, 409161.375, 319619.344, 273457.094, 261356.344, 255013.516, 250887.781, 247485.344, 244502.203, 241046.656, 238733.328, 236269.938, 234363.453, 233073.672, 231721.547, 230392.547, 229973.609, 228485.047, 227639.109, 226253.797, 225688.438, 225048.438, 224258.406, 223802.078, 223155.859, 222154.453, 222794.609, 222021.359, 220534.578, 220306.453, 219809.141, 219786.078, 219164.328, 219084.016, 217549.188, 217503.188, 216333.594, 216871.953, 214767.672, 214404.984, 213696.203, 214780.562, 213289.328, 213372.609, 212636.641, 212706.062, 212075.188, 211700.516, 211784.297, 210675.25, 211018.891, 211396.375, 209684.375, 210245.453, 208870.25, 209639.047, 208426.375, 209645.234, 208527.922, 208944.797, 208348.188, 206803.406, 207028.234, 207721.094, 206938.594, 206413.703, 206388.281, 206520.0, 205876.125, 205636.219, 206262.562, 205500.594, 205529.688, 204735.344, 205033.531, 204262.844, 204420.594, 204119.891, 204259.172, 203460.844, 203556.0, 203666.172, 203615.375, 203465.594, 203297.969, 203392.656, 202198.359, 202323.703, 202183.734, 203147.922, 200775.844, 201080.656, 201693.344, 201556.375, 200971.828, 200973.469, 200386.406, 199783.219, 200693.141, 200143.484], 'val_loss': [3843.421, 3344.177, 2836.104, 2690.608, 2695.774, 2625.761, 2626.516, 2606.163, 2524.352, 2509.807, 2482.948, 2483.689, 2495.016, 2457.385, 2429.712, 2447.856, 2431.093, 2434.994, 2421.016, 2431.29, 2389.481, 2400.079, 2413.854, 2412.113, 2389.781, 2394.513, 2360.11, 2364.217, 2341.19, 2335.794, 2326.807, 2341.19, 2324.895, 2309.219, 2339.876, 2285.004, 2297.884, 2309.94, 2299.789, 2288.407, 2288.83, 2293.493, 2280.175, 2282.739, 2278.155, 2289.138, 2286.36, 2301.782, 2295.813, 2280.859, 2279.715, 2268.781, 2256.187, 2258.298, 2285.681, 2256.386, 2281.053, 2276.606, 2262.321, 2263.086, 2262.902, 2239.771, 2258.738, 2246.972, 2260.908, 2264.513, 2257.226, 2250.485, 2252.422, 2231.738, 2268.976, 2249.038, 2244.345, 2225.043, 2221.586, 2225.312, 2223.698, 2222.808, 2238.826, 2235.059, 2203.128, 2213.289, 2236.509, 2215.344, 2218.604, 2204.342, 2221.763, 2214.081, 2221.156, 2211.825, 2201.085, 2208.83, 2213.21, 2215.852, 2198.068, 2227.549, 2203.539, 2210.31, 2217.163, 2207.877]}	100	100	True
