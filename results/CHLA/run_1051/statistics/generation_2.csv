id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	24662.59375		639490	13	-1	205.72180080413818	{'train_loss': [650107.0, 388831.438, 297343.812, 267446.375, 259038.031, 255424.422, 251044.0, 248192.922, 246424.938, 244279.25, 241227.219, 240363.156, 238373.344, 237393.312, 235478.859, 234027.938, 233126.844, 231576.172, 230289.766, 230003.672, 228666.953, 227879.719, 226283.906, 225832.125, 224591.438, 224432.219, 223985.406, 223607.734, 222878.578, 221640.375, 221217.734, 220165.953, 219887.375, 219338.281, 219317.797, 218036.828, 217849.641, 217750.547, 217549.391, 216613.812, 216270.562, 215712.969, 215693.328, 214075.562, 213980.766, 213463.281, 212343.375, 212505.531, 211949.484, 211411.766, 210261.312, 210931.578, 210896.812, 210675.109, 209974.719, 209552.891, 208990.016, 208713.312, 208654.938, 208808.922, 207677.281, 206887.297, 206985.625, 205763.266, 207749.25, 205710.078, 205693.609, 205571.844, 205375.75, 205485.312, 204826.203, 203981.75, 204644.844, 202939.375, 203742.062, 203063.625, 201864.594, 203855.109, 202517.781, 202062.578, 202194.656, 201813.703, 201872.0, 201227.562, 200570.5, 200356.281, 201474.547, 200511.266, 199763.891, 198201.5, 198954.609, 199057.984, 198806.422, 199191.875, 198502.234, 198628.5, 198532.859, 198306.984, 198065.891, 196914.891], 'val_loss': [3996.682, 3245.142, 2864.729, 2682.91, 2675.907, 2628.521, 2556.6, 2574.633, 2537.424, 2513.444, 2565.971, 2535.695, 2482.764, 2492.565, 2461.055, 2446.921, 2461.44, 2438.359, 2442.958, 2430.641, 2417.885, 2379.94, 2380.542, 2395.339, 2390.409, 2367.241, 2384.361, 2360.648, 2351.123, 2356.743, 2342.912, 2351.482, 2349.432, 2348.423, 2322.355, 2315.09, 2332.166, 2321.694, 2331.384, 2316.162, 2301.644, 2316.553, 2331.243, 2333.963, 2301.091, 2317.05, 2311.665, 2301.176, 2280.082, 2284.93, 2309.655, 2295.729, 2279.43, 2249.939, 2283.381, 2278.956, 2261.024, 2271.648, 2267.175, 2242.232, 2250.116, 2277.938, 2230.457, 2268.234, 2244.338, 2236.033, 2278.683, 2248.953, 2227.779, 2236.78, 2232.389, 2246.905, 2226.994, 2234.052, 2224.354, 2218.296, 2217.468, 2235.175, 2213.985, 2230.486, 2216.714, 2210.275, 2195.518, 2220.57, 2197.316, 2183.205, 2200.284, 2182.93, 2186.432, 2176.218, 2170.842, 2183.88, 2188.398, 2182.424, 2190.802, 2201.124, 2188.557, 2172.726, 2189.583, 2162.489]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:58 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	26698.22266		1382042	12	-1	205.49223470687866	{'train_loss': [1160264.25, 1149183.375, 1107728.0, 768498.625, 359200.906, 297036.656, 275479.75, 266971.094, 261376.25, 257827.766, 253870.406, 252208.922, 250458.828, 248589.406, 246059.0, 246184.516, 244579.531, 242906.859, 241778.5, 241169.281, 240466.484, 238892.891, 237722.047, 238037.875, 237169.297, 235880.328, 235220.609, 234540.359, 234246.234, 233417.172, 232510.203, 232376.406, 231949.625, 230664.281, 229910.594, 229316.75, 229674.891, 228334.797, 228002.375, 227593.125, 226909.703, 226429.25, 226159.609, 225403.062, 225283.5, 224646.531, 223909.188, 224694.844, 223244.25, 222927.438, 222110.234, 221841.266, 221394.203, 221354.906, 219917.766, 219039.297, 219302.469, 219341.703, 218426.234, 218546.266, 217189.531, 217277.031, 217432.234, 216546.781, 216133.781, 216101.281, 216467.047, 215394.688, 215638.047, 214409.516, 214487.422, 213905.734, 213703.406, 212832.922, 212669.781, 213403.5, 213354.422, 212088.469, 211565.594, 212146.719, 211958.734, 211357.5, 210404.641, 210501.922, 209795.562, 210234.453, 210179.875, 210235.75, 209771.484, 208849.406, 208486.5, 208789.0, 209110.016, 208440.609, 206997.078, 206981.922, 207976.484, 206798.094, 207079.375, 207025.453], 'val_loss': [11110.398, 11479.197, 10761.768, 3752.022, 3023.074, 2915.647, 2747.156, 2772.857, 2609.007, 2584.435, 2586.932, 2567.926, 2571.707, 2580.421, 2523.778, 2522.656, 2537.25, 2518.802, 2551.887, 2514.113, 2490.534, 2498.378, 2515.331, 2507.01, 2483.318, 2494.208, 2467.593, 2452.574, 2482.705, 2455.64, 2480.48, 2464.582, 2456.265, 2461.337, 2461.223, 2435.095, 2450.363, 2434.54, 2446.861, 2430.465, 2414.105, 2452.658, 2416.645, 2395.796, 2431.91, 2431.425, 2424.815, 2394.185, 2419.59, 2386.606, 2400.266, 2391.447, 2380.8, 2380.27, 2370.647, 2395.165, 2366.864, 2459.783, 2366.631, 2373.804, 2330.659, 2372.027, 2362.677, 2358.036, 2363.037, 2360.279, 2351.29, 2357.939, 2332.615, 2347.617, 2338.685, 2350.118, 2321.263, 2361.265, 2336.484, 2314.557, 2339.467, 2327.627, 2366.257, 2360.079, 2336.457, 2361.167, 2325.378, 2315.957, 2309.979, 2314.229, 2321.945, 2306.418, 2318.814, 2325.906, 2345.263, 2327.032, 2380.861, 2342.799, 2344.272, 2350.827, 2320.842, 2336.779, 2330.205, 2312.761]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:50 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.1797801723801871 beta1:0.8724928857675553 beta2:0.8042967905741668 weight_decay:7.319295158773019e-06 batch_size:32 epochs:100	100	1000	True	131121.73438		632472	14	-1	219.41790080070496	{'train_loss': [1071537.875, 1170495.5, 1202739.0, 1155916.75, 1218034.375, 1129787.5, 1086509.5, 1110846.0, 1082707.5, 1085986.125, 1076672.75, 1072964.875, 1070802.0, 1076145.25, 1071784.5, 1070901.375, 1077445.875, 1069213.625, 1074823.125, 1075890.25, 1070142.875, 1076842.5, 1073403.375, 1067669.0, 1073991.5, 1072201.375, 1069280.125, 1077571.0, 1070389.0, 1070485.125, 1074227.875, 1068572.375, 1071240.125, 1072385.5, 1070690.75, 1072157.125, 1071531.25, 1069771.875, 1073604.375, 1070199.375, 1071444.25, 1071828.625, 1072549.0, 1070273.5, 1071677.625, 1073540.25, 1074245.5, 1074668.5, 1075367.125, 1072257.25, 1073096.5, 1068924.375, 1072968.0, 1070732.125, 1072473.625, 1073222.25, 1071708.875, 1072114.625, 1070557.875, 1071920.625, 1071139.0, 1069649.75, 1069419.125, 1071414.125, 1070073.25, 1069948.5, 1071605.875, 1070101.0, 1072656.25, 1071722.25, 1070850.375, 1073626.0, 1074334.5, 1070837.875, 1070149.5, 1075554.0, 1071162.5, 1072660.875, 1072121.875, 1069352.375, 1074798.375, 1073686.625, 1068720.25, 1077534.25, 1070201.75, 1070635.625, 1072622.5, 1072998.375, 1073383.25, 1073658.125, 1073902.0, 1071555.875, 1072443.25, 1073281.625, 1070285.0, 1071820.375, 1071427.0, 1070719.75, 1070957.625, 1072283.75], 'val_loss': [11110.434, 11110.434, 14926.986, 11110.434, 11110.434, 14265.861, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11116.55, 11110.434, 11110.434, 11110.434, 11110.434, 11110.427, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11510.766, 11110.434, 11110.434, 11110.434, 11564.503, 11110.434, 11366.205, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.433, 11103.316, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11637.302, 11717.669, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11068.266, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11908.158, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11126.451, 11110.434, 11110.434, 11075.671, 11110.434, 11366.969, 11773.56, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:98 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:rmsprop lr:0.09793519194904024 alpha:0.9154707151507228 weight_decay:1.0186768761015267e-05 batch_size:32 epochs:100	100	1000	True	131132.51562		4333752	13	-1	214.47659873962402	{'train_loss': [1140367.5, 1062567.125, 1863066.625, 3904607.0, 1062567.125, 1062567.125, 7357026.0, 1062567.125, 1062567.125, 3795319.25, 1062567.0, 1107481.375, 4413061.5, 1062583.75, 1088870.375, 1258952.125, 2452331.25, 1216629.5, 1261840.5, 1244008.75, 2007980.375, 1062567.125, 1104713.625, 1148171.5, 1398580.375, 1063785.5, 1146078.25, 1213902.875, 1146512.875, 1113259.125, 1218476.5, 1372352.875, 1194799.75, 1093693.0, 1158713.125, 1280864.25, 1249599.75, 1130652.875, 1159439.75, 1280061.0, 1193020.0, 1137861.0, 1151445.25, 1347609.5, 1155473.0, 1141387.625, 1171697.5, 1190741.125, 1128902.375, 1148787.125, 1166718.625, 1226260.75, 1125253.625, 1182899.25, 1166218.875, 1167732.25, 1131602.125, 1135214.625, 1250509.875, 1199530.75, 1164011.625, 1132175.625, 1210395.0, 1210374.75, 1166588.625, 1129992.375, 1154018.5, 1217107.375, 1175666.25, 1176318.25, 1158907.625, 1193999.0, 1178252.0, 1225362.125, 1188152.625, 1205448.5, 1146830.875, 1124515.75, 1257692.375, 1206264.75, 1134695.375, 1145216.75, 1209188.5, 1248300.0, 1152871.0, 1222207.375, 1228173.25, 1209931.875, 1176697.375, 1157641.875, 1194027.25, 1254447.5, 1117293.625, 1192050.625, 1184092.75, 1260728.125, 1137271.875, 1147361.625, 1219907.0, 1208819.125], 'val_loss': [11110.434, 11110.434, 1202707.0, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 12439.479, 11110.434, 11110.434, 11110.434, 11110.434, 14118.761, 11110.434, 11110.434, 11110.434, 11448.722, 11147.789, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 25025.586, 13197.213, 11110.41, 11110.434, 11110.434, 17413.893, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11197.304, 14109.141, 13410.141, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11106.419, 13619.986, 14444.051, 16328.467, 13423.916, 11110.434, 11080.012, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 13538.326, 11110.434, 11110.434, 22504.98, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 26955.203, 11110.434]}	100	100	True
