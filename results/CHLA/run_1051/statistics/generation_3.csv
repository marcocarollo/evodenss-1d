id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	24786.44336		639490	13	-1	206.62848329544067	{'train_loss': [644724.125, 418564.188, 314623.906, 274842.625, 260556.344, 253071.578, 248351.688, 244261.719, 241585.219, 239096.641, 236261.047, 235215.891, 233989.734, 232678.766, 231478.844, 229798.328, 228849.375, 228256.969, 227068.359, 225974.891, 225145.281, 224151.766, 224548.547, 223176.266, 222178.672, 221151.953, 220165.656, 220546.719, 220005.922, 219196.828, 217609.141, 217491.562, 217036.375, 215754.516, 215383.75, 215099.781, 215626.656, 213874.109, 214368.484, 212251.047, 212648.312, 212497.469, 212054.453, 212215.703, 210893.609, 211063.516, 210181.25, 209916.609, 209678.422, 208833.594, 208770.812, 207733.688, 208489.656, 208264.188, 207908.656, 207035.516, 206112.25, 207391.609, 206719.906, 205980.75, 205838.672, 205879.984, 205487.797, 205050.281, 204722.984, 205242.234, 204238.156, 204101.812, 202827.0, 203605.734, 202674.266, 204160.969, 203000.562, 202357.797, 202746.75, 202234.031, 201521.531, 201782.734, 200815.391, 201296.25, 200840.219, 200670.234, 200272.766, 200406.25, 200643.609, 200112.203, 199628.344, 199372.797, 200013.234, 200241.234, 199864.406, 199201.062, 199233.562, 198697.719, 197310.766, 199307.344, 198221.828, 197351.594, 197341.594, 197192.188], 'val_loss': [3791.663, 3466.8, 2912.297, 2726.239, 2601.918, 2583.353, 2517.206, 2496.726, 2506.427, 2475.412, 2476.407, 2453.402, 2426.548, 2443.614, 2442.942, 2402.01, 2403.475, 2386.543, 2356.378, 2374.849, 2379.79, 2364.761, 2341.995, 2348.267, 2340.524, 2323.672, 2328.686, 2340.446, 2314.678, 2331.3, 2338.516, 2327.425, 2329.726, 2287.691, 2288.62, 2296.851, 2283.798, 2294.904, 2311.612, 2277.608, 2260.336, 2261.391, 2270.219, 2281.195, 2287.247, 2274.712, 2293.429, 2239.47, 2239.924, 2223.366, 2239.361, 2264.496, 2244.629, 2228.859, 2240.706, 2222.781, 2240.437, 2213.96, 2222.342, 2224.806, 2222.59, 2222.46, 2213.155, 2196.371, 2193.538, 2204.353, 2226.658, 2197.728, 2220.703, 2180.565, 2194.065, 2189.09, 2203.815, 2199.314, 2194.693, 2201.995, 2170.498, 2195.085, 2194.68, 2192.163, 2173.095, 2184.344, 2187.158, 2179.905, 2188.497, 2148.379, 2168.364, 2170.018, 2196.617, 2157.027, 2169.758, 2148.157, 2154.064, 2145.826, 2163.089, 2142.789, 2147.79, 2143.205, 2157.196, 2155.758]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:123 kernel_size:8 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:70 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.23206307282035943 beta1:0.9927279190013203 beta2:0.9933374823559408 weight_decay:0.0003189305642838345 batch_size:32 epochs:100	100	1000	True	131121.95312		712303	14	-1	225.55123019218445	{'train_loss': [1068045.5, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062585.25, 1062567.125, 1062567.125, 1062567.125, 1062774.25, 1062624.125, 1063233.5, 1067368.5, 1069060.0, 1066591.375, 1064328.5, 1062962.25, 1064027.625, 1063685.875, 1062567.125, 1063283.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1063028.875, 1062567.125, 1062668.25, 1064495.375, 1081262.375, 1071138.0, 1065440.875, 1063624.625, 1070461.5, 1063788.75, 1063566.875, 1062610.875, 1066798.75, 1062605.125, 1062606.125, 1062567.125, 1067187.75, 1063726.0, 1062567.125, 1062567.125, 1063249.375, 1066271.25, 1065246.375, 1074210.625, 1066908.875, 1063902.5, 1064262.0, 1063378.75, 1066805.375, 1062981.75, 1064535.25, 1062984.875, 1065774.625, 1067222.375, 1063972.75, 1062567.125, 1062642.0, 1062603.875, 1062567.125, 1062908.5, 1067605.25, 1066492.0, 1063962.0, 1065204.375, 1064800.625, 1069768.75, 1064060.375, 1064107.5, 1064852.0, 1074537.875, 1063098.875, 1072074.75, 1062925.5, 1065084.25, 1063834.75, 1064469.5, 1064657.0, 1062567.125, 1062567.125, 1064574.875, 1066458.0, 1064620.625, 1065460.0, 1069896.75, 1072155.625, 1063537.75, 1063780.125, 1063345.75, 1064003.125, 1065363.625, 1066129.75, 1063898.375, 1063559.75, 1062831.375, 1067536.25], 'val_loss': [11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	25492.72656		700610	14	-1	216.8294005393982	{'train_loss': [620428.438, 385921.281, 307070.812, 273736.781, 263138.062, 258035.5, 254234.734, 250013.969, 247289.0, 245553.453, 243680.391, 242376.234, 240808.5, 240062.141, 238307.75, 237276.344, 236936.203, 235920.484, 235254.219, 234697.734, 234116.078, 232952.391, 231801.906, 231897.656, 231225.469, 230595.172, 229652.281, 228459.828, 228305.125, 227949.031, 226699.344, 227758.078, 226580.203, 225189.438, 225424.594, 224121.672, 222552.031, 222629.703, 222159.828, 221478.922, 221133.703, 220885.0, 221005.719, 219825.125, 218761.094, 217692.812, 218944.891, 217651.219, 218359.641, 217421.094, 216918.219, 215969.375, 216358.094, 216543.484, 215821.234, 214751.703, 214722.844, 214864.125, 214061.297, 214080.422, 214161.312, 212429.984, 213039.172, 212560.156, 212704.438, 212006.719, 212357.953, 212321.719, 211361.188, 210489.109, 210831.688, 210413.172, 210402.25, 210721.031, 210004.031, 209817.547, 209444.656, 209730.797, 209082.328, 208669.141, 209635.141, 208259.266, 208747.562, 207460.109, 208005.266, 207614.047, 208702.547, 206555.547, 207008.125, 206677.703, 207073.328, 206425.219, 205778.406, 206405.984, 205210.641, 205201.5, 204877.453, 205398.703, 205363.531, 204676.031], 'val_loss': [4249.735, 3564.473, 2919.475, 2750.618, 2697.028, 2695.033, 2630.743, 2689.853, 2637.942, 2632.906, 2633.716, 2623.829, 2640.61, 2604.703, 2601.469, 2565.29, 2571.169, 2544.473, 2521.083, 2553.633, 2529.683, 2528.301, 2476.843, 2460.98, 2476.021, 2454.604, 2442.136, 2475.326, 2454.527, 2441.027, 2404.253, 2434.152, 2416.535, 2408.929, 2447.459, 2434.694, 2414.99, 2391.537, 2390.29, 2387.595, 2377.975, 2336.167, 2356.933, 2339.491, 2408.401, 2357.023, 2360.254, 2349.687, 2338.528, 2329.631, 2348.271, 2353.733, 2328.458, 2337.709, 2331.75, 2339.037, 2359.044, 2310.649, 2319.842, 2327.171, 2312.339, 2297.236, 2288.903, 2311.989, 2294.073, 2300.412, 2299.34, 2314.229, 2270.776, 2269.019, 2275.999, 2305.792, 2272.555, 2240.143, 2258.544, 2258.618, 2276.209, 2260.961, 2301.786, 2262.29, 2270.512, 2256.484, 2262.732, 2261.616, 2250.045, 2283.154, 2238.282, 2254.104, 2248.237, 2239.974, 2241.37, 2254.365, 2240.994, 2252.228, 2264.462, 2214.398, 2253.636, 2236.192, 2259.492, 2250.692]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:87 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.16772349149927257 alpha:0.9914582262137921 weight_decay:7.809900217126406e-05 batch_size:32 epochs:100	100	1000	True	462953.0		851793	12	-1	204.98322463035583	{'train_loss': [1700023.5, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 8718329.0, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062737.125, 1062567.125, 1126826.25, 1062567.125, 1310271.5, 2050545.625, 1067094.75, 2669554.0, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1075609.125, 1062567.125, 1062567.125, 2358358.0, 3435223.0, 1062567.125, 1062567.125, 1689191.125, 1062601.875, 1249421.25, 1062580.75, 1063669.125, 1062567.125, 1238003.25, 1078774.375, 1069222.375, 7773050.0, 1062567.125, 1218995.125, 1095358.625, 1211213.625, 1062567.125, 1062726.625, 1098962.25, 1063494.75, 1062758.875, 1075030.0, 1360173.0, 1062567.125, 1067862.25, 1076386.75, 1277351.25, 1073135.625, 2313368.75, 1062567.125, 1393837.625, 1335353.0, 4775229.0, 1087418.0, 1155559.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1062567.125, 1487115.125, 1273964.0, 1127784.0, 1217811.125, 1175961.625, 1553251.5, 1062567.125, 1069669.25, 1265996.125, 1077815.875, 1065027.75, 1200636.125, 1254464.375, 1401724.875, 1843863.75, 1062780.25, 1295688.375, 1137667.875, 1129454.625, 1349556.5, 1128317.0, 1234714.125, 1965470.5, 1062567.125, 1184538.5, 3019724.25], 'val_loss': [11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11077.973, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 12013.907, 11110.434, 11110.434, 11110.404, 11110.432, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11107.639, 11110.434, 11110.434, 11110.434, 160169.703, 11110.434, 11110.434, 11110.434, 13513.576, 13854.475, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 13183.458, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 11110.434, 38751.102]}	100	100	True
