id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	23901.41992		416732	11	-1	240.00346398353577	{'train_loss': [519560.031, 329052.219, 282283.531, 264753.062, 257823.875, 253372.891, 248652.906, 245799.0, 242661.172, 240003.094, 237385.203, 235493.234, 232940.781, 231409.609, 228854.578, 228222.719, 226489.844, 225242.188, 223766.531, 223132.75, 221691.797, 220585.906, 219392.562, 219315.109, 217746.578, 217160.312, 215917.891, 215823.562, 215113.0, 214744.594, 213627.719, 213623.391, 213171.578, 213027.688, 212476.719, 211216.094, 211870.391, 211733.031, 210046.594, 210537.531, 209189.094, 208514.188, 207842.391, 207474.656, 207277.469, 207935.594, 206854.141, 206351.734, 206075.797, 206022.938, 205494.438, 205162.562, 205117.219, 204504.516, 203465.719, 204710.734, 203898.766, 203739.078, 203351.219, 202660.312, 202672.469, 201498.969, 201418.359, 201158.672, 201552.781, 201188.266, 200804.219, 200546.766, 200164.578, 200281.078, 200007.516, 198404.672, 199107.953, 198564.906, 198690.312, 198316.812, 197168.219, 197893.578, 197603.0, 197594.469, 196775.656, 196660.812, 196123.391, 196504.234, 195391.547, 195596.172, 196022.188, 195521.328, 195663.047, 194011.719, 194254.0, 194076.078, 193471.906, 194115.109, 193346.422, 193495.938, 193364.703, 193236.781, 192971.172, 192626.672], 'val_loss': [3277.778, 2372.138, 2191.79, 2122.685, 2072.604, 2037.434, 2030.313, 1981.717, 1979.53, 1946.583, 1952.279, 1930.409, 1908.378, 1897.846, 1904.809, 1884.052, 1890.557, 1863.563, 1872.957, 1850.029, 1832.763, 1826.902, 1838.364, 1818.191, 1810.405, 1827.355, 1835.064, 1807.793, 1824.517, 1796.247, 1805.05, 1796.421, 1788.703, 1801.741, 1795.539, 1783.254, 1783.035, 1773.498, 1770.274, 1801.904, 1776.368, 1798.332, 1786.913, 1779.386, 1786.674, 1761.517, 1766.085, 1764.121, 1766.761, 1763.843, 1751.783, 1772.655, 1757.901, 1747.97, 1768.988, 1758.103, 1758.802, 1748.999, 1738.051, 1741.088, 1739.992, 1735.136, 1746.717, 1739.208, 1725.806, 1750.361, 1719.278, 1734.426, 1720.497, 1713.488, 1716.349, 1721.66, 1716.008, 1723.65, 1714.257, 1710.321, 1730.232, 1703.463, 1719.611, 1708.397, 1693.357, 1701.147, 1702.849, 1683.43, 1710.982, 1696.041, 1713.636, 1693.519, 1715.438, 1721.14, 1703.072, 1695.334, 1680.431, 1719.246, 1689.36, 1698.417, 1674.39, 1691.551, 1679.907, 1700.242]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:84 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	24213.85938		524720	11	-1	247.7966091632843	{'train_loss': [696749.938, 402313.406, 309359.5, 276420.406, 265995.781, 260524.391, 255892.156, 252648.375, 250645.031, 248528.453, 247776.844, 246471.438, 245093.719, 243251.203, 242518.438, 241051.109, 240933.344, 239907.266, 238871.672, 237853.641, 237288.812, 236844.062, 236176.203, 235043.016, 234258.312, 233274.578, 232471.953, 231956.203, 231419.938, 229494.719, 228961.156, 228214.219, 226810.797, 226367.891, 225627.156, 224145.781, 223993.797, 222501.797, 222283.109, 221980.641, 220652.688, 220304.219, 218853.219, 218479.656, 217366.188, 217001.094, 217028.875, 215491.391, 214455.891, 214632.047, 213156.094, 213388.0, 212267.078, 211622.688, 212243.547, 210869.906, 210315.734, 210744.594, 209679.422, 209657.25, 209096.781, 208510.281, 208083.5, 207894.266, 206518.766, 206723.266, 205497.078, 206427.531, 205652.047, 205476.344, 204511.156, 204738.094, 204099.969, 204489.859, 202406.422, 203176.469, 202962.703, 202628.25, 203044.641, 202262.641, 202623.094, 201581.203, 202159.641, 201409.453, 200918.125, 199941.828, 200984.875, 200153.938, 198965.156, 199626.703, 198274.578, 198273.688, 198198.266, 198656.172, 197823.328, 197896.625, 196704.438, 197589.797, 196796.266, 196727.578], 'val_loss': [3639.354, 2759.974, 2361.766, 2219.989, 2159.524, 2135.211, 2078.333, 2067.607, 2066.574, 2036.769, 2024.272, 2021.446, 2027.612, 1996.765, 2026.134, 2004.054, 1987.501, 2008.351, 1998.638, 1989.776, 1980.53, 1981.936, 1988.542, 1975.231, 1988.712, 1943.67, 1939.407, 1949.62, 1929.378, 1908.593, 1891.994, 1886.035, 1889.848, 1884.907, 1875.492, 1876.19, 1869.2, 1855.808, 1846.639, 1839.362, 1849.07, 1830.858, 1821.105, 1832.877, 1843.026, 1825.32, 1821.087, 1802.217, 1798.604, 1809.263, 1808.934, 1829.899, 1793.744, 1800.634, 1790.316, 1802.688, 1777.689, 1779.838, 1799.576, 1798.619, 1775.688, 1767.108, 1771.306, 1775.654, 1760.378, 1757.266, 1777.7, 1771.764, 1762.81, 1754.473, 1738.45, 1752.872, 1740.285, 1748.598, 1738.399, 1736.869, 1736.119, 1724.502, 1725.749, 1722.774, 1727.789, 1730.405, 1740.786, 1722.118, 1745.767, 1722.727, 1738.038, 1726.489, 1719.158, 1739.62, 1744.833, 1722.966, 1728.118, 1726.11, 1720.458, 1696.452, 1721.492, 1700.094, 1708.872, 1717.246]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:92 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:rmsprop lr:0.21667032932840025 alpha:0.9315739595131545 weight_decay:4.348737050626713e-05 batch_size:26 epochs:100	100	1000	True	131122.59375		820694	10	-1	232.33903884887695	{'train_loss': [1344050.5, 1062569.75, 3391826.75, 1062567.25, 1155910.625, 2362944.0, 1062631.125, 2017254.125, 1224858.75, 1261056.875, 2042794.25, 1367393.0, 1171819.375, 1365065.375, 1333869.875, 1183525.125, 1271045.375, 1400944.875, 1217642.0, 1239931.625, 1523344.875, 1176137.25, 1200570.875, 1334084.0, 1295178.375, 1192835.75, 1253031.125, 1393223.625, 1247266.125, 1301516.0, 1264909.0, 1323461.125, 1191989.375, 1364401.25, 1402704.875, 1226913.375, 1100474.25, 1400652.0, 1398739.625, 1305254.0, 1254505.375, 1374138.625, 1254028.625, 1186507.125, 1379737.375, 1462307.875, 1286836.875, 1223566.5, 1286389.625, 1323196.125, 1413425.25, 1420537.625, 1333036.25, 1305890.5, 1290965.625, 1419202.5, 1106579.625, 1680292.0, 1279909.875, 1283032.25, 1222521.375, 1252214.375, 1331366.125, 1262996.5, 1404132.375, 1297008.0, 1257377.75, 1335964.5, 1291205.25, 1331452.25, 1163943.0, 1298851.0, 1423029.875, 1433392.875, 1286463.625, 1324079.5, 1322354.75, 1311211.25, 1303471.375, 1160526.125, 1277502.625, 1304745.0, 1298479.5, 1257515.875, 1303067.125, 1321897.875, 1408034.75, 1246521.625, 1322878.75, 1381901.5, 1265802.5, 1195422.125, 1414569.5, 1272529.25, 1249959.375, 1243691.625, 1317188.75, 1269476.25, 1222550.125, 1380875.875], 'val_loss': [8888.349, 8888.349, 8888.349, 8888.349, 10923.654, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 15302.855, 8888.349, 8888.349, 8888.349, 17978.076, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:84 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:46 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:65 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adam lr:0.24497597014678835 beta1:0.9873365524045572 beta2:0.8143167358395912 weight_decay:2.510321345607212e-05 batch_size:26 epochs:100	100	1000	True	131191.59375		4017181	10	-1	262.3001217842102	{'train_loss': [1221926.5, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 179660672.0, 21594146.0, 34075132.0, 24269428.0, 101893752.0, 88332976.0, 23684910.0, 61156784.0, 89228736.0, 39641528.0, 94750184.0, 59833088.0, 124296000.0, 56640668.0, 38212860.0, 60516992.0, 33284474.0, 6493601.0, 7619363.5, 11505204.0, 28838702.0, 21941610.0, 14268448.0, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25, 1062567.25], 'val_loss': [8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349, 8888.349]}	100	100	True
