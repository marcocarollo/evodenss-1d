id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	1000	True	24339.35156		416732	11	-1	242.927969455719	{'train_loss': [514677.25, 363621.031, 321459.062, 304519.156, 297226.281, 292626.188, 272881.875, 265555.344, 261455.312, 257804.5, 254403.984, 252549.328, 250266.75, 248830.672, 247065.891, 245797.5, 243892.844, 242041.922, 240926.312, 239275.125, 237764.156, 237382.922, 236116.734, 235507.094, 234539.469, 233383.312, 233816.219, 232154.438, 231721.125, 231017.734, 230702.078, 229805.328, 228799.109, 227834.547, 227614.25, 226938.484, 226028.703, 225333.781, 223866.422, 223700.219, 222598.062, 222072.438, 221282.859, 221255.172, 219509.141, 219017.109, 218745.891, 217388.25, 216045.688, 217259.656, 216358.234, 214333.391, 215324.812, 214298.797, 214000.391, 213630.5, 212241.766, 212068.047, 212367.641, 211210.828, 211497.297, 209884.094, 210607.219, 209887.047, 210824.609, 209455.422, 208655.531, 208413.969, 208315.672, 207818.031, 207392.562, 206256.688, 206708.516, 205743.234, 206301.281, 205407.156, 205673.531, 204759.797, 203860.859, 204338.469, 204516.766, 204076.828, 204405.969, 204215.469, 202842.266, 202776.078, 201198.125, 201954.75, 201745.688, 200837.094, 201686.438, 200174.484, 201022.125, 200987.719, 200472.016, 200119.906, 200182.047, 199843.672, 199819.484, 198129.484], 'val_loss': [3110.252, 2655.441, 2551.308, 2508.333, 2456.325, 2435.057, 2237.603, 2195.803, 2157.046, 2123.434, 2125.308, 2091.372, 2078.461, 2060.832, 2044.153, 2032.353, 2017.956, 2010.873, 1999.102, 1977.484, 1976.097, 1975.696, 1988.661, 1961.986, 1976.94, 1951.415, 1947.156, 1934.12, 1928.015, 1939.486, 1931.799, 1951.774, 1920.034, 1924.884, 1929.495, 1902.413, 1911.364, 1918.037, 1883.169, 1893.623, 1874.907, 1878.79, 1864.924, 1872.485, 1870.292, 1848.4, 1866.193, 1842.162, 1834.507, 1855.548, 1862.215, 1827.698, 1821.79, 1812.977, 1832.677, 1817.365, 1810.626, 1811.697, 1817.064, 1800.701, 1800.89, 1817.818, 1799.273, 1806.05, 1798.292, 1788.32, 1791.058, 1795.891, 1767.341, 1784.832, 1776.62, 1771.721, 1784.372, 1763.84, 1767.396, 1762.946, 1779.904, 1775.571, 1760.61, 1779.586, 1761.688, 1756.527, 1757.675, 1745.991, 1744.358, 1758.595, 1751.078, 1765.345, 1740.44, 1728.977, 1732.292, 1742.273, 1725.732, 1722.336, 1730.927, 1717.082, 1720.106, 1724.663, 1732.68, 1733.881]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:26 epochs:100	100	2000	True	24040.39844		416732	11	-1	240.95374083518982	{'train_loss': [521653.5, 353874.938, 300281.0, 271830.781, 261594.188, 255273.172, 251817.703, 248642.0, 245842.641, 243854.094, 242816.297, 241049.719, 239196.578, 238281.375, 237291.547, 236353.203, 234897.719, 233835.141, 232887.469, 232774.953, 230938.406, 229480.0, 228139.844, 227361.297, 225899.719, 223743.859, 223403.75, 221193.156, 220115.656, 219057.172, 217387.672, 217028.156, 216287.75, 214892.672, 214360.422, 212770.047, 212827.547, 212333.328, 211064.094, 209567.875, 209568.188, 208889.391, 209114.422, 207784.016, 207525.844, 206413.672, 205936.078, 206248.469, 206259.609, 204012.359, 204896.312, 204028.656, 203293.516, 202857.75, 202219.766, 201832.953, 201444.219, 201662.75, 200471.25, 200509.984, 200187.297, 199811.953, 199573.984, 200002.297, 198323.266, 198945.469, 197674.203, 197615.219, 197981.453, 197546.562, 196800.078, 198023.922, 195817.453, 195510.828, 197202.266, 195688.078, 195946.266, 195558.172, 194837.625, 194843.672, 194045.734, 195078.469, 192779.922, 193631.172, 193236.484, 192474.281, 193318.516, 192579.406, 192964.141, 191224.969, 191920.375, 192626.703, 191846.219, 190182.234, 190655.875, 190896.031, 191095.703, 189958.578, 190174.0, 190676.188], 'val_loss': [3199.771, 2619.505, 2263.882, 2165.787, 2128.115, 2105.477, 2052.524, 2044.235, 2026.04, 2022.086, 1999.771, 2000.3, 1985.556, 1976.946, 1974.371, 1979.711, 1952.932, 1948.284, 1933.733, 1941.393, 1926.297, 1901.703, 1900.296, 1897.864, 1865.667, 1877.618, 1848.972, 1827.797, 1843.807, 1838.137, 1832.391, 1811.775, 1811.597, 1800.357, 1779.143, 1789.724, 1794.047, 1759.235, 1761.214, 1775.512, 1758.114, 1794.421, 1778.882, 1781.621, 1760.335, 1748.369, 1750.709, 1747.314, 1752.165, 1752.656, 1737.122, 1755.697, 1737.681, 1729.869, 1726.177, 1749.342, 1749.457, 1744.816, 1731.059, 1739.629, 1708.28, 1718.906, 1718.451, 1723.217, 1719.873, 1711.722, 1692.553, 1712.465, 1705.89, 1718.592, 1724.722, 1714.493, 1693.929, 1709.235, 1707.309, 1695.084, 1711.25, 1693.025, 1703.68, 1717.581, 1692.651, 1667.397, 1673.636, 1675.47, 1722.505, 1683.882, 1682.558, 1686.054, 1669.701, 1674.873, 1686.887, 1697.844, 1673.591, 1668.891, 1704.609, 1711.847, 1739.746, 1672.181, 1706.452, 1682.657]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:46 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:39 epochs:100	100	2000	True	24189.81641		389940	11	-1	199.46491527557373	{'train_loss': [541812.25, 372382.969, 309249.125, 282723.062, 270153.625, 262692.75, 258514.125, 255082.656, 252493.359, 250485.609, 248531.969, 246601.453, 244996.359, 243897.234, 240798.812, 239454.516, 237272.688, 235451.484, 233092.922, 231582.016, 230830.438, 228042.938, 226788.766, 226355.938, 224351.281, 224104.688, 222579.828, 221997.875, 220696.125, 219567.484, 219366.016, 218370.469, 217622.688, 216635.969, 215435.938, 215106.766, 214657.703, 213790.953, 213574.828, 213343.375, 212689.953, 212331.703, 210885.469, 211609.375, 210331.984, 209845.578, 210421.359, 209021.688, 209358.406, 207753.453, 208334.656, 207621.891, 207255.766, 206887.625, 205936.922, 206824.969, 206276.641, 204886.188, 204761.766, 203790.656, 203766.297, 203865.5, 204064.078, 203831.156, 202923.797, 202844.969, 202795.078, 202751.969, 201939.891, 200795.156, 201278.734, 201919.766, 200783.609, 200726.641, 200278.453, 199613.281, 199665.062, 199901.969, 199935.734, 198838.891, 200323.859, 197977.312, 199195.484, 197962.828, 199226.922, 197927.406, 197287.812, 197202.25, 196977.047, 196730.672, 196324.781, 197478.188, 196159.484, 195929.469, 195510.344, 196431.609, 196014.703, 195268.875, 194101.344, 195293.172], 'val_loss': [4737.036, 3791.199, 3649.338, 3325.236, 3256.411, 3194.527, 3180.37, 3168.728, 3113.965, 3105.927, 3075.594, 3064.598, 3055.417, 3019.227, 3075.192, 3011.785, 3040.591, 2964.841, 2959.911, 2964.42, 2929.028, 2900.662, 2912.533, 2927.922, 2839.862, 2845.66, 2872.2, 2821.119, 2833.861, 2831.372, 2782.221, 2753.371, 2766.291, 2819.725, 2769.366, 2733.667, 2727.15, 2697.88, 2821.178, 2763.344, 2745.068, 2767.194, 2687.874, 2672.849, 2665.928, 2703.525, 2701.741, 2630.123, 2647.203, 2645.767, 2658.906, 2644.316, 2671.216, 2649.424, 2650.306, 2629.97, 2613.313, 2671.305, 2595.591, 2606.959, 2599.692, 2598.347, 2604.849, 2603.468, 2622.353, 2629.258, 2631.661, 2624.381, 2606.968, 2618.332, 2620.234, 2601.6, 2608.119, 2581.78, 2585.377, 2566.144, 2563.241, 2571.725, 2578.335, 2582.489, 2588.78, 2561.98, 2550.92, 2552.153, 2545.794, 2586.52, 2556.666, 2513.811, 2566.437, 2535.296, 2537.331, 2510.423, 2561.929, 2519.236, 2535.794, 2520.803, 2576.375, 2519.627, 2584.523, 2535.004]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:92 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:105 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:123 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:12 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:14 epochs:100	100	1000	True	27941.12305		2278527	10	-1	353.5800995826721	{'train_loss': [1086588.75, 1097304.25, 406113.031, 294138.344, 273154.062, 261836.703, 258115.969, 254087.969, 251345.719, 247972.125, 245670.938, 243865.516, 242517.422, 240937.531, 239736.172, 238159.109, 237163.484, 235456.5, 235206.484, 234700.688, 233165.312, 232293.562, 232099.359, 230637.609, 231188.297, 229752.906, 228654.156, 228885.672, 227735.859, 227217.141, 226606.109, 226862.656, 225991.203, 225769.734, 224425.203, 224725.312, 224296.922, 222812.094, 223265.781, 221949.0, 222689.016, 221704.641, 221078.797, 220802.5, 220464.094, 219539.375, 219676.641, 219115.422, 218099.969, 219429.562, 217570.594, 216865.312, 216864.672, 216147.297, 216064.375, 215284.703, 213838.562, 213862.016, 213975.766, 212613.391, 212468.391, 212708.375, 211754.781, 211846.594, 211239.344, 210788.922, 209926.969, 210113.609, 209354.953, 208920.234, 209028.766, 208403.344, 208043.484, 207387.031, 207497.625, 206840.203, 206505.875, 206186.656, 205895.047, 205173.938, 204078.656, 204779.906, 204458.453, 204058.484, 203756.484, 202384.969, 202641.594, 201858.328, 201290.469, 201640.094, 200331.141, 199976.578, 200167.969, 200594.953, 200888.594, 199600.375, 199652.594, 198583.781, 199117.297, 197978.797], 'val_loss': [4937.971, 3788.188, 1384.966, 1281.52, 1236.843, 1233.994, 1171.756, 1163.224, 1154.105, 1139.711, 1137.157, 1145.353, 1123.026, 1129.166, 1135.517, 1126.967, 1122.773, 1118.713, 1138.481, 1153.685, 1121.109, 1134.002, 1122.273, 1147.366, 1130.658, 1118.606, 1154.332, 1118.795, 1136.874, 1112.261, 1121.736, 1113.892, 1134.417, 1120.987, 1123.442, 1118.0, 1119.167, 1134.295, 1129.532, 1112.793, 1127.42, 1131.719, 1101.989, 1118.805, 1120.35, 1119.186, 1114.72, 1111.636, 1107.642, 1120.65, 1112.608, 1107.411, 1111.391, 1135.045, 1110.068, 1125.677, 1109.808, 1106.213, 1117.911, 1137.277, 1133.246, 1125.219, 1141.107, 1118.495, 1106.904, 1126.433, 1115.917, 1122.346, 1128.769, 1130.019, 1127.306, 1113.315, 1115.268, 1112.806, 1136.707, 1116.399, 1114.872, 1129.452, 1124.913, 1112.191, 1106.751, 1105.829, 1104.556, 1118.675, 1115.167, 1091.545, 1132.743, 1119.606, 1104.429, 1080.227, 1093.915, 1119.409, 1121.307, 1104.839, 1101.296, 1091.397, 1109.097, 1102.208, 1092.972, 1081.571]}	100	100	True
