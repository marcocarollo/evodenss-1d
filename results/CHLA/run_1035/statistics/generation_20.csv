id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.09110654586796615 batch_size:51 epochs:100	100	2000	True	2658.38965		657150	18	-1	164.2200174331665	{'train_loss': [6765.455, 6281.85, 6179.047, 6109.847, 5333.059, 4750.228, 4510.643, 4362.28, 4288.814, 4216.406, 4174.26, 4110.624, 4064.726, 3984.887, 3777.115, 3671.699, 3580.6, 3498.585, 3455.24, 3411.851, 3389.51, 3350.289, 3331.84, 3286.579, 3259.278, 3235.993, 3211.753, 3180.778, 3161.491, 3151.415, 3145.082, 3126.578, 3116.646, 3095.0, 3077.296, 3069.032, 3057.566, 3042.65, 3028.574, 3025.528, 3026.653, 3015.229, 3000.858, 2994.421, 2984.48, 2978.701, 2970.43, 2979.09, 2984.259, 2956.169, 2949.914, 2934.362, 2926.564, 2920.576, 2924.076, 2915.026, 2921.863, 2905.193, 2910.995, 2901.11, 2892.966, 2899.835, 2902.927, 2878.872, 2887.879, 2884.675, 2875.779, 2873.323, 2859.165, 2867.053, 2866.333, 2867.029, 2854.608, 2837.379, 2861.844, 2839.686, 2831.714, 2834.079, 2832.533, 2818.344, 2817.098, 2831.078, 2834.667, 2816.002, 2823.022, 2802.528, 2809.646, 2810.018, 2811.391, 2792.059, 2794.755, 2800.322, 2791.224, 2806.216, 2805.739, 2795.999, 2774.401, 2785.515, 2780.714, 2778.776], 'val_loss': [5485.936, 5364.017, 5339.875, 5036.3, 4191.85, 3796.003, 3717.913, 3638.962, 3631.322, 3617.586, 3601.756, 3593.153, 3598.031, 3400.207, 3260.498, 3165.971, 3158.275, 3087.692, 3106.926, 3066.968, 3010.704, 3025.572, 3030.526, 2978.108, 2939.477, 2925.738, 2933.859, 2904.363, 2900.825, 2879.731, 2871.586, 2884.763, 2889.344, 2895.602, 2871.007, 2831.065, 2838.018, 2853.687, 2830.896, 2839.31, 2863.084, 2825.742, 2813.561, 2831.481, 2824.324, 2845.587, 2819.128, 2840.858, 2796.124, 2797.924, 2791.254, 2814.766, 2790.567, 2761.678, 2787.767, 2784.634, 2831.067, 2804.57, 2783.907, 2788.091, 2772.702, 2761.233, 2773.231, 2749.465, 2763.358, 2738.293, 2773.444, 2788.083, 2739.441, 2762.899, 2738.786, 2732.861, 2754.528, 2738.87, 2771.819, 2769.536, 2761.956, 2728.918, 2748.694, 2718.986, 2798.59, 2716.361, 2728.073, 2730.115, 2740.004, 2715.15, 2699.857, 2779.316, 2691.349, 2701.374, 2712.654, 2762.345, 2711.863, 2711.199, 2699.93, 2699.054, 2732.946, 2693.911, 2711.345, 2688.143]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:74 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:96 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:36 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:95 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:74 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:38 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adadelta lr:0.09110654586796615 batch_size:69 epochs:100	100	1000	True	3626.59058		2447158	17	-1	154.76797223091125	{'train_loss': [10687.358, 10238.595, 9456.554, 8356.41, 7561.481, 7222.081, 6849.284, 6602.815, 6504.437, 6272.275, 6197.286, 6090.529, 5955.472, 5888.977, 5795.792, 5737.135, 5649.109, 5573.838, 5511.35, 5470.561, 5368.355, 5334.662, 5311.208, 5241.285, 5173.922, 5169.523, 5078.864, 5018.995, 4937.439, 4857.886, 4794.716, 4669.206, 4627.8, 4559.982, 4503.199, 4449.691, 4397.25, 4370.202, 4344.567, 4334.389, 4305.192, 4260.45, 4240.193, 4200.391, 4168.174, 4168.84, 4158.668, 4142.821, 4145.877, 4131.118, 4096.926, 4093.417, 4076.74, 4064.519, 4069.894, 4043.203, 4055.787, 4015.506, 4024.47, 3998.19, 4033.578, 3988.949, 3975.324, 3992.521, 3966.685, 3985.328, 4049.957, 3907.376, 3908.31, 3945.845, 3941.593, 3926.676, 3917.36, 3892.563, 3928.103, 3896.044, 3877.78, 3897.82, 3879.09, 3866.331, 3877.45, 3858.652, 3886.61, 3856.812, 3833.631, 3897.627, 3840.665, 3838.054, 3839.431, 3884.829, 3804.531, 3846.243, 3829.746, 3847.259, 3813.896, 3801.931, 3779.098, 3809.869, 3791.874, 3796.016], 'val_loss': [9603.815, 11796.41, 9741.377, 7960.35, 6880.505, 6635.799, 5944.722, 5827.37, 5772.842, 5670.901, 5342.585, 5272.123, 5256.245, 5042.93, 5236.859, 4920.067, 4811.263, 4762.206, 4749.988, 4769.12, 4802.793, 4640.853, 4538.178, 4557.839, 4610.046, 4600.765, 4509.193, 4667.302, 4423.898, 4466.102, 4577.746, 4352.99, 4220.138, 4211.979, 4237.421, 4251.625, 4092.89, 3939.402, 3983.853, 3909.098, 3948.676, 3810.863, 3981.986, 3931.186, 3880.218, 3842.375, 3849.865, 3881.541, 3965.631, 3767.771, 3925.736, 3973.353, 3961.856, 3933.858, 3806.345, 3815.245, 3787.734, 3765.656, 3833.875, 3835.409, 3870.86, 3808.232, 3869.194, 3754.02, 3707.217, 3681.289, 3741.065, 3680.174, 3750.921, 3706.834, 3694.001, 3804.781, 3733.863, 3647.122, 3708.191, 3637.008, 3677.861, 3605.649, 3658.388, 3663.66, 3699.462, 3721.391, 3628.382, 3641.345, 3640.74, 3620.217, 3648.041, 3634.951, 3554.44, 3639.283, 3631.512, 3613.174, 3595.863, 3682.184, 3632.664, 3547.776, 3633.936, 3592.193, 3598.815, 3590.971]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:70 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.09110654586796615 batch_size:51 epochs:100	100	1000	True	2647.27393		646909	18	-1	161.8524875640869	{'train_loss': [6791.999, 6280.848, 6174.161, 6102.001, 5619.417, 4818.495, 4249.529, 4006.611, 3840.31, 3706.571, 3635.033, 3569.872, 3490.755, 3448.792, 3403.254, 3346.0, 3309.922, 3281.568, 3249.559, 3221.487, 3209.804, 3165.532, 3155.311, 3128.545, 3112.817, 3086.343, 3080.028, 3053.323, 3033.911, 3034.982, 3014.831, 3020.245, 3002.999, 2990.63, 2974.727, 2975.928, 2962.3, 2953.567, 2936.843, 2940.539, 2931.508, 2905.984, 2910.602, 2892.582, 2893.696, 2885.497, 2875.282, 2889.758, 2881.303, 2866.071, 2860.75, 2860.5, 2850.055, 2858.118, 2854.244, 2841.784, 2824.545, 2843.864, 2817.791, 2819.91, 2817.266, 2818.2, 2821.743, 2800.283, 2806.838, 2797.533, 2811.395, 2789.781, 2793.208, 2778.268, 2776.525, 2781.577, 2772.149, 2769.608, 2778.083, 2775.105, 2754.658, 2769.804, 2751.015, 2759.25, 2749.708, 2739.27, 2752.871, 2742.062, 2748.073, 2754.7, 2748.952, 2731.022, 2738.936, 2754.009, 2732.617, 2737.158, 2720.852, 2718.326, 2727.03, 2711.592, 2730.58, 2713.37, 2727.229, 2702.26], 'val_loss': [5494.204, 5377.186, 5342.095, 5277.042, 4110.948, 3775.56, 3540.127, 3355.939, 3246.376, 3250.215, 3156.484, 3087.109, 3054.952, 3035.582, 3038.239, 2970.862, 2930.466, 2913.436, 2907.323, 2920.422, 2928.143, 2891.781, 2922.874, 2942.167, 2875.451, 2836.212, 2853.428, 2859.612, 2797.455, 2812.655, 2853.9, 2847.632, 2826.953, 2825.776, 2781.047, 2823.745, 2820.826, 2813.553, 2804.159, 2789.916, 2772.052, 2811.534, 2800.835, 2828.992, 2806.428, 2768.487, 2768.529, 2798.544, 2761.815, 2810.866, 2803.477, 2793.147, 2750.793, 2762.148, 2787.914, 2796.478, 2721.672, 2769.467, 2718.28, 2740.46, 2699.75, 2735.543, 2694.725, 2707.927, 2758.971, 2722.379, 2725.084, 2725.463, 2730.944, 2703.786, 2696.056, 2675.251, 2696.818, 2681.51, 2714.792, 2687.267, 2716.484, 2679.323, 2700.077, 2661.723, 2697.66, 2694.761, 2685.007, 2657.573, 2683.23, 2701.125, 2664.236, 2670.068, 2685.675, 2676.973, 2700.026, 2667.355, 2670.733, 2667.294, 2642.997, 2660.259, 2613.079, 2686.56, 2674.027, 2635.834]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:82 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:18 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:78 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:8 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.08710559782690344 batch_size:51 epochs:100	100	1000	True	2617.34058		569030	19	-1	168.428941488266	{'train_loss': [6908.548, 6318.012, 6195.168, 6082.357, 5494.876, 4890.894, 4600.941, 4421.551, 4255.084, 4062.906, 3909.427, 3797.211, 3721.946, 3654.856, 3592.404, 3552.186, 3496.096, 3465.06, 3416.153, 3367.741, 3355.311, 3316.551, 3271.172, 3247.801, 3243.668, 3213.479, 3188.65, 3174.52, 3156.935, 3154.126, 3129.615, 3104.394, 3093.803, 3104.189, 3088.522, 3069.645, 3041.273, 3055.95, 3031.032, 3017.869, 3016.786, 2991.054, 2996.92, 2993.167, 2993.155, 2995.028, 2975.533, 2968.408, 2966.395, 2961.237, 2953.57, 2949.444, 2933.551, 2940.862, 2930.123, 2918.271, 2920.835, 2919.712, 2911.476, 2906.048, 2897.071, 2894.54, 2906.035, 2904.478, 2889.309, 2878.063, 2880.289, 2889.263, 2886.145, 2876.975, 2888.127, 2881.694, 2871.927, 2863.957, 2853.405, 2862.515, 2852.754, 2847.11, 2856.786, 2844.687, 2839.718, 2837.594, 2843.389, 2831.165, 2833.978, 2839.958, 2831.296, 2827.934, 2826.874, 2833.0, 2819.988, 2820.093, 2813.741, 2812.712, 2814.273, 2814.503, 2819.507, 2808.307, 2802.263, 2800.173], 'val_loss': [5590.442, 5411.548, 5341.099, 5078.413, 4145.861, 3894.716, 3790.928, 3649.006, 3526.678, 3419.568, 3293.106, 3310.982, 3261.974, 3206.394, 3169.868, 3170.332, 3176.25, 3117.848, 3068.558, 3082.463, 3042.375, 2984.492, 3034.068, 2973.429, 2956.609, 2986.252, 2968.556, 2917.236, 3000.341, 2862.943, 2853.81, 2870.819, 2832.683, 2898.549, 2852.917, 2829.406, 2821.242, 2816.031, 2838.245, 2798.462, 2832.279, 2777.104, 2824.478, 2766.317, 2747.47, 2815.092, 2765.382, 2773.651, 2732.604, 2755.456, 2764.105, 2777.36, 2768.016, 2757.195, 2744.361, 2735.603, 2749.97, 2747.874, 2741.892, 2724.365, 2738.948, 2721.445, 2701.685, 2730.737, 2714.277, 2709.529, 2718.568, 2726.858, 2736.583, 2718.436, 2704.475, 2730.861, 2694.802, 2704.865, 2678.074, 2725.609, 2702.191, 2688.781, 2698.952, 2689.058, 2713.698, 2694.847, 2690.694, 2678.517, 2689.939, 2699.511, 2672.238, 2702.577, 2672.131, 2673.911, 2710.235, 2667.246, 2698.664, 2664.156, 2674.362, 2685.628, 2671.029, 2672.49, 2664.239, 2647.539]}	100	100	True
