id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.05229362301733192 batch_size:50 epochs:100	100	1000	True	2763.40356		2075556	19	-1	156.6979467868805	{'train_loss': [7621.651, 7284.161, 6396.812, 5474.347, 5011.559, 4756.082, 4595.271, 4444.279, 4334.544, 4234.095, 4142.62, 4073.027, 4007.856, 3933.542, 3891.857, 3818.128, 3763.121, 3718.834, 3659.685, 3609.006, 3564.769, 3505.398, 3457.467, 3438.361, 3368.634, 3340.659, 3318.462, 3270.78, 3259.896, 3237.182, 3205.968, 3198.5, 3177.625, 3147.428, 3135.774, 3117.034, 3106.798, 3078.87, 3078.364, 3052.053, 3044.994, 3028.554, 3007.919, 3004.993, 2998.382, 2983.313, 2976.972, 2955.167, 2938.247, 2928.846, 2923.919, 2925.296, 2898.056, 2890.918, 2893.546, 2884.788, 2877.087, 2879.412, 2856.004, 2849.25, 2867.075, 2832.188, 2839.232, 2828.378, 2839.916, 2819.448, 2803.874, 2799.06, 2799.265, 2796.171, 2791.084, 2781.742, 2783.201, 2780.534, 2781.276, 2765.65, 2765.7, 2753.666, 2756.246, 2759.085, 2737.163, 2736.532, 2728.818, 2733.082, 2735.446, 2721.452, 2724.313, 2710.461, 2723.152, 2707.074, 2696.376, 2708.95, 2696.032, 2707.055, 2703.849, 2704.663, 2690.513, 2696.38, 2692.028, 2690.693], 'val_loss': [6770.348, 5803.384, 5106.926, 4604.047, 4169.113, 4149.019, 3740.088, 3621.238, 3706.574, 3607.095, 3493.623, 3491.039, 3449.188, 3335.969, 3263.073, 3338.671, 3263.338, 3213.938, 3193.021, 3127.043, 3129.679, 3135.16, 3085.929, 3042.452, 3101.541, 3076.127, 3059.034, 3061.09, 3029.019, 3025.12, 2955.048, 2998.408, 2974.322, 2986.059, 2930.65, 2946.86, 2978.303, 2945.476, 2947.402, 2936.61, 2906.13, 2930.322, 2928.863, 2926.347, 2868.411, 2859.31, 2868.659, 2889.542, 2827.658, 2853.328, 2834.054, 2848.606, 2833.276, 2818.451, 2809.05, 2780.054, 2785.938, 2783.802, 2795.921, 2761.648, 2756.701, 2814.466, 2753.651, 2754.112, 2745.032, 2758.051, 2785.885, 2753.706, 2766.696, 2789.65, 2783.37, 2740.686, 2783.782, 2787.79, 2762.904, 2750.792, 2733.217, 2765.433, 2725.521, 2773.172, 2747.11, 2740.417, 2730.029, 2726.727, 2758.596, 2712.62, 2752.289, 2720.637, 2670.874, 2751.028, 2702.956, 2710.946, 2690.141, 2726.313, 2696.783, 2725.601, 2725.383, 2725.528, 2760.178, 2722.272]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.05229362301733192 batch_size:50 epochs:100	100	2000	True	2799.63379		2075556	19	-1	190.01685118675232	{'train_loss': [7612.308, 7291.341, 5617.215, 5113.115, 4863.274, 4650.817, 4464.777, 4295.165, 4101.535, 3979.318, 3874.283, 3813.366, 3739.024, 3690.1, 3625.201, 3577.165, 3528.15, 3481.768, 3439.786, 3423.561, 3397.962, 3365.577, 3334.532, 3322.307, 3290.449, 3252.927, 3267.582, 3243.623, 3214.87, 3211.537, 3199.159, 3187.859, 3162.216, 3158.247, 3132.793, 3133.453, 3126.383, 3105.41, 3095.277, 3096.422, 3083.922, 3050.717, 3056.474, 3059.517, 3046.322, 3025.309, 3029.49, 3025.503, 3014.958, 3008.513, 2995.243, 2982.38, 2991.481, 2972.87, 2971.265, 2962.506, 2949.502, 2952.127, 2933.795, 2948.792, 2929.818, 2919.735, 2918.397, 2894.146, 2919.547, 2910.106, 2897.223, 2897.879, 2899.513, 2889.39, 2879.274, 2876.225, 2869.289, 2870.008, 2854.801, 2848.838, 2858.011, 2847.923, 2850.015, 2859.47, 2843.973, 2826.053, 2835.87, 2822.857, 2822.409, 2823.79, 2811.469, 2814.097, 2799.221, 2800.201, 2791.214, 2798.16, 2796.336, 2795.777, 2773.397, 2764.726, 2786.626, 2793.827, 2757.567, 2781.43], 'val_loss': [6879.651, 5524.975, 5266.48, 4421.836, 4779.173, 3882.924, 3812.167, 4173.785, 3553.952, 3796.038, 3453.021, 3553.628, 3518.738, 3340.674, 3512.485, 3279.506, 3196.193, 3171.987, 3363.237, 3167.547, 3228.458, 3146.85, 3069.201, 3241.928, 3087.269, 3205.715, 3042.066, 3068.114, 3164.243, 3068.66, 3003.689, 2984.524, 2965.892, 2978.996, 3028.326, 3023.269, 2999.817, 3031.373, 2954.25, 2972.259, 3007.719, 3010.448, 2916.542, 2956.856, 2953.945, 2973.657, 2941.996, 2895.738, 2909.906, 2944.322, 2896.595, 2877.126, 2866.658, 2863.723, 2897.496, 2949.404, 2905.667, 2842.924, 2892.244, 2917.292, 2896.941, 2847.134, 2850.59, 2907.025, 2868.68, 2850.088, 2817.111, 2879.264, 2829.021, 2785.188, 2814.9, 2837.357, 2766.399, 2797.781, 2862.57, 2819.833, 2828.535, 2766.968, 2766.461, 2795.282, 2797.406, 2791.824, 2785.901, 2753.709, 2799.168, 2818.445, 2837.837, 2757.803, 2813.309, 2732.054, 2779.271, 2830.782, 2778.414, 2795.551, 2737.093, 2764.458, 2747.92, 2781.163, 2802.249, 2758.787]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:deconv1d out_channels:90 kernel_size:7 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:deconv1d out_channels:104 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:52 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:17 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adadelta lr:0.05229362301733192 batch_size:59 epochs:100	100	1000	True	3138.13647		2135932	21	-1	175.82500791549683	{'train_loss': [8985.933, 8613.444, 6818.374, 6037.651, 5719.465, 5507.391, 5325.33, 5182.946, 5066.327, 4911.646, 4763.322, 4630.521, 4534.825, 4435.348, 4384.271, 4296.525, 4273.635, 4224.678, 4162.755, 4144.808, 4061.307, 4048.09, 3996.093, 3973.968, 3913.639, 3875.24, 3871.723, 3823.892, 3807.484, 3773.007, 3765.622, 3741.496, 3707.555, 3691.031, 3657.093, 3652.811, 3615.346, 3611.254, 3612.872, 3581.545, 3544.836, 3550.108, 3530.088, 3515.392, 3507.289, 3491.813, 3495.989, 3458.775, 3452.043, 3438.786, 3428.963, 3419.382, 3405.162, 3400.088, 3379.776, 3393.483, 3361.751, 3371.296, 3349.573, 3333.268, 3326.864, 3329.765, 3336.473, 3318.492, 3314.613, 3309.469, 3296.784, 3301.782, 3267.301, 3259.249, 3269.173, 3250.282, 3249.146, 3245.502, 3246.263, 3234.627, 3229.94, 3235.622, 3232.142, 3223.33, 3195.906, 3203.399, 3204.179, 3197.725, 3197.261, 3182.913, 3177.507, 3187.297, 3171.215, 3171.066, 3183.452, 3171.637, 3159.299, 3163.998, 3153.349, 3151.393, 3136.453, 3141.678, 3142.659, 3155.65], 'val_loss': [7922.601, 6649.56, 6197.865, 5226.425, 4925.492, 4739.132, 4550.504, 4496.22, 4358.915, 4171.182, 4115.201, 3999.458, 4130.557, 3927.276, 3899.852, 3944.467, 3845.593, 3752.821, 3864.343, 3659.866, 3759.323, 3757.71, 3621.802, 3583.684, 3491.719, 3493.964, 3557.236, 3582.959, 3518.833, 3433.794, 3421.787, 3404.888, 3400.906, 3374.18, 3494.52, 3446.31, 3409.063, 3304.419, 3307.918, 3377.676, 3349.117, 3301.78, 3341.433, 3300.793, 3303.808, 3381.088, 3257.72, 3357.462, 3264.483, 3373.456, 3304.195, 3307.359, 3224.022, 3290.307, 3206.151, 3243.944, 3273.444, 3254.858, 3241.449, 3267.466, 3229.243, 3250.869, 3220.007, 3220.05, 3288.966, 3195.508, 3204.032, 3140.82, 3157.654, 3189.608, 3164.62, 3101.004, 3208.356, 3172.195, 3158.446, 3127.765, 3138.787, 3170.442, 3155.195, 3171.314, 3161.385, 3168.134, 3146.609, 3142.85, 3157.793, 3112.986, 3091.416, 3214.918, 3103.667, 3119.368, 3175.909, 3083.131, 3101.737, 3118.366, 3091.945, 3074.647, 3107.126, 3144.823, 3140.317, 3075.191]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:81 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:deconv1d out_channels:32 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:60 kernel_size:5 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:29 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adadelta lr:0.08352169860378392 batch_size:50 epochs:100	100	1000	True	3260.93335		24657391	20	-1	247.8240683078766	{'train_loss': [9911.957, 9981.544, 10204.955, 10364.961, 10488.052, 10536.234, 10535.335, 10491.347, 10374.313, 10233.562, 10056.895, 9814.308, 9504.02, 9126.226, 8651.935, 8085.556, 7407.721, 6607.688, 5588.529, 4919.911, 4421.554, 4048.357, 3860.524, 3739.711, 3585.091, 3495.052, 3431.631, 3351.016, 3311.869, 3235.878, 3192.501, 3178.731, 3124.027, 3101.952, 3060.257, 3043.805, 3010.516, 2983.924, 2978.279, 2947.106, 2929.0, 2903.615, 2891.725, 2864.664, 2853.324, 2848.003, 2824.353, 2817.253, 2796.534, 2796.179, 2784.281, 2773.041, 2769.365, 2752.742, 2733.323, 2733.497, 2716.032, 2711.85, 2700.88, 2703.729, 2694.182, 2687.606, 2685.866, 2666.436, 2661.468, 2661.352, 2651.949, 2632.236, 2632.515, 2624.53, 2623.125, 2624.0, 2613.165, 2610.216, 2603.945, 2584.751, 2590.525, 2583.522, 2580.224, 2567.909, 2565.6, 2566.499, 2546.95, 2545.651, 2545.08, 2539.213, 2524.51, 2536.233, 2529.818, 2515.408, 2513.368, 2508.097, 2508.699, 2505.061, 2500.246, 2488.501, 2493.088, 2481.176, 2482.38, 2472.947], 'val_loss': [9306.622, 10662.209, 10593.459, 11075.229, 10361.658, 9660.838, 9119.071, 8915.479, 8419.441, 8182.032, 7162.798, 6491.604, 6898.577, 6135.817, 6126.692, 5877.825, 5957.572, 5327.704, 4864.017, 4289.922, 3753.015, 3644.297, 3580.493, 3307.08, 3373.667, 3327.599, 3092.622, 3188.913, 3082.899, 3022.361, 3058.795, 3048.113, 3036.791, 3022.29, 2996.749, 3020.069, 3056.833, 2953.961, 2971.004, 2972.248, 2943.549, 2940.343, 2898.688, 2962.277, 2956.582, 2847.367, 2923.972, 2870.209, 2885.367, 2867.882, 2906.669, 2829.928, 2841.585, 2865.816, 2809.818, 2855.797, 2828.047, 2811.999, 2802.263, 2843.388, 2885.013, 2808.681, 2784.327, 2775.667, 2765.332, 2769.344, 2769.454, 2800.924, 2760.508, 2775.585, 2752.905, 2764.57, 2718.157, 2716.629, 2684.483, 2756.27, 2696.614, 2749.936, 2719.2, 2715.494, 2725.349, 2690.033, 2681.461, 2721.525, 2741.45, 2711.515, 2699.129, 2717.925, 2691.849, 2690.913, 2687.231, 2705.183, 2687.988, 2681.59, 2671.952, 2665.423, 2667.182, 2681.783, 2672.978, 2687.555]}	100	100	True
