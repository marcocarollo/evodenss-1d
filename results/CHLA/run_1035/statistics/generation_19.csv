id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.09110654586796615 batch_size:51 epochs:100	100	2000	True	2614.28369		657150	18	-1	144.4843623638153	{'train_loss': [6787.408, 6285.181, 6155.968, 5599.942, 4963.673, 4648.829, 4468.278, 4364.309, 4277.076, 4221.0, 4154.857, 4117.812, 4053.678, 3989.218, 3894.29, 3788.863, 3650.37, 3567.873, 3488.332, 3437.664, 3401.689, 3346.021, 3309.706, 3284.436, 3240.613, 3205.421, 3197.651, 3174.448, 3152.963, 3139.561, 3122.056, 3096.744, 3073.704, 3061.195, 3050.026, 3044.222, 3036.9, 3027.491, 3013.871, 3019.796, 3000.419, 2982.317, 2982.901, 2966.268, 2963.408, 2961.054, 2954.674, 2922.337, 2931.238, 2925.907, 2913.122, 2911.108, 2914.122, 2906.584, 2889.53, 2901.728, 2890.018, 2874.793, 2889.135, 2883.693, 2861.706, 2860.411, 2855.842, 2838.127, 2856.139, 2837.652, 2855.638, 2826.408, 2846.279, 2833.458, 2823.424, 2826.346, 2835.248, 2810.144, 2826.12, 2812.645, 2819.882, 2807.58, 2804.132, 2799.634, 2794.051, 2787.514, 2788.215, 2789.251, 2782.817, 2775.02, 2784.869, 2780.036, 2774.331, 2781.558, 2779.504, 2757.484, 2754.297, 2750.859, 2763.838, 2751.985, 2755.314, 2745.575, 2738.407, 2745.176], 'val_loss': [5543.74, 5365.003, 4908.725, 4327.506, 3948.382, 3748.731, 3699.463, 3612.93, 3607.598, 3580.039, 3593.113, 3558.562, 3551.434, 3482.436, 3379.93, 3270.876, 3184.631, 3118.915, 3113.026, 3032.387, 2974.407, 2980.396, 2952.071, 2921.145, 2911.692, 2885.833, 2861.499, 2878.895, 2868.812, 2855.215, 2835.061, 2826.428, 2854.543, 2818.05, 2845.337, 2819.419, 2779.372, 2869.227, 2770.842, 2804.192, 2785.522, 2801.377, 2772.69, 2777.885, 2797.491, 2769.847, 2761.8, 2780.207, 2728.35, 2744.357, 2765.575, 2730.823, 2747.805, 2742.115, 2696.06, 2712.763, 2723.635, 2698.337, 2749.37, 2694.662, 2696.037, 2713.326, 2704.125, 2730.022, 2731.882, 2746.552, 2731.99, 2706.599, 2712.411, 2689.041, 2672.589, 2692.156, 2677.383, 2667.226, 2697.519, 2697.91, 2703.041, 2667.024, 2657.424, 2670.656, 2716.047, 2648.545, 2650.901, 2631.735, 2648.771, 2642.051, 2667.858, 2664.135, 2675.571, 2625.38, 2659.552, 2662.593, 2639.383, 2631.801, 2620.298, 2614.841, 2648.971, 2621.017, 2606.701, 2604.052]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:41 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:77 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:9 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:84 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:15 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adadelta lr:0.09110654586796615 batch_size:95 epochs:100	100	1000	True	5912.9624		1268338	20	-1	133.4126386642456	{'train_loss': [14674.445, 14101.026, 14017.824, 12195.326, 10038.705, 9382.342, 8990.104, 8670.727, 8421.326, 8238.631, 7991.763, 8176.713, 7679.854, 7535.028, 7434.173, 7363.52, 7269.235, 7159.753, 7101.508, 7050.672, 6954.11, 6841.739, 6844.042, 6757.769, 6748.085, 6654.864, 6650.691, 6621.475, 6547.409, 6500.913, 6493.093, 6384.419, 6388.419, 6376.152, 6322.398, 6320.104, 6262.779, 6234.456, 6234.532, 6194.268, 6182.251, 6146.714, 6166.228, 6101.425, 6124.329, 6082.865, 6043.517, 6049.499, 5987.898, 6010.38, 5993.68, 5994.552, 5977.45, 5952.608, 5943.846, 5918.177, 5936.638, 5866.785, 5870.318, 5888.095, 5850.027, 5845.262, 5845.344, 5819.574, 5812.613, 5840.938, 5787.936, 5802.39, 5802.759, 5784.755, 5752.861, 5772.611, 5780.682, 5751.783, 5725.167, 5723.984, 5708.959, 5710.266, 5668.438, 5704.374, 5701.731, 5667.854, 5679.153, 5668.181, 5640.9, 5630.312, 5621.929, 5598.052, 5630.693, 5628.9, 5618.403, 5616.296, 5600.058, 5581.403, 5576.814, 5555.966, 5557.766, 5565.785, 5562.792, 5583.936], 'val_loss': [14013.015, 13610.954, 17463.65, 9145.588, 8656.676, 8135.245, 8001.788, 7734.29, 7662.048, 7575.611, 7751.533, 7374.431, 7130.72, 6939.952, 7262.999, 7533.66, 6948.85, 7267.586, 6784.071, 7060.388, 6643.319, 6798.188, 6957.897, 6564.817, 6885.795, 7025.004, 6795.19, 6551.835, 6442.195, 6657.159, 6425.745, 6337.588, 6411.887, 6331.115, 6345.283, 6287.183, 6590.55, 6467.245, 6067.698, 6473.298, 6243.931, 6303.523, 6341.043, 6208.451, 6190.569, 6418.504, 6115.042, 6038.289, 6185.696, 6069.28, 6162.285, 6227.64, 6219.442, 6331.417, 6590.547, 6324.197, 6140.778, 6228.906, 6255.285, 6444.757, 6149.375, 6096.125, 6009.398, 6055.39, 6216.454, 5923.787, 6411.922, 6063.504, 6027.355, 5954.464, 6077.518, 5969.559, 5898.929, 5959.503, 5913.168, 6087.805, 5981.322, 5792.862, 5765.723, 5990.52, 5973.471, 5828.863, 6003.597, 5888.452, 5785.15, 5854.13, 5785.812, 6089.058, 5874.766, 5764.096, 5702.162, 5824.516, 5795.734, 5783.625, 5794.263, 5905.601, 5862.8, 5823.513, 5914.591, 5813.537]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:58 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:87 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:121 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:58 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:79 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.08886846507692439 batch_size:51 epochs:100	100	1000	True	2709.56372		952611	19	-1	149.76177549362183	{'train_loss': [7131.568, 6603.442, 6400.233, 5712.818, 5007.475, 4710.206, 4451.356, 4191.501, 4019.753, 3904.447, 3829.542, 3762.442, 3700.106, 3634.15, 3591.342, 3526.109, 3472.89, 3447.537, 3402.292, 3363.846, 3310.964, 3296.904, 3245.204, 3231.042, 3190.199, 3180.067, 3156.469, 3140.473, 3124.678, 3104.78, 3089.924, 3075.445, 3052.196, 3050.466, 3046.258, 3037.233, 3028.029, 3002.941, 2998.968, 2988.808, 2962.483, 2957.609, 2965.471, 2955.697, 2940.682, 2929.44, 2945.096, 2925.446, 2916.672, 2898.366, 2905.678, 2894.144, 2898.921, 2883.908, 2897.353, 2873.14, 2875.128, 2870.519, 2844.502, 2848.646, 2853.268, 2851.892, 2845.929, 2841.999, 2835.24, 2828.832, 2822.623, 2817.013, 2799.878, 2807.807, 2805.771, 2808.793, 2794.811, 2790.421, 2797.146, 2801.156, 2797.63, 2789.085, 2773.24, 2779.533, 2790.15, 2768.663, 2783.341, 2771.038, 2761.039, 2756.59, 2758.356, 2755.172, 2748.371, 2745.08, 2750.349, 2745.58, 2754.706, 2728.585, 2734.384, 2740.817, 2733.995, 2729.453, 2722.482, 2731.907], 'val_loss': [5786.41, 5574.403, 4982.22, 4383.094, 4099.112, 3960.476, 3806.526, 3694.587, 3450.403, 3403.037, 3403.953, 3329.451, 3227.275, 3178.315, 3288.495, 3146.093, 3168.161, 3233.919, 3137.261, 3062.307, 3146.251, 3076.83, 3214.844, 3129.462, 3008.964, 2968.702, 3067.345, 3094.946, 2962.912, 3004.11, 3049.603, 3031.775, 2971.398, 3031.016, 3051.97, 2897.466, 3058.193, 2940.718, 2950.602, 2977.902, 3007.923, 2987.548, 3012.385, 2946.957, 2973.933, 2891.363, 2947.412, 2941.18, 2898.413, 2876.531, 2976.009, 2863.322, 2860.948, 2961.893, 2828.231, 2926.506, 2815.863, 2822.515, 2856.823, 2796.732, 2836.682, 2816.975, 2786.631, 2818.768, 2813.508, 2883.696, 2813.394, 2887.1, 2817.836, 2741.293, 2824.759, 2777.51, 2828.034, 2733.748, 2744.614, 2843.615, 2788.96, 2729.732, 2858.842, 2771.698, 2749.694, 2718.192, 2714.801, 2773.983, 2780.532, 2707.271, 2781.51, 2716.216, 2750.442, 2690.787, 2842.719, 2702.375, 2709.545, 2665.356, 2716.879, 2767.932, 2720.344, 2691.055, 2767.113, 2713.139]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:deconv1d out_channels:43 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.09110654586796615 batch_size:52 epochs:100	100	2000	True	2679.12939		749586	18	-1	150.92227220535278	{'train_loss': [7234.09, 6692.94, 6538.533, 5877.725, 4846.287, 4506.973, 4279.008, 4133.971, 4019.691, 3924.474, 3841.009, 3760.335, 3707.169, 3635.137, 3583.974, 3535.602, 3486.644, 3451.606, 3414.135, 3375.955, 3354.764, 3323.934, 3288.37, 3271.118, 3245.219, 3218.445, 3202.661, 3191.66, 3159.823, 3153.546, 3133.761, 3122.032, 3110.48, 3095.707, 3085.201, 3077.104, 3072.905, 3056.543, 3052.446, 3043.423, 3040.436, 3028.107, 3013.404, 3008.781, 2990.671, 3006.13, 2983.396, 2990.335, 2978.927, 2971.849, 2957.281, 2964.653, 2961.404, 2956.931, 2950.461, 2943.863, 2945.146, 2937.386, 2945.636, 2920.018, 2921.048, 2920.401, 2914.961, 2913.95, 2916.282, 2911.55, 2905.416, 2905.86, 2885.993, 2897.302, 2884.745, 2892.26, 2873.266, 2888.646, 2884.172, 2881.071, 2867.36, 2874.456, 2868.148, 2863.757, 2873.831, 2853.355, 2859.87, 2867.283, 2849.549, 2853.069, 2847.278, 2841.747, 2844.835, 2853.076, 2848.342, 2847.49, 2831.008, 2835.997, 2828.145, 2834.615, 2839.362, 2833.444, 2823.185, 2817.485], 'val_loss': [5966.379, 5708.953, 5532.819, 4326.267, 3947.958, 3580.362, 3730.764, 3510.605, 3408.922, 3326.328, 3151.354, 3234.662, 3176.954, 3181.796, 3111.133, 3099.272, 2984.198, 3010.775, 3067.924, 2962.815, 2995.851, 3077.897, 3030.359, 2953.886, 2922.923, 2951.454, 2988.572, 2936.482, 2962.428, 2948.374, 2910.384, 2901.025, 2971.376, 2903.573, 2870.132, 2881.784, 2913.183, 2929.635, 2911.893, 2829.079, 2878.751, 2803.98, 2804.63, 2999.869, 2810.137, 2882.66, 2853.142, 2808.979, 2792.062, 2828.046, 2852.905, 2817.91, 2792.06, 2747.531, 2831.754, 2815.781, 2919.55, 2850.652, 2792.838, 2818.855, 2834.444, 2810.398, 2798.156, 2823.801, 2762.334, 2760.75, 2743.445, 2774.731, 2747.319, 2717.38, 2814.339, 2710.936, 2736.459, 2733.068, 2749.053, 2738.877, 2728.337, 2730.127, 2764.425, 2791.929, 2728.552, 2758.175, 2724.224, 2756.401, 2753.844, 2664.759, 2755.223, 2735.953, 2715.232, 2748.507, 2732.991, 2746.376, 2713.449, 2714.6, 2710.308, 2705.542, 2694.468, 2693.104, 2701.221, 2708.059]}	0	100	True
