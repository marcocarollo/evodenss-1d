id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.0012243082232335878 batch_size:74 epochs:100	100	1000	True	5968.03271		452251	14	-1	107.83720326423645	{'train_loss': [13602.187, 13054.069, 12465.172, 11852.759, 11270.324, 10732.619, 10288.582, 9922.131, 9650.608, 9437.348, 9264.084, 9129.787, 9031.526, 8946.918, 8881.057, 8832.701, 8780.773, 8757.326, 8731.971, 8702.957, 8687.979, 8668.245, 8657.033, 8636.198, 8630.671, 8618.186, 8613.707, 8604.217, 8595.184, 8594.387, 8581.511, 8573.487, 8571.126, 8566.604, 8557.628, 8558.817, 8551.357, 8545.016, 8543.516, 8535.129, 8532.318, 8531.87, 8524.496, 8520.104, 8515.905, 8513.491, 8513.774, 8501.427, 8501.156, 8492.933, 8489.57, 8481.183, 8478.97, 8475.526, 8469.466, 8464.811, 8458.897, 8448.813, 8443.28, 8437.896, 8429.139, 8413.689, 8406.745, 8390.705, 8381.657, 8359.393, 8347.526, 8321.905, 8301.327, 8274.284, 8244.063, 8214.895, 8178.594, 8151.296, 8110.675, 8078.101, 8045.746, 8000.477, 7962.236, 7931.594, 7894.111, 7854.888, 7816.97, 7783.81, 7744.814, 7708.135, 7678.187, 7649.695, 7609.918, 7570.761, 7537.887, 7515.551, 7478.653, 7447.208, 7407.54, 7382.694, 7353.134, 7338.765, 7299.45, 7268.12], 'val_loss': [10297.143, 10093.549, 9773.878, 9344.707, 8920.678, 8577.61, 8297.798, 8097.763, 7947.646, 7819.015, 7716.237, 7632.247, 7566.303, 7512.611, 7469.883, 7432.833, 7403.222, 7379.204, 7362.343, 7342.604, 7326.309, 7310.008, 7299.541, 7283.491, 7274.084, 7262.091, 7253.905, 7241.924, 7232.861, 7224.673, 7215.059, 7210.556, 7201.084, 7194.745, 7187.348, 7181.752, 7175.027, 7168.376, 7163.803, 7158.24, 7152.019, 7147.312, 7143.65, 7138.179, 7133.0, 7125.447, 7121.496, 7118.413, 7112.573, 7107.633, 7102.218, 7096.946, 7091.951, 7086.019, 7079.268, 7073.305, 7065.914, 7058.9, 7050.255, 7038.734, 7028.612, 7016.274, 7003.575, 6986.363, 6971.018, 6949.554, 6927.447, 6901.641, 6873.996, 6841.406, 6808.615, 6771.661, 6734.172, 6695.8, 6656.146, 6617.16, 6576.996, 6539.666, 6499.998, 6462.814, 6426.876, 6391.249, 6356.257, 6324.612, 6291.433, 6261.464, 6230.771, 6202.34, 6172.553, 6145.283, 6120.165, 6092.708, 6068.884, 6043.492, 6019.432, 5996.467, 5973.551, 5951.259, 5930.094, 5909.391]}	100	100	True
