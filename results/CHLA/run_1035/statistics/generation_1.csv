id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.0012243082232335878 batch_size:74 epochs:100	100	1000	True	6241.7627		452251	14	-1	132.6730933189392	{'train_loss': [13872.224, 13325.829, 12751.146, 12130.727, 11507.302, 10961.57, 10466.881, 10056.248, 9731.576, 9502.208, 9306.867, 9165.851, 9060.577, 8965.481, 8904.62, 8848.258, 8805.329, 8774.787, 8750.873, 8723.616, 8708.908, 8688.714, 8677.432, 8657.217, 8644.073, 8637.556, 8623.486, 8614.476, 8608.249, 8598.239, 8599.016, 8587.364, 8582.512, 8577.313, 8572.104, 8564.731, 8563.446, 8559.875, 8553.966, 8548.07, 8540.941, 8537.715, 8541.383, 8534.36, 8531.7, 8529.094, 8527.749, 8516.452, 8516.594, 8515.152, 8513.152, 8503.707, 8501.556, 8504.528, 8498.462, 8491.518, 8484.151, 8481.969, 8477.71, 8467.935, 8468.815, 8460.908, 8454.127, 8442.881, 8437.248, 8429.944, 8417.747, 8408.69, 8398.875, 8387.606, 8378.431, 8361.202, 8341.78, 8327.068, 8309.561, 8288.038, 8266.546, 8241.103, 8220.125, 8186.954, 8161.247, 8131.476, 8104.088, 8075.819, 8049.958, 8014.83, 7989.0, 7956.907, 7917.889, 7885.595, 7863.688, 7839.052, 7795.9, 7763.065, 7734.443, 7712.172, 7678.307, 7656.632, 7619.227, 7605.392], 'val_loss': [10281.744, 10047.135, 9877.638, 9602.059, 9204.514, 8764.712, 8405.402, 8153.517, 7981.696, 7851.706, 7744.792, 7662.895, 7597.83, 7550.114, 7513.104, 7481.896, 7449.219, 7429.945, 7404.388, 7382.422, 7361.472, 7343.999, 7329.174, 7313.808, 7300.958, 7288.402, 7278.513, 7267.098, 7256.066, 7246.096, 7237.285, 7228.954, 7223.098, 7215.749, 7208.993, 7201.713, 7196.297, 7189.11, 7185.053, 7178.671, 7174.549, 7168.182, 7164.346, 7159.464, 7155.064, 7150.268, 7146.186, 7141.644, 7138.857, 7134.857, 7129.578, 7125.383, 7121.08, 7115.248, 7111.389, 7106.51, 7099.605, 7093.819, 7088.271, 7080.212, 7073.704, 7066.364, 7058.323, 7049.456, 7039.629, 7030.05, 7020.093, 7008.088, 6995.07, 6981.252, 6965.946, 6947.637, 6927.544, 6905.337, 6881.021, 6855.106, 6829.016, 6800.128, 6768.925, 6737.188, 6705.558, 6671.476, 6639.644, 6606.372, 6573.096, 6539.566, 6507.559, 6475.847, 6443.366, 6411.124, 6381.12, 6351.899, 6323.401, 6294.064, 6264.549, 6239.361, 6213.001, 6188.13, 6162.942, 6139.034]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:13 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.003133221785214082 batch_size:74 epochs:100	100	1000	True	4130.68945		2525399	15	-1	162.5578784942627	{'train_loss': [12391.354, 11043.954, 10938.19, 10817.056, 10675.2, 10506.294, 10392.947, 10266.06, 10139.536, 10040.137, 9949.084, 9896.942, 9813.438, 9736.197, 9686.424, 9643.308, 9599.964, 9563.462, 9509.68, 9460.79, 9406.372, 9281.495, 9046.297, 8472.209, 8072.237, 7836.646, 7616.786, 7453.794, 7306.182, 7190.494, 7084.012, 7016.049, 6957.348, 6895.564, 6830.055, 6761.469, 6736.808, 6689.092, 6640.398, 6603.869, 6555.646, 6533.022, 6494.408, 6445.654, 6405.378, 6377.473, 6338.443, 6317.353, 6294.922, 6269.235, 6239.191, 6211.997, 6200.792, 6181.327, 6158.283, 6129.013, 6115.299, 6080.253, 6060.925, 6056.747, 6053.145, 6007.627, 5987.301, 5995.9, 5958.779, 5946.843, 5923.34, 5924.291, 5884.51, 5892.947, 5866.375, 5837.185, 5831.257, 5807.036, 5808.89, 5784.638, 5762.557, 5751.504, 5750.577, 5720.917, 5721.25, 5696.5, 5686.495, 5692.074, 5665.621, 5635.908, 5634.27, 5633.788, 5600.531, 5585.002, 5568.48, 5568.509, 5543.8, 5548.332, 5539.926, 5525.035, 5530.718, 5506.495, 5480.674, 5454.413], 'val_loss': [9171.331, 9508.047, 9221.235, 8886.755, 8549.924, 8328.224, 8122.406, 7978.264, 7832.016, 7711.044, 7625.532, 7538.763, 7490.862, 7447.188, 7389.274, 7374.756, 7338.494, 7303.769, 7272.24, 7206.517, 7136.879, 6965.151, 6527.572, 6083.077, 6007.577, 5805.852, 5651.312, 5515.807, 5397.094, 5347.657, 5254.388, 5189.61, 5115.662, 5066.357, 5002.021, 4970.195, 4907.547, 4861.666, 4824.083, 4786.136, 4728.511, 4695.887, 4662.862, 4646.795, 4614.178, 4572.187, 4572.068, 4539.592, 4525.9, 4496.029, 4473.021, 4463.878, 4445.93, 4437.027, 4414.87, 4403.643, 4397.476, 4365.671, 4348.004, 4343.521, 4325.816, 4322.337, 4304.69, 4299.054, 4287.701, 4286.937, 4258.546, 4258.407, 4247.503, 4237.491, 4228.952, 4212.693, 4201.564, 4200.117, 4206.18, 4190.472, 4175.83, 4182.031, 4169.381, 4169.525, 4158.292, 4144.679, 4140.0, 4147.271, 4129.783, 4123.515, 4121.885, 4119.551, 4105.352, 4100.059, 4098.03, 4092.502, 4084.355, 4077.293, 4076.086, 4079.344, 4057.188, 4054.057, 4054.578, 4054.737]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:49 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:55 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.0012243082232335878 batch_size:82 epochs:100	100	1000	True	8379.59863		513072	16	-1	156.18102765083313	{'train_loss': [15376.123, 14989.796, 14309.325, 13382.808, 12402.864, 11598.411, 10990.28, 10582.739, 10302.562, 10122.661, 10021.661, 9923.602, 9859.29, 9799.517, 9763.755, 9726.427, 9690.933, 9669.979, 9648.536, 9623.529, 9614.124, 9595.859, 9573.904, 9565.646, 9557.367, 9547.444, 9531.716, 9527.828, 9520.842, 9516.331, 9499.007, 9497.912, 9489.22, 9482.746, 9485.001, 9475.191, 9469.627, 9466.491, 9460.861, 9459.167, 9459.017, 9453.994, 9450.054, 9440.493, 9437.802, 9435.378, 9429.556, 9431.169, 9429.984, 9426.358, 9425.784, 9426.062, 9416.937, 9419.501, 9415.558, 9414.167, 9410.757, 9407.017, 9406.286, 9404.386, 9402.247, 9399.849, 9401.356, 9394.65, 9391.854, 9394.242, 9392.928, 9389.534, 9389.337, 9384.938, 9387.637, 9391.671, 9381.25, 9383.002, 9379.927, 9380.83, 9375.723, 9372.05, 9375.834, 9372.939, 9372.486, 9374.735, 9366.89, 9368.633, 9367.512, 9363.896, 9357.816, 9359.837, 9360.583, 9359.676, 9360.893, 9357.44, 9352.845, 9353.829, 9350.214, 9350.919, 9348.492, 9349.23, 9345.152, 9343.102], 'val_loss': [12417.513, 12135.221, 11879.57, 11398.602, 10827.35, 10388.502, 10116.477, 9935.274, 9789.474, 9662.425, 9552.57, 9452.604, 9361.157, 9277.002, 9198.622, 9136.376, 9079.256, 9032.48, 8981.715, 8942.333, 8905.431, 8873.875, 8846.309, 8820.436, 8798.539, 8773.567, 8757.224, 8742.035, 8727.362, 8712.095, 8701.348, 8689.661, 8679.404, 8673.266, 8663.192, 8653.935, 8645.559, 8639.471, 8631.619, 8625.566, 8620.354, 8612.732, 8607.516, 8604.325, 8598.055, 8592.532, 8591.081, 8586.706, 8582.317, 8578.746, 8577.245, 8574.602, 8571.965, 8568.321, 8566.261, 8561.819, 8559.534, 8556.343, 8554.441, 8551.756, 8549.419, 8547.078, 8546.234, 8544.401, 8542.517, 8539.926, 8538.803, 8536.837, 8534.349, 8532.838, 8530.609, 8530.062, 8527.142, 8526.385, 8525.886, 8523.46, 8522.053, 8520.14, 8519.729, 8517.862, 8516.613, 8515.045, 8513.042, 8510.804, 8509.646, 8509.266, 8507.415, 8505.803, 8503.952, 8502.936, 8502.04, 8499.972, 8497.56, 8496.149, 8494.18, 8492.597, 8490.514, 8489.077, 8486.515, 8483.778]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:81 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:81 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:17 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.0012243082232335878 batch_size:96 epochs:100	100	1000	True	8180.66016		22726408	14	-1	243.94748330116272	{'train_loss': [14801.486, 14274.473, 14263.645, 14221.187, 14183.271, 14163.869, 14133.989, 14101.96, 14075.265, 14048.353, 14005.415, 13981.127, 13923.284, 13910.461, 13861.466, 13800.678, 13795.734, 13762.324, 13693.845, 13641.195, 13637.185, 13552.878, 13541.255, 13466.93, 13446.645, 13379.077, 13321.204, 13269.529, 13218.155, 13158.07, 13099.451, 13024.877, 12953.113, 12853.729, 12744.859, 12620.884, 12505.401, 12356.871, 12150.972, 11939.437, 11663.965, 11494.906, 11283.532, 11183.288, 11089.252, 11038.416, 10940.342, 10894.044, 10807.966, 10716.752, 10695.276, 10629.133, 10575.674, 10523.323, 10455.448, 10394.753, 10405.749, 10347.021, 10307.8, 10254.479, 10218.587, 10187.312, 10181.105, 10128.624, 10075.577, 10043.265, 10034.353, 9966.383, 9959.836, 9911.817, 9921.717, 9882.788, 9826.647, 9808.632, 9804.95, 9753.109, 9731.786, 9724.71, 9685.626, 9665.897, 9621.845, 9614.263, 9561.47, 9556.959, 9522.183, 9520.695, 9498.256, 9473.419, 9453.04, 9413.678, 9383.906, 9368.423, 9314.335, 9307.357, 9310.45, 9304.139, 9263.957, 9258.655, 9237.434, 9196.141], 'val_loss': [12766.91, 13484.536, 13516.604, 13465.989, 13362.003, 13208.057, 13272.263, 13082.942, 13125.641, 13012.248, 12859.395, 12919.184, 12776.811, 12774.0, 12685.03, 12606.11, 12552.739, 12427.969, 12368.437, 12261.134, 12265.574, 12089.459, 12011.371, 12021.643, 11891.931, 11875.736, 11768.834, 11655.773, 11553.522, 11488.773, 11491.662, 11241.322, 11191.896, 11030.826, 10823.662, 10738.467, 10554.249, 10298.146, 10074.071, 9846.348, 9619.92, 9624.85, 9730.816, 9739.502, 9619.878, 9703.701, 9632.826, 9500.17, 9450.942, 9404.998, 9300.094, 9198.846, 9139.311, 9113.753, 9028.532, 9031.913, 9007.368, 8902.297, 8793.117, 8885.865, 8826.391, 8718.446, 8725.795, 8682.331, 8595.109, 8579.569, 8529.921, 8570.79, 8481.372, 8436.176, 8432.77, 8344.391, 8308.046, 8251.724, 8318.465, 8278.138, 8228.271, 8151.065, 8136.74, 8055.678, 8089.18, 8142.388, 8034.126, 7980.011, 7933.89, 7921.693, 7883.112, 7867.889, 7873.191, 7889.983, 7789.022, 7764.111, 7781.829, 7757.644, 7716.083, 7776.233, 7790.144, 7639.737, 7693.401, 7697.439]}	100	100	True
