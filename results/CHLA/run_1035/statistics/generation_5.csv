id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:conv1d out_channels:123 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adadelta lr:0.05229362301733192 batch_size:74 epochs:100	100	1000	True	3800.55566		2066438	17	-1	130.93350672721863	{'train_loss': [11350.98, 10601.716, 8533.218, 7857.827, 7490.487, 7272.48, 6972.124, 6803.2, 6606.199, 6433.99, 6327.261, 6268.514, 6119.333, 6058.707, 5937.011, 5838.758, 5741.84, 5684.987, 5576.625, 5514.071, 5426.899, 5366.907, 5329.987, 5255.771, 5209.447, 5181.952, 5120.033, 5076.838, 5034.04, 4997.58, 4960.281, 4947.507, 4935.886, 4903.032, 4839.967, 4837.135, 4787.218, 4780.515, 4732.979, 4761.155, 4693.279, 4698.779, 4676.578, 4662.125, 4656.055, 4613.613, 4612.141, 4588.406, 4571.946, 4569.676, 4532.087, 4529.847, 4520.018, 4516.342, 4475.267, 4490.915, 4489.875, 4448.306, 4446.026, 4439.391, 4424.417, 4410.742, 4441.18, 4388.488, 4376.243, 4373.826, 4360.173, 4356.742, 4341.59, 4352.56, 4360.322, 4329.054, 4333.489, 4310.916, 4300.557, 4300.924, 4301.618, 4296.988, 4280.782, 4283.469, 4272.824, 4259.238, 4254.504, 4271.586, 4255.519, 4257.748, 4227.736, 4238.578, 4217.657, 4252.46, 4219.716, 4193.308, 4197.335, 4180.458, 4201.191, 4175.349, 4202.61, 4184.934, 4183.04, 4156.618], 'val_loss': [8449.694, 8152.394, 9029.915, 6606.066, 6089.299, 5764.941, 5537.625, 5501.225, 5147.743, 5031.088, 4798.964, 4997.208, 4777.91, 4822.386, 4795.662, 4547.132, 4690.312, 4602.526, 4451.713, 4467.307, 4751.966, 4442.346, 4269.251, 4374.843, 4375.885, 4371.329, 4228.234, 4476.653, 4294.343, 4307.767, 4424.13, 4188.198, 4291.305, 4220.574, 4210.09, 4237.881, 4047.842, 4163.47, 4068.289, 4057.572, 4026.234, 4101.329, 4022.52, 4030.417, 3964.926, 4083.609, 4038.904, 3909.764, 3949.945, 3952.476, 3931.045, 3968.631, 3981.104, 3974.142, 3942.935, 3993.229, 3900.854, 3945.62, 3956.496, 3895.27, 3883.477, 3971.829, 3943.014, 3885.723, 3954.192, 3914.515, 3991.01, 3828.493, 3856.74, 4000.692, 3833.796, 3914.852, 3806.867, 3839.768, 3880.987, 3839.431, 3941.14, 3829.51, 3838.39, 3873.607, 3838.419, 3914.811, 3864.105, 3809.284, 3791.259, 3798.194, 3906.894, 3828.681, 3751.34, 3815.727, 3725.815, 3907.326, 3836.947, 3810.797, 3837.344, 3794.486, 3703.603, 3767.225, 3778.371, 3788.19]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:conv1d out_channels:123 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:34 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:deconv1d out_channels:72 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:gradient_descent lr:0.05229362301733192 momentum:0.7255906626321399 weight_decay:1.0140846408843317e-05 nesterov:False batch_size:74 epochs:100	100	1000	True	22228.16797		2082306	18	-1	131.84253334999084	{'train_loss': [54786.879, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143], 'val_loss': [22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:74 epochs:100	100	1000	True	3641.83179		1272847	18	-1	126.00817465782166	{'train_loss': [11042.361, 10004.303, 7939.708, 7308.499, 6961.282, 6748.172, 6597.503, 6412.375, 6269.341, 6132.414, 6013.301, 5909.52, 5816.946, 5752.519, 5673.988, 5576.268, 5519.254, 5471.146, 5419.948, 5359.622, 5318.482, 5260.852, 5246.661, 5189.663, 5127.775, 5099.949, 5056.817, 5029.523, 5012.354, 4959.088, 4946.629, 4896.727, 4884.926, 4895.891, 4831.84, 4812.467, 4794.308, 4760.503, 4752.542, 4719.04, 4694.145, 4679.433, 4659.016, 4652.999, 4610.394, 4616.306, 4621.091, 4572.281, 4558.644, 4537.671, 4531.596, 4533.453, 4515.56, 4508.577, 4494.02, 4494.688, 4459.847, 4456.871, 4434.013, 4440.804, 4423.644, 4389.974, 4397.309, 4363.875, 4370.387, 4371.755, 4371.174, 4335.92, 4348.947, 4356.67, 4326.803, 4339.97, 4315.925, 4308.456, 4306.071, 4289.328, 4283.916, 4263.947, 4267.426, 4262.459, 4247.977, 4236.444, 4263.866, 4251.273, 4220.736, 4225.313, 4211.563, 4222.263, 4189.307, 4216.299, 4198.779, 4186.268, 4174.756, 4169.764, 4177.457, 4171.277, 4175.972, 4179.338, 4174.867, 4147.373], 'val_loss': [8204.768, 6508.429, 5738.159, 5422.938, 5453.063, 5396.147, 4854.752, 4892.074, 4746.232, 4630.338, 4587.197, 4569.407, 4528.5, 4464.287, 4440.498, 4478.63, 4422.451, 4367.455, 4265.345, 4311.605, 4253.184, 4207.799, 4159.081, 4114.485, 4099.275, 4166.624, 4068.57, 4085.692, 4098.481, 4043.694, 4060.715, 4028.551, 4010.274, 4056.69, 3977.883, 4005.996, 3953.608, 3882.11, 3969.699, 3936.235, 3899.247, 3929.931, 3933.836, 3904.248, 3824.629, 3945.95, 3884.449, 3894.145, 3836.138, 3835.867, 3928.689, 3814.648, 3846.401, 3898.689, 3824.244, 3792.322, 3840.948, 3782.261, 3819.74, 3824.078, 3831.394, 3816.843, 3823.08, 3773.014, 3806.405, 3742.212, 3732.905, 3707.211, 3777.661, 3721.683, 3761.343, 3684.601, 3746.342, 3705.07, 3715.217, 3699.706, 3693.945, 3668.765, 3735.29, 3746.289, 3729.696, 3691.4, 3669.119, 3740.096, 3697.62, 3619.028, 3622.475, 3688.34, 3676.222, 3716.243, 3649.977, 3619.469, 3696.193, 3648.88, 3664.418, 3596.544, 3640.189, 3620.669, 3605.879, 3574.761]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:118 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:123 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:deconv1d out_channels:62 kernel_size:6 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adadelta lr:0.05229362301733192 batch_size:68 epochs:100	100	1000	True	3821.16577		3484950	17	-1	150.49701976776123	{'train_loss': [10313.349, 8633.725, 7885.283, 7695.449, 7030.567, 6715.636, 6406.564, 6129.244, 5967.435, 5722.483, 5619.017, 5522.318, 5323.96, 5240.28, 5176.522, 5143.188, 5061.465, 4957.224, 4974.176, 4891.129, 4846.711, 4834.375, 4746.846, 4747.342, 4706.614, 4681.144, 4657.026, 4603.777, 4614.849, 4582.065, 4565.612, 4537.221, 4506.513, 4473.062, 4468.592, 4424.967, 4414.795, 4419.015, 4394.688, 4363.174, 4379.571, 4338.38, 4301.933, 4304.689, 4292.439, 4300.667, 4275.34, 4268.541, 4242.375, 4232.507, 4224.306, 4219.565, 4224.756, 4218.373, 4198.544, 4157.019, 4166.156, 4185.201, 4148.652, 4169.582, 4152.814, 4140.715, 4118.24, 4124.269, 4106.875, 4118.792, 4099.044, 4093.802, 4093.402, 4060.218, 4059.086, 4050.397, 4069.205, 4047.862, 4078.608, 4030.135, 4035.108, 4029.149, 4021.912, 4023.885, 4019.966, 4010.758, 4006.698, 3973.024, 3999.737, 3990.219, 3976.415, 3983.33, 3949.198, 3992.183, 3967.115, 3963.063, 3942.254, 3946.785, 3957.961, 3939.474, 3939.459, 3927.74, 3924.047, 3920.008], 'val_loss': [9454.573, 8549.237, 7478.82, 6775.696, 6397.205, 5974.783, 5575.663, 5381.006, 5078.559, 5084.402, 5122.454, 4729.551, 4780.009, 4635.623, 4782.093, 4473.815, 4410.174, 4587.305, 4533.017, 4552.711, 4298.607, 4209.852, 4243.23, 4309.349, 4199.902, 4217.224, 4123.793, 4138.364, 4041.111, 4052.755, 4115.426, 4089.244, 4041.965, 4071.188, 3974.073, 4053.188, 4055.768, 4006.163, 4062.049, 4040.817, 3944.121, 3986.738, 3987.247, 3930.394, 3963.85, 3950.294, 3932.066, 3957.924, 3910.793, 3938.256, 3905.499, 3974.838, 3901.662, 3937.249, 3865.718, 3938.343, 3951.027, 3870.685, 3941.24, 3860.434, 3866.164, 3879.152, 3940.347, 3966.947, 3837.823, 3889.685, 3872.705, 3892.582, 3854.502, 3847.748, 3844.026, 3863.469, 3828.5, 3891.528, 3812.861, 3839.944, 3842.229, 3849.944, 3860.183, 3818.376, 3846.646, 3868.347, 3848.615, 3839.612, 3770.518, 3805.998, 3775.153, 3749.524, 3812.693, 3829.675, 3768.589, 3754.454, 3725.757, 3794.841, 3762.329, 3756.359, 3751.557, 3769.585, 3777.046, 3745.634]}	100	100	True
