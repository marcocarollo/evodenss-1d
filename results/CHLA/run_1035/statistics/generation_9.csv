id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.05229362301733192 batch_size:50 epochs:100	100	1000	True	2799.63379		2075556	19	-1	189.51239204406738	{'train_loss': [7612.308, 7291.341, 5617.215, 5113.115, 4863.274, 4650.817, 4464.777, 4295.165, 4101.535, 3979.318, 3874.283, 3813.366, 3739.024, 3690.1, 3625.201, 3577.165, 3528.15, 3481.768, 3439.786, 3423.561, 3397.962, 3365.577, 3334.532, 3322.307, 3290.449, 3252.927, 3267.582, 3243.623, 3214.87, 3211.537, 3199.159, 3187.859, 3162.216, 3158.247, 3132.793, 3133.453, 3126.383, 3105.41, 3095.277, 3096.422, 3083.922, 3050.717, 3056.474, 3059.517, 3046.322, 3025.309, 3029.49, 3025.503, 3014.958, 3008.513, 2995.243, 2982.38, 2991.481, 2972.87, 2971.265, 2962.506, 2949.502, 2952.127, 2933.795, 2948.792, 2929.818, 2919.735, 2918.397, 2894.146, 2919.547, 2910.106, 2897.223, 2897.879, 2899.513, 2889.39, 2879.274, 2876.225, 2869.289, 2870.008, 2854.801, 2848.838, 2858.011, 2847.923, 2850.015, 2859.47, 2843.973, 2826.053, 2835.87, 2822.857, 2822.409, 2823.79, 2811.469, 2814.097, 2799.221, 2800.201, 2791.214, 2798.16, 2796.336, 2795.777, 2773.397, 2764.726, 2786.626, 2793.827, 2757.567, 2781.43], 'val_loss': [6879.651, 5524.975, 5266.48, 4421.836, 4779.173, 3882.924, 3812.167, 4173.785, 3553.952, 3796.038, 3453.021, 3553.628, 3518.738, 3340.674, 3512.485, 3279.506, 3196.193, 3171.987, 3363.237, 3167.547, 3228.458, 3146.85, 3069.201, 3241.928, 3087.269, 3205.715, 3042.066, 3068.114, 3164.243, 3068.66, 3003.689, 2984.524, 2965.892, 2978.996, 3028.326, 3023.269, 2999.817, 3031.373, 2954.25, 2972.259, 3007.719, 3010.448, 2916.542, 2956.856, 2953.945, 2973.657, 2941.996, 2895.738, 2909.906, 2944.322, 2896.595, 2877.126, 2866.658, 2863.723, 2897.496, 2949.404, 2905.667, 2842.924, 2892.244, 2917.292, 2896.941, 2847.134, 2850.59, 2907.025, 2868.68, 2850.088, 2817.111, 2879.264, 2829.021, 2785.188, 2814.9, 2837.357, 2766.399, 2797.781, 2862.57, 2819.833, 2828.535, 2766.968, 2766.461, 2795.282, 2797.406, 2791.824, 2785.901, 2753.709, 2799.168, 2818.445, 2837.837, 2757.803, 2813.309, 2732.054, 2779.271, 2830.782, 2778.414, 2795.551, 2737.093, 2764.458, 2747.92, 2781.163, 2802.249, 2758.787]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.05229362301733192 batch_size:50 epochs:100	100	2000	True	2888.64551		2075556	19	-1	188.51155400276184	{'train_loss': [7647.615, 6160.926, 5291.758, 4928.576, 4741.792, 4568.906, 4454.083, 4323.327, 4204.251, 4108.007, 3996.78, 3889.239, 3837.989, 3789.677, 3709.032, 3665.449, 3626.921, 3585.009, 3538.677, 3515.867, 3487.281, 3446.936, 3423.181, 3393.365, 3350.364, 3328.964, 3329.531, 3299.55, 3278.242, 3254.474, 3241.49, 3233.036, 3210.549, 3194.427, 3173.136, 3165.85, 3152.218, 3136.244, 3122.071, 3124.044, 3093.356, 3088.026, 3076.48, 3069.344, 3062.817, 3056.103, 3037.502, 3025.28, 3020.305, 3002.669, 3010.904, 3009.857, 2987.471, 2997.731, 2982.508, 2972.896, 2955.47, 2956.674, 2946.382, 2945.681, 2927.572, 2944.094, 2936.967, 2932.85, 2924.223, 2913.963, 2903.539, 2903.092, 2903.33, 2903.372, 2898.026, 2883.942, 2883.233, 2889.041, 2886.593, 2872.797, 2864.152, 2868.069, 2847.963, 2859.1, 2840.001, 2831.713, 2851.315, 2840.015, 2821.053, 2818.762, 2823.191, 2812.097, 2811.675, 2816.683, 2801.782, 2792.954, 2800.594, 2780.628, 2777.694, 2771.975, 2776.365, 2775.465, 2770.178, 2760.363], 'val_loss': [6721.826, 5368.787, 4727.364, 4284.245, 4063.366, 4035.874, 3917.476, 3899.881, 3538.587, 3583.543, 3655.657, 3668.246, 3534.59, 3385.835, 3428.68, 3324.213, 3359.522, 3539.288, 3357.995, 3208.317, 3217.615, 3163.44, 3288.053, 3159.354, 3126.212, 3197.165, 3153.745, 3040.91, 3063.864, 3092.795, 3006.836, 3035.103, 3060.518, 3065.999, 2996.387, 3032.874, 2983.976, 2964.503, 2984.57, 2939.007, 2935.745, 3050.362, 2975.89, 3009.81, 2973.186, 3032.052, 2983.188, 2969.012, 2931.243, 2926.317, 2959.25, 2957.781, 2939.05, 2996.596, 2994.912, 2899.63, 2960.365, 2917.304, 2895.196, 2952.552, 2849.325, 2891.532, 2999.888, 2939.914, 2858.727, 2967.764, 2889.773, 2871.689, 2892.377, 2847.021, 2981.254, 2924.458, 2878.242, 2866.815, 2970.55, 2891.524, 2886.371, 2824.921, 2869.622, 2859.343, 2893.68, 2816.699, 2848.695, 2765.493, 2860.227, 2819.271, 2838.121, 2849.491, 2833.301, 2810.589, 2901.033, 2768.71, 2798.915, 2777.018, 2783.848, 2783.23, 2769.238, 2855.41, 2795.854, 2805.594]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:deconv1d out_channels:111 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:91 kernel_size:5 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:83 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:gradient_descent lr:0.05229362301733192 momentum:0.9369192784213591 weight_decay:7.952097225106694e-05 nesterov:False batch_size:50 epochs:100	100	1000	True	16812.67188		6542627	20	-1	233.6182017326355	{'train_loss': [180481072.0, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408], 'val_loss': [16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:86 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:44 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:39 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adadelta lr:0.05229362301733192 batch_size:50 epochs:100	100	1000	True	2899.32715		3426978	20	-1	203.54360914230347	{'train_loss': [7689.489, 7489.924, 7204.437, 6342.257, 5730.021, 5420.592, 5149.025, 5003.805, 4795.293, 4636.302, 4479.723, 4333.618, 4225.686, 4162.338, 4056.681, 4015.449, 3934.19, 3885.906, 3814.937, 3759.595, 3733.733, 3658.216, 3614.591, 3574.921, 3543.845, 3519.2, 3466.999, 3455.865, 3412.734, 3383.298, 3322.377, 3283.408, 3256.067, 3204.544, 3196.227, 3179.906, 3152.76, 3156.148, 3125.249, 3116.23, 3095.594, 3076.687, 3059.717, 3048.865, 3043.954, 3027.667, 3004.035, 3013.401, 2994.252, 2983.267, 2970.054, 2971.336, 2960.662, 2945.185, 2941.423, 2930.494, 2924.487, 2906.605, 2903.394, 2905.96, 2897.157, 2887.507, 2880.192, 2855.778, 2863.444, 2863.828, 2866.332, 2851.769, 2850.454, 2846.051, 2828.355, 2826.092, 2814.999, 2825.792, 2817.225, 2813.823, 2797.256, 2798.919, 2806.826, 2777.088, 2780.323, 2786.907, 2784.285, 2767.412, 2768.156, 2771.217, 2752.537, 2754.703, 2757.689, 2745.414, 2750.599, 2737.221, 2728.287, 2731.711, 2733.347, 2730.476, 2715.396, 2710.786, 2711.432, 2700.83], 'val_loss': [8179.168, 7688.156, 7437.122, 6803.434, 6141.169, 5961.354, 5742.44, 4925.647, 4997.589, 4717.509, 4535.434, 4273.716, 4078.171, 3816.678, 3983.256, 3711.445, 3638.573, 3611.832, 3604.49, 3542.882, 3479.426, 3358.331, 3288.936, 3342.358, 3237.125, 3267.151, 3280.188, 3146.035, 3132.681, 3153.46, 3142.219, 3075.271, 2983.681, 2988.227, 2989.562, 3027.128, 2929.073, 2943.117, 2957.349, 2966.767, 2908.108, 2910.212, 2970.837, 2914.496, 2887.923, 2881.21, 2882.281, 2913.148, 2897.847, 2822.805, 2932.47, 2868.492, 2869.438, 2867.213, 2845.368, 2895.549, 2860.322, 2840.562, 2874.248, 2854.625, 2860.78, 2847.067, 2848.985, 2852.866, 2821.975, 2836.019, 2822.848, 2810.202, 2827.146, 2850.701, 2811.247, 2879.967, 2854.555, 2807.749, 2805.062, 2801.693, 2829.256, 2823.503, 2838.897, 2795.936, 2801.06, 2805.699, 2854.778, 2820.914, 2776.362, 2816.589, 2798.096, 2836.039, 2850.248, 2801.34, 2777.629, 2783.721, 2825.542, 2820.019, 2819.893, 2785.806, 2801.655, 2757.874, 2768.075, 2795.064]}	100	100	True
