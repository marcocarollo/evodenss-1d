id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.05229362301733192 batch_size:50 epochs:100	100	1000	True	2894.1311		2075556	19	-1	183.39777374267578	{'train_loss': [7623.271, 7408.691, 6088.447, 5259.476, 4932.753, 4707.324, 4539.364, 4409.119, 4288.019, 4170.665, 4022.229, 3925.276, 3830.293, 3757.606, 3720.143, 3662.422, 3618.082, 3573.924, 3541.427, 3486.006, 3465.497, 3406.229, 3363.627, 3351.632, 3317.889, 3289.156, 3293.923, 3241.491, 3244.288, 3216.552, 3200.284, 3179.155, 3160.828, 3149.023, 3159.812, 3127.844, 3102.989, 3101.676, 3091.324, 3077.708, 3070.813, 3060.323, 3046.641, 3044.3, 3033.195, 3031.267, 3014.649, 3002.57, 2997.271, 2987.886, 2983.629, 2976.916, 2959.979, 2958.758, 2955.197, 2941.43, 2938.422, 2935.299, 2923.568, 2907.588, 2912.584, 2911.453, 2903.869, 2898.394, 2882.943, 2893.626, 2888.566, 2885.875, 2875.181, 2859.349, 2871.547, 2853.418, 2835.447, 2848.938, 2842.512, 2833.063, 2824.802, 2833.605, 2814.593, 2818.863, 2802.362, 2809.981, 2815.586, 2811.323, 2799.259, 2792.283, 2796.056, 2792.837, 2786.182, 2793.852, 2785.751, 2776.194, 2755.038, 2771.021, 2753.636, 2771.134, 2761.928, 2760.753, 2767.04, 2747.892], 'val_loss': [6784.678, 6390.515, 5205.467, 4525.7, 4119.145, 4002.39, 3834.384, 3814.32, 3693.651, 3575.67, 3546.951, 3579.913, 3348.436, 3354.406, 3365.28, 3437.82, 3243.467, 3286.821, 3254.885, 3222.632, 3175.674, 3111.598, 3091.225, 3021.041, 3048.049, 3019.619, 2999.914, 3015.503, 2952.309, 2941.015, 2930.52, 2944.647, 2955.409, 2961.441, 2920.515, 2959.81, 2969.3, 2961.043, 2959.991, 2932.123, 2904.257, 2910.056, 2894.7, 2842.419, 2857.7, 2915.072, 2863.576, 2851.528, 2863.958, 2860.315, 2815.434, 2820.26, 2876.038, 2843.128, 2858.854, 2833.692, 2829.999, 2805.758, 2827.583, 2787.41, 2813.054, 2784.467, 2883.094, 2754.275, 2765.815, 2771.985, 2851.749, 2765.347, 2738.337, 2771.238, 2796.54, 2791.277, 2765.526, 2724.461, 2771.394, 2730.611, 2788.365, 2807.952, 2768.928, 2776.593, 2755.349, 2768.561, 2771.907, 2764.87, 2739.688, 2782.562, 2818.207, 2785.088, 2736.65, 2734.1, 2784.386, 2743.766, 2779.749, 2746.991, 2773.257, 2741.63, 2743.887, 2754.074, 2767.131, 2821.608]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:44 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:13 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:19 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:47 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:77 kernel_size:5 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:rmsprop lr:0.05229362301733192 alpha:0.992859255097563 weight_decay:3.280420028645099e-05 batch_size:50 epochs:100	100	1000	True	16788.34766		6494956	20	-1	275.4727592468262	{'train_loss': [25804.787, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.434, 17422.408, 17422.408, 17422.545, 17422.408, 47549.707, 72911.375, 17736.432, 56872.586, 17422.408, 17422.408, 149131.641, 17715.178, 30409.164, 107007.453, 17462.65, 17429.826, 17429.055, 17423.541, 17422.408, 17422.408, 17422.408, 17424.037, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17436.963, 17422.408, 17422.408, 17422.408, 17422.408, 521782.312, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408, 17422.408], 'val_loss': [16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:51 epochs:100	100	1000	True	2837.08179		3472828	18	-1	207.84369659423828	{'train_loss': [7813.949, 6645.874, 5960.607, 5813.55, 5376.711, 5222.288, 4871.188, 4646.417, 4472.952, 4308.459, 4219.277, 4128.71, 4042.989, 3943.204, 3932.641, 3832.387, 3797.113, 3750.343, 3703.878, 3668.075, 3628.955, 3594.699, 3559.305, 3539.708, 3510.882, 3485.132, 3444.39, 3448.685, 3408.222, 3390.385, 3376.939, 3377.704, 3334.933, 3344.196, 3313.682, 3301.508, 3292.603, 3273.499, 3273.604, 3233.985, 3216.958, 3217.289, 3187.529, 3198.198, 3175.474, 3177.235, 3148.679, 3143.11, 3120.347, 3097.419, 3107.572, 3092.029, 3079.392, 3094.811, 3057.457, 3064.903, 3064.102, 3058.864, 3040.124, 3028.689, 3020.001, 3011.798, 2999.236, 3005.505, 2981.671, 2998.236, 2976.664, 2971.893, 2956.824, 2962.734, 2966.616, 2936.591, 2928.822, 2944.465, 2926.541, 2924.654, 2927.121, 2890.645, 2897.515, 2897.498, 2896.509, 2885.625, 2879.569, 2875.656, 2868.015, 2876.374, 2859.063, 2848.728, 2848.276, 2832.72, 2844.739, 2837.188, 2817.848, 2814.194, 2817.741, 2793.95, 2789.344, 2807.875, 2784.085, 2786.57], 'val_loss': [7045.868, 6642.991, 6078.283, 5447.219, 5176.896, 4789.735, 4580.258, 4373.81, 4197.887, 4116.141, 3824.829, 3756.401, 3666.216, 3701.767, 3714.201, 3580.925, 3558.629, 3573.563, 3498.355, 3525.14, 3385.909, 3456.981, 3365.817, 3454.501, 3396.872, 3402.748, 3416.713, 3474.929, 3310.667, 3300.624, 3383.855, 3337.726, 3260.667, 3287.422, 3206.507, 3198.255, 3333.665, 3313.238, 3236.988, 3183.181, 3264.756, 3073.946, 3144.744, 3070.325, 3173.993, 3141.413, 3087.44, 3056.911, 3067.189, 3110.772, 3086.976, 3091.41, 3024.836, 3060.542, 3122.162, 3051.901, 3038.399, 3002.521, 2978.004, 3030.141, 2955.262, 3095.47, 3010.396, 2952.761, 2975.533, 2898.893, 3054.358, 2979.854, 2910.241, 2912.787, 2847.835, 2915.452, 2870.601, 2821.896, 2883.608, 2830.794, 2790.693, 2957.35, 2854.422, 2819.443, 2843.65, 2848.114, 2855.115, 2810.777, 2790.675, 2792.018, 2783.966, 2845.779, 2822.411, 2803.243, 2875.596, 2867.438, 2776.583, 2825.713, 2781.141, 2822.268, 2828.026, 2786.217, 2794.548, 2787.211]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:77 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:conv1d out_channels:77 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:105 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:86 kernel_size:9 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.05229362301733192 beta1:0.5014609043081573 beta2:0.8751842880644027 weight_decay:4.620145759705479e-05 batch_size:50 epochs:100	100	1000	True	4219.87012		739767	19	-1	170.59933233261108	{'train_loss': [20072.141, 15361.927, 7214.81, 6929.889, 6843.388, 6822.003, 6960.041, 7967.054, 6716.258, 6841.992, 6321.196, 5819.57, 5564.336, 5250.134, 5193.011, 5172.229, 5066.283, 5004.103, 5055.435, 4898.286, 4941.512, 4973.854, 4883.36, 4828.317, 4849.174, 4873.042, 4776.722, 4870.011, 4854.498, 4765.227, 4785.463, 4728.463, 4779.029, 4705.545, 4751.633, 4810.071, 4652.73, 4736.766, 4721.077, 4647.367, 4684.207, 4628.959, 4656.666, 4617.734, 4622.87, 4602.653, 4578.375, 4616.704, 4587.311, 4605.202, 4565.097, 4549.289, 4540.88, 4525.158, 4506.451, 4435.856, 4475.932, 4447.039, 4399.056, 4426.606, 4444.381, 4444.013, 4417.157, 4387.403, 4444.574, 4398.048, 4421.739, 4402.245, 4358.773, 4411.089, 4415.929, 4372.429, 4507.612, 4318.234, 4339.379, 4390.232, 4379.6, 4374.474, 4316.971, 4339.268, 4330.318, 4315.461, 4391.265, 4386.584, 4309.228, 4289.772, 4297.76, 4312.459, 4322.281, 4329.974, 4338.392, 4323.149, 4377.443, 4340.492, 4347.108, 4311.249, 4360.91, 4300.007, 4340.404, 4352.755], 'val_loss': [16395.939, 8752.704, 7324.913, 6028.859, 12797.313, 6314.141, 5678.133, 7182.252, 6059.779, 6053.112, 6091.695, 4811.651, 5080.181, 4643.223, 4522.072, 4245.824, 4664.975, 4462.713, 4661.464, 4317.415, 4246.415, 4768.747, 4209.57, 4749.436, 4621.475, 4541.696, 4480.029, 4394.309, 4302.864, 4578.869, 4508.973, 4381.086, 4419.899, 4356.307, 4229.105, 4998.686, 4301.711, 4625.681, 4380.535, 4298.849, 4251.183, 4316.426, 4400.243, 4225.797, 4149.519, 4349.12, 4523.834, 4424.889, 4010.715, 4351.088, 4521.632, 4297.517, 4258.333, 4178.994, 3965.07, 4018.111, 4212.735, 4225.881, 3938.027, 4088.434, 4494.378, 4090.633, 4033.527, 4523.15, 4478.245, 4676.731, 4366.364, 4891.367, 4720.68, 4508.709, 4225.687, 5149.155, 4557.913, 4520.812, 4536.302, 4188.074, 4058.043, 4034.26, 3845.347, 4027.392, 4580.976, 4502.271, 4367.674, 3865.592, 4148.122, 4579.216, 3915.82, 4737.266, 4853.778, 4062.334, 4146.106, 3943.704, 5789.216, 4477.827, 4321.903, 4598.975, 4871.211, 3791.385, 4408.686, 4356.718]}	100	100	True
