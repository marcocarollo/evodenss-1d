id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:74 epochs:100	100	2000	True	3733.96362		1272847	18	-1	152.59650015830994	{'train_loss': [11015.082, 10477.562, 8985.267, 7368.722, 6913.295, 6594.561, 6380.655, 6239.104, 6128.138, 6030.218, 5941.583, 5855.135, 5765.87, 5697.297, 5637.853, 5593.62, 5532.092, 5464.178, 5405.877, 5360.031, 5324.655, 5275.733, 5221.047, 5173.97, 5163.166, 5098.469, 5056.439, 5030.643, 4997.353, 4992.807, 4971.519, 4939.03, 4887.067, 4848.187, 4854.423, 4813.879, 4838.196, 4805.678, 4784.14, 4747.366, 4737.315, 4722.416, 4691.011, 4699.447, 4680.562, 4659.333, 4627.036, 4618.045, 4611.909, 4610.383, 4587.094, 4573.037, 4575.948, 4544.231, 4539.14, 4533.573, 4506.961, 4502.201, 4489.931, 4482.825, 4480.095, 4468.727, 4431.143, 4437.797, 4426.763, 4418.561, 4416.511, 4407.385, 4404.008, 4392.563, 4385.254, 4369.558, 4352.578, 4359.314, 4362.971, 4340.14, 4338.267, 4318.298, 4325.704, 4325.358, 4284.367, 4314.17, 4292.341, 4297.619, 4285.304, 4278.48, 4272.319, 4262.759, 4276.115, 4240.156, 4225.481, 4237.585, 4240.692, 4247.762, 4223.154, 4219.353, 4226.498, 4199.769, 4197.121, 4221.375], 'val_loss': [8197.729, 7772.249, 6135.753, 5926.962, 5063.863, 5117.63, 4821.356, 4623.121, 4551.779, 4491.783, 4396.853, 4366.614, 4299.673, 4273.78, 4282.814, 4180.304, 4144.759, 4150.968, 4226.175, 4161.043, 4142.879, 4060.418, 4085.127, 4081.7, 4099.01, 4003.585, 4053.647, 4048.481, 4102.026, 4047.036, 3994.238, 3984.496, 3976.762, 3901.375, 4068.482, 3935.447, 3957.97, 3946.363, 3993.033, 3906.749, 3941.861, 3919.604, 3890.818, 3925.5, 3937.37, 3881.078, 3903.738, 3995.5, 3910.059, 3856.388, 3895.192, 3882.828, 3987.504, 3937.718, 3930.083, 3960.983, 3874.474, 3829.01, 3862.742, 3888.956, 3837.135, 3840.704, 3834.871, 3832.785, 3799.804, 3807.983, 3815.381, 3745.285, 3782.292, 3848.555, 3805.134, 3762.307, 3888.348, 3834.897, 3757.363, 3776.381, 3859.26, 3794.717, 3772.505, 3727.57, 3738.825, 3796.283, 3787.722, 3745.139, 3740.06, 3741.138, 3741.858, 3726.4, 3740.458, 3722.349, 3748.998, 3778.688, 3701.921, 3673.574, 3749.777, 3727.662, 3755.405, 3681.964, 3715.811, 3735.898]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:67 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:89 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:deconv1d out_channels:47 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:deconv1d out_channels:76 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:10 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:93 epochs:100	100	1000	True	5207.54443		24788448	18	-1	275.5274338722229	{'train_loss': [15405.458, 14801.576, 14259.71, 14272.423, 14330.375, 14342.14, 14394.725, 14397.508, 14460.131, 14527.816, 14585.443, 14495.631, 14573.169, 14521.22, 14496.083, 14513.442, 14475.041, 14383.481, 14332.663, 14314.423, 14203.193, 14195.02, 14066.161, 13965.897, 13884.162, 13659.002, 13465.241, 13193.849, 12841.548, 12130.746, 11325.305, 10556.105, 9655.467, 9098.119, 8510.341, 7944.727, 7649.54, 7279.929, 7049.972, 6818.477, 6774.998, 6495.007, 6406.473, 6363.288, 6317.275, 6236.996, 6224.689, 6066.478, 6029.829, 5978.522, 6026.05, 5908.978, 5905.722, 5875.214, 5853.42, 5710.08, 5729.728, 5756.761, 5679.272, 5743.914, 5624.187, 5653.456, 5593.966, 5616.214, 5571.771, 5610.238, 5510.322, 5495.94, 5520.677, 5486.108, 5552.2, 5448.305, 5459.654, 5431.126, 5404.048, 5391.392, 5443.648, 5438.84, 5341.455, 5324.414, 5347.501, 5290.784, 5352.614, 5338.764, 5396.523, 5220.846, 5240.44, 5270.255, 5272.471, 5258.585, 5342.997, 5216.538, 5194.688, 5218.666, 5202.389, 5198.181, 5154.601, 5173.47, 5184.061, 5144.195], 'val_loss': [14081.025, 18916.746, 18729.391, 17905.42, 15866.747, 14724.157, 14988.628, 15344.271, 13713.146, 14572.388, 12978.627, 13102.633, 12818.176, 12206.852, 12038.793, 11636.675, 11267.929, 11395.262, 11070.662, 10422.791, 9900.691, 9778.544, 10064.856, 9486.636, 9514.075, 9148.176, 9057.296, 8310.151, 8182.458, 9779.029, 7988.554, 7310.995, 7152.724, 6660.695, 6284.23, 6550.814, 6534.834, 6095.699, 5941.292, 6166.756, 6015.425, 6002.154, 5685.285, 6263.271, 5736.191, 5891.396, 6019.0, 6039.592, 5679.751, 5625.914, 5696.773, 5468.643, 5864.157, 5427.365, 5195.236, 5595.659, 5621.843, 5890.519, 6105.714, 5238.353, 5333.28, 5218.26, 5293.833, 5192.624, 5154.099, 4911.365, 5138.294, 5052.387, 4999.073, 5077.556, 4915.331, 5147.115, 4982.638, 5021.829, 4996.194, 5063.502, 4958.836, 4954.652, 5161.047, 4904.173, 4886.23, 4981.752, 4793.561, 4813.162, 4724.412, 4851.802, 4816.226, 4819.7, 4869.379, 4868.957, 4748.138, 4730.238, 4809.366, 4665.704, 4737.671, 4655.867, 4697.008, 4878.333, 4695.613, 4676.614]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:78 kernel_size:6 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.05229362301733192 batch_size:74 epochs:100	100	1000	True	3785.56348		2076259	19	-1	171.57341504096985	{'train_loss': [11409.325, 11090.589, 10570.602, 8696.461, 7631.167, 7295.5, 7057.121, 6811.884, 6644.817, 6510.149, 6410.044, 6310.374, 6195.144, 6050.611, 5950.231, 5782.211, 5688.387, 5618.712, 5534.748, 5494.095, 5433.361, 5406.48, 5345.379, 5303.926, 5260.143, 5251.887, 5178.751, 5163.327, 5128.42, 5087.418, 5058.747, 5032.583, 4975.215, 4962.043, 4927.529, 4897.276, 4888.854, 4845.216, 4820.181, 4830.582, 4770.019, 4774.918, 4736.443, 4724.009, 4699.977, 4659.063, 4670.551, 4648.537, 4631.791, 4613.867, 4603.694, 4598.91, 4558.073, 4547.195, 4541.559, 4519.411, 4522.744, 4523.098, 4498.379, 4510.82, 4480.953, 4459.628, 4456.333, 4451.638, 4439.19, 4427.283, 4420.847, 4411.437, 4400.975, 4396.875, 4403.225, 4382.989, 4374.321, 4363.291, 4357.676, 4336.504, 4320.417, 4321.878, 4340.568, 4303.504, 4328.686, 4308.474, 4290.545, 4297.727, 4280.473, 4268.566, 4274.123, 4266.215, 4268.672, 4259.265, 4247.09, 4252.179, 4251.395, 4227.269, 4234.485, 4230.411, 4235.601, 4237.323, 4204.584, 4184.531], 'val_loss': [8561.215, 8414.023, 7286.957, 7459.671, 6769.845, 6799.627, 5974.37, 5639.924, 5941.401, 5547.127, 5442.578, 5207.644, 4990.215, 5171.549, 4731.959, 4821.216, 4696.494, 4673.833, 4624.059, 4506.166, 4784.312, 4408.856, 4452.257, 4419.946, 4517.337, 4339.744, 4290.45, 4296.288, 4140.114, 4281.661, 4221.171, 4316.075, 4272.324, 4080.352, 4202.628, 4100.43, 4121.196, 4118.074, 4172.855, 4056.445, 4137.933, 4167.224, 4097.201, 4073.424, 4016.6, 4017.25, 4033.377, 4087.722, 4155.658, 3943.099, 4150.279, 4009.649, 4002.555, 3892.863, 4006.468, 3976.908, 3991.233, 3944.552, 3953.334, 4035.186, 4003.303, 3863.194, 4000.59, 3917.857, 3942.822, 3958.279, 3878.959, 3889.535, 3861.724, 3903.801, 3839.898, 3939.46, 3842.31, 3761.7, 3779.455, 3868.255, 3814.985, 3755.984, 3870.765, 3873.203, 3859.741, 3738.394, 3929.77, 3816.939, 3885.864, 3798.407, 3826.008, 3877.729, 3814.358, 3738.704, 3749.344, 3831.939, 3805.927, 3856.35, 3801.958, 3800.301, 3817.943, 3706.896, 3732.136, 3731.781]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:74 epochs:100	100	2000	True	3641.83179		1272847	18	-1	126.55913972854614	{'train_loss': [11042.361, 10004.303, 7939.708, 7308.499, 6961.282, 6748.172, 6597.503, 6412.375, 6269.341, 6132.414, 6013.301, 5909.52, 5816.946, 5752.519, 5673.988, 5576.268, 5519.254, 5471.146, 5419.948, 5359.622, 5318.482, 5260.852, 5246.661, 5189.663, 5127.775, 5099.949, 5056.817, 5029.523, 5012.354, 4959.088, 4946.629, 4896.727, 4884.926, 4895.891, 4831.84, 4812.467, 4794.308, 4760.503, 4752.542, 4719.04, 4694.145, 4679.433, 4659.016, 4652.999, 4610.394, 4616.306, 4621.091, 4572.281, 4558.644, 4537.671, 4531.596, 4533.453, 4515.56, 4508.577, 4494.02, 4494.688, 4459.847, 4456.871, 4434.013, 4440.804, 4423.644, 4389.974, 4397.309, 4363.875, 4370.387, 4371.755, 4371.174, 4335.92, 4348.947, 4356.67, 4326.803, 4339.97, 4315.925, 4308.456, 4306.071, 4289.328, 4283.916, 4263.947, 4267.426, 4262.459, 4247.977, 4236.444, 4263.866, 4251.273, 4220.736, 4225.313, 4211.563, 4222.263, 4189.307, 4216.299, 4198.779, 4186.268, 4174.756, 4169.764, 4177.457, 4171.277, 4175.972, 4179.338, 4174.867, 4147.373], 'val_loss': [8204.768, 6508.429, 5738.159, 5422.938, 5453.063, 5396.147, 4854.752, 4892.074, 4746.232, 4630.338, 4587.197, 4569.407, 4528.5, 4464.287, 4440.498, 4478.63, 4422.451, 4367.455, 4265.345, 4311.605, 4253.184, 4207.799, 4159.081, 4114.485, 4099.275, 4166.624, 4068.57, 4085.692, 4098.481, 4043.694, 4060.715, 4028.551, 4010.274, 4056.69, 3977.883, 4005.996, 3953.608, 3882.11, 3969.699, 3936.235, 3899.247, 3929.931, 3933.836, 3904.248, 3824.629, 3945.95, 3884.449, 3894.145, 3836.138, 3835.867, 3928.689, 3814.648, 3846.401, 3898.689, 3824.244, 3792.322, 3840.948, 3782.261, 3819.74, 3824.078, 3831.394, 3816.843, 3823.08, 3773.014, 3806.405, 3742.212, 3732.905, 3707.211, 3777.661, 3721.683, 3761.343, 3684.601, 3746.342, 3705.07, 3715.217, 3699.706, 3693.945, 3668.765, 3735.29, 3746.289, 3729.696, 3691.4, 3669.119, 3740.096, 3697.62, 3619.028, 3622.475, 3688.34, 3676.222, 3716.243, 3649.977, 3619.469, 3696.193, 3648.88, 3664.418, 3596.544, 3640.189, 3620.669, 3605.879, 3574.761]}	0	100	True
