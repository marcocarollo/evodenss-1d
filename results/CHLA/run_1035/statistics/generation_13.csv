id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:51 epochs:100	100	1000	True	2882.03247		3472828	18	-1	158.74333453178406	{'train_loss': [7630.664, 6375.953, 5967.661, 5678.379, 5396.046, 5141.486, 4767.738, 4587.603, 4397.029, 4277.677, 4225.985, 4147.688, 4066.89, 4030.947, 3937.698, 3902.117, 3866.836, 3779.295, 3776.867, 3758.569, 3685.113, 3651.023, 3616.566, 3600.222, 3581.445, 3533.497, 3515.831, 3515.265, 3468.521, 3463.639, 3464.645, 3398.373, 3426.302, 3391.114, 3376.484, 3353.202, 3348.27, 3309.853, 3319.824, 3300.143, 3285.427, 3265.693, 3280.206, 3257.961, 3270.646, 3229.054, 3223.327, 3203.727, 3202.455, 3198.842, 3178.943, 3190.466, 3168.131, 3159.956, 3154.116, 3140.442, 3146.082, 3129.684, 3125.013, 3120.193, 3105.429, 3086.344, 3078.382, 3089.486, 3090.835, 3077.185, 3082.175, 3064.175, 3056.059, 3055.09, 3031.942, 3029.897, 3034.793, 3030.023, 3006.368, 3002.516, 2994.927, 2998.576, 2994.468, 2981.221, 2981.723, 2973.006, 2980.131, 2969.384, 2975.669, 2948.654, 2939.448, 2943.462, 2927.545, 2931.655, 2924.469, 2921.715, 2910.656, 2918.134, 2897.673, 2909.003, 2916.903, 2892.667, 2896.58, 2885.794], 'val_loss': [5849.717, 6294.82, 5690.1, 5436.912, 5012.689, 4570.348, 4199.469, 4153.803, 3998.982, 3921.049, 3778.096, 3929.672, 3786.427, 3782.401, 3766.203, 3631.855, 3649.68, 3710.288, 3691.045, 3512.777, 3552.125, 3530.441, 3364.953, 3590.022, 3523.459, 3498.682, 3563.869, 3389.411, 3387.337, 3518.446, 3353.8, 3365.905, 3275.211, 3342.087, 3243.823, 3299.842, 3470.413, 3290.607, 3295.407, 3279.665, 3345.294, 3421.574, 3183.748, 3166.306, 3053.924, 3099.828, 3206.3, 3077.459, 3106.816, 3167.324, 3108.885, 2965.595, 3097.704, 3034.774, 3202.734, 3153.403, 2998.683, 3012.308, 2988.458, 3045.912, 2959.043, 3043.249, 2991.89, 2970.576, 2911.969, 2985.939, 3002.096, 2986.523, 2993.698, 2936.519, 2913.57, 2902.078, 2939.777, 2889.016, 2988.128, 3012.219, 2944.485, 2928.631, 2856.235, 2936.329, 2930.415, 2934.065, 2871.907, 2864.943, 2895.352, 2871.685, 2958.189, 2861.683, 2858.023, 2851.598, 2911.068, 2828.094, 2794.533, 2873.165, 2880.163, 2854.827, 2820.217, 2827.515, 2771.404, 2779.443]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:69 kernel_size:6 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:90 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adadelta lr:0.041092959355921145 batch_size:51 epochs:100	100	1000	True	2887.75366		6610083	20	-1	210.03642988204956	{'train_loss': [7866.847, 7765.663, 7630.43, 6755.048, 6381.31, 6172.173, 6000.241, 5797.742, 5633.106, 5451.016, 5277.496, 5185.528, 4912.652, 5125.256, 4582.978, 4410.694, 4254.373, 4134.976, 4045.974, 3946.141, 3850.185, 3756.744, 3683.561, 3628.481, 3587.372, 3516.96, 3493.167, 3446.344, 3402.322, 3398.252, 3365.166, 3318.126, 3313.351, 3267.487, 3246.99, 3255.071, 3231.633, 3190.336, 3204.225, 3176.577, 3149.788, 3149.209, 3153.768, 3121.99, 3098.061, 3092.929, 3081.358, 3067.505, 3051.501, 3058.328, 3042.48, 3043.424, 3009.165, 3028.502, 2993.045, 3003.519, 2978.51, 2983.378, 2973.312, 2952.44, 2938.2, 2942.744, 2928.485, 2944.107, 2938.753, 2918.079, 2908.877, 2906.924, 2888.175, 2879.517, 2875.365, 2883.883, 2865.884, 2849.905, 2856.571, 2876.034, 2853.539, 2838.479, 2854.634, 2833.936, 2832.429, 2828.358, 2827.747, 2823.04, 2805.073, 2807.584, 2823.582, 2806.726, 2787.859, 2778.154, 2782.287, 2786.867, 2787.208, 2777.566, 2776.706, 2765.569, 2766.002, 2765.88, 2759.752, 2740.821], 'val_loss': [8291.778, 8510.664, 7950.587, 7518.221, 7128.847, 7265.209, 5905.912, 6043.095, 5710.884, 5448.452, 4909.383, 4971.796, 4882.33, 6409.836, 4219.05, 4036.234, 4217.255, 3790.97, 3710.164, 3571.704, 3487.519, 3378.286, 3307.442, 3230.321, 3226.862, 3161.142, 3161.558, 3206.768, 3118.4, 3104.626, 3072.483, 3076.524, 3060.038, 3056.348, 3042.22, 3015.201, 3029.799, 2958.516, 2988.859, 2989.693, 2954.182, 2993.181, 2952.967, 2913.231, 2951.534, 2880.625, 2968.295, 2900.62, 2912.547, 2912.747, 2919.969, 2898.791, 2858.813, 2914.267, 2866.689, 2910.868, 2899.169, 2858.729, 2861.9, 2893.683, 2881.783, 2881.11, 2887.323, 2880.457, 2865.574, 2879.51, 2856.029, 2859.429, 2871.724, 2853.829, 2851.653, 2846.277, 2852.117, 2895.954, 2808.137, 2836.733, 2839.467, 2814.639, 2798.41, 2799.324, 2775.553, 2801.268, 2834.24, 2785.512, 2875.747, 2780.339, 2815.241, 2769.362, 2806.927, 2807.802, 2808.119, 2761.268, 2734.067, 2789.029, 2774.006, 2757.615, 2797.937, 2819.574, 2777.965, 2759.384]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:51 epochs:100	100	1000	True	2679.81079		763315	18	-1	124.45658826828003	{'train_loss': [7056.767, 6446.413, 6357.629, 6303.57, 5957.106, 5334.312, 4992.163, 4740.615, 4599.515, 4499.433, 4442.513, 4387.25, 4331.566, 4305.269, 4261.329, 4211.488, 4201.66, 4171.323, 4142.226, 4109.506, 4083.124, 4069.107, 4035.903, 4024.0, 3998.85, 3973.411, 3944.582, 3935.719, 3912.468, 3888.034, 3857.844, 3843.23, 3809.876, 3779.059, 3672.062, 3585.41, 3512.848, 3455.22, 3414.217, 3388.132, 3362.856, 3340.239, 3317.219, 3295.948, 3267.372, 3245.74, 3226.153, 3203.236, 3200.745, 3179.817, 3162.916, 3148.186, 3148.043, 3128.789, 3112.517, 3098.606, 3083.875, 3085.095, 3072.72, 3054.12, 3048.811, 3036.179, 3035.035, 3023.292, 3026.355, 2996.06, 3009.807, 3001.281, 2984.73, 2978.448, 2976.475, 2977.804, 2957.775, 2954.142, 2946.185, 2938.072, 2938.741, 2939.321, 2925.125, 2914.591, 2909.848, 2906.906, 2908.305, 2902.236, 2893.934, 2881.926, 2871.15, 2873.883, 2886.03, 2877.99, 2877.724, 2873.755, 2867.284, 2867.87, 2856.249, 2845.554, 2847.727, 2844.056, 2842.587, 2849.316], 'val_loss': [5563.687, 5443.563, 5392.314, 5336.538, 4550.248, 4113.955, 3833.379, 3785.431, 3711.007, 3698.727, 3700.572, 3639.07, 3644.362, 3613.812, 3601.945, 3598.275, 3601.121, 3577.781, 3580.121, 3565.477, 3559.207, 3555.345, 3562.571, 3525.775, 3535.607, 3544.139, 3532.527, 3552.927, 3503.691, 3519.672, 3487.451, 3465.758, 3492.365, 3396.506, 3316.616, 3218.858, 3191.863, 3155.707, 3110.535, 3079.7, 3063.21, 3020.289, 3013.992, 3013.016, 2973.899, 2986.366, 2965.039, 2949.626, 2939.572, 2928.377, 2915.709, 2952.601, 2921.955, 2901.963, 2880.57, 2881.442, 2911.826, 2888.143, 2857.62, 2874.052, 2844.971, 2858.319, 2834.57, 2824.736, 2856.317, 2857.892, 2834.628, 2818.519, 2833.291, 2820.571, 2811.322, 2817.861, 2803.143, 2831.889, 2792.895, 2802.557, 2807.74, 2804.906, 2793.664, 2805.128, 2796.307, 2770.755, 2777.19, 2815.684, 2794.549, 2770.547, 2764.366, 2767.152, 2762.041, 2786.662, 2754.818, 2774.353, 2784.437, 2733.921, 2754.908, 2732.436, 2752.657, 2773.651, 2747.401, 2717.075]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:27 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:58 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:deconv1d out_channels:8 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.05229362301733192 batch_size:51 epochs:100	100	1000	True	3218.32495		24130455	19	-1	335.42978858947754	{'train_loss': [7897.774, 7701.575, 7850.082, 7990.008, 8085.876, 8131.155, 8195.826, 8232.099, 8274.334, 8281.194, 8269.523, 8225.19, 8196.108, 8152.038, 8113.435, 8029.133, 7950.755, 7822.131, 7702.466, 7547.182, 7403.195, 7234.032, 7056.307, 6845.205, 6638.319, 6379.653, 6156.38, 5877.896, 5560.053, 5288.762, 4976.281, 4688.993, 4397.5, 4130.177, 3884.705, 3710.67, 3553.594, 3409.012, 3317.176, 3214.096, 3173.886, 3111.814, 3067.894, 3011.041, 2975.384, 2956.423, 2916.675, 2897.985, 2869.363, 2843.695, 2826.398, 2806.29, 2790.789, 2778.064, 2759.452, 2750.916, 2749.232, 2720.026, 2707.951, 2697.588, 2690.765, 2687.211, 2666.223, 2659.647, 2656.975, 2650.684, 2631.124, 2624.096, 2615.901, 2607.263, 2581.867, 2591.679, 2576.432, 2564.553, 2566.632, 2559.151, 2544.698, 2543.076, 2524.368, 2520.322, 2521.546, 2509.175, 2495.263, 2492.275, 2489.642, 2481.506, 2466.034, 2461.763, 2464.193, 2449.4, 2437.19, 2443.151, 2441.914, 2429.314, 2424.145, 2413.186, 2409.319, 2397.082, 2386.87, 2400.197], 'val_loss': [12926.741, 11659.118, 11595.776, 10552.429, 8892.887, 8836.925, 9376.231, 9022.14, 8066.024, 7915.121, 8329.13, 7014.786, 7894.818, 7304.862, 7991.312, 7270.759, 7135.894, 7091.961, 7083.183, 7084.562, 6520.307, 6263.497, 5978.303, 5600.599, 5387.396, 5312.697, 4918.421, 5050.395, 4490.369, 4345.132, 4041.079, 3968.345, 3927.559, 3640.205, 3464.046, 3385.593, 3292.083, 3226.571, 3184.154, 3112.01, 3087.657, 3080.073, 2972.313, 2922.044, 2953.558, 2930.352, 2895.192, 2902.22, 2853.537, 2877.683, 2837.283, 2822.686, 2826.828, 2832.484, 2784.147, 2795.957, 2764.414, 2784.686, 2787.297, 2749.782, 2765.71, 2715.009, 2716.592, 2733.314, 2745.73, 2694.544, 2697.197, 2684.536, 2661.666, 2687.336, 2718.427, 2683.888, 2691.001, 2668.196, 2673.659, 2681.007, 2651.224, 2655.558, 2648.479, 2646.31, 2659.642, 2641.507, 2639.292, 2650.078, 2624.312, 2623.933, 2627.631, 2634.987, 2627.77, 2633.053, 2637.921, 2660.72, 2629.4, 2638.217, 2640.6, 2632.13, 2654.21, 2634.65, 2651.952, 2624.249]}	100	100	True
