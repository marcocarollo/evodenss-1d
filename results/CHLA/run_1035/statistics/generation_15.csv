id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:51 epochs:100	100	1000	True	2633.46826		763315	18	-1	172.92344498634338	{'train_loss': [7255.512, 6504.942, 6388.915, 6237.806, 5579.781, 5103.778, 4819.904, 4657.192, 4560.827, 4473.321, 4410.137, 4361.581, 4294.276, 4244.451, 4099.532, 3941.278, 3803.8, 3698.86, 3635.62, 3567.531, 3505.784, 3462.836, 3413.851, 3378.404, 3346.799, 3319.51, 3283.437, 3263.398, 3231.351, 3214.35, 3202.629, 3191.123, 3169.485, 3139.097, 3134.889, 3123.679, 3100.518, 3097.762, 3078.962, 3081.075, 3058.851, 3058.982, 3042.629, 3035.591, 3020.62, 3024.133, 3006.415, 3010.748, 2997.922, 2979.401, 2983.917, 2984.692, 2965.372, 2959.354, 2940.809, 2944.362, 2955.767, 2942.532, 2912.771, 2919.174, 2917.802, 2906.565, 2912.223, 2909.59, 2904.031, 2895.149, 2881.36, 2902.038, 2886.135, 2884.338, 2882.742, 2887.436, 2864.585, 2868.647, 2870.452, 2857.568, 2840.508, 2845.851, 2845.501, 2845.862, 2834.189, 2844.679, 2830.319, 2815.71, 2832.199, 2832.265, 2819.533, 2806.762, 2808.186, 2808.61, 2812.956, 2804.787, 2809.533, 2788.34, 2791.579, 2796.55, 2781.873, 2788.797, 2796.168, 2789.489], 'val_loss': [5690.03, 5497.577, 5418.204, 4884.102, 4151.062, 4096.758, 3870.298, 3822.846, 3775.835, 3727.833, 3715.367, 3667.568, 3651.611, 3598.736, 3462.024, 3316.425, 3224.931, 3140.614, 3130.358, 3113.909, 3039.888, 3016.259, 2964.436, 2997.943, 2934.667, 2969.107, 2927.955, 2926.908, 2896.334, 2922.724, 2866.765, 2852.397, 2876.076, 2889.984, 2872.408, 2835.738, 2870.996, 2819.279, 2843.684, 2826.686, 2854.092, 2817.934, 2822.344, 2818.76, 2822.405, 2806.575, 2831.528, 2824.729, 2806.075, 2778.95, 2812.69, 2769.47, 2804.671, 2812.596, 2811.949, 2758.44, 2738.041, 2790.94, 2777.713, 2739.166, 2764.88, 2761.866, 2742.048, 2745.515, 2727.06, 2770.455, 2759.496, 2755.347, 2735.365, 2761.419, 2775.999, 2712.344, 2737.34, 2748.02, 2705.51, 2721.719, 2749.571, 2733.344, 2757.524, 2780.647, 2747.07, 2692.991, 2720.648, 2717.427, 2705.949, 2756.601, 2684.603, 2687.565, 2699.425, 2716.41, 2733.735, 2721.096, 2695.751, 2694.357, 2748.754, 2691.277, 2671.63, 2684.993, 2689.157, 2657.919]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:77 epochs:100	100	1000	True	4461.64795		748315	18	-1	139.68506288528442	{'train_loss': [10882.662, 9749.536, 9625.054, 9557.803, 9511.232, 9472.495, 9440.072, 9395.817, 8848.285, 7890.437, 7635.211, 7390.308, 7221.985, 7019.058, 6925.535, 6803.982, 6702.438, 6601.167, 6513.574, 6449.217, 6389.449, 6332.567, 6273.511, 6231.165, 6169.016, 6141.362, 6100.037, 6051.168, 6037.318, 5996.433, 5973.339, 5941.911, 5907.195, 5878.65, 5863.327, 5840.911, 5809.582, 5794.493, 5765.27, 5740.545, 5724.082, 5706.538, 5673.874, 5637.155, 5597.803, 5546.731, 5461.46, 5357.56, 5295.638, 5230.974, 5192.058, 5166.127, 5097.226, 5072.188, 5069.515, 4989.271, 4959.607, 4956.222, 4916.607, 4900.148, 4863.467, 4820.416, 4842.95, 4825.714, 4795.691, 4796.773, 4766.568, 4739.767, 4742.249, 4720.93, 4694.59, 4707.024, 4688.789, 4680.782, 4639.106, 4667.696, 4638.264, 4619.819, 4615.249, 4592.318, 4620.429, 4588.321, 4543.207, 4572.285, 4539.351, 4546.773, 4550.844, 4528.787, 4521.03, 4524.897, 4526.496, 4491.166, 4496.06, 4488.066, 4486.097, 4491.127, 4434.796, 4470.562, 4469.268, 4463.555], 'val_loss': [9196.703, 8831.752, 8725.438, 8644.496, 8631.161, 8600.541, 8583.474, 8522.456, 6824.627, 6372.583, 6198.946, 6055.31, 5967.351, 5900.752, 5853.122, 5827.617, 5837.591, 5813.206, 5831.726, 5802.873, 5759.216, 5791.572, 5747.886, 5725.687, 5721.968, 5683.881, 5731.915, 5720.261, 5675.622, 5678.151, 5659.387, 5662.748, 5649.884, 5622.372, 5657.295, 5646.398, 5630.017, 5629.113, 5619.182, 5608.862, 5615.238, 5521.266, 5532.604, 5482.731, 5467.125, 5309.646, 5162.185, 5136.872, 5029.873, 5057.333, 4999.994, 4914.413, 4884.257, 4873.539, 4858.399, 4843.585, 4752.586, 4758.441, 4677.226, 4661.317, 4648.545, 4659.491, 4657.989, 4664.212, 4667.406, 4583.863, 4655.054, 4596.374, 4577.329, 4616.897, 4582.989, 4604.086, 4590.906, 4557.5, 4544.588, 4548.733, 4563.305, 4546.623, 4531.479, 4493.12, 4513.515, 4506.489, 4533.452, 4526.621, 4473.156, 4432.126, 4477.626, 4468.514, 4446.7, 4512.389, 4442.192, 4466.827, 4463.201, 4484.814, 4419.814, 4447.624, 4443.966, 4444.141, 4403.245, 4445.724]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:11 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:11 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:gradient_descent lr:0.05229362301733192 momentum:0.732087939002516 weight_decay:0.0003103703986362322 nesterov:True batch_size:51 epochs:100	100	1000	True	16651.19922		843721	19	-1	172.95757961273193	{'train_loss': [54580.605, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783], 'val_loss': [16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:102 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.05229362301733192 beta1:0.9180216121310151 beta2:0.94612125453066 weight_decay:0.0001191607870506377 batch_size:51 epochs:100	100	1000	True	6085.51367		844526	19	-1	178.7557852268219	{'train_loss': [23236.635, 17712.752, 17712.783, 17712.783, 17712.783, 17712.783, 17814.465, 18159.611, 17701.623, 17712.781, 17716.994, 17624.01, 11986.973, 6446.085, 6032.721, 6113.858, 6131.754, 6137.708, 6177.4, 6187.182, 6170.618, 6195.057, 6196.81, 6207.938, 6190.267, 6192.39, 6200.165, 6182.149, 6222.901, 6198.195, 6301.915, 6321.195, 6509.387, 6336.35, 6384.434, 6445.937, 6506.679, 6455.436, 6649.097, 6540.385, 6512.027, 6461.061, 6661.312, 6666.081, 6710.799, 6547.981, 6506.001, 6617.06, 6476.392, 6558.646, 6476.913, 6491.712, 6489.854, 6497.428, 6508.303, 6477.07, 6459.859, 6477.9, 6707.456, 7031.178, 6405.514, 6511.215, 6456.679, 6440.668, 6591.426, 6544.158, 6673.295, 6612.567, 6431.592, 6645.935, 6718.377, 6483.284, 6419.59, 6476.471, 6713.666, 6514.979, 6446.771, 6555.203, 6456.896, 6518.977, 6788.272, 6496.284, 6472.336, 6503.727, 6477.684, 6504.321, 6409.376, 6516.436, 6556.15, 6453.48, 6515.732, 6578.201, 6688.345, 6562.317, 6498.062, 6582.256, 6581.985, 6692.662, 6649.068, 6591.793], 'val_loss': [16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16622.221, 16829.746, 16640.615, 16640.615, 16619.785, 16280.98, 10070.916, 5808.224, 5849.588, 6209.54, 6176.562, 6466.788, 6279.673, 5793.406, 6195.165, 6559.498, 6419.911, 6194.192, 6443.764, 6552.24, 5962.573, 6202.202, 5984.316, 6083.723, 6043.054, 6110.468, 6183.427, 6215.366, 6150.053, 6134.028, 5794.886, 5942.498, 6080.369, 6266.097, 6552.896, 6076.908, 6023.503, 5862.989, 6300.638, 6070.562, 5985.346, 6315.789, 6591.581, 6205.655, 5993.043, 5823.178, 5873.109, 6699.468, 6075.53, 6651.293, 6284.428, 6301.223, 6827.38, 5958.543, 5799.336, 5895.283, 5800.1, 6315.762, 6261.199, 6278.304, 6693.235, 5796.063, 6614.154, 7514.974, 5912.944, 5874.194, 6041.149, 5937.397, 6089.584, 5911.825, 6298.659, 6532.0, 5719.201, 6272.169, 6440.143, 5966.603, 6583.059, 6286.896, 6578.413, 6101.97, 6260.985, 6213.688, 6137.323, 6492.367, 6338.634, 6192.014, 5809.748, 6532.452, 6194.361, 6345.334, 6432.41, 6224.979, 7165.409, 6120.208]}	100	100	True
