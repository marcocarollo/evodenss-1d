id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:13 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.009145972964182274 batch_size:74 epochs:100	100	1000	True	3829.64819		1498441	16	-1	149.3625898361206	{'train_loss': [11770.394, 10792.949, 10485.077, 10190.807, 9960.585, 9793.778, 9682.279, 9602.384, 9505.396, 9292.44, 7986.977, 7238.371, 7002.695, 6812.127, 6673.0, 6539.618, 6426.257, 6332.217, 6269.7, 6206.67, 6126.953, 6091.735, 6031.235, 5986.737, 5930.555, 5899.74, 5862.763, 5834.782, 5805.059, 5770.226, 5741.208, 5705.883, 5670.628, 5654.035, 5616.989, 5589.859, 5571.345, 5551.262, 5537.127, 5506.642, 5493.119, 5455.123, 5445.889, 5430.806, 5407.626, 5391.413, 5379.558, 5360.555, 5337.233, 5329.614, 5304.316, 5292.406, 5268.109, 5253.534, 5244.184, 5212.511, 5225.112, 5197.147, 5184.413, 5169.554, 5155.261, 5152.163, 5122.468, 5102.639, 5089.856, 5085.077, 5063.804, 5046.721, 5058.272, 5044.621, 5020.795, 5021.668, 4995.101, 5003.442, 4979.554, 4960.727, 4969.987, 4953.716, 4938.985, 4922.246, 4911.382, 4891.405, 4897.675, 4878.743, 4869.608, 4862.119, 4864.304, 4841.081, 4831.534, 4826.764, 4829.918, 4804.756, 4805.419, 4798.346, 4770.205, 4774.024, 4760.795, 4761.588, 4749.216, 4738.736], 'val_loss': [9599.58, 8896.977, 8218.503, 7819.914, 7619.167, 7494.375, 7371.018, 7347.388, 7212.747, 6748.242, 5751.429, 5343.266, 5155.242, 4943.578, 4855.549, 4803.778, 4719.317, 4738.444, 4648.021, 4523.903, 4495.314, 4447.582, 4380.113, 4343.975, 4353.471, 4324.684, 4282.294, 4327.753, 4239.25, 4245.441, 4224.377, 4227.602, 4179.576, 4190.186, 4158.216, 4159.523, 4161.667, 4103.37, 4105.265, 4103.51, 4077.99, 4044.365, 4065.885, 4035.143, 4014.6, 4078.606, 4020.251, 4005.249, 4007.302, 4002.076, 4003.851, 3989.655, 3961.486, 3961.456, 3961.649, 3968.034, 3953.164, 3937.356, 3940.918, 3929.282, 3928.389, 3931.119, 3913.107, 3939.33, 3892.17, 3868.302, 3876.824, 3917.844, 3879.144, 3852.318, 3845.444, 3862.204, 3861.831, 3875.88, 3836.323, 3857.065, 3819.354, 3845.663, 3856.21, 3829.735, 3844.973, 3818.924, 3822.53, 3799.756, 3825.075, 3823.293, 3803.853, 3797.285, 3822.85, 3786.161, 3790.2, 3802.962, 3798.853, 3776.035, 3785.307, 3784.294, 3785.026, 3787.075, 3782.022, 3797.638]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:conv1d out_channels:123 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adadelta lr:0.05229362301733192 batch_size:74 epochs:100	100	1000	True	3764.76367		2066438	17	-1	153.06728148460388	{'train_loss': [11344.296, 10267.783, 8304.771, 7833.523, 7505.514, 7232.264, 6976.08, 6821.035, 6683.961, 6589.111, 6440.493, 6325.638, 6268.911, 6138.416, 5988.871, 5880.353, 5737.207, 5656.312, 5571.858, 5477.748, 5393.888, 5336.806, 5273.07, 5187.821, 5148.768, 5109.471, 5084.9, 5017.066, 4996.894, 4931.717, 4929.55, 4924.632, 4864.82, 4855.044, 4827.742, 4787.532, 4772.119, 4760.056, 4742.922, 4722.954, 4691.261, 4666.344, 4671.054, 4624.832, 4639.595, 4613.406, 4590.724, 4606.815, 4574.543, 4559.504, 4542.769, 4541.933, 4500.152, 4505.977, 4520.12, 4479.264, 4478.191, 4461.696, 4452.42, 4442.081, 4433.554, 4417.236, 4424.909, 4417.921, 4397.109, 4391.178, 4392.961, 4381.13, 4360.7, 4371.282, 4366.758, 4356.492, 4335.32, 4328.705, 4337.438, 4326.185, 4309.488, 4297.583, 4300.404, 4310.011, 4296.607, 4289.138, 4260.372, 4256.882, 4254.429, 4252.564, 4250.708, 4237.172, 4232.59, 4256.864, 4229.326, 4216.966, 4222.779, 4205.199, 4205.785, 4190.613, 4204.181, 4209.664, 4187.828, 4194.216], 'val_loss': [8437.953, 7371.854, 7120.023, 6275.015, 5943.815, 5580.761, 5439.548, 5321.956, 5214.775, 5171.494, 5033.559, 4983.755, 5001.31, 4730.183, 4804.667, 4640.897, 4692.186, 4503.507, 4707.814, 4494.393, 4466.651, 4352.645, 4324.085, 4293.796, 4159.33, 4190.585, 4163.986, 4094.776, 4122.403, 4098.314, 4157.446, 4122.507, 4020.474, 4184.273, 4013.501, 3947.836, 3983.725, 3989.159, 4011.689, 3970.695, 3937.396, 4105.887, 3935.898, 3934.893, 3936.356, 3952.632, 3970.839, 3936.792, 3901.793, 3809.822, 4014.196, 3862.411, 3949.571, 3936.422, 3832.827, 3866.734, 3871.037, 3880.541, 3886.064, 3770.322, 3883.519, 3851.509, 3822.926, 3798.984, 3800.856, 3867.416, 3764.719, 3770.615, 3841.248, 3796.148, 3844.906, 3749.616, 3796.45, 3719.528, 3869.063, 3782.485, 3814.509, 3796.32, 3739.541, 3857.654, 3709.298, 3727.154, 3723.795, 3749.945, 3784.072, 3717.54, 3777.602, 3716.445, 3743.153, 3813.321, 3777.317, 3758.47, 3737.027, 3739.709, 3808.681, 3706.413, 3717.976, 3720.869, 3697.994, 3751.646]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:deconv1d out_channels:42 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:58 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:13 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:rmsprop lr:0.009145972964182274 alpha:0.8260304341664251 weight_decay:5.403182002937404e-05 batch_size:74 epochs:100	100	1000	True	20293.76562		4593743	17	-1	156.21180725097656	{'train_loss': [33109.68, 25921.143, 25921.143, 49162.305, 26117.83, 25921.143, 25921.143, 117171.656, 25921.143, 25921.143, 25921.143, 54692.309, 70016.281, 25921.143, 26444.285, 49995.215, 57438.906, 26361.461, 31581.439, 33408.301, 30216.852, 15849.623, 14962.18, 13936.259, 13511.771, 12872.603, 12180.357, 12578.85, 11756.231, 12425.904, 11670.856, 11538.458, 11243.081, 11462.181, 10940.423, 10689.836, 10698.677, 11104.558, 10723.639, 11377.469, 10590.773, 10772.26, 10097.083, 11387.438, 10152.922, 10511.778, 10235.778, 11075.342, 10358.271, 10898.063, 10681.076, 9942.002, 10346.311, 11358.022, 10296.052, 10387.49, 9832.732, 10035.205, 10311.512, 10581.322, 10151.059, 10502.886, 10008.066, 9868.891, 10463.817, 10478.868, 9991.825, 10172.678, 10460.702, 9299.448, 10630.713, 9711.485, 10058.115, 10569.648, 10560.209, 10607.393, 9694.645, 10218.17, 9976.789, 9804.418, 10158.154, 9597.116, 9925.026, 10998.761, 9937.313, 10050.991, 9919.635, 10122.57, 9690.934, 9439.894, 10033.836, 9856.097, 10117.827, 10030.08, 9980.99, 10085.23, 9644.098, 9534.114, 9864.396, 9437.128], 'val_loss': [22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.479, 26907.58, 22187.49, 22187.01, 22187.488, 16196.101, 11143.347, 14457.891, 13777.259, 14303.338, 14471.607, 8955.096, 7713.222, 6740.882, 6835.781, 10719.897, 8496.873, 12795.298, 10081.486, 8450.85, 9919.021, 12832.676, 8889.822, 8578.666, 14426.154, 10543.774, 8903.131, 6346.694, 8070.869, 14285.88, 9844.847, 6234.83, 8285.38, 8625.408, 9359.038, 11051.012, 12799.015, 8756.401, 6775.009, 7338.734, 12948.569, 8263.456, 7296.313, 5915.939, 7809.512, 6172.613, 7335.689, 15140.637, 6137.114, 5825.658, 6534.491, 10725.639, 14090.989, 5935.326, 15293.543, 6179.886, 8914.368, 5763.266, 6914.995, 6855.042, 6497.517, 12045.415, 5573.173, 5954.879, 8034.989, 6977.815, 7027.556, 6489.978, 6771.829, 7264.976, 7575.62, 4730.285, 7132.593, 6079.451, 6236.255, 6689.917, 6335.178, 5920.695, 7943.401, 6882.364, 10154.412, 8336.124, 8243.625, 7938.752, 20188.699]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:88 kernel_size:6 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:49 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:56 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:deconv1d out_channels:13 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:gradient_descent lr:0.009145972964182274 momentum:0.8875217193419879 weight_decay:6.464229164137047e-05 nesterov:True batch_size:74 epochs:100	100	1000	True	22203.83984		1079948	18	-1	158.34501218795776	{'train_loss': [30145.045, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143], 'val_loss': [22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49]}	100	100	True
