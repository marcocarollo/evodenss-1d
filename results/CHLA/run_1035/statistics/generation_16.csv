id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:51 epochs:100	100	1000	True	2683.36572		763315	18	-1	174.10114693641663	{'train_loss': [7199.127, 6480.004, 6374.955, 5965.302, 5387.667, 5062.985, 4854.257, 4728.118, 4620.358, 4544.0, 4495.45, 4440.374, 4383.584, 4325.176, 4220.918, 4065.378, 3948.887, 3844.271, 3768.592, 3711.426, 3652.375, 3591.459, 3545.232, 3515.761, 3465.803, 3440.963, 3418.777, 3381.395, 3365.18, 3333.151, 3304.554, 3299.56, 3257.348, 3235.975, 3222.31, 3214.293, 3199.238, 3172.847, 3170.486, 3149.93, 3140.478, 3108.448, 3107.084, 3098.383, 3094.863, 3091.629, 3062.791, 3073.336, 3053.714, 3046.579, 3030.792, 3012.982, 3009.643, 3006.297, 2986.031, 2987.399, 2977.421, 2966.986, 2974.403, 2964.435, 2952.368, 2948.281, 2941.472, 2938.575, 2939.53, 2926.106, 2911.994, 2925.301, 2900.489, 2913.297, 2905.448, 2898.356, 2890.772, 2882.15, 2895.359, 2892.898, 2864.786, 2871.702, 2867.492, 2860.673, 2860.802, 2865.13, 2850.793, 2858.344, 2859.434, 2846.003, 2853.258, 2840.887, 2833.473, 2834.094, 2825.997, 2817.897, 2814.924, 2822.555, 2820.256, 2803.353, 2806.61, 2819.578, 2810.373, 2800.49], 'val_loss': [5655.669, 5451.866, 5307.771, 4583.285, 4174.918, 3990.931, 3814.885, 3764.047, 3758.555, 3752.112, 3722.431, 3713.907, 3696.768, 3637.075, 3507.047, 3383.538, 3259.027, 3209.959, 3203.72, 3141.652, 3094.356, 3042.58, 3035.561, 2987.34, 2988.973, 2954.741, 2933.781, 2923.867, 2918.649, 2904.981, 2897.048, 2870.906, 2892.145, 2876.377, 2871.047, 2872.434, 2856.976, 2830.429, 2851.922, 2845.326, 2825.719, 2835.849, 2824.911, 2811.629, 2813.22, 2810.5, 2808.337, 2796.052, 2794.572, 2797.678, 2794.202, 2790.103, 2822.693, 2788.873, 2767.69, 2763.952, 2800.398, 2769.408, 2761.081, 2741.749, 2734.99, 2768.121, 2738.284, 2753.294, 2764.762, 2733.848, 2733.726, 2712.633, 2747.31, 2739.229, 2698.919, 2759.489, 2735.315, 2721.747, 2714.472, 2722.498, 2717.909, 2711.875, 2703.592, 2733.045, 2700.335, 2682.842, 2686.618, 2683.377, 2702.211, 2699.828, 2681.62, 2696.475, 2688.966, 2676.097, 2691.383, 2673.514, 2658.818, 2706.894, 2662.058, 2665.733, 2652.988, 2680.827, 2686.016, 2670.688]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:51 epochs:100	100	1000	True	2722.01392		763315	18	-1	171.45149326324463	{'train_loss': [7160.184, 6474.693, 6356.0, 6266.817, 5845.794, 5254.968, 4863.114, 4677.902, 4514.9, 4420.679, 4352.385, 4307.419, 4258.344, 4222.963, 4182.654, 4132.205, 4112.216, 4066.688, 4028.292, 3943.832, 3783.933, 3726.358, 3684.442, 3638.799, 3608.615, 3569.087, 3542.067, 3511.658, 3485.927, 3472.131, 3451.579, 3425.659, 3398.074, 3388.534, 3373.654, 3356.639, 3330.045, 3314.89, 3309.265, 3300.848, 3275.575, 3262.942, 3253.652, 3223.43, 3230.3, 3205.537, 3205.33, 3182.898, 3177.274, 3172.971, 3154.108, 3154.096, 3117.534, 3115.819, 3106.681, 3110.833, 3109.516, 3093.16, 3085.022, 3060.636, 3071.624, 3046.159, 3055.776, 3045.351, 3039.544, 3028.754, 3015.195, 3007.725, 3011.812, 3014.433, 2981.539, 2993.133, 2987.846, 2975.463, 2980.511, 2982.784, 2955.051, 2966.558, 2947.605, 2950.798, 2945.457, 2942.289, 2927.669, 2925.62, 2930.835, 2933.743, 2921.777, 2921.331, 2910.086, 2900.956, 2893.315, 2898.116, 2897.287, 2897.896, 2879.445, 2877.562, 2873.923, 2868.556, 2881.611, 2853.681], 'val_loss': [5571.255, 5448.688, 5396.661, 5126.663, 4552.056, 3902.562, 3815.551, 3739.165, 3697.782, 3692.003, 3636.58, 3612.672, 3614.297, 3596.079, 3584.418, 3566.437, 3555.235, 3524.519, 3489.576, 3353.949, 3323.193, 3240.281, 3243.676, 3186.807, 3164.074, 3161.004, 3136.656, 3135.889, 3148.728, 3098.757, 3095.567, 3100.211, 3075.375, 3061.97, 3074.159, 3049.68, 3041.2, 3044.548, 3026.165, 3020.962, 3004.071, 2975.504, 2955.624, 2964.101, 2962.83, 3027.694, 2948.583, 2922.493, 2932.318, 2911.712, 2923.166, 2907.993, 2924.729, 2909.796, 2911.903, 2876.936, 2910.881, 2872.653, 2888.518, 2853.097, 2893.035, 2844.596, 2832.749, 2833.872, 2820.168, 2813.76, 2834.058, 2846.046, 2801.509, 2769.434, 2795.754, 2808.639, 2780.268, 2802.846, 2835.237, 2797.084, 2766.886, 2768.679, 2785.693, 2777.452, 2783.541, 2774.516, 2747.073, 2734.045, 2783.597, 2761.241, 2750.663, 2750.888, 2760.691, 2746.22, 2732.519, 2759.797, 2745.962, 2734.537, 2752.95, 2731.73, 2732.092, 2732.968, 2739.195, 2727.176]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.09110654586796615 batch_size:51 epochs:100	100	1000	True	2616.90356		657150	18	-1	171.86119318008423	{'train_loss': [6768.751, 6297.757, 6183.668, 6068.056, 5616.338, 5010.227, 4597.499, 4237.635, 3994.571, 3846.283, 3767.086, 3682.67, 3605.137, 3544.91, 3487.414, 3443.035, 3404.108, 3370.535, 3334.158, 3315.03, 3284.301, 3253.35, 3219.204, 3199.87, 3182.304, 3165.067, 3159.023, 3138.998, 3128.751, 3103.431, 3078.075, 3066.787, 3063.328, 3048.713, 3019.388, 3022.01, 3007.581, 2990.312, 3000.823, 2992.297, 2977.337, 2971.842, 2965.775, 2950.884, 2949.641, 2936.232, 2928.724, 2943.342, 2910.263, 2908.768, 2917.534, 2896.629, 2919.138, 2899.015, 2899.06, 2886.116, 2873.274, 2870.347, 2856.514, 2865.615, 2875.969, 2859.446, 2854.06, 2851.316, 2838.477, 2847.311, 2824.809, 2829.429, 2821.945, 2833.433, 2828.32, 2814.55, 2809.412, 2812.625, 2797.108, 2812.402, 2792.689, 2773.871, 2787.054, 2794.7, 2777.681, 2774.581, 2766.598, 2779.574, 2760.441, 2767.062, 2764.751, 2775.727, 2768.084, 2762.903, 2767.845, 2755.509, 2735.985, 2761.291, 2754.656, 2728.393, 2743.191, 2744.402, 2742.421, 2729.449], 'val_loss': [5498.9, 5383.782, 5344.088, 4911.864, 4555.694, 4025.979, 3676.136, 3414.97, 3285.188, 3300.101, 3158.239, 3103.229, 3083.378, 3066.934, 3049.788, 3018.213, 3004.095, 2994.46, 2949.164, 2936.672, 2924.763, 2876.091, 2916.407, 2936.394, 2904.286, 2873.341, 2860.949, 2880.912, 2844.537, 2859.669, 2830.896, 2851.651, 2848.942, 2846.915, 2836.458, 2794.66, 2748.304, 2811.939, 2785.524, 2776.618, 2806.704, 2809.18, 2803.384, 2820.942, 2753.541, 2797.327, 2779.91, 2763.484, 2772.64, 2764.366, 2798.928, 2741.442, 2746.823, 2726.434, 2779.684, 2734.67, 2741.788, 2708.664, 2719.049, 2725.347, 2687.901, 2736.335, 2695.79, 2690.061, 2691.931, 2701.634, 2716.334, 2728.016, 2705.695, 2690.727, 2671.341, 2661.824, 2685.51, 2673.608, 2663.981, 2661.993, 2684.698, 2708.589, 2651.295, 2639.471, 2651.071, 2677.384, 2648.474, 2661.732, 2632.335, 2620.514, 2647.542, 2672.917, 2635.433, 2602.952, 2643.451, 2656.688, 2645.943, 2624.854, 2637.171, 2632.66, 2647.37, 2639.883, 2615.246, 2620.522]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:51 epochs:100	100	2000	True	2633.46826		763315	18	-1	173.47387075424194	{'train_loss': [7255.512, 6504.942, 6388.915, 6237.806, 5579.781, 5103.778, 4819.904, 4657.192, 4560.827, 4473.321, 4410.137, 4361.581, 4294.276, 4244.451, 4099.532, 3941.278, 3803.8, 3698.86, 3635.62, 3567.531, 3505.784, 3462.836, 3413.851, 3378.404, 3346.799, 3319.51, 3283.437, 3263.398, 3231.351, 3214.35, 3202.629, 3191.123, 3169.485, 3139.097, 3134.889, 3123.679, 3100.518, 3097.762, 3078.962, 3081.075, 3058.851, 3058.982, 3042.629, 3035.591, 3020.62, 3024.133, 3006.415, 3010.748, 2997.922, 2979.401, 2983.917, 2984.692, 2965.372, 2959.354, 2940.809, 2944.362, 2955.767, 2942.532, 2912.771, 2919.174, 2917.802, 2906.565, 2912.223, 2909.59, 2904.031, 2895.149, 2881.36, 2902.038, 2886.135, 2884.338, 2882.742, 2887.436, 2864.585, 2868.647, 2870.452, 2857.568, 2840.508, 2845.851, 2845.501, 2845.862, 2834.189, 2844.679, 2830.319, 2815.71, 2832.199, 2832.265, 2819.533, 2806.762, 2808.186, 2808.61, 2812.956, 2804.787, 2809.533, 2788.34, 2791.579, 2796.55, 2781.873, 2788.797, 2796.168, 2789.489], 'val_loss': [5690.03, 5497.577, 5418.204, 4884.102, 4151.062, 4096.758, 3870.298, 3822.846, 3775.835, 3727.833, 3715.367, 3667.568, 3651.611, 3598.736, 3462.024, 3316.425, 3224.931, 3140.614, 3130.358, 3113.909, 3039.888, 3016.259, 2964.436, 2997.943, 2934.667, 2969.107, 2927.955, 2926.908, 2896.334, 2922.724, 2866.765, 2852.397, 2876.076, 2889.984, 2872.408, 2835.738, 2870.996, 2819.279, 2843.684, 2826.686, 2854.092, 2817.934, 2822.344, 2818.76, 2822.405, 2806.575, 2831.528, 2824.729, 2806.075, 2778.95, 2812.69, 2769.47, 2804.671, 2812.596, 2811.949, 2758.44, 2738.041, 2790.94, 2777.713, 2739.166, 2764.88, 2761.866, 2742.048, 2745.515, 2727.06, 2770.455, 2759.496, 2755.347, 2735.365, 2761.419, 2775.999, 2712.344, 2737.34, 2748.02, 2705.51, 2721.719, 2749.571, 2733.344, 2757.524, 2780.647, 2747.07, 2692.991, 2720.648, 2717.427, 2705.949, 2756.601, 2684.603, 2687.565, 2699.425, 2716.41, 2733.735, 2721.096, 2695.751, 2694.357, 2748.754, 2691.277, 2671.63, 2684.993, 2689.157, 2657.919]}	0	100	True
