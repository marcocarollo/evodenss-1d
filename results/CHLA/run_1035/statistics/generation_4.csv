id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:conv1d out_channels:123 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adadelta lr:0.05229362301733192 batch_size:74 epochs:100	100	1000	True	3727.50146		2066438	17	-1	150.91529512405396	{'train_loss': [11368.287, 10883.985, 8877.368, 7833.087, 7403.381, 7222.535, 6850.004, 6733.56, 6601.587, 6353.333, 6241.391, 6236.398, 5958.751, 5901.77, 5798.689, 5706.348, 5631.556, 5560.14, 5541.933, 5427.472, 5393.827, 5340.518, 5286.319, 5244.892, 5179.005, 5145.047, 5122.813, 5092.246, 5051.435, 5031.027, 4985.721, 4974.606, 4929.375, 4899.754, 4875.771, 4843.362, 4836.648, 4787.105, 4783.547, 4769.174, 4727.505, 4729.341, 4709.549, 4692.942, 4658.609, 4638.202, 4649.986, 4605.785, 4618.298, 4590.606, 4571.763, 4546.944, 4541.602, 4526.852, 4518.53, 4499.808, 4484.503, 4480.319, 4474.477, 4447.614, 4438.458, 4433.111, 4440.57, 4411.5, 4411.087, 4405.92, 4385.49, 4382.208, 4352.309, 4359.667, 4343.972, 4346.934, 4329.022, 4339.34, 4313.437, 4312.654, 4297.682, 4283.155, 4273.845, 4278.468, 4275.529, 4261.767, 4251.982, 4254.438, 4230.508, 4265.243, 4230.666, 4221.585, 4235.18, 4214.315, 4222.835, 4207.194, 4213.217, 4193.412, 4175.678, 4197.28, 4180.884, 4145.231, 4177.743, 4168.593], 'val_loss': [8556.645, 7492.927, 6852.059, 6180.488, 5817.256, 5804.761, 5485.337, 5308.395, 5021.853, 4902.223, 5236.563, 4761.67, 4862.678, 4690.854, 4686.351, 4894.345, 4663.387, 4529.533, 4516.564, 4573.691, 4415.584, 4367.923, 4489.415, 4313.552, 4508.987, 4579.555, 4412.063, 4280.84, 4128.9, 4414.213, 4278.868, 4293.438, 4318.992, 4280.245, 4151.743, 4175.512, 4213.196, 4190.43, 4053.994, 4050.313, 4028.649, 4039.356, 4037.99, 4016.371, 4063.119, 4121.956, 4019.028, 4035.482, 4016.326, 4061.881, 3941.438, 3998.766, 3901.832, 3933.96, 3923.625, 4009.177, 4051.753, 3973.623, 3944.302, 3879.911, 3870.918, 3978.407, 3857.513, 3864.01, 3903.131, 3863.676, 3946.719, 3818.023, 3830.211, 3744.67, 3797.528, 3781.295, 3792.087, 3773.385, 3824.91, 3760.523, 3747.862, 3722.206, 3777.926, 3807.573, 3780.07, 3725.887, 3720.184, 3794.531, 3744.568, 3729.517, 3727.472, 3672.531, 3781.146, 3821.23, 3697.798, 3704.916, 3772.713, 3692.294, 3669.557, 3711.003, 3629.907, 3705.871, 3616.67, 3694.54]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:92 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:123 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adadelta lr:0.002088813897834851 batch_size:74 epochs:100	100	1000	True	4605.18555		9614374	17	-1	184.37569093704224	{'train_loss': [11993.493, 11239.12, 11204.021, 11170.094, 11113.499, 11076.765, 11039.25, 10992.676, 10960.702, 10892.35, 10839.388, 10776.769, 10725.9, 10658.749, 10567.824, 10506.272, 10422.979, 10344.842, 10199.882, 9954.894, 9409.022, 8963.604, 8742.524, 8512.379, 8313.862, 8138.949, 8026.544, 7905.766, 7821.819, 7710.787, 7635.741, 7530.762, 7452.214, 7411.318, 7340.487, 7303.169, 7255.396, 7220.34, 7181.278, 7161.021, 7137.145, 7108.708, 7065.762, 7037.774, 7029.129, 6979.967, 6961.023, 6944.004, 6940.826, 6911.033, 6898.6, 6886.054, 6840.372, 6817.305, 6821.079, 6795.411, 6774.406, 6758.622, 6740.11, 6732.964, 6705.842, 6707.708, 6689.863, 6677.894, 6649.438, 6656.476, 6628.61, 6616.988, 6584.107, 6586.824, 6574.983, 6586.408, 6537.039, 6542.048, 6525.899, 6490.676, 6522.964, 6492.408, 6467.68, 6461.407, 6441.108, 6426.061, 6412.587, 6411.622, 6398.707, 6372.414, 6380.97, 6377.349, 6357.16, 6348.561, 6312.268, 6332.22, 6324.5, 6307.248, 6302.845, 6312.032, 6281.688, 6264.358, 6249.306, 6248.521], 'val_loss': [11450.854, 11844.928, 11560.498, 11487.93, 11172.002, 10955.831, 10794.645, 10538.062, 10273.523, 10013.623, 9746.019, 9538.768, 9271.369, 9068.263, 8806.415, 8624.636, 8471.78, 8177.177, 7954.178, 7401.389, 7376.03, 7473.299, 7137.511, 6840.104, 6576.912, 6362.833, 6157.248, 5997.682, 5810.263, 5730.383, 5565.615, 5516.492, 5447.6, 5382.639, 5358.007, 5335.313, 5249.124, 5173.778, 5155.451, 5144.306, 5105.881, 5069.859, 5061.066, 5033.85, 5010.274, 4974.809, 4964.382, 4959.222, 4953.41, 4911.958, 4900.003, 4874.516, 4895.281, 4853.847, 4836.572, 4836.813, 4843.635, 4766.457, 4782.655, 4794.832, 4752.993, 4745.175, 4738.172, 4691.446, 4707.57, 4692.064, 4690.452, 4687.756, 4751.354, 4648.856, 4661.224, 4692.634, 4658.449, 4677.564, 4634.039, 4632.144, 4603.36, 4612.724, 4575.874, 4599.231, 4577.864, 4590.506, 4550.06, 4556.663, 4565.21, 4512.762, 4535.503, 4531.668, 4511.555, 4501.919, 4527.882, 4485.973, 4478.833, 4492.129, 4466.131, 4452.112, 4450.88, 4441.102, 4444.235, 4413.699]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:43 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:conv1d out_channels:123 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:43 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:70 epochs:100	100	1000	True	3937.70776		6509960	18	-1	208.66956043243408	{'train_loss': [10796.515, 10541.525, 9127.537, 8687.112, 8399.173, 8095.751, 7916.446, 7752.151, 7605.737, 7495.71, 7363.846, 7208.448, 7083.651, 6925.348, 6794.532, 6641.712, 6444.374, 6279.466, 5989.631, 5820.808, 5769.895, 5422.653, 5410.192, 5202.458, 5125.709, 5064.636, 4923.45, 4845.923, 4800.246, 4765.052, 4721.551, 4675.644, 4618.765, 4599.349, 4563.728, 4538.286, 4526.493, 4497.982, 4461.771, 4441.101, 4433.517, 4411.048, 4393.479, 4365.807, 4338.616, 4350.802, 4290.397, 4290.021, 4263.636, 4255.124, 4229.181, 4198.621, 4201.486, 4195.811, 4176.446, 4165.14, 4138.033, 4141.543, 4107.957, 4112.316, 4094.862, 4064.931, 4081.295, 4053.212, 4037.028, 4041.594, 4025.443, 4018.395, 3983.365, 4011.771, 3987.765, 3974.143, 3961.356, 3977.541, 3944.601, 3944.51, 3925.991, 3915.89, 3913.081, 3890.65, 3886.247, 3876.052, 3883.671, 3867.41, 3856.119, 3865.556, 3834.558, 3844.765, 3836.052, 3818.013, 3809.075, 3807.717, 3804.054, 3783.031, 3780.502, 3783.225, 3770.613, 3754.791, 3770.239, 3744.976], 'val_loss': [11074.678, 14388.059, 11570.794, 10470.923, 10099.467, 9102.961, 8503.222, 8559.02, 7985.855, 8036.706, 7579.408, 8108.918, 7166.5, 7152.068, 6388.222, 6737.239, 5973.363, 5703.913, 5720.742, 5116.583, 5142.453, 5040.305, 4857.744, 4873.125, 4737.618, 4475.999, 4596.641, 4555.17, 4520.914, 4294.62, 4300.915, 4263.518, 4188.64, 4265.737, 4108.159, 4072.436, 4179.552, 4137.583, 4153.276, 4117.544, 4017.626, 3969.718, 4104.436, 4014.994, 4007.665, 3990.685, 4011.205, 3905.6, 3934.274, 3975.748, 3880.012, 3951.478, 3872.772, 3829.912, 3938.011, 3917.453, 3845.429, 3839.529, 3963.005, 3842.506, 3806.485, 3792.066, 3804.955, 3811.099, 3839.338, 3891.024, 3782.463, 3816.021, 3881.643, 3767.805, 3723.347, 3757.178, 3722.644, 3886.729, 3787.39, 3792.194, 3747.037, 3829.823, 3817.664, 3698.149, 3826.401, 3761.4, 3858.577, 3738.117, 3809.823, 3833.477, 3809.227, 3698.249, 3818.192, 3653.396, 3721.04, 3834.148, 3733.087, 3762.383, 3730.755, 3663.004, 3707.887, 3720.985, 3653.168, 3788.817]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:127 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:65 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:gradient_descent lr:0.05229362301733192 momentum:0.8088966628343623 weight_decay:4.217074009964229e-05 nesterov:True batch_size:74 epochs:100	100	1000	True	22335.84766		6517352	18	-1	204.12696814537048	{'train_loss': [575695.0, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143, 25921.143], 'val_loss': [22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 22187.49]}	100	100	True
