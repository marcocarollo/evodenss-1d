id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.09110654586796615 batch_size:51 epochs:100	100	2000	True	2613.71118		657150	18	-1	119.94041156768799	{'train_loss': [6788.053, 6299.959, 6192.87, 5774.725, 5000.912, 4638.046, 4444.175, 4297.461, 4021.121, 3844.697, 3730.579, 3647.888, 3584.791, 3517.476, 3456.04, 3415.564, 3375.22, 3330.784, 3302.584, 3250.097, 3227.336, 3215.257, 3173.205, 3152.136, 3133.412, 3111.97, 3111.884, 3090.28, 3057.484, 3056.528, 3036.778, 3034.266, 3032.356, 2995.039, 2985.007, 2988.03, 2970.989, 2960.154, 2958.768, 2935.791, 2942.549, 2934.692, 2929.944, 2906.007, 2908.775, 2905.891, 2897.956, 2893.816, 2887.141, 2879.031, 2872.405, 2881.313, 2859.549, 2869.313, 2861.398, 2842.101, 2858.896, 2844.851, 2833.695, 2844.98, 2844.625, 2828.815, 2816.335, 2810.605, 2823.181, 2825.69, 2809.229, 2812.326, 2805.597, 2805.655, 2789.955, 2810.312, 2812.48, 2792.751, 2795.969, 2788.562, 2787.563, 2779.922, 2770.475, 2779.977, 2772.853, 2776.752, 2776.142, 2767.671, 2761.011, 2755.382, 2770.056, 2745.671, 2754.649, 2744.345, 2756.206, 2769.431, 2753.756, 2749.764, 2752.715, 2748.195, 2743.506, 2743.835, 2725.763, 2729.131], 'val_loss': [5508.472, 5376.369, 5326.432, 4444.634, 4055.028, 3748.435, 3666.848, 3511.15, 3321.656, 3243.509, 3170.1, 3119.261, 3072.998, 3002.773, 2978.29, 2980.035, 2955.459, 2880.074, 2900.531, 2845.993, 2875.323, 2848.973, 2847.372, 2811.488, 2800.988, 2814.44, 2803.686, 2774.791, 2778.197, 2773.771, 2732.716, 2742.927, 2753.508, 2745.904, 2717.297, 2727.869, 2756.306, 2729.845, 2698.391, 2709.392, 2713.411, 2701.854, 2690.411, 2689.258, 2707.157, 2696.882, 2678.344, 2688.199, 2674.902, 2664.85, 2694.339, 2659.442, 2678.922, 2669.451, 2673.323, 2648.265, 2657.692, 2644.875, 2657.648, 2680.056, 2682.184, 2656.384, 2642.981, 2659.127, 2636.634, 2626.232, 2646.644, 2620.405, 2628.564, 2644.507, 2641.803, 2636.363, 2633.755, 2659.326, 2622.922, 2617.713, 2616.378, 2619.917, 2611.395, 2654.388, 2603.529, 2615.815, 2606.93, 2602.044, 2596.18, 2625.292, 2608.189, 2626.874, 2605.69, 2616.553, 2586.762, 2594.48, 2593.623, 2585.797, 2615.022, 2591.231, 2587.745, 2570.967, 2594.401, 2586.821]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.09110654586796615 batch_size:51 epochs:100	100	2000	True	2719.58594		882568	19	-1	124.76861691474915	{'train_loss': [7060.033, 6567.694, 5937.687, 5147.071, 4768.876, 4536.476, 4287.299, 4122.663, 3983.198, 3878.672, 3770.281, 3696.866, 3627.914, 3566.607, 3533.574, 3488.018, 3426.296, 3398.131, 3367.926, 3337.499, 3311.295, 3289.011, 3255.041, 3220.213, 3207.479, 3196.974, 3183.645, 3153.304, 3141.197, 3112.773, 3117.328, 3088.208, 3073.34, 3063.866, 3056.559, 3057.229, 3039.63, 3027.891, 3014.624, 3003.092, 2996.903, 2989.419, 2982.96, 2959.892, 2946.68, 2952.941, 2946.101, 2943.882, 2925.091, 2924.597, 2922.013, 2918.832, 2912.548, 2900.715, 2892.18, 2893.104, 2880.719, 2877.308, 2888.479, 2869.884, 2879.29, 2864.581, 2869.622, 2865.867, 2857.222, 2854.35, 2840.932, 2856.069, 2846.162, 2833.281, 2830.726, 2835.737, 2824.112, 2817.193, 2823.1, 2836.789, 2803.872, 2815.318, 2813.553, 2820.983, 2806.039, 2809.124, 2796.474, 2802.448, 2786.663, 2792.449, 2793.022, 2782.487, 2796.029, 2779.879, 2788.456, 2781.267, 2782.151, 2776.269, 2778.595, 2771.016, 2766.509, 2766.351, 2772.286, 2766.703], 'val_loss': [5740.68, 5582.223, 4461.407, 4314.815, 4103.586, 3904.776, 3787.688, 3691.982, 3444.54, 3381.736, 3348.588, 3286.915, 3313.845, 3213.159, 3185.605, 3203.45, 3200.342, 3106.531, 3109.18, 3104.669, 3082.833, 3088.244, 3055.388, 3055.368, 3037.96, 3040.34, 2986.625, 3064.51, 3006.031, 3018.728, 2993.911, 2960.906, 2936.503, 2962.189, 2963.577, 2943.836, 2943.603, 2929.071, 2934.278, 2912.92, 2892.368, 2887.271, 2850.634, 2924.182, 2921.931, 2860.797, 2876.105, 2848.511, 2869.413, 2888.343, 2876.064, 2825.926, 2886.289, 2837.441, 2837.769, 2816.786, 2895.707, 2861.922, 2797.167, 2790.01, 2810.047, 2811.984, 2820.606, 2823.597, 2818.162, 2843.359, 2811.014, 2786.854, 2796.233, 2769.352, 2805.786, 2829.034, 2782.299, 2803.974, 2765.388, 2745.259, 2779.469, 2858.143, 2829.772, 2764.518, 2727.992, 2787.08, 2780.367, 2828.234, 2762.807, 2764.723, 2764.171, 2815.65, 2762.536, 2760.931, 2763.246, 2807.917, 2730.23, 2820.643, 2778.107, 2788.651, 2698.585, 2749.06, 2770.18, 2737.036]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:116 kernel_size:6 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:15 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adadelta lr:0.09110654586796615 batch_size:60 epochs:100	100	1000	True	3010.34351		741618	20	-1	117.00871253013611	{'train_loss': [8105.108, 7447.22, 7298.227, 7203.535, 7140.547, 6962.689, 6402.373, 5777.127, 5342.981, 5044.233, 4732.885, 4539.792, 4414.305, 4335.188, 4215.98, 4157.367, 4105.662, 4043.792, 4011.408, 3966.909, 3912.636, 3882.161, 3849.437, 3827.423, 3791.477, 3771.144, 3725.514, 3698.543, 3713.277, 3688.663, 3671.263, 3635.362, 3621.852, 3626.429, 3598.615, 3594.877, 3565.622, 3554.315, 3541.232, 3534.009, 3516.649, 3500.902, 3514.966, 3510.698, 3486.416, 3481.931, 3458.202, 3438.832, 3443.617, 3466.723, 3420.524, 3428.76, 3418.943, 3406.801, 3406.708, 3422.785, 3391.177, 3395.466, 3388.31, 3380.484, 3373.904, 3371.264, 3368.805, 3353.812, 3367.169, 3340.472, 3347.62, 3344.886, 3338.317, 3337.261, 3331.263, 3320.52, 3317.421, 3308.814, 3317.253, 3316.57, 3310.29, 3305.491, 3299.85, 3303.455, 3281.439, 3281.176, 3280.844, 3291.302, 3273.562, 3281.444, 3280.192, 3268.22, 3253.217, 3267.073, 3249.066, 3243.84, 3255.436, 3244.962, 3245.284, 3228.218, 3252.903, 3240.585, 3258.749, 3237.371], 'val_loss': [6411.348, 6191.253, 6138.445, 6098.455, 6087.329, 5534.615, 4983.14, 4328.662, 4239.595, 3976.257, 3754.796, 3687.633, 3568.692, 3637.793, 3593.718, 3523.207, 3419.987, 3489.508, 3349.979, 3324.976, 3358.379, 3340.293, 3347.087, 3308.93, 3281.1, 3255.106, 3214.302, 3244.575, 3189.682, 3201.705, 3209.633, 3189.464, 3231.882, 3209.964, 3295.603, 3162.073, 3227.366, 3190.9, 3127.117, 3180.925, 3152.174, 3156.915, 3136.588, 3125.671, 3137.347, 3086.056, 3089.938, 3094.8, 3102.212, 3098.719, 3070.215, 3099.76, 3093.01, 3056.705, 3089.654, 3130.037, 3088.685, 3112.512, 3115.022, 3136.005, 3078.386, 3042.64, 3064.676, 3056.198, 3088.72, 3053.475, 3058.312, 3050.184, 3061.932, 3078.652, 3024.164, 3107.456, 3058.219, 3033.574, 3030.909, 3034.492, 3039.081, 3012.028, 3070.684, 3019.807, 2990.677, 3016.389, 3030.048, 3003.613, 3032.062, 3039.07, 3018.594, 3006.257, 3081.779, 3012.719, 3020.281, 2989.441, 2984.657, 3016.925, 3009.503, 3029.272, 2990.558, 3029.086, 3052.187, 3015.868]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.09110654586796615 batch_size:51 epochs:100	100	3000	True	2616.90356		657150	18	-1	172.7930223941803	{'train_loss': [6768.751, 6297.757, 6183.668, 6068.056, 5616.338, 5010.227, 4597.499, 4237.635, 3994.571, 3846.283, 3767.086, 3682.67, 3605.137, 3544.91, 3487.414, 3443.035, 3404.108, 3370.535, 3334.158, 3315.03, 3284.301, 3253.35, 3219.204, 3199.87, 3182.304, 3165.067, 3159.023, 3138.998, 3128.751, 3103.431, 3078.075, 3066.787, 3063.328, 3048.713, 3019.388, 3022.01, 3007.581, 2990.312, 3000.823, 2992.297, 2977.337, 2971.842, 2965.775, 2950.884, 2949.641, 2936.232, 2928.724, 2943.342, 2910.263, 2908.768, 2917.534, 2896.629, 2919.138, 2899.015, 2899.06, 2886.116, 2873.274, 2870.347, 2856.514, 2865.615, 2875.969, 2859.446, 2854.06, 2851.316, 2838.477, 2847.311, 2824.809, 2829.429, 2821.945, 2833.433, 2828.32, 2814.55, 2809.412, 2812.625, 2797.108, 2812.402, 2792.689, 2773.871, 2787.054, 2794.7, 2777.681, 2774.581, 2766.598, 2779.574, 2760.441, 2767.062, 2764.751, 2775.727, 2768.084, 2762.903, 2767.845, 2755.509, 2735.985, 2761.291, 2754.656, 2728.393, 2743.191, 2744.402, 2742.421, 2729.449], 'val_loss': [5498.9, 5383.782, 5344.088, 4911.864, 4555.694, 4025.979, 3676.136, 3414.97, 3285.188, 3300.101, 3158.239, 3103.229, 3083.378, 3066.934, 3049.788, 3018.213, 3004.095, 2994.46, 2949.164, 2936.672, 2924.763, 2876.091, 2916.407, 2936.394, 2904.286, 2873.341, 2860.949, 2880.912, 2844.537, 2859.669, 2830.896, 2851.651, 2848.942, 2846.915, 2836.458, 2794.66, 2748.304, 2811.939, 2785.524, 2776.618, 2806.704, 2809.18, 2803.384, 2820.942, 2753.541, 2797.327, 2779.91, 2763.484, 2772.64, 2764.366, 2798.928, 2741.442, 2746.823, 2726.434, 2779.684, 2734.67, 2741.788, 2708.664, 2719.049, 2725.347, 2687.901, 2736.335, 2695.79, 2690.061, 2691.931, 2701.634, 2716.334, 2728.016, 2705.695, 2690.727, 2671.341, 2661.824, 2685.51, 2673.608, 2663.981, 2661.993, 2684.698, 2708.589, 2651.295, 2639.471, 2651.071, 2677.384, 2648.474, 2661.732, 2632.335, 2620.514, 2647.542, 2672.917, 2635.433, 2602.952, 2643.451, 2656.688, 2645.943, 2624.854, 2637.171, 2632.66, 2647.37, 2639.883, 2615.246, 2620.522]}	0	100	True
