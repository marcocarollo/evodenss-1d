id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.09110654586796615 batch_size:51 epochs:100	100	1000	True	2686.92896		657150	18	-1	171.46573138237	{'train_loss': [6789.409, 6289.172, 6176.849, 6105.131, 5681.257, 4897.699, 4406.907, 4104.343, 3949.821, 3781.122, 3706.44, 3625.719, 3560.351, 3527.245, 3456.842, 3428.469, 3380.564, 3362.6, 3320.105, 3291.68, 3296.642, 3254.894, 3227.388, 3217.282, 3187.071, 3166.731, 3150.093, 3131.281, 3123.033, 3107.297, 3097.122, 3092.855, 3083.28, 3058.11, 3057.654, 3044.716, 3029.527, 3011.895, 3008.638, 3007.887, 3003.393, 3001.002, 2980.794, 2978.637, 2971.113, 2965.77, 2970.689, 2950.479, 2935.539, 2947.465, 2933.628, 2928.127, 2933.129, 2919.475, 2922.298, 2916.134, 2910.792, 2908.611, 2906.473, 2909.279, 2889.977, 2893.29, 2893.259, 2888.621, 2890.577, 2879.074, 2881.075, 2874.225, 2860.465, 2858.167, 2861.483, 2848.301, 2849.975, 2845.885, 2846.795, 2835.842, 2836.996, 2845.184, 2831.477, 2835.371, 2843.84, 2829.562, 2824.906, 2830.661, 2820.309, 2825.469, 2817.129, 2827.327, 2821.161, 2812.652, 2802.482, 2804.311, 2807.111, 2817.079, 2800.717, 2813.316, 2800.918, 2800.072, 2801.005, 2797.63], 'val_loss': [5535.525, 5385.038, 5346.383, 5154.882, 4620.452, 4014.822, 3501.913, 3405.405, 3375.152, 3240.082, 3122.531, 3154.329, 3186.713, 3212.15, 3090.246, 3077.851, 3075.34, 3045.656, 3114.73, 2975.335, 3020.362, 3026.979, 2941.903, 3021.958, 2961.668, 2931.787, 2914.4, 2944.959, 2957.484, 2891.684, 3001.414, 2909.251, 2916.498, 2965.853, 2958.013, 2859.229, 2852.779, 2871.195, 2847.654, 2903.612, 2872.524, 2851.01, 2865.505, 2839.555, 2834.025, 2837.417, 2841.377, 2832.975, 2819.248, 2832.068, 2851.303, 2821.534, 2810.728, 2803.504, 2762.783, 2814.996, 2831.472, 2803.209, 2780.495, 2829.547, 2746.212, 2770.122, 2748.021, 2751.43, 2737.41, 2751.288, 2763.534, 2754.823, 2757.864, 2772.329, 2774.283, 2758.113, 2738.969, 2744.799, 2783.451, 2774.477, 2763.232, 2783.642, 2742.341, 2758.082, 2733.921, 2723.405, 2718.32, 2735.588, 2749.747, 2725.888, 2735.367, 2728.087, 2699.945, 2722.787, 2720.477, 2698.732, 2690.042, 2700.728, 2742.567, 2751.005, 2698.473, 2724.386, 2723.007, 2727.302]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:44 kernel_size:6 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:106 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.09110654586796615 batch_size:51 epochs:100	100	2000	True	2665.94824		941554	19	-1	182.79204845428467	{'train_loss': [6989.675, 6453.366, 6306.65, 6098.5, 5327.415, 4992.074, 4725.484, 4517.064, 4366.849, 4259.411, 4114.894, 3880.174, 3733.078, 3636.652, 3566.805, 3533.319, 3485.113, 3412.2, 3374.943, 3331.098, 3306.978, 3268.521, 3264.135, 3220.627, 3209.142, 3190.883, 3169.092, 3160.7, 3139.09, 3128.987, 3115.648, 3112.025, 3097.372, 3092.034, 3052.639, 3062.537, 3049.307, 3023.699, 3049.806, 3014.475, 3018.287, 2998.763, 3001.742, 2987.126, 2986.805, 2980.274, 2961.26, 2964.698, 2962.646, 2958.202, 2955.164, 2944.458, 2934.021, 2930.247, 2931.063, 2928.968, 2922.21, 2926.649, 2924.189, 2915.916, 2898.743, 2897.814, 2897.755, 2892.971, 2895.479, 2874.717, 2883.14, 2878.845, 2877.867, 2866.856, 2863.744, 2870.51, 2860.028, 2857.032, 2851.613, 2847.849, 2837.233, 2825.35, 2845.875, 2824.384, 2821.831, 2826.247, 2803.584, 2816.503, 2811.774, 2812.698, 2810.049, 2806.057, 2809.754, 2804.711, 2798.779, 2782.274, 2795.198, 2787.365, 2797.22, 2774.211, 2771.384, 2774.597, 2786.987, 2781.993], 'val_loss': [5661.195, 5496.241, 5396.492, 4790.455, 4064.215, 3891.931, 3731.42, 3680.044, 3715.223, 3670.079, 3422.246, 3328.963, 3243.972, 3215.34, 3166.145, 3124.072, 3154.585, 3057.97, 3048.545, 3009.925, 3000.735, 2988.968, 3032.993, 2936.374, 2925.629, 2925.8, 2943.729, 2920.473, 2888.25, 2921.451, 2889.956, 2875.573, 2867.603, 2865.774, 2840.123, 2881.883, 2865.669, 2871.488, 2823.129, 2867.176, 2845.245, 2831.22, 2798.028, 2823.895, 2818.521, 2802.037, 2821.058, 2803.349, 2786.742, 2784.028, 2795.963, 2783.393, 2775.356, 2793.254, 2787.204, 2754.753, 2746.104, 2761.023, 2795.603, 2736.706, 2782.201, 2746.174, 2732.618, 2746.015, 2743.577, 2787.597, 2750.94, 2726.334, 2757.14, 2748.317, 2737.552, 2732.732, 2728.189, 2762.597, 2702.339, 2726.422, 2706.962, 2732.583, 2683.228, 2709.107, 2726.433, 2724.831, 2698.568, 2697.341, 2687.561, 2690.546, 2711.693, 2689.347, 2683.533, 2681.647, 2700.79, 2671.527, 2687.462, 2682.531, 2674.699, 2685.986, 2667.385, 2656.703, 2641.542, 2669.678]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:3 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.057941479317861484 batch_size:51 epochs:100	100	1000	True	2707.28003		827799	19	-1	173.00184392929077	{'train_loss': [7084.822, 6471.19, 6392.217, 6318.17, 6103.245, 5367.511, 5023.704, 4767.942, 4606.641, 4456.335, 4355.877, 4242.356, 4086.971, 3989.159, 3892.381, 3827.673, 3743.145, 3687.642, 3635.032, 3580.523, 3540.185, 3485.056, 3442.385, 3419.707, 3384.728, 3355.956, 3332.219, 3311.223, 3286.193, 3247.231, 3236.965, 3206.856, 3192.293, 3171.236, 3157.013, 3138.827, 3112.142, 3114.218, 3085.582, 3068.102, 3063.563, 3051.997, 3044.872, 3038.54, 3017.701, 3012.454, 3005.635, 2988.666, 2982.725, 2972.572, 2953.001, 2946.135, 2958.876, 2938.333, 2922.552, 2928.321, 2917.037, 2911.279, 2901.078, 2897.232, 2894.441, 2875.507, 2890.01, 2870.638, 2877.281, 2876.123, 2860.533, 2871.224, 2859.679, 2861.467, 2854.018, 2849.214, 2853.778, 2840.075, 2839.481, 2826.534, 2829.426, 2816.675, 2824.814, 2813.482, 2815.923, 2801.572, 2815.054, 2813.1, 2796.384, 2806.378, 2795.238, 2788.066, 2788.996, 2778.485, 2780.135, 2795.259, 2777.792, 2772.836, 2770.659, 2779.564, 2770.813, 2760.145, 2761.44, 2775.214], 'val_loss': [5599.077, 5478.196, 5426.603, 5403.428, 4785.236, 4253.272, 3941.181, 3890.621, 3874.493, 3690.849, 3596.993, 3552.965, 3500.38, 3398.144, 3278.642, 3227.94, 3180.128, 3104.65, 3091.955, 3076.879, 3069.275, 3032.777, 3077.384, 2982.482, 2967.533, 2980.552, 2963.356, 2942.889, 2917.67, 2899.893, 2863.387, 2857.252, 2872.139, 2861.688, 2864.582, 2841.718, 2810.173, 2851.519, 2822.882, 2804.385, 2821.313, 2803.546, 2798.056, 2794.115, 2778.727, 2765.965, 2805.816, 2813.393, 2779.48, 2777.765, 2768.212, 2812.446, 2762.94, 2759.859, 2772.067, 2748.42, 2745.566, 2747.292, 2748.629, 2769.991, 2766.203, 2739.016, 2743.122, 2714.208, 2755.782, 2710.149, 2746.354, 2765.867, 2737.954, 2731.853, 2737.171, 2748.712, 2759.755, 2727.617, 2757.718, 2764.521, 2761.594, 2732.048, 2719.893, 2716.197, 2710.486, 2721.291, 2728.138, 2734.938, 2706.186, 2703.497, 2709.339, 2709.382, 2715.759, 2719.353, 2685.616, 2701.245, 2723.183, 2710.382, 2724.528, 2708.048, 2702.599, 2691.753, 2686.472, 2708.605]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.09110654586796615 batch_size:51 epochs:100	100	2000	True	2616.90356		657150	18	-1	172.41294503211975	{'train_loss': [6768.751, 6297.757, 6183.668, 6068.056, 5616.338, 5010.227, 4597.499, 4237.635, 3994.571, 3846.283, 3767.086, 3682.67, 3605.137, 3544.91, 3487.414, 3443.035, 3404.108, 3370.535, 3334.158, 3315.03, 3284.301, 3253.35, 3219.204, 3199.87, 3182.304, 3165.067, 3159.023, 3138.998, 3128.751, 3103.431, 3078.075, 3066.787, 3063.328, 3048.713, 3019.388, 3022.01, 3007.581, 2990.312, 3000.823, 2992.297, 2977.337, 2971.842, 2965.775, 2950.884, 2949.641, 2936.232, 2928.724, 2943.342, 2910.263, 2908.768, 2917.534, 2896.629, 2919.138, 2899.015, 2899.06, 2886.116, 2873.274, 2870.347, 2856.514, 2865.615, 2875.969, 2859.446, 2854.06, 2851.316, 2838.477, 2847.311, 2824.809, 2829.429, 2821.945, 2833.433, 2828.32, 2814.55, 2809.412, 2812.625, 2797.108, 2812.402, 2792.689, 2773.871, 2787.054, 2794.7, 2777.681, 2774.581, 2766.598, 2779.574, 2760.441, 2767.062, 2764.751, 2775.727, 2768.084, 2762.903, 2767.845, 2755.509, 2735.985, 2761.291, 2754.656, 2728.393, 2743.191, 2744.402, 2742.421, 2729.449], 'val_loss': [5498.9, 5383.782, 5344.088, 4911.864, 4555.694, 4025.979, 3676.136, 3414.97, 3285.188, 3300.101, 3158.239, 3103.229, 3083.378, 3066.934, 3049.788, 3018.213, 3004.095, 2994.46, 2949.164, 2936.672, 2924.763, 2876.091, 2916.407, 2936.394, 2904.286, 2873.341, 2860.949, 2880.912, 2844.537, 2859.669, 2830.896, 2851.651, 2848.942, 2846.915, 2836.458, 2794.66, 2748.304, 2811.939, 2785.524, 2776.618, 2806.704, 2809.18, 2803.384, 2820.942, 2753.541, 2797.327, 2779.91, 2763.484, 2772.64, 2764.366, 2798.928, 2741.442, 2746.823, 2726.434, 2779.684, 2734.67, 2741.788, 2708.664, 2719.049, 2725.347, 2687.901, 2736.335, 2695.79, 2690.061, 2691.931, 2701.634, 2716.334, 2728.016, 2705.695, 2690.727, 2671.341, 2661.824, 2685.51, 2673.608, 2663.981, 2661.993, 2684.698, 2708.589, 2651.295, 2639.471, 2651.071, 2677.384, 2648.474, 2661.732, 2632.335, 2620.514, 2647.542, 2672.917, 2635.433, 2602.952, 2643.451, 2656.688, 2645.943, 2624.854, 2637.171, 2632.66, 2647.37, 2639.883, 2615.246, 2620.522]}	0	100	True
