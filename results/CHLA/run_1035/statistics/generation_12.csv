id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:51 epochs:100	100	1000	True	2789.3208		3472828	18	-1	230.31133317947388	{'train_loss': [7787.568, 6304.895, 6027.091, 5669.277, 5278.917, 4930.857, 4578.295, 4432.814, 4313.439, 4161.205, 4085.207, 3972.844, 3889.81, 3818.98, 3735.708, 3710.763, 3651.3, 3618.759, 3570.645, 3533.743, 3492.483, 3485.245, 3445.674, 3431.382, 3404.333, 3357.04, 3373.369, 3349.923, 3324.845, 3302.572, 3303.588, 3274.115, 3255.992, 3234.172, 3228.346, 3219.198, 3207.348, 3192.446, 3170.362, 3165.942, 3135.5, 3144.018, 3123.573, 3115.989, 3110.026, 3088.764, 3074.517, 3074.921, 3082.73, 3054.005, 3043.707, 3047.242, 3030.277, 3013.276, 3015.804, 2994.025, 2991.107, 2990.86, 2976.628, 2967.983, 2973.844, 2966.79, 2971.81, 2940.783, 2925.117, 2943.985, 2920.774, 2935.487, 2907.115, 2907.367, 2922.702, 2901.386, 2899.956, 2900.838, 2893.303, 2871.106, 2866.945, 2872.321, 2856.637, 2863.467, 2854.974, 2846.278, 2829.601, 2840.242, 2831.648, 2839.233, 2813.118, 2826.167, 2813.906, 2802.483, 2806.031, 2807.658, 2798.407, 2802.7, 2792.163, 2773.478, 2778.526, 2788.182, 2776.553, 2765.405], 'val_loss': [6852.05, 6078.583, 6200.424, 5297.649, 5407.611, 4831.457, 4218.696, 4221.065, 4049.642, 3989.704, 3687.376, 3722.272, 3704.805, 3563.816, 3477.736, 3481.714, 3479.229, 3309.861, 3273.245, 3259.713, 3316.325, 3168.169, 3324.402, 3165.156, 3164.803, 3176.148, 3135.507, 3197.11, 3182.493, 3159.42, 3159.432, 3097.327, 3046.663, 3143.156, 3062.915, 3082.321, 3112.349, 3137.56, 3051.453, 3032.002, 2984.849, 2987.992, 2971.576, 2959.355, 2955.958, 2896.649, 2962.174, 2957.25, 2891.284, 2928.488, 2926.415, 2866.426, 2900.926, 2895.23, 2839.468, 2838.223, 2899.396, 2934.92, 2842.717, 2853.091, 2859.778, 2810.517, 2806.945, 2800.311, 2829.511, 2830.966, 2817.155, 2791.448, 2768.015, 2796.582, 2783.309, 2800.251, 2780.787, 2781.089, 2787.062, 2756.952, 2783.103, 2753.946, 2738.163, 2760.082, 2755.74, 2783.694, 2755.421, 2707.544, 2726.865, 2705.496, 2747.052, 2684.627, 2748.583, 2721.429, 2748.829, 2757.066, 2728.06, 2690.552, 2698.442, 2721.096, 2736.06, 2712.367, 2764.449, 2710.718]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:83 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:50 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:43 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.05229362301733192 beta1:0.8666172791462436 beta2:0.8650646291501731 weight_decay:9.289808040566646e-05 batch_size:51 epochs:100	100	1000	True	17349.78516		2009850	19	-1	227.85820937156677	{'train_loss': [21394.508, 18488.414, 20085.072, 18939.104, 17866.514, 19041.887, 17935.949, 17789.355, 18216.543, 17840.104, 17791.275, 18296.674, 18072.492, 17753.014, 18102.656, 18041.525, 17806.385, 17807.395, 18351.295, 17808.184, 17829.711, 18173.527, 17996.445, 17802.133, 17888.834, 18208.752, 17794.926, 17818.668, 18239.678, 17923.355, 17758.828, 18145.768, 18109.465, 17738.93, 17867.695, 18317.352, 17909.227, 17796.24, 18182.719, 17913.82, 17800.586, 17900.93, 18200.818, 17827.787, 17882.746, 18246.627, 17891.602, 17809.627, 18059.057, 18055.656, 17818.586, 17894.955, 18436.215, 17901.291, 17869.645, 18147.971, 17948.514, 17830.76, 17925.764, 18146.164, 17916.727, 17807.336, 18183.02, 17995.248, 17804.068, 18096.592, 18091.986, 17862.586, 17887.889, 18244.096, 17952.922, 17774.557, 18262.029, 18058.648, 17872.865, 17884.287, 18288.568, 17877.141, 17857.346, 18036.852, 18026.912, 17917.018, 18020.936, 18095.623, 17908.844, 17998.564, 18144.354, 17960.891, 17882.479, 18034.852, 18110.867, 17875.52, 17949.803, 18170.02, 17917.938, 17911.932, 18040.762, 18162.762, 17871.566, 17966.842], 'val_loss': [1268816.875, 16640.615, 18251.199, 16640.609, 16890.209, 16640.615, 17335.418, 16639.84, 17969.844, 16640.615, 16640.615, 18570.377, 78046.758, 16767.137, 16640.543, 16629.619, 16640.615, 16958.975, 17191.455, 16640.615, 17356.51, 17137.785, 16640.615, 16640.615, 24115.973, 16615.092, 16740.518, 26822.672, 17384.975, 16640.615, 16590.256, 17025.598, 16640.615, 16640.613, 17146.617, 16639.352, 16640.615, 16640.615, 17975.627, 21273.697, 16623.576, 16640.537, 16961.029, 16640.615, 16640.615, 30429.004, 17325.127, 17644.656, 17013.832, 19073.477, 16640.615, 18420.857, 17343.197, 16640.609, 16640.338, 17593.539, 16623.367, 16640.615, 16575.941, 16640.615, 16640.562, 16893.248, 16624.811, 17151.424, 17385.816, 17316.762, 16863.48, 16640.615, 16640.443, 16640.615, 16640.615, 16672.48, 17597.031, 16640.615, 16640.615, 16640.615, 16638.676, 16629.479, 16640.229, 17229.111, 16640.615, 16640.615, 16640.615, 17714.141, 16640.615, 17228.908, 16951.004, 16636.824, 16795.6, 16605.914, 16640.615, 16913.789, 16640.594, 16640.596, 16640.615, 16635.021, 17340.949, 18553.881, 16640.615, 17306.926]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:89 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:84 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adadelta lr:0.05229362301733192 batch_size:60 epochs:100	100	1000	True	3289.92896		2076733	20	-1	191.79182767868042	{'train_loss': [9128.979, 8262.039, 7221.945, 6530.214, 6171.967, 5862.124, 5621.046, 5442.093, 5302.478, 5179.169, 5024.057, 4910.851, 4822.309, 4745.15, 4694.37, 4659.058, 4596.708, 4545.23, 4487.513, 4460.147, 4406.069, 4392.162, 4355.673, 4319.9, 4262.24, 4242.542, 4209.742, 4201.428, 4155.828, 4141.572, 4117.991, 4093.628, 4071.023, 4069.33, 4057.835, 4037.114, 4012.992, 4014.39, 3998.567, 3981.365, 3965.765, 3953.362, 3934.592, 3919.101, 3895.158, 3900.647, 3890.147, 3875.697, 3849.973, 3850.802, 3859.674, 3831.951, 3804.41, 3837.351, 3794.339, 3796.394, 3784.417, 3750.213, 3774.98, 3738.814, 3733.698, 3730.551, 3722.264, 3723.566, 3706.375, 3694.694, 3699.313, 3703.969, 3680.854, 3668.661, 3656.855, 3634.403, 3647.644, 3643.4, 3640.771, 3621.97, 3627.058, 3612.04, 3612.984, 3601.542, 3577.231, 3575.843, 3601.148, 3575.206, 3569.057, 3551.614, 3567.432, 3558.472, 3542.469, 3550.658, 3539.143, 3517.794, 3531.143, 3521.205, 3514.999, 3509.031, 3497.699, 3516.286, 3484.282, 3503.601], 'val_loss': [7718.442, 6248.292, 5874.734, 5721.844, 5566.036, 4766.79, 4694.444, 4656.94, 4527.199, 4368.57, 4307.559, 4324.046, 4147.917, 4257.245, 4346.383, 4240.38, 4218.48, 4292.424, 4083.866, 4156.882, 3945.731, 3990.701, 4041.863, 3978.937, 3906.893, 3853.196, 3661.369, 3735.008, 3744.92, 3760.829, 3741.297, 3721.239, 3663.978, 3873.2, 3628.166, 3597.222, 3709.014, 3614.982, 3477.913, 3663.52, 3504.961, 3587.375, 3528.827, 3472.467, 3491.283, 3580.209, 3442.546, 3472.451, 3430.181, 3479.356, 3459.601, 3512.365, 3435.309, 3424.245, 3515.99, 3367.3, 3443.315, 3389.415, 3445.167, 3473.124, 3352.687, 3330.962, 3330.209, 3351.068, 3341.379, 3348.172, 3366.34, 3328.41, 3309.415, 3338.422, 3341.043, 3354.561, 3327.124, 3335.875, 3366.711, 3311.13, 3309.904, 3270.774, 3290.382, 3289.505, 3282.185, 3319.902, 3388.499, 3280.029, 3287.781, 3313.415, 3269.369, 3232.627, 3266.343, 3254.306, 3279.973, 3250.599, 3305.933, 3312.169, 3254.196, 3263.904, 3247.692, 3230.894, 3246.961, 3216.071]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:95 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:44 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:41 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:85 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:65 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:95 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:gradient_descent lr:0.05229362301733192 momentum:0.9872416178771388 weight_decay:7.589989371113975e-05 nesterov:True batch_size:51 epochs:100	100	1000	True	16945.20508		12598346	19	-1	393.24985241889954	{'train_loss': [5099011.5, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783, 17712.783], 'val_loss': [16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615, 16640.615]}	100	100	True
