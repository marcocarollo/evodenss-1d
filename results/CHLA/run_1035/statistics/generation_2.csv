id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:13 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.003133221785214082 batch_size:74 epochs:100	100	1000	True	4287.66602		2525399	15	-1	182.13665962219238	{'train_loss': [12249.396, 11017.508, 10898.857, 10777.448, 10660.558, 10520.645, 10394.989, 10277.093, 10191.794, 10098.325, 10030.485, 9948.793, 9896.271, 9849.455, 9789.596, 9742.646, 9712.909, 9669.5, 9639.266, 9610.611, 9554.018, 9519.836, 9476.606, 9401.707, 9230.891, 8936.033, 8713.678, 8483.917, 8094.494, 7749.116, 7531.427, 7419.499, 7318.782, 7245.802, 7166.043, 7078.876, 7019.703, 6959.244, 6908.055, 6849.539, 6816.621, 6769.919, 6725.887, 6704.239, 6662.373, 6636.129, 6613.354, 6576.987, 6560.763, 6532.243, 6500.914, 6482.353, 6463.278, 6433.245, 6431.11, 6400.588, 6381.884, 6352.145, 6317.805, 6316.504, 6302.741, 6264.924, 6262.452, 6252.834, 6230.856, 6220.773, 6183.778, 6167.423, 6147.544, 6163.329, 6125.132, 6118.751, 6089.152, 6088.991, 6075.571, 6030.043, 6034.375, 6024.51, 5996.323, 5984.773, 5982.178, 5958.966, 5959.999, 5932.593, 5936.234, 5908.587, 5894.797, 5883.58, 5858.253, 5847.365, 5844.23, 5831.174, 5806.17, 5818.915, 5787.814, 5755.876, 5771.137, 5754.339, 5746.458, 5741.298], 'val_loss': [9270.78, 9472.589, 9131.189, 8801.522, 8540.259, 8312.071, 8145.408, 7977.438, 7861.393, 7751.718, 7668.16, 7579.437, 7549.909, 7510.059, 7450.649, 7436.563, 7390.979, 7345.036, 7336.878, 7304.841, 7277.48, 7255.479, 7204.413, 7059.133, 6800.341, 6560.004, 6416.969, 6132.611, 5833.783, 5690.45, 5622.351, 5511.276, 5415.031, 5350.452, 5271.627, 5220.279, 5144.905, 5133.961, 5082.954, 5052.733, 4999.667, 4994.042, 4975.21, 4932.972, 4933.035, 4896.964, 4892.12, 4872.746, 4838.258, 4839.28, 4824.129, 4814.955, 4796.483, 4785.675, 4762.239, 4738.923, 4750.426, 4732.067, 4716.584, 4678.762, 4700.619, 4679.902, 4652.621, 4636.225, 4628.301, 4600.609, 4609.244, 4591.911, 4577.38, 4574.086, 4556.19, 4537.067, 4522.759, 4518.898, 4508.177, 4483.914, 4487.655, 4460.887, 4461.512, 4420.915, 4434.821, 4417.635, 4398.227, 4382.445, 4381.856, 4378.045, 4367.005, 4344.997, 4338.413, 4341.053, 4303.937, 4301.852, 4294.131, 4288.752, 4281.146, 4270.66, 4252.636, 4249.865, 4242.113, 4243.212]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:13 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.009145972964182274 batch_size:74 epochs:100	100	1000	True	3665.2666		1498441	16	-1	166.76241612434387	{'train_loss': [11849.416, 10789.603, 10401.084, 10037.002, 9780.331, 9636.567, 9528.787, 9463.319, 9406.873, 9380.208, 9339.672, 9296.081, 9043.461, 8394.158, 7910.365, 7641.44, 7440.775, 7188.068, 7081.044, 6904.355, 6805.204, 6724.179, 6633.991, 6615.021, 6570.661, 6504.747, 6482.067, 6454.582, 6414.205, 6398.384, 6368.933, 6341.344, 6302.292, 6278.848, 6248.346, 6210.351, 6205.404, 6160.978, 6129.988, 6118.04, 6077.947, 6038.874, 6001.211, 5984.344, 5962.669, 5908.273, 5867.965, 5802.417, 5763.269, 5679.111, 5644.644, 5589.705, 5560.098, 5503.871, 5469.783, 5443.708, 5383.705, 5364.938, 5320.249, 5282.15, 5251.716, 5210.358, 5203.617, 5174.237, 5148.167, 5116.755, 5090.713, 5072.316, 5054.668, 5028.163, 5016.171, 4988.529, 4978.494, 4946.262, 4933.758, 4927.488, 4903.363, 4873.011, 4901.348, 4857.997, 4841.428, 4835.553, 4820.715, 4802.869, 4784.358, 4770.797, 4762.419, 4743.071, 4722.767, 4733.216, 4710.751, 4710.67, 4697.399, 4691.948, 4680.403, 4664.43, 4653.507, 4636.105, 4630.24, 4626.709], 'val_loss': [9642.571, 8799.772, 8076.162, 7655.781, 7440.578, 7354.388, 7272.667, 7250.988, 7212.91, 7193.196, 7184.442, 7146.798, 6375.349, 5957.09, 5667.206, 5392.171, 5234.966, 5134.09, 4977.455, 4927.899, 4933.481, 4900.331, 4862.962, 4842.394, 4846.27, 4814.991, 4812.075, 4797.604, 4770.142, 4764.522, 4751.334, 4723.475, 4718.025, 4710.383, 4683.775, 4683.565, 4644.964, 4638.26, 4616.877, 4600.774, 4569.27, 4541.84, 4530.837, 4504.42, 4493.21, 4463.857, 4429.817, 4390.262, 4366.062, 4319.927, 4311.903, 4294.511, 4251.206, 4233.604, 4198.308, 4189.714, 4150.318, 4165.808, 4128.983, 4077.092, 4056.393, 4061.639, 4032.423, 4034.154, 4000.592, 3984.865, 3967.736, 3958.499, 3926.897, 3921.645, 3906.978, 3900.357, 3883.166, 3887.097, 3869.47, 3860.929, 3842.669, 3845.443, 3820.54, 3834.057, 3810.87, 3806.822, 3799.768, 3796.907, 3784.491, 3783.48, 3773.788, 3770.462, 3765.877, 3759.41, 3752.678, 3761.73, 3757.629, 3746.9, 3732.479, 3735.123, 3736.155, 3721.722, 3713.734, 3714.855]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:50 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:13 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.003133221785214082 batch_size:81 epochs:100	100	1000	True	6070.71729		678301	16	-1	139.5912640094757	{'train_loss': [15021.126, 13649.178, 12461.461, 11786.378, 11522.484, 11381.12, 11231.163, 11115.835, 10988.878, 10902.852, 10814.418, 10709.518, 10636.603, 10568.152, 10511.032, 10471.142, 10428.253, 10369.247, 10344.856, 10311.438, 10284.688, 10244.57, 10230.711, 10205.926, 10190.052, 10164.755, 10149.396, 10148.797, 10125.629, 10111.077, 10102.483, 10084.73, 10066.724, 10059.705, 10049.105, 10034.323, 10035.049, 10027.869, 10009.732, 9994.947, 9985.363, 9982.207, 9970.336, 9960.67, 9949.537, 9932.073, 9915.606, 9893.796, 9868.038, 9810.542, 9694.871, 9508.467, 9276.511, 9085.26, 8930.168, 8809.355, 8716.131, 8655.422, 8590.542, 8547.03, 8510.497, 8466.313, 8454.974, 8389.108, 8353.283, 8335.488, 8283.299, 8239.878, 8200.558, 8179.133, 8109.834, 8086.294, 8038.161, 7997.3, 7952.062, 7900.307, 7850.37, 7805.41, 7751.226, 7715.559, 7678.682, 7635.701, 7605.36, 7570.986, 7550.804, 7518.396, 7494.847, 7478.566, 7454.658, 7423.557, 7410.982, 7403.609, 7369.711, 7363.728, 7324.353, 7311.708, 7304.512, 7296.927, 7276.936, 7258.104], 'val_loss': [10485.313, 9783.625, 10122.758, 10391.94, 10357.057, 10172.177, 9897.532, 9679.615, 9495.047, 9355.258, 9223.946, 9128.262, 9059.188, 8999.718, 8946.458, 8908.107, 8870.215, 8846.936, 8816.445, 8798.535, 8779.856, 8767.905, 8761.553, 8737.791, 8723.773, 8720.817, 8707.05, 8698.588, 8692.565, 8690.157, 8681.093, 8676.146, 8673.292, 8665.657, 8652.664, 8650.413, 8647.025, 8642.646, 8633.599, 8625.829, 8618.472, 8614.904, 8614.215, 8601.959, 8593.23, 8583.15, 8567.311, 8535.177, 8489.199, 8388.089, 8180.256, 7894.953, 7652.117, 7480.263, 7379.319, 7291.086, 7240.455, 7200.744, 7157.232, 7120.301, 7097.565, 7061.76, 7028.294, 6979.367, 6944.562, 6903.905, 6854.604, 6820.9, 6787.821, 6734.934, 6682.95, 6640.862, 6575.419, 6532.741, 6470.888, 6407.792, 6372.62, 6339.51, 6308.29, 6281.683, 6237.342, 6228.339, 6184.779, 6195.052, 6161.855, 6175.75, 6154.572, 6146.145, 6130.883, 6121.539, 6131.812, 6121.697, 6114.638, 6106.04, 6084.732, 6095.561, 6087.274, 6074.806, 6069.202, 6053.888]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:109 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:79 kernel_size:5 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:50 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:40 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:85 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:13 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adadelta lr:0.003133221785214082 batch_size:69 epochs:100	100	1000	True	4613.80469		17697338	17	-1	271.52179169654846	{'train_loss': [10805.424, 10498.284, 10451.994, 10440.234, 10407.643, 10383.418, 10353.022, 10318.172, 10264.822, 10219.634, 10187.578, 10125.261, 10045.159, 9935.291, 9565.678, 8941.544, 8669.257, 8552.413, 8438.814, 8358.734, 8262.467, 8178.181, 8080.081, 7978.662, 7887.563, 7798.3, 7693.148, 7607.392, 7555.332, 7480.925, 7415.631, 7316.404, 7275.632, 7217.668, 7146.923, 7091.496, 7025.243, 6949.912, 6854.188, 6791.864, 6716.872, 6646.641, 6587.518, 6558.442, 6480.28, 6405.319, 6376.548, 6310.82, 6257.697, 6204.053, 6154.894, 6140.552, 6102.557, 6039.36, 6029.273, 5984.701, 5967.777, 5948.109, 5896.408, 5865.113, 5849.125, 5822.076, 5781.575, 5753.773, 5745.989, 5726.354, 5674.552, 5682.614, 5656.336, 5636.199, 5620.439, 5603.397, 5581.26, 5554.467, 5550.707, 5523.604, 5517.643, 5487.566, 5462.199, 5447.202, 5430.522, 5420.541, 5409.12, 5396.882, 5382.892, 5371.131, 5345.909, 5340.177, 5327.253, 5308.492, 5301.437, 5276.085, 5261.709, 5242.419, 5232.918, 5224.507, 5234.415, 5204.848, 5195.092, 5197.402], 'val_loss': [11489.139, 11488.096, 11321.979, 11192.793, 10863.088, 10802.684, 10569.478, 10134.325, 9998.607, 9754.636, 9673.136, 9438.951, 9258.396, 8801.007, 8369.357, 9023.775, 8886.805, 8545.547, 8199.15, 7998.36, 7757.766, 7447.925, 7214.06, 7113.848, 6664.016, 6685.62, 6567.903, 6345.208, 6263.069, 6259.436, 6133.748, 5954.081, 5978.424, 5783.795, 5697.556, 5690.256, 5634.457, 5535.473, 5399.82, 5346.057, 5203.18, 5291.357, 5126.479, 5023.324, 5107.792, 5011.473, 4972.825, 4944.246, 4920.882, 4854.827, 4882.975, 4800.831, 4789.325, 4794.231, 4783.145, 4752.775, 4737.497, 4676.325, 4671.44, 4632.447, 4661.538, 4571.32, 4561.653, 4574.279, 4526.273, 4561.955, 4536.344, 4542.529, 4505.354, 4437.038, 4481.516, 4490.942, 4443.394, 4433.602, 4409.175, 4419.425, 4397.191, 4377.893, 4353.593, 4363.594, 4350.406, 4321.708, 4315.669, 4316.711, 4292.015, 4318.649, 4312.01, 4305.298, 4254.579, 4266.704, 4226.955, 4266.106, 4239.819, 4207.785, 4203.4, 4217.722, 4174.211, 4194.756, 4197.926, 4180.963]}	100	100	True
