id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:51 epochs:100	100	1000	True	2682.68701		763315	18	-1	153.90916061401367	{'train_loss': [7020.095, 6425.408, 6336.669, 6007.774, 5080.345, 4612.126, 4382.631, 4234.723, 4132.438, 4045.017, 3963.169, 3888.544, 3840.982, 3774.685, 3731.3, 3695.499, 3637.169, 3614.175, 3566.513, 3547.276, 3513.885, 3489.811, 3456.902, 3424.893, 3402.168, 3380.874, 3360.688, 3342.565, 3326.557, 3299.345, 3284.854, 3266.928, 3247.094, 3226.388, 3206.788, 3196.123, 3180.257, 3178.269, 3151.876, 3135.392, 3135.222, 3106.696, 3110.165, 3091.637, 3082.543, 3070.264, 3060.527, 3050.837, 3029.56, 3037.391, 3024.469, 3018.672, 3001.567, 2989.881, 2988.443, 2984.119, 2976.154, 2975.38, 2950.823, 2941.948, 2935.704, 2957.88, 2945.968, 2927.575, 2927.827, 2902.806, 2894.268, 2906.425, 2909.291, 2898.321, 2893.122, 2883.757, 2867.365, 2883.034, 2871.113, 2865.066, 2868.66, 2862.04, 2858.952, 2850.91, 2844.751, 2830.455, 2823.752, 2840.28, 2827.592, 2819.031, 2825.051, 2817.588, 2818.942, 2809.413, 2819.166, 2802.249, 2804.315, 2797.771, 2795.519, 2787.792, 2789.985, 2781.381, 2784.553, 2777.393], 'val_loss': [5562.513, 5457.743, 5395.101, 4523.804, 4258.851, 3674.65, 3538.807, 3410.415, 3318.307, 3269.438, 3247.024, 3246.726, 3204.11, 3207.755, 3158.942, 3153.114, 3089.825, 3123.628, 3146.307, 3106.972, 3049.918, 3040.406, 3108.285, 3002.866, 3089.112, 3039.873, 3085.547, 3039.317, 2983.153, 2970.56, 2989.935, 2966.329, 2976.943, 2956.84, 2968.603, 2957.433, 2934.975, 2922.144, 2896.49, 2911.104, 2953.618, 2981.575, 2924.802, 2868.498, 2923.411, 2910.598, 2954.22, 2886.86, 2865.128, 2887.307, 2872.468, 2864.539, 2914.915, 2893.965, 2820.582, 2812.614, 2795.18, 2797.245, 2836.97, 2834.846, 2799.386, 2764.835, 2800.582, 2794.094, 2793.936, 2839.152, 2852.521, 2782.607, 2771.6, 2856.309, 2760.017, 2780.384, 2744.317, 2826.472, 2760.896, 2745.83, 2751.256, 2768.719, 2759.229, 2732.599, 2726.468, 2787.533, 2795.328, 2713.367, 2739.326, 2762.894, 2734.815, 2739.182, 2707.453, 2751.758, 2736.853, 2735.153, 2708.535, 2739.697, 2694.448, 2750.627, 2766.654, 2743.673, 2721.295, 2714.134]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:41 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:126 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:40 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:41 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:47 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.004809640334397808 batch_size:51 epochs:100	100	1000	True	3667.72607		585785	19	-1	149.32617688179016	{'train_loss': [9301.062, 7955.462, 7107.326, 6784.379, 6646.689, 6544.893, 6487.77, 6453.435, 6427.198, 6391.165, 6378.898, 6359.645, 6350.556, 6329.111, 6324.104, 6310.219, 6290.911, 6290.763, 6287.181, 6274.743, 6261.64, 6256.763, 6244.557, 6242.988, 6234.466, 6228.414, 6217.854, 6215.128, 6204.791, 6202.169, 6199.767, 6194.476, 6186.542, 6180.722, 6179.652, 6170.952, 6163.397, 6165.03, 6159.723, 6147.581, 6130.886, 6088.339, 5980.446, 5868.717, 5796.442, 5744.174, 5701.011, 5693.377, 5678.065, 5676.511, 5640.614, 5642.921, 5629.326, 5613.967, 5605.371, 5586.41, 5571.366, 5560.335, 5536.218, 5528.676, 5521.317, 5488.201, 5468.931, 5468.286, 5430.659, 5412.521, 5376.003, 5363.247, 5339.799, 5314.471, 5288.204, 5273.119, 5237.52, 5199.689, 5176.861, 5132.687, 5114.463, 5046.774, 4999.156, 4991.16, 4932.396, 4905.663, 4879.366, 4842.969, 4814.868, 4785.945, 4769.189, 4710.169, 4700.778, 4652.763, 4626.539, 4613.446, 4575.479, 4559.572, 4525.315, 4524.914, 4489.721, 4487.734, 4454.618, 4431.885], 'val_loss': [6686.748, 6211.29, 6051.462, 5879.66, 5723.542, 5643.999, 5575.513, 5532.691, 5501.325, 5490.647, 5463.014, 5459.761, 5446.189, 5438.471, 5431.336, 5420.159, 5410.144, 5406.559, 5404.463, 5401.405, 5393.327, 5390.956, 5390.385, 5386.393, 5379.884, 5378.516, 5374.877, 5370.71, 5372.56, 5371.658, 5368.583, 5366.546, 5359.504, 5357.376, 5357.521, 5354.603, 5352.619, 5346.709, 5345.388, 5335.371, 5303.455, 5203.986, 5009.679, 4895.828, 4838.297, 4794.896, 4781.332, 4762.721, 4746.735, 4736.978, 4729.23, 4718.495, 4701.395, 4693.858, 4692.334, 4672.917, 4662.426, 4650.44, 4635.83, 4625.13, 4608.653, 4599.557, 4577.207, 4554.805, 4532.7, 4507.543, 4485.468, 4460.926, 4436.011, 4421.752, 4408.461, 4378.056, 4357.819, 4309.906, 4299.938, 4256.398, 4241.458, 4211.584, 4185.333, 4156.603, 4139.267, 4131.863, 4079.763, 4086.001, 4050.157, 4039.346, 3994.545, 3992.999, 3966.018, 3928.612, 3914.559, 3897.289, 3859.723, 3831.036, 3791.488, 3775.332, 3767.695, 3744.204, 3729.44, 3701.905]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:83 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:31 kernel_size:4 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:deconv1d out_channels:108 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:rmsprop lr:0.05229362301733192 alpha:0.8502199841683117 weight_decay:0.0008660202172945878 batch_size:51 epochs:100	100	1000	True	18633.0		1869949	20	-1	165.29714012145996	{'train_loss': [18244.141, 23144.646, 27409.23, 22179.039, 22522.285, 21329.711, 21862.178, 20915.049, 21030.557, 18297.732, 22381.385, 18577.094, 21181.766, 19349.533, 19469.473, 19967.963, 19872.219, 20008.643, 19379.91, 19913.924, 19612.914, 19813.566, 19663.297, 19582.078, 20003.273, 19605.117, 20122.701, 19561.691, 20070.525, 20459.168, 19111.744, 19907.914, 19516.061, 19661.721, 19693.824, 19911.672, 19892.32, 19662.932, 19532.656, 19534.949, 19844.393, 19661.699, 20230.299, 19625.789, 19701.545, 20211.422, 19190.793, 20220.48, 19873.871, 19726.086, 20048.885, 19655.955, 19520.07, 20039.42, 19573.311, 19643.984, 19628.65, 19552.531, 19807.967, 20133.453, 19314.678, 20215.104, 19448.91, 19568.762, 19922.053, 19639.49, 19985.91, 19349.406, 19643.666, 19793.229, 19661.01, 19460.1, 20378.715, 20026.867, 19332.229, 19594.922, 20387.232, 19371.561, 19871.293, 19812.377, 19308.25, 19928.965, 20043.854, 19485.107, 19708.303, 19970.023, 19984.064, 19470.654, 19908.857, 19596.607, 19994.865, 19336.312, 19716.131, 20184.871, 19340.352, 19466.758, 19732.24, 19876.162, 19572.758, 20056.816], 'val_loss': [16640.615, 206961.578, 16640.615, 36328.691, 16640.615, 33859.273, 16640.615, 16640.605, 16639.525, 16640.318, 16640.396, 19860.039, 16621.643, 16640.607, 18647.166, 19713.33, 16624.926, 17014.646, 20780.754, 17061.395, 22523.648, 16630.928, 16941.881, 19726.154, 17162.807, 17874.51, 24405.924, 17463.375, 17591.371, 16946.695, 25740.006, 16480.279, 16640.084, 16636.514, 17414.205, 22335.703, 16821.986, 16640.605, 16639.303, 17150.84, 17302.662, 18694.379, 17637.379, 19314.473, 16678.188, 16882.762, 17059.533, 16570.697, 16889.906, 17449.768, 16605.678, 16640.506, 21016.645, 18393.891, 17944.41, 16851.646, 18211.922, 16588.729, 16445.715, 16631.291, 16639.703, 18154.904, 17530.215, 20174.426, 17731.904, 16971.982, 19228.414, 20438.375, 16638.686, 17791.506, 16630.461, 16564.045, 17793.281, 20332.488, 16893.582, 20223.74, 16607.275, 18026.867, 31219.73, 16640.557, 17737.012, 16634.877, 17851.461, 22898.361, 17128.555, 21664.012, 16640.615, 17718.635, 16863.91, 16639.961, 16503.994, 16640.592, 16624.818, 20594.045, 17382.402, 16640.541, 21039.85, 16907.021, 16903.322, 18607.408]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:41 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:102 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:41 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:46 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.05229362301733192 batch_size:54 epochs:100	100	1000	True	3103.57446		1192751	19	-1	156.74483060836792	{'train_loss': [8057.923, 7626.542, 7309.253, 6170.71, 5301.265, 5009.31, 4802.052, 4618.29, 4524.735, 4423.999, 4349.234, 4276.537, 4230.523, 4174.324, 4129.717, 4080.548, 4037.733, 4000.153, 3960.751, 3907.91, 3876.963, 3844.511, 3811.195, 3776.923, 3732.718, 3716.427, 3692.688, 3649.759, 3624.14, 3610.811, 3598.123, 3552.337, 3544.751, 3529.053, 3511.771, 3475.046, 3477.95, 3444.971, 3419.988, 3434.806, 3418.419, 3395.586, 3384.686, 3381.877, 3355.32, 3330.11, 3314.487, 3329.458, 3314.832, 3308.442, 3281.741, 3288.889, 3266.501, 3251.401, 3253.579, 3232.801, 3246.621, 3239.569, 3233.153, 3207.317, 3202.053, 3201.812, 3193.612, 3190.469, 3176.372, 3170.268, 3153.889, 3162.081, 3158.97, 3144.619, 3140.706, 3127.88, 3134.025, 3111.121, 3126.803, 3107.534, 3107.325, 3090.473, 3098.583, 3107.071, 3098.271, 3074.109, 3093.837, 3074.936, 3067.646, 3078.28, 3068.347, 3070.978, 3059.282, 3045.186, 3037.054, 3053.358, 3053.627, 3044.353, 3053.859, 3045.12, 3035.395, 3027.162, 3028.194, 3013.489], 'val_loss': [7446.776, 7157.998, 6029.729, 5170.514, 4499.722, 4215.5, 4086.388, 4040.555, 3939.977, 3871.438, 3798.867, 3784.854, 3732.725, 3737.766, 3664.229, 3674.212, 3597.765, 3541.679, 3591.015, 3528.298, 3493.913, 3464.717, 3460.333, 3453.571, 3408.605, 3409.044, 3396.015, 3387.445, 3376.234, 3384.298, 3391.217, 3346.154, 3360.79, 3343.562, 3385.365, 3339.872, 3394.351, 3342.175, 3365.761, 3371.715, 3309.475, 3340.468, 3323.401, 3295.688, 3321.269, 3300.221, 3314.533, 3303.455, 3317.598, 3313.543, 3285.713, 3314.271, 3299.53, 3289.988, 3311.982, 3247.876, 3265.848, 3279.859, 3197.919, 3262.052, 3248.698, 3236.328, 3233.822, 3228.101, 3222.334, 3163.042, 3218.389, 3193.796, 3171.102, 3194.161, 3190.695, 3210.132, 3195.384, 3180.456, 3198.1, 3191.813, 3196.714, 3216.854, 3159.772, 3164.085, 3149.388, 3136.56, 3118.079, 3185.671, 3119.356, 3120.544, 3104.29, 3169.737, 3154.802, 3138.12, 3125.965, 3168.399, 3148.024, 3125.865, 3137.927, 3116.608, 3095.526, 3171.5, 3095.915, 3076.312]}	100	100	True
