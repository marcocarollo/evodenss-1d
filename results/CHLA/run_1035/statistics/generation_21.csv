id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:82 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:18 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:78 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:8 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.08710559782690344 batch_size:51 epochs:100	100	1000	True	2649.41479		569030	19	-1	158.38072323799133	{'train_loss': [6926.967, 6262.896, 6146.187, 6066.872, 5949.093, 5308.62, 4783.207, 4508.877, 4356.195, 4168.737, 3990.067, 3848.379, 3741.589, 3669.096, 3602.751, 3549.237, 3490.293, 3444.983, 3403.562, 3361.091, 3350.388, 3303.009, 3276.746, 3238.113, 3247.994, 3199.321, 3187.167, 3171.095, 3139.964, 3144.521, 3139.297, 3110.806, 3098.671, 3090.358, 3081.996, 3060.401, 3050.813, 3053.431, 3018.789, 3035.083, 3030.17, 3003.896, 3007.349, 2994.987, 2991.345, 2975.423, 2959.605, 2967.021, 2961.887, 2944.274, 2955.222, 2946.659, 2944.203, 2926.867, 2922.92, 2930.031, 2923.441, 2919.51, 2932.744, 2909.244, 2923.252, 2902.49, 2901.287, 2887.85, 2872.907, 2898.818, 2884.772, 2877.722, 2868.505, 2867.385, 2860.853, 2876.216, 2858.552, 2872.148, 2852.313, 2847.024, 2858.144, 2860.157, 2844.425, 2851.995, 2856.304, 2838.238, 2824.089, 2830.465, 2835.028, 2841.148, 2835.989, 2829.179, 2815.445, 2821.724, 2821.215, 2820.061, 2805.19, 2806.536, 2817.53, 2798.489, 2806.126, 2797.327, 2816.707, 2797.347], 'val_loss': [5610.783, 5410.674, 5353.685, 5318.649, 4860.527, 4106.621, 3864.626, 3761.87, 3642.084, 3506.868, 3371.011, 3348.017, 3256.77, 3196.182, 3149.848, 3088.342, 3092.64, 3020.401, 3009.009, 3017.349, 2958.267, 2941.501, 2934.747, 2900.127, 2906.097, 2899.866, 2866.032, 2904.068, 2857.693, 2850.451, 2836.096, 2850.98, 2835.499, 2791.041, 2785.865, 2787.011, 2804.179, 2781.264, 2783.32, 2741.023, 2786.897, 2776.258, 2776.866, 2752.617, 2768.732, 2746.769, 2742.949, 2751.082, 2766.094, 2726.507, 2700.906, 2752.071, 2735.375, 2725.497, 2731.511, 2729.755, 2693.644, 2708.168, 2699.754, 2714.635, 2688.568, 2692.183, 2698.408, 2697.001, 2709.493, 2706.265, 2701.18, 2708.696, 2681.562, 2680.438, 2685.773, 2703.218, 2677.643, 2657.804, 2675.189, 2656.246, 2662.229, 2653.795, 2667.865, 2664.837, 2672.172, 2662.558, 2646.159, 2651.216, 2663.657, 2687.147, 2650.619, 2637.08, 2660.844, 2643.784, 2657.967, 2647.511, 2656.052, 2642.262, 2631.528, 2653.11, 2651.484, 2654.216, 2654.481, 2665.456]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:76 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:82 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:18 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:72 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:76 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:78 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:8 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.08710559782690344 batch_size:89 epochs:100	100	1000	True	4272.33545		622478	19	-1	139.61504483222961	{'train_loss': [12460.512, 11304.71, 11101.866, 10928.052, 10434.526, 9485.435, 8903.914, 8486.3, 8240.243, 7929.851, 7648.484, 7315.127, 7084.037, 6878.536, 6742.055, 6622.498, 6506.542, 6404.419, 6335.767, 6261.046, 6194.987, 6124.889, 6078.114, 6004.852, 5933.475, 5908.409, 5836.489, 5805.592, 5739.993, 5699.646, 5684.211, 5639.745, 5588.767, 5580.072, 5522.092, 5495.996, 5507.665, 5472.354, 5453.539, 5408.375, 5400.987, 5385.423, 5343.525, 5312.245, 5330.145, 5332.164, 5322.144, 5294.745, 5271.482, 5269.192, 5248.148, 5241.732, 5235.434, 5215.066, 5214.171, 5182.416, 5196.899, 5157.936, 5159.648, 5148.09, 5135.413, 5134.072, 5119.462, 5146.666, 5108.384, 5105.926, 5092.651, 5073.72, 5081.938, 5100.532, 5067.877, 5093.534, 5045.959, 5049.883, 5058.348, 5036.928, 5042.472, 5043.243, 5043.806, 5027.757, 5015.222, 5014.992, 5007.925, 5012.28, 5008.954, 5002.449, 4979.235, 4981.867, 4997.211, 5001.6, 4994.709, 4967.905, 4940.512, 4962.576, 4976.252, 4957.779, 4937.808, 4947.278, 4933.518, 4948.271], 'val_loss': [9295.054, 8866.871, 8704.654, 8363.687, 7526.215, 6640.427, 6253.991, 6409.225, 5946.64, 5812.681, 5633.079, 5450.067, 5326.635, 5242.598, 5191.011, 5155.28, 5074.336, 5031.323, 5002.679, 4966.697, 4922.885, 4873.074, 4872.229, 4821.617, 4801.642, 4775.602, 4777.977, 4717.967, 4624.856, 4593.89, 4618.837, 4580.731, 4605.713, 4607.752, 4558.646, 4519.524, 4560.744, 4516.312, 4522.883, 4460.571, 4452.621, 4441.752, 4467.327, 4436.839, 4418.115, 4424.669, 4410.024, 4403.305, 4396.035, 4417.032, 4387.309, 4395.723, 4356.977, 4369.305, 4379.394, 4380.775, 4345.072, 4342.62, 4346.379, 4367.564, 4363.146, 4335.898, 4353.245, 4332.733, 4332.006, 4328.101, 4322.188, 4325.071, 4365.126, 4329.428, 4362.307, 4309.574, 4336.444, 4309.133, 4313.819, 4282.961, 4296.587, 4306.146, 4305.729, 4291.898, 4315.038, 4295.185, 4287.693, 4294.688, 4281.525, 4298.466, 4275.414, 4310.113, 4273.41, 4297.866, 4281.88, 4278.587, 4271.236, 4283.985, 4257.129, 4268.639, 4296.236, 4257.287, 4250.462, 4270.375]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:82 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:18 kernel_size:7 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:78 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:8 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adadelta lr:0.08710559782690344 batch_size:51 epochs:100	100	1000	True	2621.13452		614525	20	-1	166.2659707069397	{'train_loss': [6974.823, 6303.294, 6191.583, 6071.936, 5443.489, 4916.119, 4629.387, 4442.16, 4320.002, 4232.212, 4144.024, 4061.895, 3962.913, 3861.276, 3777.598, 3693.506, 3623.211, 3573.122, 3521.5, 3480.201, 3432.948, 3413.257, 3353.864, 3340.716, 3313.259, 3272.692, 3246.671, 3234.154, 3218.362, 3186.824, 3181.995, 3165.67, 3138.725, 3134.78, 3116.465, 3090.946, 3089.855, 3077.223, 3069.481, 3038.792, 3048.639, 3061.453, 3026.652, 3026.36, 3015.784, 3022.318, 2984.432, 2994.583, 2979.096, 2976.369, 2976.566, 2950.987, 2958.531, 2956.746, 2950.136, 2950.26, 2943.017, 2933.262, 2927.073, 2920.087, 2921.119, 2916.975, 2922.604, 2911.399, 2895.904, 2916.255, 2901.495, 2889.725, 2890.606, 2886.478, 2862.048, 2876.308, 2877.631, 2876.742, 2850.321, 2869.925, 2849.078, 2845.849, 2867.993, 2843.302, 2854.245, 2841.058, 2835.044, 2838.586, 2840.246, 2824.9, 2830.248, 2828.513, 2835.339, 2821.646, 2818.464, 2828.137, 2812.49, 2790.978, 2798.51, 2816.381, 2794.344, 2811.897, 2802.656, 2804.508], 'val_loss': [5586.496, 5409.957, 5341.721, 5069.001, 4100.033, 4112.4, 3801.667, 3753.924, 3687.507, 3660.306, 3601.427, 3521.213, 3459.844, 3375.869, 3346.984, 3278.029, 3225.717, 3165.564, 3136.897, 3123.187, 3172.919, 3081.921, 3064.171, 3012.429, 3070.982, 3025.822, 2967.014, 2946.205, 2971.111, 2920.667, 2918.494, 2888.689, 2896.569, 2886.562, 2959.736, 2933.289, 2879.072, 2853.915, 2838.821, 2890.273, 2834.346, 2838.542, 2853.456, 2852.567, 2847.474, 2870.332, 2825.361, 2815.301, 2810.199, 2794.852, 2779.413, 2778.968, 2798.32, 2781.444, 2797.397, 2800.962, 2754.498, 2767.615, 2744.325, 2806.601, 2765.503, 2726.509, 2742.395, 2743.312, 2739.705, 2761.335, 2754.914, 2739.884, 2726.06, 2729.34, 2780.353, 2734.28, 2705.508, 2735.355, 2730.493, 2729.06, 2703.932, 2720.398, 2705.815, 2769.997, 2728.878, 2726.062, 2693.439, 2719.62, 2765.073, 2718.227, 2713.327, 2711.433, 2705.851, 2664.576, 2739.586, 2671.842, 2686.482, 2662.881, 2691.812, 2677.886, 2676.633, 2685.354, 2690.963, 2651.427]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:24 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:82 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:18 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:78 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:81 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:8 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adadelta lr:0.08710559782690344 batch_size:71 epochs:100	100	1000	True	3571.38037		585286	20	-1	152.84716749191284	{'train_loss': [9741.586, 8822.481, 8643.152, 7754.621, 6875.19, 6489.946, 6311.819, 6181.432, 6052.742, 5945.868, 5861.188, 5771.648, 5703.041, 5616.02, 5501.052, 5357.524, 5232.873, 5138.738, 5045.52, 4948.732, 4903.438, 4821.308, 4773.354, 4726.409, 4663.126, 4642.66, 4599.434, 4547.792, 4515.318, 4485.878, 4464.472, 4429.787, 4403.558, 4382.025, 4352.62, 4319.964, 4302.107, 4281.808, 4276.932, 4266.549, 4256.187, 4234.821, 4218.221, 4187.365, 4188.042, 4175.729, 4178.817, 4157.436, 4145.842, 4133.308, 4141.138, 4117.071, 4104.111, 4105.945, 4097.181, 4097.88, 4085.772, 4066.999, 4069.068, 4050.043, 4058.675, 4044.135, 4037.007, 4027.223, 4027.258, 4020.026, 4019.742, 4019.772, 3997.934, 4003.434, 4000.004, 3989.589, 3987.136, 3994.814, 3986.849, 3985.672, 3969.921, 3954.751, 3956.502, 3947.721, 3947.26, 3956.685, 3953.069, 3928.76, 3945.238, 3933.161, 3928.237, 3929.548, 3922.269, 3923.115, 3932.879, 3913.075, 3915.832, 3916.164, 3912.208, 3903.674, 3908.552, 3914.378, 3897.741, 3902.266], 'val_loss': [7750.763, 7327.225, 6802.47, 5478.898, 5165.192, 5204.624, 4883.922, 4922.754, 4910.08, 4848.922, 4771.206, 4737.207, 4731.46, 4620.951, 4507.621, 4373.751, 4385.825, 4242.647, 4161.232, 4164.656, 4124.353, 4081.55, 4040.999, 4031.91, 3979.052, 3935.029, 3954.052, 3900.295, 3888.658, 3871.91, 3921.716, 3952.912, 3804.427, 3797.36, 3799.146, 3833.618, 3777.988, 3787.995, 3766.018, 3820.946, 3741.605, 3767.071, 3754.982, 3765.556, 3767.961, 3746.271, 3717.04, 3722.059, 3702.689, 3712.585, 3652.924, 3690.276, 3685.124, 3672.056, 3681.044, 3683.934, 3679.395, 3661.023, 3650.931, 3658.245, 3669.018, 3683.708, 3653.134, 3642.385, 3618.712, 3646.453, 3652.469, 3635.462, 3623.544, 3603.447, 3602.747, 3643.828, 3623.355, 3633.706, 3605.234, 3617.795, 3584.095, 3594.573, 3581.011, 3594.143, 3601.652, 3616.999, 3598.611, 3570.538, 3585.232, 3583.212, 3562.962, 3565.277, 3554.597, 3568.392, 3553.618, 3583.464, 3569.563, 3545.774, 3555.416, 3574.663, 3557.127, 3567.384, 3513.139, 3539.718]}	100	100	True
