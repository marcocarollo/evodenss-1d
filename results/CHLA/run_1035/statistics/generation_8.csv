id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adadelta lr:0.05229362301733192 batch_size:74 epochs:100	100	2000	True	3729.36987		1272847	18	-1	154.0715835094452	{'train_loss': [11066.197, 10372.377, 8458.686, 7386.142, 6995.144, 6812.644, 6594.858, 6438.307, 6313.011, 6197.38, 6074.861, 6022.787, 5914.783, 5859.546, 5775.336, 5719.246, 5623.513, 5577.69, 5510.072, 5437.416, 5396.893, 5327.156, 5261.348, 5205.861, 5140.877, 5124.969, 5064.975, 5036.29, 4976.406, 4971.361, 4930.297, 4910.296, 4885.94, 4826.072, 4815.224, 4797.464, 4746.153, 4764.917, 4717.012, 4684.636, 4680.578, 4650.646, 4634.411, 4631.945, 4608.848, 4586.896, 4569.094, 4531.12, 4551.76, 4520.329, 4497.395, 4489.658, 4465.354, 4469.659, 4433.22, 4411.02, 4419.594, 4409.912, 4381.74, 4393.955, 4377.984, 4375.818, 4351.165, 4331.019, 4338.446, 4330.358, 4330.474, 4317.246, 4294.664, 4299.081, 4289.301, 4247.09, 4265.138, 4242.293, 4251.492, 4239.759, 4229.73, 4215.925, 4204.5, 4219.982, 4182.68, 4185.75, 4192.864, 4185.868, 4185.913, 4178.939, 4179.849, 4153.372, 4151.37, 4144.067, 4147.845, 4149.855, 4136.158, 4138.51, 4127.409, 4114.743, 4131.437, 4085.766, 4102.702, 4117.077], 'val_loss': [7920.534, 6991.057, 6088.249, 5718.101, 6360.335, 5514.41, 5297.171, 5253.135, 5030.182, 4887.261, 4897.477, 4851.196, 4576.155, 4440.604, 4460.621, 4327.929, 4231.533, 4383.733, 4299.325, 4240.504, 4229.542, 4150.35, 4304.478, 4172.267, 4222.979, 4201.567, 4133.592, 4153.419, 4011.898, 3996.413, 4073.368, 4040.899, 4019.689, 3927.292, 3931.537, 3890.737, 3966.08, 3889.357, 3924.929, 3904.613, 3875.367, 3918.046, 3871.849, 3925.49, 3824.976, 3872.606, 3930.87, 3886.265, 3935.473, 3868.721, 3870.994, 3854.466, 3804.337, 3876.695, 3933.712, 3849.0, 3865.384, 3818.625, 3877.31, 3829.074, 3789.73, 3930.467, 3822.696, 3800.994, 3766.592, 3828.884, 3848.624, 3802.329, 3879.296, 3816.215, 3732.866, 3831.706, 3823.596, 3755.269, 3724.881, 3779.402, 3857.224, 3851.32, 3725.977, 3754.128, 3807.222, 3776.027, 3718.227, 3682.426, 3707.681, 3790.344, 3690.176, 3640.545, 3705.192, 3686.959, 3743.245, 3649.165, 3692.291, 3723.433, 3816.371, 3725.874, 3673.227, 3783.04, 3796.582, 3739.491]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.05229362301733192 batch_size:50 epochs:100	100	1000	True	2888.64551		2075556	19	-1	187.87329268455505	{'train_loss': [7647.615, 6160.926, 5291.758, 4928.576, 4741.792, 4568.906, 4454.083, 4323.327, 4204.251, 4108.007, 3996.78, 3889.239, 3837.989, 3789.677, 3709.032, 3665.449, 3626.921, 3585.009, 3538.677, 3515.867, 3487.281, 3446.936, 3423.181, 3393.365, 3350.364, 3328.964, 3329.531, 3299.55, 3278.242, 3254.474, 3241.49, 3233.036, 3210.549, 3194.427, 3173.136, 3165.85, 3152.218, 3136.244, 3122.071, 3124.044, 3093.356, 3088.026, 3076.48, 3069.344, 3062.817, 3056.103, 3037.502, 3025.28, 3020.305, 3002.669, 3010.904, 3009.857, 2987.471, 2997.731, 2982.508, 2972.896, 2955.47, 2956.674, 2946.382, 2945.681, 2927.572, 2944.094, 2936.967, 2932.85, 2924.223, 2913.963, 2903.539, 2903.092, 2903.33, 2903.372, 2898.026, 2883.942, 2883.233, 2889.041, 2886.593, 2872.797, 2864.152, 2868.069, 2847.963, 2859.1, 2840.001, 2831.713, 2851.315, 2840.015, 2821.053, 2818.762, 2823.191, 2812.097, 2811.675, 2816.683, 2801.782, 2792.954, 2800.594, 2780.628, 2777.694, 2771.975, 2776.365, 2775.465, 2770.178, 2760.363], 'val_loss': [6721.826, 5368.787, 4727.364, 4284.245, 4063.366, 4035.874, 3917.476, 3899.881, 3538.587, 3583.543, 3655.657, 3668.246, 3534.59, 3385.835, 3428.68, 3324.213, 3359.522, 3539.288, 3357.995, 3208.317, 3217.615, 3163.44, 3288.053, 3159.354, 3126.212, 3197.165, 3153.745, 3040.91, 3063.864, 3092.795, 3006.836, 3035.103, 3060.518, 3065.999, 2996.387, 3032.874, 2983.976, 2964.503, 2984.57, 2939.007, 2935.745, 3050.362, 2975.89, 3009.81, 2973.186, 3032.052, 2983.188, 2969.012, 2931.243, 2926.317, 2959.25, 2957.781, 2939.05, 2996.596, 2994.912, 2899.63, 2960.365, 2917.304, 2895.196, 2952.552, 2849.325, 2891.532, 2999.888, 2939.914, 2858.727, 2967.764, 2889.773, 2871.689, 2892.377, 2847.021, 2981.254, 2924.458, 2878.242, 2866.815, 2970.55, 2891.524, 2886.371, 2824.921, 2869.622, 2859.343, 2893.68, 2816.699, 2848.695, 2765.493, 2860.227, 2819.271, 2838.121, 2849.491, 2833.301, 2810.589, 2901.033, 2768.71, 2798.915, 2777.018, 2783.848, 2783.23, 2769.238, 2855.41, 2795.854, 2805.594]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:66 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:81 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:deconv1d out_channels:48 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:124 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:53 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adam lr:0.05229362301733192 beta1:0.604394084856289 beta2:0.5417888395367102 weight_decay:7.684912792352154e-05 batch_size:74 epochs:100	100	1000	True	63133.76172		9112470	20	-1	258.70068860054016	{'train_loss': [28834.727, 101197.078, 50118.746, 64690.418, 32568.756, 43276.508, 33932.512, 90207.703, 73136.531, 31034.467, 55134.535, 51215.812, 51896.664, 30593.096, 50561.316, 47412.859, 35059.059, 28355.951, 43239.359, 38346.27, 42977.066, 31019.941, 43426.285, 40480.848, 32742.799, 38390.375, 40802.48, 35297.367, 37967.648, 39032.371, 39099.09, 35390.027, 36412.25, 42751.762, 40387.863, 37285.906, 38698.488, 36927.508, 45665.484, 39638.5, 47490.848, 35872.434, 34296.062, 43903.695, 34169.602, 36236.074, 36709.387, 37529.039, 34034.438, 40255.277, 36327.969, 33283.301, 34593.754, 34309.297, 37497.625, 36219.25, 39759.547, 34873.93, 38363.051, 41899.934, 38048.621, 38799.387, 36683.477, 37991.078, 36027.543, 32415.977, 35029.871, 36706.812, 34802.984, 39658.082, 31049.818, 40236.191, 42797.355, 35300.078, 32115.994, 43105.148, 35505.57, 36237.66, 38785.285, 38837.258, 31889.184, 47495.195, 32369.564, 33064.105, 34553.371, 36177.059, 32724.855, 37859.035, 39099.953, 31952.01, 36354.398, 39960.922, 37655.91, 32440.25, 35470.691, 38067.988, 32984.754, 39997.32, 35140.117, 32708.748], 'val_loss': [22187.49, 22187.49, 22187.49, 22187.49, 22187.49, 72652.242, 22187.49, 22187.49, 22187.49, 81294.945, 22187.49, 22187.49, 335002.031, 29142.568, 22187.49, 22187.49, 23039.07, 32567.27, 22187.49, 27959.471, 22187.406, 31392.6, 37201.395, 27008.068, 37167.207, 33166.395, 26946.701, 27996.703, 25634.137, 22187.49, 22273.734, 57278.918, 32443.703, 57952.395, 46824.258, 25470.754, 29466.57, 36830.613, 22860.281, 22938.107, 22187.49, 22187.398, 30637.777, 22187.475, 34213.109, 23269.65, 23711.748, 59504.188, 45101.117, 24290.369, 32212.994, 23096.512, 26773.828, 61681.242, 59857.844, 30267.324, 29259.176, 44865.293, 22187.49, 55259.887, 48062.809, 30398.699, 22187.49, 24810.973, 27315.064, 22187.49, 41700.551, 41910.73, 22187.488, 22186.959, 87319.297, 23607.467, 22187.49, 30323.414, 33560.668, 22187.49, 25615.307, 48834.652, 22187.49, 27790.309, 29828.934, 22187.49, 60213.613, 32428.279, 35181.844, 62184.102, 31069.076, 22187.479, 24044.502, 34895.812, 22187.49, 24559.139, 42674.371, 22187.49, 34144.078, 174451.5, 37085.316, 22941.619, 22824.006, 62929.852]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:42 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:30 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:conv1d out_channels:96 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:113 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:15 layer:conv1d out_channels:75 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.05229362301733192 batch_size:82 epochs:100	100	1000	True	4403.92188		877788	19	-1	152.04297304153442	{'train_loss': [12149.974, 10863.625, 10612.401, 10477.781, 10226.979, 8511.556, 7716.049, 7340.667, 7055.736, 6794.524, 6617.112, 6493.907, 6376.788, 6259.499, 6182.847, 6088.594, 6018.526, 5962.166, 5901.854, 5844.16, 5776.142, 5719.661, 5681.099, 5638.452, 5592.666, 5570.254, 5510.408, 5481.902, 5424.187, 5413.711, 5390.433, 5327.0, 5334.756, 5294.287, 5252.921, 5248.313, 5209.06, 5205.575, 5149.149, 5129.383, 5110.28, 5101.805, 5090.994, 5036.612, 5049.354, 5007.451, 5020.509, 4981.383, 4971.123, 4932.06, 4931.581, 4927.977, 4906.609, 4887.952, 4881.821, 4857.563, 4866.835, 4836.748, 4802.734, 4795.123, 4815.916, 4781.517, 4779.481, 4763.749, 4774.087, 4764.196, 4739.916, 4732.754, 4707.79, 4706.935, 4686.547, 4697.042, 4692.298, 4673.892, 4695.09, 4648.045, 4644.643, 4634.154, 4627.717, 4629.137, 4628.438, 4614.337, 4617.352, 4594.437, 4597.589, 4601.932, 4587.453, 4581.257, 4589.925, 4542.51, 4575.808, 4570.831, 4588.372, 4553.458, 4539.992, 4541.328, 4539.15, 4533.688, 4519.208, 4523.677], 'val_loss': [9501.347, 9036.556, 8863.756, 8769.02, 7758.012, 6382.815, 6161.755, 5911.324, 5497.057, 5494.452, 5229.425, 5191.462, 5030.098, 5061.245, 4997.606, 4859.944, 4922.765, 4923.152, 4808.326, 4743.87, 4755.987, 4718.163, 4702.219, 4707.318, 4715.317, 4634.203, 4730.933, 4641.441, 4694.554, 4649.846, 4656.646, 4679.507, 4639.494, 4590.229, 4645.697, 4616.219, 4595.531, 4612.595, 4534.505, 4512.515, 4630.708, 4539.809, 4519.657, 4652.157, 4561.425, 4578.611, 4531.312, 4537.047, 4574.712, 4532.308, 4519.621, 4555.492, 4545.482, 4600.447, 4479.625, 4502.421, 4514.757, 4460.473, 4463.513, 4514.801, 4477.924, 4438.53, 4520.756, 4507.874, 4485.384, 4473.746, 4499.773, 4444.124, 4458.174, 4434.5, 4416.208, 4386.104, 4360.003, 4419.578, 4397.302, 4410.854, 4405.875, 4481.1, 4394.39, 4457.822, 4407.142, 4424.823, 4365.21, 4417.131, 4381.826, 4351.751, 4361.628, 4346.849, 4364.921, 4382.105, 4418.392, 4329.883, 4378.518, 4356.469, 4408.185, 4398.661, 4358.396, 4382.735, 4335.824, 4378.258]}	100	100	True
