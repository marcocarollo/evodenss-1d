id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.009632013248587246 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:3.5253704020870216e-05 batch_size:15 epochs:100	100	1000	True	995.06915		452251	14	-1	354.6277151107788	{'train_loss': [1430.452, 1242.931, 1162.921, 1123.745, 1101.839, 1090.991, 1079.987, 1073.819, 1070.123, 1064.188, 1058.346, 1057.342, 1052.748, 1050.258, 1046.618, 1043.098, 1041.35, 1039.939, 1040.318, 1034.728, 1033.883, 1033.073, 1032.404, 1030.175, 1026.871, 1024.461, 1027.818, 1024.14, 1022.714, 1024.428, 1022.567, 1022.608, 1019.243, 1021.404, 1017.946, 1019.432, 1015.394, 1012.873, 1014.468, 1017.675, 1013.188, 1013.932, 1012.252, 1011.784, 1011.91, 1011.656, 1010.49, 1007.497, 1011.464, 1008.628, 1009.875, 1007.706, 1006.913, 1006.077, 1005.299, 1005.401, 1007.444, 1007.189, 1003.329, 1002.158, 1002.294, 1002.828, 999.537, 1001.448, 1003.021, 997.264, 1000.397, 1001.955, 1000.027, 998.121, 999.732, 999.027, 997.758, 998.365, 998.033, 994.662, 997.331, 997.893, 997.338, 995.124, 996.886, 998.044, 994.162, 995.376, 993.395, 991.798, 993.872, 994.86, 993.039, 994.556, 994.288, 990.961, 991.848, 990.478, 993.456, 990.367, 993.906, 990.042, 988.289, 987.505], 'val_loss': [1308.283, 1175.321, 1130.192, 1115.541, 1095.251, 1091.293, 1085.954, 1075.052, 1068.042, 1064.623, 1066.161, 1060.549, 1056.293, 1059.475, 1048.089, 1072.038, 1057.245, 1052.515, 1040.143, 1058.876, 1074.536, 1040.947, 1054.837, 1047.444, 1053.38, 1080.536, 1056.269, 1049.495, 1048.319, 1053.287, 1043.12, 1062.287, 1047.006, 1051.296, 1029.464, 1038.074, 1039.46, 1045.085, 1050.088, 1037.718, 1037.095, 1041.167, 1044.35, 1047.252, 1034.13, 1030.439, 1029.716, 1033.873, 1040.039, 1042.486, 1041.009, 1040.188, 1052.29, 1032.239, 1038.491, 1038.032, 1041.122, 1033.153, 1048.001, 1036.85, 1054.066, 1036.653, 1032.413, 1032.429, 1036.254, 1031.496, 1031.136, 1027.771, 1031.567, 1037.186, 1039.265, 1041.001, 1038.918, 1037.302, 1038.594, 1042.478, 1043.372, 1035.663, 1035.212, 1037.078, 1050.894, 1033.733, 1031.512, 1037.937, 1032.119, 1033.906, 1031.363, 1028.788, 1031.639, 1031.389, 1027.451, 1032.73, 1049.904, 1032.915, 1027.795, 1060.693, 1031.166, 1038.697, 1038.274, 1037.476]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:46 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:42 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:116 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.009632013248587246 alpha:0.8640614072874051 weight_decay:3.5253704020870216e-05 batch_size:15 epochs:100	100	1000	True	1129.05566		644827	14	-1	364.33806920051575	{'train_loss': [5155.027, 1409.915, 1330.366, 1285.167, 1272.344, 1265.103, 1254.925, 1255.18, 1250.137, 1247.448, 1243.07, 1241.079, 1235.734, 1234.76, 1226.768, 1226.628, 1219.16, 1213.424, 1210.222, 1207.173, 1205.875, 1208.941, 1202.913, 1204.729, 1201.66, 1199.059, 1195.512, 1192.236, 1193.342, 1194.381, 1189.229, 1189.94, 1190.695, 1186.157, 1186.974, 1186.929, 1183.232, 1186.066, 1182.878, 1183.489, 1180.793, 1178.933, 1181.283, 1177.447, 1180.365, 1175.115, 1176.262, 1173.848, 1170.542, 1175.301, 1172.173, 1172.381, 1170.933, 1169.279, 1169.418, 1166.284, 1167.551, 1171.198, 1168.136, 1163.701, 1166.33, 1165.406, 1160.764, 1163.975, 1163.008, 1159.567, 1162.848, 1162.977, 1159.311, 1155.671, 1159.006, 1155.0, 1152.82, 1161.082, 1154.938, 1160.424, 1156.081, 1153.813, 1149.818, 1149.945, 1147.488, 1151.304, 1151.632, 1150.828, 1147.434, 1148.918, 1150.118, 1148.482, 1142.712, 1145.511, 1145.445, 1147.477, 1146.395, 1141.709, 1142.165, 1141.304, 1141.026, 1138.34, 1140.767, 1142.38], 'val_loss': [1373.334, 1307.498, 1273.112, 1215.0, 1253.807, 1208.154, 1188.431, 1201.981, 1191.835, 1302.854, 1249.392, 1189.615, 1186.68, 1191.673, 1190.548, 1191.867, 1161.71, 1241.814, 1167.843, 1208.17, 1163.38, 1176.803, 1194.588, 1216.165, 1157.623, 1294.415, 1143.451, 1146.913, 1187.569, 1236.494, 1167.694, 1137.534, 1190.551, 1239.865, 1170.7, 1178.708, 1200.075, 1155.488, 1187.063, 1146.216, 1159.729, 1221.913, 1160.528, 1158.903, 1159.889, 1143.129, 1139.433, 1181.686, 1327.903, 1127.312, 1132.029, 1193.025, 1142.857, 1143.396, 1131.883, 1140.099, 1181.576, 1177.046, 1140.862, 1136.964, 1132.123, 1173.458, 1148.34, 1144.194, 1143.007, 1149.934, 1131.872, 1148.852, 1134.302, 1139.498, 1276.374, 1162.907, 1176.139, 1179.52, 1130.714, 1132.662, 1130.895, 1127.746, 1149.203, 1130.141, 1155.035, 1135.655, 1130.426, 1146.988, 1148.025, 1145.759, 1131.976, 1141.551, 1129.843, 1206.029, 1170.227, 1132.467, 1143.77, 1165.89, 1204.792, 1224.833, 1156.979, 1166.786, 1158.367, 1142.035]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:65 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:67 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:43 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.009632013248587246 batch_size:15 epochs:100	100	1000	True	1030.94019		416552	15	-1	334.11907410621643	{'train_loss': [1769.625, 1402.584, 1383.126, 1372.272, 1367.517, 1362.894, 1358.652, 1354.7, 1344.714, 1331.293, 1318.642, 1311.437, 1306.451, 1304.198, 1300.05, 1295.73, 1292.225, 1287.67, 1279.914, 1271.553, 1260.662, 1249.592, 1241.166, 1231.097, 1223.257, 1215.804, 1209.792, 1206.051, 1199.812, 1197.136, 1193.383, 1190.23, 1186.206, 1183.756, 1179.145, 1178.926, 1175.382, 1174.452, 1171.343, 1167.065, 1167.329, 1165.006, 1163.506, 1161.705, 1158.571, 1157.382, 1154.793, 1153.917, 1152.486, 1150.183, 1149.337, 1147.701, 1146.612, 1145.567, 1142.325, 1141.472, 1139.123, 1137.871, 1137.101, 1135.468, 1134.36, 1133.264, 1131.772, 1130.375, 1130.665, 1127.901, 1127.224, 1125.053, 1125.246, 1122.566, 1122.493, 1120.421, 1119.832, 1118.984, 1116.522, 1117.717, 1117.04, 1114.085, 1113.943, 1112.036, 1112.001, 1110.564, 1112.009, 1109.531, 1110.043, 1109.592, 1106.938, 1106.321, 1104.855, 1103.458, 1103.999, 1102.568, 1101.559, 1100.323, 1100.132, 1099.96, 1098.524, 1098.913, 1097.053, 1097.039], 'val_loss': [1413.923, 1360.216, 1343.272, 1335.088, 1330.896, 1328.044, 1324.473, 1318.509, 1308.155, 1301.052, 1295.162, 1292.35, 1290.085, 1288.905, 1286.922, 1283.495, 1279.533, 1272.614, 1263.56, 1254.086, 1241.266, 1225.742, 1212.404, 1201.784, 1194.606, 1185.535, 1183.23, 1177.732, 1173.963, 1168.296, 1164.392, 1160.537, 1159.021, 1155.016, 1153.149, 1152.589, 1144.757, 1145.91, 1143.909, 1142.422, 1141.995, 1136.247, 1135.957, 1133.006, 1133.184, 1130.738, 1129.732, 1124.764, 1124.408, 1120.968, 1122.018, 1117.488, 1118.626, 1113.476, 1113.081, 1114.405, 1111.829, 1108.121, 1105.531, 1103.789, 1104.272, 1105.568, 1100.281, 1102.619, 1098.919, 1099.08, 1095.888, 1097.127, 1093.637, 1091.382, 1090.016, 1087.907, 1087.252, 1085.854, 1086.257, 1085.554, 1080.126, 1082.25, 1082.197, 1080.869, 1079.755, 1077.82, 1078.952, 1076.217, 1076.035, 1074.088, 1076.328, 1074.911, 1071.631, 1072.62, 1070.332, 1071.088, 1070.707, 1070.254, 1067.511, 1068.092, 1071.64, 1069.097, 1065.545, 1065.681]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:3.5253704020870216e-05 batch_size:15 epochs:100	100	1000	True	989.95111		500851	14	-1	323.1001570224762	{'train_loss': [1580.504, 1265.018, 1230.693, 1215.368, 1209.231, 1202.932, 1196.186, 1156.6, 1116.455, 1095.888, 1082.589, 1071.483, 1062.74, 1056.015, 1050.51, 1046.435, 1041.932, 1039.689, 1036.294, 1033.833, 1028.029, 1028.115, 1025.37, 1020.686, 1020.51, 1015.97, 1014.01, 1012.465, 1012.622, 1009.389, 1008.917, 1007.549, 1005.873, 1004.873, 1002.017, 1000.596, 997.587, 999.451, 995.5, 995.686, 994.697, 993.194, 992.364, 988.472, 987.378, 986.639, 983.636, 984.342, 984.462, 982.236, 981.685, 981.251, 978.164, 976.986, 979.304, 974.982, 973.988, 972.158, 970.555, 971.602, 971.735, 971.32, 971.605, 970.827, 968.262, 970.183, 968.814, 968.805, 966.807, 964.763, 964.329, 964.801, 962.156, 961.284, 959.229, 961.05, 960.563, 959.865, 960.581, 957.184, 955.085, 957.121, 954.906, 959.123, 953.716, 954.375, 951.147, 953.632, 952.456, 950.432, 949.583, 951.178, 949.704, 947.464, 950.203, 948.855, 944.28, 947.439, 945.506, 944.155], 'val_loss': [1376.312, 1224.633, 1201.415, 1193.202, 1193.404, 1178.881, 1172.401, 1106.792, 1090.514, 1069.603, 1056.929, 1045.125, 1052.251, 1047.217, 1038.368, 1047.708, 1041.68, 1031.789, 1035.986, 1036.453, 1033.621, 1033.369, 1031.669, 1036.099, 1030.539, 1036.185, 1032.68, 1037.555, 1027.698, 1031.465, 1025.655, 1030.987, 1025.519, 1028.207, 1024.475, 1027.557, 1029.434, 1023.912, 1021.44, 1022.653, 1025.247, 1025.596, 1021.113, 1023.188, 1016.124, 1021.797, 1022.693, 1019.366, 1018.723, 1019.643, 1022.396, 1020.477, 1021.023, 1020.722, 1020.443, 1020.991, 1023.401, 1023.871, 1024.452, 1024.383, 1023.318, 1021.642, 1030.014, 1021.609, 1015.741, 1023.409, 1025.469, 1017.71, 1026.413, 1019.213, 1029.772, 1020.112, 1020.977, 1023.132, 1025.396, 1023.105, 1020.52, 1035.374, 1033.608, 1026.352, 1027.746, 1025.329, 1028.309, 1039.435, 1031.331, 1029.168, 1063.73, 1037.964, 1023.224, 1025.277, 1025.709, 1034.527, 1027.678, 1027.094, 1028.896, 1027.258, 1026.785, 1032.469, 1030.703, 1035.832]}	100	100	True
