id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	849.26691		548637	18	-1	386.7831802368164	{'train_loss': [1349.362, 1106.048, 1073.131, 1064.542, 1053.943, 1050.616, 1046.418, 1044.539, 1040.233, 1039.64, 1040.448, 1032.571, 1032.359, 1025.329, 1027.458, 1027.6, 1027.909, 1017.717, 1014.959, 1015.045, 1008.442, 1010.09, 1007.451, 1005.333, 1005.963, 1003.401, 1000.988, 1000.266, 997.018, 995.415, 996.132, 995.508, 990.084, 991.502, 992.08, 986.971, 988.85, 987.604, 985.096, 983.07, 980.897, 971.014, 965.358, 957.087, 956.777, 948.633, 942.619, 939.818, 932.17, 931.475, 926.971, 924.604, 917.817, 918.522, 913.185, 912.928, 911.261, 908.131, 905.31, 906.603, 903.596, 902.534, 899.735, 899.263, 897.769, 893.296, 894.419, 890.382, 892.183, 890.255, 889.503, 890.088, 886.162, 885.121, 884.595, 883.742, 889.507, 882.767, 884.528, 882.332, 881.187, 882.409, 881.837, 878.707, 880.5, 877.185, 878.666, 877.637, 875.649, 874.53, 873.259, 873.413, 871.367, 872.55, 874.89, 869.347, 870.806, 869.306, 872.041, 869.604], 'val_loss': [1114.68, 1063.951, 1046.779, 1032.094, 1027.693, 1014.467, 1023.179, 1006.744, 1013.828, 1009.593, 1010.419, 1014.666, 1004.777, 1009.575, 1005.354, 1005.032, 995.413, 998.304, 1007.832, 993.053, 997.335, 995.272, 992.973, 991.565, 986.72, 990.899, 985.809, 989.29, 986.104, 990.299, 986.187, 980.068, 988.079, 985.945, 976.601, 980.197, 984.029, 976.004, 981.487, 977.167, 965.497, 960.353, 947.385, 936.485, 934.575, 926.927, 931.628, 908.604, 927.822, 909.938, 913.639, 911.829, 912.413, 901.061, 922.974, 907.5, 899.915, 898.685, 896.418, 897.933, 900.33, 903.775, 899.209, 898.209, 899.094, 899.87, 894.808, 897.049, 896.481, 898.714, 912.359, 907.203, 892.831, 905.981, 911.173, 895.363, 898.434, 905.452, 907.102, 904.135, 907.316, 905.32, 895.726, 905.429, 902.008, 894.223, 904.401, 897.326, 900.919, 899.392, 889.907, 901.818, 894.891, 903.736, 897.135, 902.367, 900.083, 894.947, 892.658, 893.441]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:37 kernel_size:6 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:77 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:76 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:76 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.8128123931059622 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	871.21289		623888	19	-1	389.4332857131958	{'train_loss': [1323.777, 1124.44, 1076.388, 1059.721, 1019.163, 988.97, 971.81, 957.831, 949.484, 942.723, 940.591, 935.546, 933.278, 927.028, 925.601, 923.176, 921.029, 918.331, 917.563, 914.501, 912.948, 910.622, 909.363, 907.725, 905.854, 903.374, 903.272, 902.971, 899.713, 900.37, 902.134, 901.939, 898.204, 894.035, 895.648, 895.596, 892.54, 891.469, 890.72, 892.055, 891.335, 889.521, 887.675, 887.28, 886.402, 885.737, 884.356, 884.298, 886.023, 884.181, 887.857, 883.541, 882.843, 880.601, 882.858, 878.986, 881.245, 878.777, 880.467, 876.911, 878.995, 877.856, 875.269, 876.548, 876.57, 874.546, 876.148, 878.242, 873.237, 873.825, 873.304, 873.978, 870.138, 872.808, 870.411, 870.851, 870.94, 871.555, 868.812, 868.788, 873.96, 867.77, 865.804, 868.487, 868.215, 866.273, 867.218, 865.844, 867.146, 868.719, 868.661, 867.921, 865.261, 864.199, 865.358, 862.027, 862.008, 862.332, 862.907, 862.786], 'val_loss': [1149.155, 1071.091, 1043.766, 1019.433, 967.811, 952.913, 950.463, 935.275, 909.17, 907.141, 911.578, 911.34, 915.105, 912.586, 912.232, 906.947, 909.376, 907.764, 911.856, 908.079, 901.57, 907.3, 901.422, 904.484, 921.299, 947.099, 902.847, 908.954, 939.309, 900.011, 910.078, 906.179, 901.482, 906.025, 909.986, 914.626, 903.594, 903.64, 909.961, 907.131, 902.211, 895.954, 895.226, 917.795, 903.253, 898.752, 919.119, 907.614, 897.995, 901.686, 905.76, 906.759, 899.871, 902.185, 914.919, 899.641, 902.525, 907.383, 905.363, 907.376, 905.668, 903.162, 907.804, 902.374, 900.986, 908.627, 904.968, 904.355, 908.047, 898.408, 905.167, 901.367, 912.005, 896.775, 902.086, 913.875, 911.899, 905.419, 906.683, 896.653, 900.657, 908.001, 903.729, 900.737, 894.808, 904.866, 902.16, 899.682, 906.236, 899.667, 907.679, 902.808, 899.914, 895.566, 902.345, 904.705, 910.308, 894.352, 899.257, 897.455]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:38 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:deconv1d out_channels:19 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:14 epochs:100	100	1000	True	1055.70227		1770635	19	-1	564.2108869552612	{'train_loss': [6195.509, 4949.781, 3942.223, 1306.942, 1215.758, 1179.396, 1177.6, 1173.876, 1170.948, 1166.571, 1157.744, 1116.518, 1088.513, 1077.135, 1065.589, 1052.479, 1041.971, 1032.903, 1034.886, 1026.713, 1023.332, 1014.991, 1006.486, 1010.661, 1003.976, 1001.023, 995.595, 990.318, 994.0, 988.274, 989.228, 990.326, 983.706, 984.329, 975.682, 975.384, 976.874, 967.494, 965.198, 968.5, 963.156, 961.249, 962.345, 959.32, 964.714, 958.405, 950.622, 951.374, 952.417, 947.439, 947.783, 935.642, 939.23, 938.518, 929.528, 951.275, 949.297, 941.357, 933.871, 929.887, 937.985, 925.428, 927.855, 933.225, 926.671, 925.469, 921.065, 939.257, 926.256, 920.028, 924.897, 920.095, 929.402, 919.919, 922.568, 920.516, 917.545, 917.712, 924.883, 927.056, 914.959, 914.81, 915.147, 908.584, 917.61, 916.977, 910.606, 903.606, 911.187, 897.43, 905.515, 903.467, 902.702, 897.42, 890.877, 897.884, 905.292, 901.402, 890.76, 892.04], 'val_loss': [4781.682, 4781.682, 1619.76, 1221.344, 1258.691, 1284.443, 1118.552, 1160.32, 1141.695, 1398.979, 1407.845, 1059.479, 1089.964, 1037.518, 1013.418, 982.665, 1110.066, 1108.207, 1164.932, 1020.132, 986.265, 1080.181, 972.736, 1111.302, 1027.259, 992.334, 1030.751, 1103.236, 1042.615, 993.796, 1035.388, 1050.451, 1084.616, 1005.541, 1343.767, 989.545, 1038.351, 1013.025, 1058.614, 974.629, 991.895, 974.323, 998.71, 1197.582, 998.826, 991.348, 1015.895, 1010.743, 1040.219, 1034.763, 1008.426, 1083.062, 1032.264, 1148.508, 1042.887, 1031.797, 995.981, 1049.626, 1009.626, 1016.849, 1003.654, 1004.512, 1020.054, 1005.511, 1035.332, 990.222, 1151.67, 1005.443, 1039.403, 1012.462, 1047.992, 1102.074, 1124.68, 1024.617, 1086.82, 1046.401, 1021.821, 1084.849, 1342.685, 1038.639, 1082.499, 1009.233, 1009.344, 1089.524, 1058.262, 1149.902, 1055.039, 1102.237, 1155.07, 1045.25, 1039.701, 1113.797, 1141.007, 1062.587, 1098.892, 1150.076, 1185.03, 1052.516, 1115.931, 1062.55]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:72 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:125 kernel_size:10 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:23 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:72 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.8256214883617998 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	984.9519		516504	19	-1	371.294438123703	{'train_loss': [1238.182, 1100.648, 1072.345, 1059.359, 1053.793, 1048.505, 1045.154, 1043.306, 1040.505, 1037.653, 1036.913, 1032.804, 1033.744, 1030.017, 1029.849, 1030.979, 1027.166, 1025.023, 1023.723, 1026.58, 1020.904, 1019.427, 1019.556, 1019.219, 1015.196, 1016.189, 1015.523, 1016.277, 1017.496, 1012.62, 1012.494, 1014.576, 1012.577, 1011.867, 1009.931, 1009.677, 1009.436, 1008.385, 1008.15, 1012.278, 1007.304, 1006.688, 1008.57, 1008.118, 1006.886, 1005.487, 1003.109, 1005.331, 1004.68, 1004.149, 1000.499, 1003.11, 1004.063, 1001.726, 1002.677, 1003.028, 1003.46, 998.348, 1000.545, 1002.647, 1000.206, 1000.392, 999.335, 999.976, 1000.076, 997.924, 995.769, 997.494, 998.903, 997.112, 996.957, 995.562, 993.722, 994.125, 996.132, 993.573, 991.428, 993.811, 994.569, 991.603, 993.03, 993.625, 995.849, 992.754, 990.845, 989.18, 990.708, 991.877, 989.475, 993.207, 990.112, 991.667, 989.47, 989.759, 988.192, 988.179, 989.014, 989.188, 986.018, 986.16], 'val_loss': [1141.782, 1056.389, 1039.365, 1030.942, 1027.997, 1031.106, 1020.158, 1019.919, 1023.24, 1016.648, 1024.011, 1018.02, 1020.501, 1019.302, 1016.397, 1008.48, 1010.604, 1014.338, 1011.414, 1012.803, 1008.706, 1011.692, 1016.097, 1015.951, 1001.009, 1009.508, 1009.609, 1007.648, 1004.214, 1007.106, 1007.204, 1008.675, 1000.687, 1007.906, 1014.919, 1008.217, 999.949, 1011.702, 1010.091, 999.379, 1001.485, 1004.233, 1008.793, 1005.558, 1012.55, 1017.461, 1013.224, 999.84, 1008.895, 1013.726, 1009.223, 1019.67, 1018.52, 1001.837, 1016.947, 1006.304, 999.399, 999.907, 1004.642, 1006.067, 1012.377, 1007.645, 1012.14, 1004.078, 1014.038, 1005.281, 1008.989, 1013.097, 1007.178, 1003.437, 1013.08, 1013.953, 1002.139, 998.152, 1004.215, 1003.53, 1014.652, 1005.312, 1007.592, 1007.472, 990.99, 999.227, 1002.52, 1010.173, 1009.562, 1011.856, 1005.417, 1007.24, 1013.236, 1007.807, 994.987, 1007.991, 1008.994, 994.909, 1026.602, 1008.559, 1008.21, 1007.286, 998.182, 1010.249]}	100	100	True
