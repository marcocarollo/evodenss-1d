id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.009097544753807963 beta1:0.804697104124598 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	1018.15533		455044	17	-1	352.48997926712036	{'train_loss': [1330.833, 1106.058, 1073.918, 1065.087, 1053.281, 1050.412, 1045.83, 1043.872, 1041.723, 1036.7, 1037.04, 1035.122, 1031.969, 1032.121, 1029.282, 1029.122, 1027.21, 1024.461, 1026.047, 1021.723, 1022.384, 1020.966, 1018.848, 1018.484, 1016.599, 1014.576, 1014.172, 1015.617, 1011.234, 1008.683, 1010.737, 1008.244, 1008.624, 1011.008, 1004.859, 1006.114, 1006.563, 1003.875, 1004.013, 1004.338, 1004.038, 1000.476, 1001.568, 999.645, 1003.402, 999.152, 1001.538, 999.704, 1001.302, 996.459, 995.304, 996.418, 994.43, 992.131, 993.782, 993.509, 993.946, 993.191, 992.907, 992.289, 989.293, 991.529, 985.601, 990.17, 987.367, 986.588, 989.439, 984.676, 985.944, 987.149, 986.202, 987.306, 987.571, 983.779, 983.061, 985.375, 983.118, 985.939, 979.299, 978.568, 978.74, 979.794, 980.361, 979.77, 980.258, 977.474, 977.919, 978.171, 974.716, 980.17, 976.096, 977.181, 975.392, 974.586, 973.909, 973.571, 972.489, 976.08, 974.178, 974.568], 'val_loss': [1117.853, 1069.619, 1038.286, 1036.749, 1024.095, 1035.965, 1035.734, 1018.228, 1023.709, 1025.265, 1021.109, 1022.625, 1013.591, 1023.602, 1012.836, 1018.734, 1009.398, 1024.032, 1014.712, 1009.863, 1039.438, 1028.296, 1035.006, 1040.0, 1034.961, 1031.649, 1024.183, 1016.335, 1018.643, 1027.695, 1022.124, 1028.456, 1022.743, 1027.199, 1021.85, 1028.262, 1017.003, 1019.102, 1016.866, 1007.265, 1012.704, 1013.775, 1020.347, 1021.864, 1016.888, 1016.729, 1020.931, 1014.13, 1043.739, 1011.663, 1022.471, 1022.728, 1002.004, 998.661, 1007.106, 1006.138, 1017.72, 1011.689, 1014.87, 1011.799, 1007.409, 1010.897, 997.742, 1006.652, 1011.185, 1005.168, 1007.459, 1047.013, 1034.851, 1004.18, 1024.314, 1028.766, 1010.725, 1010.156, 1019.447, 1023.333, 1084.745, 1022.632, 1063.562, 1038.853, 1022.545, 1042.724, 1015.265, 1031.753, 1017.378, 1036.4, 1013.325, 1071.781, 1010.236, 1030.763, 1062.86, 1028.442, 1019.657, 1033.898, 1012.11, 1016.029, 1003.539, 1017.752, 1018.596, 1015.141]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	881.91144		548637	18	-1	390.93258357048035	{'train_loss': [1388.815, 1139.214, 1091.411, 1075.088, 1066.37, 1058.946, 1053.814, 1050.496, 1045.977, 1041.353, 1036.837, 1035.692, 1033.851, 1023.873, 997.298, 972.74, 963.327, 952.039, 946.106, 939.352, 937.321, 931.245, 926.826, 920.745, 919.603, 915.29, 912.891, 913.098, 907.809, 905.576, 906.002, 902.185, 902.123, 898.515, 897.038, 893.545, 892.703, 892.45, 890.724, 891.606, 890.049, 888.856, 886.183, 885.614, 884.744, 886.52, 883.89, 885.483, 882.589, 879.676, 878.038, 882.022, 877.441, 876.514, 877.697, 877.851, 876.202, 874.898, 878.207, 874.065, 871.98, 870.169, 869.415, 869.651, 867.718, 869.286, 868.062, 862.703, 863.687, 868.89, 865.177, 863.38, 861.414, 865.325, 859.754, 858.796, 862.972, 857.71, 860.112, 856.148, 855.607, 857.476, 855.161, 853.819, 855.772, 851.352, 852.305, 856.456, 852.039, 853.983, 854.623, 849.717, 850.87, 852.286, 851.037, 844.44, 848.299, 850.675, 851.273, 847.312], 'val_loss': [1163.123, 1100.288, 1061.416, 1040.069, 1028.502, 1019.281, 1013.502, 1022.531, 1013.939, 1026.925, 1013.661, 1020.878, 1011.47, 986.568, 958.925, 931.035, 920.672, 918.685, 923.246, 923.763, 910.173, 913.595, 900.753, 903.771, 897.075, 907.827, 900.315, 900.437, 906.982, 904.748, 904.653, 916.817, 901.032, 903.84, 908.103, 916.082, 903.367, 901.544, 927.567, 898.269, 908.649, 911.193, 902.596, 897.59, 899.902, 909.679, 898.934, 897.895, 901.787, 906.577, 891.742, 892.098, 901.683, 907.357, 923.531, 905.31, 912.044, 898.009, 893.361, 903.015, 912.542, 914.24, 896.53, 898.61, 894.192, 905.719, 898.485, 897.349, 899.821, 905.314, 904.176, 915.659, 894.101, 905.131, 897.046, 894.828, 893.539, 899.934, 903.327, 905.642, 896.424, 908.345, 896.296, 906.33, 917.547, 901.986, 974.242, 897.219, 904.468, 895.299, 899.631, 914.438, 965.302, 909.065, 909.179, 921.38, 918.271, 906.353, 957.769, 908.144]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.9793050058648446 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	881.9574		398918	18	-1	364.8404767513275	{'train_loss': [1219.225, 1130.527, 1108.98, 1067.095, 1058.547, 1051.228, 1043.518, 1040.092, 1040.038, 1035.246, 1033.002, 1024.513, 1027.12, 1024.7, 1024.587, 1019.025, 1018.696, 1016.288, 1019.945, 1016.758, 1014.595, 1011.481, 1007.18, 1003.566, 1010.05, 1008.9, 1003.767, 1000.556, 998.721, 999.417, 997.019, 996.492, 996.767, 996.028, 994.598, 991.021, 992.903, 987.797, 993.065, 990.7, 987.623, 978.063, 974.643, 966.847, 964.344, 961.615, 955.645, 950.717, 946.437, 949.872, 944.488, 945.913, 938.641, 936.78, 936.947, 934.763, 932.874, 926.555, 921.819, 932.948, 933.202, 930.977, 920.716, 920.515, 926.436, 923.8, 913.219, 912.542, 915.719, 910.485, 906.807, 914.453, 905.784, 906.191, 906.59, 906.046, 905.516, 908.481, 906.585, 916.536, 915.813, 905.732, 904.405, 900.201, 903.445, 902.194, 898.597, 903.255, 897.91, 899.809, 904.474, 906.661, 893.802, 901.387, 899.24, 894.035, 897.424, 900.886, 896.488, 897.629], 'val_loss': [1133.378, 1110.713, 1053.561, 1044.819, 1049.514, 1027.197, 1021.486, 1050.067, 1042.322, 1055.483, 1011.927, 1011.909, 1035.717, 1007.852, 1014.004, 1013.654, 1009.209, 1015.692, 1010.955, 1015.779, 1018.257, 1001.369, 994.184, 1001.343, 993.606, 999.795, 999.36, 986.874, 988.664, 991.224, 999.151, 997.326, 1025.84, 993.464, 999.453, 985.416, 994.241, 986.288, 968.369, 980.796, 996.787, 961.436, 953.474, 968.672, 944.615, 941.083, 945.196, 939.553, 945.034, 934.431, 934.132, 950.167, 918.371, 932.925, 927.351, 924.019, 922.244, 926.182, 944.384, 938.108, 920.9, 931.535, 919.077, 932.699, 934.215, 916.34, 907.466, 913.967, 900.772, 923.763, 917.429, 898.156, 909.876, 927.873, 915.118, 898.054, 902.19, 895.469, 931.533, 950.836, 925.232, 908.603, 899.707, 910.885, 913.024, 911.226, 920.917, 914.871, 903.845, 901.812, 929.793, 901.481, 917.698, 904.68, 913.146, 916.562, 891.929, 911.348, 910.542, 922.824]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:86 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:85 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:23 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:43 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.009097544753807963 batch_size:13 epochs:100	100	1000	True	901.13666		398535	19	-1	366.87913250923157	{'train_loss': [1581.241, 1204.214, 1187.111, 1181.152, 1178.922, 1176.613, 1175.134, 1172.545, 1168.709, 1158.685, 1144.95, 1133.695, 1126.961, 1122.431, 1118.368, 1114.18, 1108.348, 1103.042, 1097.638, 1091.776, 1083.682, 1078.031, 1071.2, 1065.817, 1059.914, 1054.139, 1048.933, 1045.647, 1040.071, 1037.713, 1037.127, 1032.324, 1029.812, 1029.178, 1023.74, 1021.36, 1019.946, 1016.424, 1014.501, 1012.99, 1012.089, 1009.283, 1006.743, 1004.996, 1003.689, 1002.717, 1001.552, 1000.218, 999.658, 996.568, 995.693, 994.929, 994.245, 992.919, 990.631, 989.385, 990.088, 987.887, 986.288, 985.882, 983.954, 983.678, 982.398, 981.854, 980.621, 978.909, 981.109, 977.747, 976.465, 976.033, 975.612, 974.731, 974.295, 973.365, 971.711, 970.733, 970.175, 970.115, 968.996, 969.857, 967.878, 967.109, 966.199, 965.822, 963.649, 964.375, 962.777, 962.966, 960.69, 961.541, 960.021, 961.343, 961.171, 960.764, 958.765, 957.588, 957.948, 956.785, 956.439, 955.971], 'val_loss': [1314.187, 1203.693, 1175.426, 1163.737, 1158.439, 1154.641, 1152.114, 1149.134, 1143.544, 1132.735, 1124.4, 1119.187, 1116.417, 1113.545, 1110.045, 1105.419, 1100.512, 1094.615, 1086.74, 1077.839, 1069.392, 1060.742, 1052.574, 1046.413, 1041.066, 1035.741, 1031.747, 1027.259, 1022.883, 1019.22, 1016.126, 1013.778, 1009.942, 1007.106, 1004.887, 1001.54, 1000.38, 995.876, 993.917, 992.179, 990.659, 987.425, 986.227, 983.759, 981.957, 982.352, 979.067, 978.022, 979.821, 976.435, 977.993, 977.771, 974.595, 974.224, 975.083, 972.372, 969.983, 969.217, 971.444, 968.857, 968.328, 966.96, 964.799, 962.808, 959.56, 960.132, 960.464, 958.868, 959.238, 959.446, 953.316, 955.206, 954.299, 951.378, 950.828, 953.797, 952.547, 948.974, 950.16, 951.916, 946.556, 946.717, 946.743, 945.648, 951.837, 950.837, 941.69, 941.72, 945.192, 942.302, 942.392, 940.446, 940.078, 940.204, 939.345, 939.146, 941.049, 942.095, 939.477, 938.348]}	100	100	True
