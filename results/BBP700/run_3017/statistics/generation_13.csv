id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	1010.83936		548637	18	-1	499.4299967288971	{'train_loss': [1337.363, 1089.78, 1076.72, 1065.533, 1055.996, 1050.775, 1048.177, 1043.59, 1039.569, 1037.578, 1036.144, 1034.387, 1032.047, 1029.989, 1030.61, 1027.165, 1026.01, 1024.31, 1020.407, 1022.507, 1022.262, 1020.454, 1016.339, 1015.18, 1016.973, 1016.13, 1010.973, 1013.484, 1010.931, 1007.681, 1010.893, 1006.579, 1009.19, 1004.444, 1003.919, 1002.993, 1000.991, 1002.387, 1001.496, 1001.417, 1001.792, 998.93, 994.362, 997.897, 994.95, 994.501, 994.443, 995.389, 993.579, 992.229, 992.783, 991.515, 989.628, 990.693, 991.075, 991.519, 990.353, 990.176, 990.275, 985.361, 987.311, 988.048, 988.198, 990.0, 986.711, 986.822, 988.633, 984.164, 985.789, 984.487, 983.998, 984.678, 983.254, 983.073, 981.318, 979.99, 977.969, 980.403, 980.046, 977.475, 976.329, 978.293, 975.434, 976.671, 972.845, 972.818, 973.539, 973.321, 973.365, 970.26, 971.187, 972.469, 972.763, 968.652, 969.594, 970.858, 969.823, 969.157, 970.364, 968.871], 'val_loss': [1074.79, 1050.843, 1044.963, 1035.486, 1027.223, 1023.93, 1018.817, 1011.124, 1019.908, 1021.47, 1009.079, 1015.221, 1019.412, 1008.766, 1013.732, 1020.65, 1003.089, 1023.194, 1019.738, 1013.633, 1015.747, 1024.364, 1019.867, 1022.233, 1020.983, 1022.839, 1014.461, 1007.1, 1021.003, 1012.36, 1010.586, 1002.252, 1008.088, 1015.938, 1018.068, 1007.322, 1017.841, 1031.835, 1016.33, 1007.47, 1022.924, 1001.917, 1015.886, 1006.12, 1015.797, 1013.023, 1008.125, 1007.441, 1003.903, 1013.909, 1020.113, 1003.582, 1013.886, 1010.919, 1023.042, 1015.23, 1010.676, 1011.551, 1003.479, 1003.523, 1013.571, 998.871, 1008.228, 1015.419, 1019.221, 1005.731, 1035.441, 1018.557, 1008.233, 1025.876, 1015.77, 1026.91, 1003.414, 1022.99, 1002.664, 1000.523, 1007.922, 1010.377, 1004.411, 1014.903, 1008.106, 1006.861, 1000.002, 1000.659, 1001.941, 999.203, 1012.84, 1009.091, 1012.732, 994.216, 1000.112, 997.242, 1007.607, 999.48, 1008.41, 998.339, 1013.326, 1021.705, 1017.164, 1023.091]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	2000	True	849.26691		548637	18	-1	387.46561884880066	{'train_loss': [1349.362, 1106.048, 1073.131, 1064.542, 1053.943, 1050.616, 1046.418, 1044.539, 1040.233, 1039.64, 1040.448, 1032.571, 1032.359, 1025.329, 1027.458, 1027.6, 1027.909, 1017.717, 1014.959, 1015.045, 1008.442, 1010.09, 1007.451, 1005.333, 1005.963, 1003.401, 1000.988, 1000.266, 997.018, 995.415, 996.132, 995.508, 990.084, 991.502, 992.08, 986.971, 988.85, 987.604, 985.096, 983.07, 980.897, 971.014, 965.358, 957.087, 956.777, 948.633, 942.619, 939.818, 932.17, 931.475, 926.971, 924.604, 917.817, 918.522, 913.185, 912.928, 911.261, 908.131, 905.31, 906.603, 903.596, 902.534, 899.735, 899.263, 897.769, 893.296, 894.419, 890.382, 892.183, 890.255, 889.503, 890.088, 886.162, 885.121, 884.595, 883.742, 889.507, 882.767, 884.528, 882.332, 881.187, 882.409, 881.837, 878.707, 880.5, 877.185, 878.666, 877.637, 875.649, 874.53, 873.259, 873.413, 871.367, 872.55, 874.89, 869.347, 870.806, 869.306, 872.041, 869.604], 'val_loss': [1114.68, 1063.951, 1046.779, 1032.094, 1027.693, 1014.467, 1023.179, 1006.744, 1013.828, 1009.593, 1010.419, 1014.666, 1004.777, 1009.575, 1005.354, 1005.032, 995.413, 998.304, 1007.832, 993.053, 997.335, 995.272, 992.973, 991.565, 986.72, 990.899, 985.809, 989.29, 986.104, 990.299, 986.187, 980.068, 988.079, 985.945, 976.601, 980.197, 984.029, 976.004, 981.487, 977.167, 965.497, 960.353, 947.385, 936.485, 934.575, 926.927, 931.628, 908.604, 927.822, 909.938, 913.639, 911.829, 912.413, 901.061, 922.974, 907.5, 899.915, 898.685, 896.418, 897.933, 900.33, 903.775, 899.209, 898.209, 899.094, 899.87, 894.808, 897.049, 896.481, 898.714, 912.359, 907.203, 892.831, 905.981, 911.173, 895.363, 898.434, 905.452, 907.102, 904.135, 907.316, 905.32, 895.726, 905.429, 902.008, 894.223, 904.401, 897.326, 900.919, 899.392, 889.907, 901.818, 894.891, 903.736, 897.135, 902.367, 900.083, 894.947, 892.658, 893.441]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:38 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:3 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9077294959182282 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	2000	True	861.90015		393210	18	-1	407.11348509788513	{'train_loss': [1219.642, 1103.926, 1070.936, 1062.119, 1018.964, 995.469, 981.214, 969.796, 963.29, 959.005, 952.197, 950.328, 947.89, 943.746, 943.667, 938.262, 936.591, 935.838, 932.815, 930.852, 931.181, 927.341, 927.389, 925.128, 923.665, 923.615, 920.746, 919.533, 917.523, 917.938, 915.701, 914.777, 915.22, 914.171, 911.676, 911.076, 910.974, 910.53, 908.694, 907.598, 908.596, 908.896, 906.987, 906.599, 906.66, 905.704, 903.657, 900.618, 903.921, 902.291, 900.947, 902.517, 899.43, 901.063, 902.289, 901.158, 898.757, 899.011, 898.645, 898.469, 899.47, 896.583, 896.927, 897.58, 897.027, 898.253, 895.175, 894.357, 894.882, 896.172, 895.788, 892.38, 891.617, 892.414, 891.362, 893.802, 892.113, 890.516, 892.365, 892.69, 893.471, 891.049, 889.64, 890.355, 891.872, 889.02, 887.796, 889.811, 888.698, 888.255, 888.373, 889.085, 888.57, 887.985, 887.266, 884.162, 885.8, 887.638, 885.969, 882.606], 'val_loss': [1149.384, 1045.799, 1048.138, 1035.92, 980.222, 964.84, 947.733, 947.331, 947.621, 947.669, 943.603, 941.337, 939.255, 940.831, 932.466, 928.894, 927.871, 925.237, 913.275, 919.753, 914.488, 909.777, 912.365, 912.866, 909.034, 907.107, 907.28, 905.476, 902.01, 937.77, 902.407, 900.43, 895.436, 898.903, 904.655, 894.956, 898.196, 895.64, 899.553, 898.023, 897.589, 907.599, 896.745, 900.788, 896.149, 892.903, 909.39, 896.899, 894.542, 892.43, 893.65, 898.963, 894.296, 891.235, 900.958, 905.472, 890.774, 894.971, 908.235, 895.47, 895.198, 891.283, 900.768, 903.116, 888.778, 889.729, 898.053, 894.366, 891.935, 892.887, 897.86, 888.846, 893.503, 903.108, 899.217, 885.259, 890.422, 882.88, 906.216, 887.938, 898.982, 890.585, 892.0, 884.321, 887.762, 891.394, 887.012, 890.343, 894.139, 885.905, 886.694, 891.897, 884.284, 891.182, 892.827, 884.844, 900.478, 884.056, 898.486, 894.722]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:95 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:110 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:deconv1d out_channels:78 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:6.125616164740852e-05 batch_size:13 epochs:100	100	1000	True	864.24402		712058	20	-1	487.0818600654602	{'train_loss': [1327.116, 1085.56, 1072.409, 1067.229, 1057.276, 1055.209, 1048.822, 1030.565, 1011.015, 995.011, 978.294, 967.924, 960.751, 952.506, 948.969, 942.609, 939.881, 937.544, 938.682, 934.865, 933.193, 926.318, 925.657, 924.521, 921.514, 923.128, 921.57, 918.462, 919.355, 915.365, 913.555, 910.193, 912.287, 910.013, 914.419, 908.114, 908.796, 911.635, 908.407, 908.347, 906.986, 903.19, 902.804, 901.271, 902.338, 902.751, 904.633, 901.6, 900.544, 900.946, 895.738, 898.278, 895.263, 897.281, 898.236, 894.894, 892.848, 898.835, 893.973, 891.973, 889.666, 891.143, 889.685, 891.667, 889.184, 892.051, 892.771, 889.881, 889.783, 885.377, 886.585, 887.527, 888.64, 886.548, 884.754, 884.689, 885.678, 880.7, 885.686, 882.856, 885.826, 887.164, 882.54, 884.929, 884.968, 883.227, 880.224, 882.7, 879.837, 880.426, 879.058, 879.882, 879.814, 878.162, 878.797, 879.714, 875.987, 879.341, 875.246, 877.779], 'val_loss': [1079.349, 1056.687, 1049.99, 1041.203, 1045.526, 1025.944, 1020.499, 992.162, 970.603, 964.894, 947.856, 942.21, 942.046, 947.707, 943.947, 943.851, 935.995, 922.2, 931.473, 928.925, 953.014, 942.571, 918.2, 917.8, 922.459, 911.081, 906.357, 907.489, 913.127, 931.201, 897.726, 904.182, 902.783, 912.972, 905.727, 914.662, 902.895, 905.989, 897.373, 901.895, 906.714, 900.385, 911.955, 905.936, 907.816, 900.712, 916.37, 899.277, 896.466, 898.91, 904.857, 900.663, 907.06, 908.286, 899.613, 899.208, 906.304, 908.666, 898.328, 905.624, 910.555, 904.253, 908.823, 903.132, 914.247, 905.076, 902.42, 899.108, 906.129, 898.286, 903.28, 901.678, 900.482, 908.486, 906.869, 898.794, 901.195, 905.276, 901.774, 902.563, 906.582, 910.404, 901.943, 902.021, 904.049, 918.857, 896.371, 899.747, 903.215, 909.953, 903.184, 917.296, 908.717, 897.88, 912.871, 902.426, 908.886, 909.299, 898.081, 903.753]}	100	100	True
