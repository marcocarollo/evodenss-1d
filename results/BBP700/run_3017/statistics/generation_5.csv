id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:15 epochs:100	100	2000	True	993.56525		501251	14	-1	362.57744336128235	{'train_loss': [1569.551, 1270.747, 1194.116, 1147.045, 1119.882, 1100.847, 1087.277, 1076.427, 1069.533, 1064.833, 1055.023, 1049.801, 1045.091, 1042.333, 1040.187, 1037.028, 1030.397, 1027.563, 1026.165, 1023.026, 1021.368, 1018.92, 1016.583, 1012.711, 1008.79, 1007.43, 1006.393, 1003.477, 1004.544, 1000.524, 999.373, 997.007, 996.801, 994.237, 992.281, 993.523, 988.634, 988.836, 988.186, 983.383, 983.263, 983.525, 982.756, 978.404, 977.516, 979.188, 978.627, 976.78, 975.769, 974.11, 972.208, 972.015, 970.264, 969.355, 967.652, 968.547, 967.644, 967.166, 965.807, 963.828, 964.643, 962.011, 964.105, 961.387, 963.193, 960.838, 962.061, 960.381, 959.42, 958.177, 957.694, 956.859, 956.367, 954.638, 956.91, 958.41, 955.63, 956.879, 953.955, 952.556, 955.162, 952.206, 954.29, 950.92, 950.955, 951.187, 950.635, 946.823, 948.274, 947.321, 946.544, 946.738, 945.77, 946.529, 945.36, 946.101, 949.287, 948.4, 944.867, 942.172], 'val_loss': [1439.854, 1203.67, 1172.895, 1110.891, 1084.304, 1070.57, 1068.784, 1061.934, 1055.572, 1067.626, 1058.973, 1053.019, 1045.118, 1056.234, 1054.601, 1062.113, 1056.812, 1046.587, 1050.774, 1061.723, 1055.505, 1051.962, 1056.486, 1057.469, 1054.186, 1053.688, 1051.811, 1048.503, 1050.273, 1050.784, 1058.442, 1046.245, 1053.816, 1046.597, 1048.154, 1056.96, 1051.391, 1052.057, 1043.007, 1050.653, 1044.425, 1053.054, 1047.157, 1046.803, 1042.522, 1040.576, 1052.193, 1047.283, 1043.74, 1041.807, 1045.846, 1044.365, 1041.001, 1053.362, 1041.066, 1052.095, 1041.311, 1052.687, 1043.904, 1041.164, 1053.836, 1047.088, 1043.579, 1047.559, 1050.916, 1051.988, 1048.212, 1049.939, 1048.804, 1041.666, 1052.624, 1056.906, 1044.314, 1050.757, 1056.068, 1051.181, 1042.136, 1046.022, 1044.64, 1065.531, 1054.783, 1057.202, 1046.366, 1049.304, 1049.981, 1053.608, 1050.378, 1061.331, 1057.766, 1048.664, 1059.104, 1058.558, 1061.239, 1060.082, 1060.505, 1056.862, 1050.538, 1056.898, 1058.389, 1058.758]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:15 epochs:100	100	3000	True	976.18646		501251	14	-1	324.71504259109497	{'train_loss': [1550.931, 1290.981, 1182.624, 1136.187, 1108.037, 1091.14, 1080.161, 1068.971, 1062.472, 1057.518, 1052.686, 1047.553, 1045.32, 1040.966, 1037.918, 1036.342, 1032.0, 1030.858, 1028.313, 1025.216, 1024.889, 1020.752, 1020.567, 1017.87, 1013.853, 1014.055, 1008.581, 1009.899, 1008.78, 1007.384, 1006.239, 1005.229, 1001.789, 1005.135, 999.7, 1000.108, 997.532, 997.987, 995.185, 995.051, 994.101, 993.607, 991.406, 989.409, 986.764, 986.247, 986.444, 987.07, 983.92, 982.046, 982.393, 981.72, 980.957, 979.281, 979.416, 976.452, 977.536, 974.995, 975.018, 973.0, 971.893, 972.896, 969.194, 970.696, 966.449, 964.828, 967.681, 965.501, 968.975, 962.845, 965.223, 966.994, 963.129, 962.068, 960.359, 960.422, 963.398, 960.696, 958.298, 957.759, 958.358, 958.581, 956.25, 958.462, 957.514, 953.19, 956.012, 955.244, 953.312, 953.366, 951.881, 953.367, 952.139, 953.574, 952.12, 950.494, 951.612, 948.554, 952.762, 949.674], 'val_loss': [1348.389, 1198.701, 1143.046, 1092.738, 1081.426, 1063.907, 1067.128, 1055.697, 1056.893, 1052.976, 1044.301, 1071.938, 1055.211, 1059.633, 1042.813, 1045.913, 1042.956, 1056.826, 1043.409, 1042.478, 1044.953, 1047.253, 1037.065, 1027.821, 1050.712, 1034.42, 1032.728, 1034.586, 1042.852, 1030.439, 1029.581, 1031.602, 1028.635, 1026.859, 1029.667, 1030.813, 1031.671, 1027.305, 1025.682, 1032.645, 1030.508, 1026.366, 1033.518, 1031.786, 1030.962, 1027.168, 1037.437, 1030.901, 1035.318, 1031.38, 1034.751, 1033.641, 1031.386, 1035.936, 1033.206, 1037.481, 1041.086, 1036.856, 1035.576, 1035.306, 1040.287, 1037.488, 1043.118, 1032.845, 1036.143, 1046.763, 1042.951, 1031.164, 1033.612, 1038.199, 1038.205, 1032.803, 1041.368, 1031.042, 1042.239, 1037.485, 1037.002, 1032.845, 1028.147, 1035.978, 1037.142, 1031.959, 1030.617, 1057.898, 1031.628, 1037.549, 1027.707, 1028.038, 1038.342, 1031.062, 1041.566, 1039.224, 1038.846, 1033.959, 1033.869, 1029.632, 1036.721, 1031.857, 1031.526, 1032.001]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:96 kernel_size:6 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:96 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.004345102422989143 beta1:0.9233605225839873 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:15 epochs:100	100	1000	True	1014.5238		658766	15	-1	372.930965423584	{'train_loss': [1575.906, 1295.092, 1198.463, 1150.51, 1119.498, 1100.805, 1088.926, 1077.559, 1067.935, 1062.911, 1058.657, 1051.326, 1045.993, 1040.406, 1037.391, 1032.978, 1029.568, 1030.567, 1029.493, 1028.189, 1026.671, 1023.112, 1022.811, 1019.354, 1015.617, 1012.997, 1013.23, 1018.551, 1007.629, 1006.598, 1011.084, 1003.38, 1005.722, 998.893, 1002.692, 1002.504, 1002.399, 997.972, 997.87, 993.428, 995.103, 997.263, 997.662, 990.485, 997.203, 996.116, 992.568, 988.312, 988.728, 995.108, 993.13, 991.465, 999.126, 993.872, 994.18, 990.454, 987.963, 985.354, 984.98, 983.976, 980.965, 984.573, 984.554, 980.741, 980.698, 981.806, 983.058, 978.631, 978.405, 979.834, 972.942, 975.314, 971.826, 971.17, 969.179, 969.79, 970.163, 972.039, 970.416, 969.146, 973.269, 972.798, 969.281, 964.089, 968.798, 967.72, 966.989, 963.724, 965.094, 964.343, 965.289, 961.694, 960.728, 960.731, 958.374, 957.444, 959.845, 956.063, 959.842, 956.645], 'val_loss': [1414.644, 1222.299, 1143.585, 1111.607, 1085.509, 1079.82, 1067.547, 1068.61, 1073.002, 1058.239, 1071.292, 1055.808, 1063.657, 1057.095, 1058.45, 1057.93, 1049.379, 1058.849, 1065.533, 1051.812, 1047.687, 1049.927, 1046.651, 1056.709, 1059.881, 1072.281, 1056.03, 1067.774, 1063.964, 1082.27, 1090.071, 1071.456, 1076.796, 1065.141, 1091.046, 1070.095, 1089.373, 1080.399, 1066.769, 1074.574, 1080.969, 1079.859, 1065.7, 1079.593, 1091.567, 1087.746, 1064.413, 1060.202, 1095.925, 1094.644, 1085.084, 1115.207, 1088.678, 1094.829, 1078.221, 1087.197, 1083.014, 1088.488, 1080.802, 1080.852, 1081.397, 1078.046, 1070.766, 1076.508, 1072.607, 1082.689, 1068.782, 1073.158, 1089.09, 1076.469, 1076.461, 1082.974, 1088.963, 1076.519, 1090.177, 1075.281, 1078.43, 1091.879, 1078.377, 1089.763, 1072.598, 1093.581, 1078.674, 1079.873, 1074.068, 1072.679, 1066.839, 1081.412, 1093.377, 1096.634, 1085.889, 1064.613, 1084.114, 1094.356, 1073.44, 1070.589, 1072.134, 1072.516, 1075.907, 1081.56]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	890.57367		732867	16	-1	471.03292965888977	{'train_loss': [2039.478, 1116.214, 1079.033, 1068.77, 1061.24, 1055.822, 1049.348, 1045.037, 1042.211, 1039.422, 1036.286, 1033.243, 1000.94, 965.903, 944.697, 933.616, 925.363, 919.373, 913.322, 909.852, 903.509, 901.646, 898.961, 893.149, 892.615, 890.928, 884.93, 884.514, 880.221, 881.559, 876.961, 871.879, 870.116, 867.008, 865.679, 864.056, 862.773, 859.815, 858.63, 856.471, 854.768, 852.549, 852.369, 849.926, 847.983, 847.174, 844.13, 841.475, 838.19, 838.813, 837.346, 836.6, 837.965, 834.625, 832.901, 830.045, 830.124, 828.414, 829.637, 828.208, 826.008, 825.433, 824.789, 824.342, 820.523, 819.631, 820.802, 820.009, 818.945, 814.971, 816.087, 813.888, 817.698, 812.922, 811.57, 814.385, 811.635, 810.489, 807.327, 806.527, 805.556, 805.47, 800.686, 801.278, 802.635, 802.216, 801.895, 798.512, 804.948, 798.408, 797.932, 800.755, 795.833, 796.922, 797.619, 799.15, 796.095, 796.397, 794.983, 796.036], 'val_loss': [1195.96, 1055.824, 1047.458, 1042.577, 1042.009, 1040.788, 1028.865, 1017.663, 1014.532, 1012.586, 1023.184, 1003.231, 955.958, 935.576, 920.042, 917.215, 917.523, 910.442, 911.078, 898.614, 910.827, 900.834, 904.222, 891.781, 892.482, 901.165, 891.295, 892.789, 899.69, 898.533, 897.705, 899.573, 895.896, 902.838, 906.979, 899.576, 902.254, 901.612, 906.745, 906.509, 913.991, 911.183, 903.201, 905.415, 905.679, 914.114, 912.166, 907.63, 913.727, 928.956, 912.451, 925.708, 917.018, 924.46, 906.082, 910.772, 908.407, 910.41, 909.787, 905.305, 906.483, 911.408, 910.184, 910.084, 907.172, 911.875, 908.227, 909.174, 910.321, 907.579, 909.325, 904.793, 902.606, 903.798, 905.747, 903.082, 901.229, 901.466, 903.505, 900.891, 906.539, 911.191, 904.031, 898.316, 903.776, 903.299, 908.052, 904.566, 908.638, 901.486, 903.005, 902.621, 908.298, 911.404, 897.959, 910.828, 910.189, 906.689, 905.054, 907.006]}	100	100	True
