id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:15 epochs:100	100	2000	True	993.51947		501251	14	-1	278.26923155784607	{'train_loss': [1607.445, 1267.742, 1220.48, 1151.786, 1119.129, 1095.959, 1082.416, 1072.529, 1064.333, 1059.659, 1052.004, 1047.388, 1041.003, 1038.105, 1034.625, 1028.807, 1026.535, 1024.541, 1021.482, 1017.473, 1013.65, 1012.276, 1008.195, 1006.937, 1003.676, 1002.12, 999.698, 1000.191, 995.901, 992.732, 991.04, 989.485, 988.258, 985.987, 985.661, 982.325, 981.509, 980.667, 978.208, 976.733, 975.336, 977.516, 973.352, 975.209, 972.751, 973.347, 969.632, 967.801, 970.972, 967.21, 969.115, 966.496, 966.248, 965.812, 963.3, 963.65, 960.767, 961.309, 961.706, 957.775, 957.565, 956.162, 956.147, 954.56, 955.418, 954.219, 954.64, 952.651, 953.522, 950.071, 949.973, 950.523, 948.83, 950.407, 949.57, 950.282, 947.558, 946.386, 945.122, 940.917, 946.457, 945.093, 943.845, 945.337, 941.942, 944.744, 943.652, 942.896, 941.179, 940.453, 938.407, 942.638, 938.142, 938.373, 938.282, 938.337, 940.737, 940.15, 937.015, 935.378], 'val_loss': [1315.021, 1219.24, 1163.354, 1132.055, 1098.356, 1074.878, 1069.66, 1071.43, 1064.847, 1066.629, 1052.403, 1056.22, 1045.878, 1054.058, 1049.391, 1056.405, 1057.515, 1055.545, 1047.926, 1054.117, 1050.784, 1058.958, 1044.56, 1068.712, 1050.778, 1047.485, 1052.228, 1049.294, 1040.791, 1040.158, 1044.081, 1045.797, 1044.665, 1051.973, 1039.406, 1044.607, 1041.341, 1053.448, 1049.932, 1037.866, 1043.783, 1042.246, 1041.13, 1051.189, 1048.424, 1043.365, 1039.524, 1048.849, 1047.044, 1044.534, 1034.147, 1039.656, 1046.723, 1036.421, 1044.199, 1039.006, 1031.677, 1034.532, 1042.493, 1042.273, 1050.722, 1050.766, 1050.151, 1036.898, 1047.476, 1036.954, 1051.329, 1048.752, 1041.478, 1038.155, 1032.105, 1040.631, 1030.076, 1040.096, 1034.14, 1031.61, 1032.318, 1025.972, 1036.097, 1028.047, 1032.141, 1029.043, 1030.153, 1033.962, 1037.269, 1037.265, 1041.89, 1038.159, 1042.93, 1047.698, 1040.517, 1032.899, 1039.844, 1044.558, 1040.087, 1037.507, 1040.711, 1039.139, 1048.106, 1045.628]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:15 epochs:100	100	2000	True	981.68439		501251	14	-1	349.7202112674713	{'train_loss': [1586.878, 1279.157, 1229.815, 1215.74, 1205.564, 1162.127, 1117.919, 1096.832, 1080.685, 1067.514, 1061.243, 1054.862, 1048.434, 1045.008, 1040.009, 1035.315, 1033.208, 1028.053, 1025.667, 1024.304, 1020.328, 1018.347, 1018.979, 1014.818, 1013.681, 1010.385, 1010.746, 1007.63, 1003.896, 1002.79, 1003.774, 999.89, 1000.89, 999.852, 995.226, 995.48, 994.602, 993.19, 992.117, 990.975, 990.953, 988.011, 987.737, 985.461, 986.017, 985.32, 981.069, 983.338, 981.508, 980.345, 981.838, 978.183, 979.253, 976.643, 977.112, 976.182, 974.374, 975.318, 973.303, 971.879, 975.184, 972.475, 968.585, 969.679, 971.187, 966.831, 967.306, 965.843, 965.837, 965.565, 963.108, 963.744, 964.095, 963.823, 962.968, 960.916, 962.442, 958.52, 958.013, 958.237, 959.728, 955.841, 957.727, 955.275, 953.056, 956.112, 950.963, 951.923, 951.919, 952.597, 952.733, 953.196, 950.63, 951.35, 948.53, 952.956, 948.513, 949.703, 948.052, 949.775], 'val_loss': [1414.864, 1225.562, 1210.422, 1205.907, 1173.392, 1120.366, 1082.628, 1064.681, 1073.392, 1049.55, 1073.924, 1052.354, 1064.07, 1057.965, 1052.521, 1054.385, 1052.403, 1046.433, 1053.856, 1057.176, 1057.354, 1068.034, 1056.478, 1052.89, 1044.302, 1046.364, 1056.453, 1050.221, 1045.172, 1053.071, 1047.158, 1042.494, 1053.239, 1042.283, 1050.628, 1042.458, 1051.16, 1041.355, 1048.146, 1041.286, 1042.869, 1053.745, 1047.932, 1044.259, 1037.916, 1044.593, 1047.19, 1041.597, 1041.782, 1036.287, 1032.753, 1042.437, 1035.087, 1038.627, 1032.191, 1040.547, 1036.536, 1037.324, 1035.735, 1042.686, 1035.002, 1032.394, 1040.936, 1034.953, 1033.81, 1037.217, 1032.539, 1033.883, 1032.843, 1033.177, 1032.605, 1032.105, 1039.283, 1037.94, 1035.173, 1033.453, 1040.359, 1036.486, 1037.549, 1032.59, 1033.341, 1038.177, 1032.685, 1037.713, 1034.863, 1034.779, 1031.782, 1031.47, 1041.692, 1036.987, 1033.997, 1039.429, 1034.146, 1027.741, 1031.912, 1029.213, 1028.717, 1030.848, 1030.64, 1028.066]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:15 epochs:100	100	2000	True	981.68451		501251	14	-1	349.7124078273773	{'train_loss': [1586.878, 1279.157, 1229.815, 1215.74, 1205.564, 1162.127, 1117.919, 1096.832, 1080.685, 1067.514, 1061.243, 1054.862, 1048.434, 1045.008, 1040.009, 1035.315, 1033.208, 1028.053, 1025.667, 1024.304, 1020.328, 1018.347, 1018.979, 1014.818, 1013.681, 1010.385, 1010.746, 1007.63, 1003.896, 1002.79, 1003.774, 999.89, 1000.89, 999.852, 995.226, 995.48, 994.602, 993.19, 992.117, 990.975, 990.953, 988.011, 987.737, 985.461, 986.017, 985.32, 981.069, 983.338, 981.508, 980.345, 981.838, 978.183, 979.253, 976.643, 977.112, 976.182, 974.374, 975.318, 973.303, 971.879, 975.184, 972.475, 968.585, 969.679, 971.187, 966.831, 967.306, 965.843, 965.837, 965.565, 963.108, 963.744, 964.095, 963.823, 962.968, 960.916, 962.442, 958.52, 958.013, 958.237, 959.728, 955.841, 957.727, 955.275, 953.056, 956.112, 950.963, 951.923, 951.919, 952.597, 952.733, 953.196, 950.63, 951.35, 948.53, 952.956, 948.513, 949.703, 948.052, 949.775], 'val_loss': [1414.864, 1225.562, 1210.422, 1205.907, 1173.392, 1120.366, 1082.628, 1064.681, 1073.392, 1049.55, 1073.924, 1052.354, 1064.07, 1057.965, 1052.521, 1054.385, 1052.403, 1046.433, 1053.856, 1057.176, 1057.354, 1068.034, 1056.478, 1052.89, 1044.302, 1046.364, 1056.453, 1050.221, 1045.172, 1053.071, 1047.158, 1042.494, 1053.239, 1042.283, 1050.628, 1042.458, 1051.16, 1041.355, 1048.146, 1041.286, 1042.869, 1053.745, 1047.932, 1044.259, 1037.916, 1044.593, 1047.19, 1041.597, 1041.782, 1036.287, 1032.753, 1042.437, 1035.087, 1038.627, 1032.191, 1040.547, 1036.536, 1037.324, 1035.735, 1042.686, 1035.002, 1032.394, 1040.936, 1034.953, 1033.81, 1037.217, 1032.539, 1033.883, 1032.843, 1033.177, 1032.605, 1032.105, 1039.283, 1037.94, 1035.173, 1033.453, 1040.359, 1036.486, 1037.549, 1032.59, 1033.341, 1038.177, 1032.685, 1037.713, 1034.863, 1034.779, 1031.782, 1031.47, 1041.692, 1036.987, 1033.997, 1039.429, 1034.146, 1027.741, 1031.912, 1029.213, 1028.717, 1030.848, 1030.64, 1028.066]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:48 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:123 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:86 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.00648996491282109 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:15 epochs:100	100	1000	True	1042.89612		438059	15	-1	290.33792066574097	{'train_loss': [1506.318, 1271.953, 1192.57, 1137.085, 1113.128, 1094.809, 1083.149, 1073.163, 1066.153, 1058.667, 1055.024, 1050.501, 1048.051, 1044.276, 1040.384, 1039.419, 1035.167, 1032.48, 1030.507, 1028.079, 1026.796, 1025.067, 1024.796, 1020.897, 1019.364, 1019.292, 1017.002, 1015.125, 1013.037, 1012.329, 1010.42, 1009.079, 1008.536, 1006.358, 1005.226, 1006.618, 1002.879, 1001.938, 1003.854, 1002.139, 1000.767, 999.933, 997.296, 999.766, 995.331, 994.442, 995.129, 993.885, 993.443, 994.643, 990.463, 992.277, 993.406, 991.496, 989.942, 990.615, 988.446, 989.169, 986.328, 985.73, 985.942, 982.944, 985.266, 982.631, 981.415, 984.472, 981.086, 985.996, 982.492, 980.37, 981.681, 977.762, 978.664, 978.856, 979.436, 979.373, 978.489, 977.381, 977.922, 978.43, 975.01, 975.305, 975.391, 973.911, 972.58, 972.487, 973.041, 973.016, 972.476, 972.418, 972.815, 973.542, 972.665, 969.52, 969.985, 972.117, 970.07, 967.667, 971.601, 971.356], 'val_loss': [1328.38, 1223.158, 1138.177, 1104.868, 1075.527, 1081.028, 1073.267, 1060.819, 1066.29, 1056.835, 1049.813, 1045.252, 1051.374, 1050.493, 1050.757, 1050.867, 1045.736, 1039.662, 1043.007, 1036.768, 1040.171, 1039.432, 1034.062, 1027.395, 1034.437, 1052.282, 1033.867, 1036.699, 1045.028, 1053.282, 1028.485, 1038.526, 1041.431, 1046.564, 1036.278, 1028.635, 1035.127, 1045.332, 1037.932, 1050.648, 1045.374, 1034.632, 1031.309, 1036.448, 1054.35, 1040.71, 1034.531, 1033.028, 1052.204, 1032.88, 1042.892, 1035.051, 1030.648, 1034.308, 1032.716, 1061.936, 1042.547, 1030.082, 1023.43, 1040.389, 1056.789, 1023.88, 1030.809, 1026.611, 1022.46, 1026.069, 1025.033, 1034.854, 1049.367, 1026.749, 1031.587, 1039.387, 1030.591, 1035.58, 1032.235, 1050.832, 1032.225, 1050.001, 1052.33, 1069.646, 1042.23, 1033.632, 1044.005, 1054.508, 1042.673, 1049.129, 1054.84, 1070.361, 1051.764, 1045.299, 1073.107, 1089.475, 1040.841, 1050.343, 1047.122, 1044.931, 1042.891, 1067.575, 1040.499, 1068.944]}	100	100	True
