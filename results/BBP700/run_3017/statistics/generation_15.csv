id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	3000	True	856.27484		548637	18	-1	441.46866750717163	{'train_loss': [1336.745, 1116.41, 1082.219, 1066.68, 1011.687, 980.789, 962.03, 949.592, 943.674, 934.822, 927.744, 925.867, 921.058, 917.531, 913.807, 910.662, 907.85, 904.134, 902.893, 902.398, 900.062, 896.626, 898.636, 894.637, 893.939, 893.41, 892.289, 894.592, 891.021, 887.409, 886.954, 887.974, 882.288, 884.939, 882.871, 883.622, 881.244, 881.027, 879.927, 880.192, 879.043, 879.037, 876.184, 878.1, 872.108, 876.026, 873.132, 874.553, 871.69, 869.804, 871.175, 871.279, 869.554, 867.101, 869.384, 868.304, 867.183, 862.348, 864.402, 862.582, 863.408, 865.663, 861.365, 860.806, 860.207, 861.406, 862.764, 861.72, 859.23, 857.361, 862.228, 860.613, 859.63, 857.894, 855.922, 857.642, 853.929, 858.696, 855.087, 851.42, 853.539, 849.595, 852.184, 852.426, 845.635, 849.833, 850.068, 847.17, 847.566, 846.909, 848.898, 849.303, 844.826, 843.596, 843.268, 841.641, 841.288, 840.798, 840.593, 840.443], 'val_loss': [1143.007, 1067.029, 1055.101, 1021.271, 969.838, 929.145, 920.419, 917.873, 910.033, 907.242, 900.46, 899.885, 912.14, 894.151, 886.074, 889.558, 893.525, 888.453, 895.373, 897.956, 886.801, 881.75, 887.506, 889.743, 889.654, 890.004, 887.04, 895.689, 889.017, 883.196, 886.842, 890.974, 885.897, 886.278, 887.596, 889.56, 881.381, 889.844, 888.307, 885.121, 896.868, 893.83, 898.939, 884.574, 888.692, 894.191, 893.178, 904.733, 892.184, 890.243, 897.096, 898.145, 892.623, 889.563, 895.386, 898.028, 889.638, 901.286, 893.334, 897.204, 902.023, 894.842, 897.028, 893.132, 902.007, 899.495, 896.275, 895.213, 886.685, 895.522, 901.483, 902.504, 901.534, 896.787, 917.729, 897.238, 894.09, 897.248, 897.854, 894.419, 902.098, 904.789, 898.284, 915.922, 899.19, 901.573, 900.8, 893.683, 905.389, 901.509, 906.112, 902.487, 919.375, 899.49, 910.443, 905.505, 895.863, 904.792, 901.889, 904.555]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:54 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:18 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:7.292751458799289e-05 batch_size:13 epochs:100	100	1000	True	881.74219		593118	19	-1	518.7322273254395	{'train_loss': [1656.462, 1136.052, 1089.476, 1076.12, 1069.329, 1069.613, 1063.728, 1058.551, 1055.107, 1048.385, 1019.539, 990.142, 975.659, 964.271, 956.052, 948.068, 939.97, 936.894, 932.953, 926.984, 928.363, 922.197, 918.499, 918.643, 915.142, 914.018, 911.288, 903.486, 907.701, 904.591, 904.689, 902.878, 899.556, 898.998, 897.451, 891.357, 893.398, 889.32, 894.602, 882.507, 885.829, 885.67, 888.326, 880.758, 880.539, 883.842, 877.018, 881.867, 877.79, 880.625, 874.656, 871.996, 874.052, 870.651, 871.631, 871.045, 867.122, 871.336, 869.527, 869.475, 868.247, 870.219, 864.955, 863.666, 863.539, 868.396, 863.365, 860.9, 856.327, 859.102, 855.824, 854.918, 858.613, 853.903, 856.068, 858.667, 855.565, 857.66, 851.297, 851.316, 850.379, 856.412, 859.187, 852.828, 855.474, 851.44, 850.154, 848.821, 848.67, 848.152, 856.382, 852.507, 853.146, 847.635, 851.25, 847.633, 846.188, 845.529, 845.397, 846.43], 'val_loss': [1195.06, 1085.684, 1057.179, 1034.271, 1037.178, 1032.885, 1025.518, 1034.11, 1018.071, 1010.684, 976.03, 968.109, 959.573, 953.378, 944.112, 932.682, 929.419, 925.14, 922.055, 917.47, 911.544, 913.581, 913.103, 913.805, 910.445, 916.635, 912.614, 918.496, 919.729, 914.298, 917.828, 910.053, 900.303, 907.63, 904.663, 910.881, 914.411, 905.837, 913.386, 905.212, 909.064, 909.151, 910.6, 908.939, 908.262, 923.883, 913.803, 911.845, 912.694, 926.137, 931.532, 908.004, 919.511, 918.69, 903.796, 911.737, 936.803, 925.191, 923.143, 922.645, 932.024, 919.109, 936.56, 926.51, 910.616, 902.991, 906.362, 918.468, 910.549, 910.538, 916.194, 908.33, 1019.235, 897.458, 901.437, 911.259, 900.433, 909.842, 921.033, 922.75, 913.316, 927.339, 912.426, 926.371, 927.409, 925.765, 903.731, 927.066, 915.34, 961.052, 934.762, 926.844, 908.302, 919.119, 906.557, 912.365, 914.943, 905.507, 917.547, 900.923]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:65 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.0037291634219010156 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	854.56885		564971	19	-1	460.1519088745117	{'train_loss': [1298.596, 1104.439, 1072.925, 1058.413, 1034.303, 983.661, 961.043, 946.854, 936.34, 929.207, 923.596, 916.43, 914.397, 908.123, 904.815, 900.979, 900.375, 895.954, 894.638, 892.829, 890.818, 888.73, 885.783, 885.886, 883.575, 882.625, 880.513, 876.936, 878.24, 875.796, 875.581, 872.416, 873.968, 870.411, 869.007, 867.948, 868.771, 865.229, 863.529, 866.022, 863.501, 863.144, 861.901, 862.576, 861.102, 860.958, 859.853, 859.653, 857.818, 856.086, 853.987, 853.845, 854.113, 853.087, 852.751, 851.822, 850.896, 851.662, 848.978, 849.496, 847.676, 848.457, 846.028, 846.772, 846.747, 843.974, 844.935, 844.1, 842.682, 841.638, 841.875, 841.22, 842.491, 841.538, 840.162, 840.335, 839.091, 837.791, 838.351, 839.309, 837.989, 837.744, 837.014, 836.683, 838.641, 836.993, 833.93, 835.545, 833.574, 833.887, 833.275, 830.705, 830.649, 829.32, 834.492, 831.619, 829.817, 830.616, 827.424, 830.239], 'val_loss': [1152.925, 1066.726, 1050.846, 1027.558, 974.069, 949.557, 925.255, 920.397, 914.345, 909.825, 907.572, 901.754, 897.274, 890.718, 899.61, 894.922, 890.163, 895.149, 894.87, 883.67, 885.786, 885.755, 884.606, 886.792, 887.742, 890.966, 885.137, 883.969, 884.577, 890.562, 883.466, 887.823, 881.668, 885.803, 890.412, 885.3, 889.528, 888.708, 895.76, 889.447, 887.904, 887.85, 886.119, 885.767, 884.089, 887.114, 886.491, 887.283, 886.033, 893.645, 890.064, 891.977, 885.839, 887.616, 886.985, 889.836, 896.199, 888.66, 890.908, 899.168, 896.391, 891.071, 894.903, 886.425, 896.241, 905.607, 896.593, 894.128, 895.115, 892.762, 891.141, 895.448, 887.701, 896.387, 899.587, 897.024, 894.133, 900.104, 899.213, 901.161, 896.373, 898.973, 905.873, 898.208, 893.064, 892.956, 892.54, 901.17, 889.195, 895.56, 903.143, 901.383, 901.62, 894.277, 903.992, 898.043, 896.274, 902.771, 898.793, 899.977]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.009097544753807963 beta1:0.8850908524635891 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	878.14075		514392	19	-1	459.08506631851196	{'train_loss': [1289.402, 1113.796, 1077.682, 1063.296, 1030.37, 1003.557, 989.205, 975.688, 965.557, 953.42, 948.714, 941.087, 932.776, 930.221, 927.02, 923.425, 920.716, 921.403, 914.538, 913.022, 912.721, 912.416, 907.868, 909.223, 905.797, 906.749, 902.969, 899.76, 899.264, 896.365, 897.906, 896.629, 894.949, 894.432, 894.356, 892.705, 894.537, 892.605, 889.005, 889.11, 893.93, 886.429, 885.594, 884.47, 885.312, 883.461, 883.428, 883.517, 882.466, 881.564, 881.997, 883.909, 881.172, 883.908, 881.948, 878.583, 878.765, 876.981, 878.506, 877.028, 874.347, 875.923, 874.247, 875.34, 875.501, 872.798, 873.118, 874.064, 871.189, 870.326, 870.863, 870.492, 872.443, 868.572, 870.974, 869.338, 867.89, 868.088, 869.368, 867.094, 866.61, 866.43, 866.136, 867.211, 866.171, 862.562, 866.982, 865.24, 865.798, 864.275, 863.269, 866.207, 861.596, 860.11, 861.403, 862.022, 859.46, 860.922, 863.201, 858.915], 'val_loss': [1150.298, 1065.402, 1052.874, 1042.64, 993.237, 986.405, 967.521, 964.604, 945.625, 938.564, 935.496, 923.713, 928.394, 922.981, 907.008, 908.304, 900.883, 906.899, 905.168, 905.951, 913.267, 902.333, 898.223, 902.503, 908.276, 914.903, 906.782, 917.096, 928.064, 919.055, 919.228, 905.67, 909.416, 909.211, 906.783, 902.72, 905.041, 902.465, 895.473, 913.074, 901.247, 908.134, 909.842, 906.687, 904.147, 906.771, 907.265, 900.995, 907.06, 906.192, 906.599, 920.657, 909.727, 920.984, 906.354, 918.522, 900.025, 901.039, 909.25, 907.901, 921.703, 919.409, 904.97, 913.704, 907.144, 921.836, 941.35, 911.376, 913.571, 924.311, 907.416, 928.352, 924.761, 902.005, 915.288, 922.719, 905.596, 910.956, 930.768, 917.41, 934.083, 911.421, 914.817, 918.915, 905.306, 918.952, 936.417, 930.448, 942.05, 950.776, 925.82, 912.376, 913.202, 958.926, 933.253, 922.696, 922.365, 933.654, 915.47, 916.854]}	100	100	True
