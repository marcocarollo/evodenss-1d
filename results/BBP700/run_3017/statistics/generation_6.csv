id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	880.8255		732867	16	-1	370.1920516490936	{'train_loss': [1822.94, 1094.715, 1013.651, 974.895, 960.039, 946.788, 934.736, 926.407, 919.373, 913.87, 910.195, 905.072, 899.351, 897.967, 895.0, 890.885, 888.976, 886.263, 882.409, 879.871, 877.649, 874.673, 873.006, 871.176, 869.517, 864.813, 865.427, 862.614, 860.875, 860.659, 858.655, 856.071, 856.858, 856.07, 851.909, 851.892, 852.249, 851.057, 850.543, 847.244, 844.303, 844.764, 844.812, 843.165, 841.529, 841.266, 840.523, 840.491, 838.849, 837.046, 837.779, 839.578, 837.098, 838.417, 833.761, 835.967, 830.136, 832.97, 834.15, 835.34, 832.103, 828.029, 827.982, 829.082, 829.121, 828.489, 826.677, 825.602, 826.961, 822.76, 825.332, 823.973, 824.123, 823.026, 823.044, 824.619, 823.394, 823.643, 824.833, 823.088, 821.927, 821.303, 820.76, 820.713, 820.049, 820.768, 819.718, 822.414, 821.951, 818.208, 818.788, 819.861, 816.074, 818.75, 815.119, 818.445, 815.988, 815.982, 815.216, 815.703], 'val_loss': [1172.712, 1006.394, 962.7, 951.566, 942.214, 930.735, 943.837, 941.061, 943.271, 921.718, 920.524, 924.493, 927.503, 913.84, 923.94, 930.657, 931.747, 913.178, 918.167, 929.78, 912.564, 932.246, 916.362, 912.639, 914.487, 927.583, 911.705, 924.178, 911.94, 914.834, 919.481, 913.693, 910.33, 911.416, 909.663, 910.45, 905.523, 919.877, 918.683, 905.754, 910.167, 910.797, 906.721, 909.826, 917.838, 911.226, 902.597, 910.401, 929.9, 914.224, 921.691, 914.192, 910.838, 915.931, 920.465, 916.64, 932.019, 915.29, 917.94, 925.247, 918.184, 929.568, 926.934, 921.067, 922.932, 924.58, 918.719, 919.537, 915.073, 925.038, 922.074, 916.822, 915.789, 917.889, 920.857, 923.32, 928.021, 919.907, 924.689, 920.401, 918.779, 920.156, 912.873, 924.036, 917.323, 926.986, 917.353, 919.391, 922.21, 913.118, 915.457, 914.738, 916.35, 927.153, 922.103, 908.984, 917.441, 908.974, 905.891, 920.116]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:10 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:49 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:9.613902847245783e-05 batch_size:13 epochs:100	100	1000	True	906.85645		727556	16	-1	367.28217029571533	{'train_loss': [1539.964, 1043.887, 993.186, 970.242, 957.59, 946.357, 937.146, 929.097, 924.664, 919.503, 915.854, 911.713, 909.915, 907.629, 905.413, 902.69, 899.685, 895.743, 894.487, 892.895, 889.407, 888.714, 884.053, 884.933, 882.388, 882.482, 879.63, 876.972, 878.848, 876.03, 873.13, 872.63, 870.593, 870.061, 868.396, 867.537, 865.996, 865.552, 863.23, 860.135, 861.198, 860.221, 861.512, 858.73, 857.298, 854.089, 852.593, 855.304, 855.712, 854.16, 851.656, 852.351, 850.727, 848.073, 846.45, 848.275, 847.738, 844.319, 844.388, 842.914, 840.379, 839.352, 843.5, 838.425, 838.146, 841.139, 839.714, 835.734, 836.403, 837.995, 835.59, 836.523, 836.439, 834.275, 832.749, 833.617, 832.46, 833.064, 830.173, 831.115, 828.245, 830.677, 828.372, 828.476, 826.933, 825.574, 826.22, 825.566, 824.468, 825.208, 826.459, 827.498, 823.205, 821.612, 823.206, 821.133, 822.483, 824.7, 822.441, 820.245], 'val_loss': [1101.1, 971.287, 952.074, 951.196, 943.947, 941.923, 935.986, 928.221, 935.356, 932.626, 929.779, 915.99, 917.722, 913.177, 917.835, 915.386, 909.824, 911.808, 915.821, 913.608, 917.226, 924.822, 916.863, 909.537, 914.601, 928.276, 916.074, 910.199, 912.019, 911.834, 909.368, 906.113, 915.501, 913.074, 909.226, 914.028, 914.272, 919.577, 918.84, 917.518, 921.085, 927.92, 915.761, 919.979, 915.802, 918.425, 911.784, 924.911, 914.948, 906.041, 920.6, 921.613, 919.305, 918.523, 914.353, 916.818, 915.697, 915.211, 912.789, 917.321, 918.017, 927.275, 914.575, 914.664, 918.86, 918.225, 922.032, 923.188, 920.408, 919.213, 917.135, 915.773, 922.34, 923.046, 922.204, 919.858, 924.257, 927.013, 921.01, 925.991, 930.073, 935.813, 937.378, 925.681, 930.692, 927.493, 925.053, 935.367, 926.792, 937.881, 928.308, 926.948, 930.936, 927.457, 926.168, 927.993, 925.376, 929.915, 930.278, 930.54]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:rmsprop lr:0.004345102422989143 alpha:0.9729613741942711 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	891.60022		813738	17	-1	390.0677092075348	{'train_loss': [4918.188, 4736.202, 3338.837, 1172.064, 1140.171, 1091.025, 1074.571, 1064.719, 1059.121, 1054.086, 1045.499, 1036.048, 1009.56, 988.932, 967.845, 959.164, 950.16, 940.903, 936.076, 928.107, 925.608, 920.427, 917.939, 916.326, 913.861, 906.515, 904.912, 898.689, 898.644, 895.006, 893.026, 887.638, 888.531, 884.465, 883.062, 880.43, 878.597, 875.937, 872.526, 871.469, 868.89, 866.807, 864.467, 861.615, 862.726, 859.021, 861.64, 857.427, 856.266, 854.85, 852.765, 852.337, 847.968, 847.713, 848.242, 843.469, 850.351, 842.188, 841.71, 839.901, 839.288, 837.652, 835.632, 835.037, 833.313, 832.194, 832.225, 829.459, 827.685, 827.8, 827.33, 825.276, 823.087, 822.888, 824.507, 821.489, 822.049, 818.778, 817.997, 815.968, 814.923, 815.397, 814.419, 815.617, 813.337, 812.188, 807.642, 810.797, 809.196, 809.946, 807.738, 810.745, 806.105, 804.641, 804.848, 804.346, 803.347, 801.239, 802.507, 799.664], 'val_loss': [4446.079, 4435.002, 1156.574, 1121.125, 1077.881, 1047.996, 1042.773, 1041.196, 1023.539, 1021.211, 1013.784, 993.015, 975.622, 952.582, 939.363, 920.521, 923.606, 919.101, 925.099, 912.493, 919.044, 910.477, 923.828, 907.039, 923.279, 910.643, 915.879, 905.37, 897.416, 903.203, 914.06, 915.808, 908.979, 913.359, 905.749, 913.235, 918.606, 904.746, 912.103, 900.6, 913.125, 912.114, 910.32, 917.307, 908.958, 909.532, 913.041, 913.427, 930.458, 914.521, 942.166, 927.103, 925.421, 934.372, 925.434, 952.021, 924.865, 926.149, 927.772, 924.467, 930.882, 928.849, 923.136, 935.845, 916.886, 931.167, 914.015, 921.5, 925.433, 931.732, 928.549, 916.111, 939.943, 945.207, 926.319, 925.272, 918.672, 927.271, 942.867, 919.611, 929.38, 1051.789, 931.022, 932.625, 932.629, 932.462, 931.884, 927.875, 913.933, 922.581, 963.84, 918.108, 929.534, 950.767, 927.229, 934.598, 917.14, 917.236, 936.334, 918.914]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	878.76422		750874	17	-1	383.5370433330536	{'train_loss': [1757.905, 1073.107, 1004.272, 973.612, 953.908, 943.633, 935.989, 930.561, 923.871, 917.875, 912.306, 909.169, 906.695, 903.645, 899.596, 895.095, 892.779, 889.161, 887.693, 885.851, 881.807, 880.382, 880.183, 876.48, 876.652, 872.694, 870.779, 869.604, 868.168, 867.339, 864.537, 864.823, 861.004, 860.638, 856.624, 855.522, 854.982, 853.746, 852.36, 851.494, 849.095, 848.455, 847.308, 845.703, 844.262, 844.118, 844.5, 842.369, 838.667, 839.756, 839.132, 841.364, 834.851, 835.676, 835.976, 834.414, 833.59, 827.923, 828.427, 831.688, 829.242, 826.523, 826.834, 827.054, 824.328, 822.0, 821.854, 822.705, 822.766, 823.066, 819.718, 820.382, 820.123, 820.235, 817.909, 818.138, 819.135, 816.552, 819.417, 819.674, 816.737, 814.853, 817.673, 816.058, 813.852, 813.793, 816.285, 817.079, 814.681, 811.572, 817.685, 811.128, 816.848, 811.341, 811.856, 814.031, 812.732, 813.101, 814.603, 809.274], 'val_loss': [1152.283, 1003.285, 960.817, 926.034, 921.444, 914.572, 910.438, 917.21, 913.522, 913.881, 905.831, 914.113, 906.398, 907.707, 909.774, 920.262, 922.66, 916.264, 920.376, 923.665, 916.987, 914.686, 911.819, 921.046, 916.882, 919.709, 911.147, 917.212, 912.702, 906.213, 915.397, 919.912, 917.411, 909.998, 918.059, 915.688, 923.216, 922.056, 926.691, 923.048, 918.785, 918.943, 922.409, 925.148, 916.162, 922.775, 924.778, 917.203, 923.276, 915.529, 924.888, 922.374, 917.668, 915.341, 917.117, 913.519, 918.29, 915.402, 919.224, 932.583, 916.231, 918.545, 919.832, 916.645, 925.238, 923.68, 930.981, 923.563, 925.735, 922.347, 924.776, 925.724, 927.958, 936.855, 931.375, 932.657, 930.907, 935.514, 927.954, 941.707, 925.034, 932.474, 928.662, 934.758, 931.741, 939.346, 928.27, 927.374, 932.503, 926.351, 927.651, 928.884, 924.844, 930.748, 930.29, 931.216, 934.667, 936.587, 943.797, 936.999]}	100	100	True
