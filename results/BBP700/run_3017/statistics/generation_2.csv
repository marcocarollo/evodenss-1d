id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:3.5253704020870216e-05 batch_size:15 epochs:100	100	1000	True	983.3222		500851	14	-1	356.6513509750366	{'train_loss': [1579.101, 1282.403, 1232.624, 1218.029, 1207.854, 1155.507, 1117.572, 1098.58, 1084.275, 1076.145, 1065.537, 1059.631, 1053.938, 1050.525, 1045.374, 1041.564, 1038.806, 1034.729, 1031.832, 1027.793, 1025.652, 1021.949, 1022.529, 1018.075, 1015.209, 1013.164, 1012.872, 1011.252, 1007.4, 1006.156, 1005.708, 1001.234, 1003.644, 999.757, 995.562, 995.39, 995.148, 992.854, 992.89, 991.297, 989.399, 989.512, 986.58, 984.322, 985.173, 981.865, 984.21, 979.943, 978.278, 979.452, 980.022, 979.21, 977.691, 977.457, 976.775, 976.242, 976.588, 975.358, 974.417, 973.795, 971.702, 969.978, 968.876, 970.444, 971.888, 969.446, 969.618, 969.603, 968.656, 965.423, 964.392, 965.854, 965.52, 965.945, 963.887, 963.966, 964.204, 962.783, 962.44, 961.251, 961.44, 960.396, 957.584, 957.038, 957.462, 958.821, 956.282, 957.078, 954.308, 952.548, 954.312, 952.59, 954.088, 951.613, 953.0, 953.773, 952.82, 951.33, 947.332, 950.116], 'val_loss': [1420.848, 1221.106, 1207.92, 1196.097, 1165.579, 1108.467, 1088.637, 1072.858, 1057.273, 1066.004, 1051.617, 1059.929, 1057.795, 1052.9, 1049.628, 1043.532, 1047.588, 1045.349, 1042.042, 1047.516, 1042.066, 1041.042, 1035.093, 1044.828, 1042.508, 1031.386, 1034.882, 1034.262, 1038.8, 1043.261, 1027.598, 1042.155, 1037.99, 1032.128, 1030.562, 1037.147, 1046.157, 1040.944, 1030.854, 1029.134, 1049.28, 1029.26, 1033.773, 1044.552, 1032.133, 1032.47, 1026.566, 1028.209, 1038.939, 1035.995, 1031.013, 1036.6, 1031.081, 1034.227, 1033.7, 1033.653, 1037.557, 1037.173, 1028.253, 1036.321, 1034.215, 1032.255, 1033.174, 1030.637, 1041.685, 1043.479, 1043.064, 1028.053, 1041.305, 1035.792, 1036.458, 1036.233, 1037.976, 1036.704, 1047.175, 1047.97, 1038.346, 1035.022, 1033.211, 1032.025, 1038.293, 1037.76, 1042.582, 1047.59, 1046.301, 1041.833, 1035.884, 1047.083, 1034.106, 1033.604, 1042.768, 1045.803, 1047.78, 1035.006, 1048.446, 1044.862, 1036.885, 1042.657, 1045.153, 1039.848]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:15 epochs:100	100	1000	True	981.68439		501251	14	-1	349.2526876926422	{'train_loss': [1586.878, 1279.157, 1229.815, 1215.74, 1205.564, 1162.127, 1117.919, 1096.832, 1080.685, 1067.514, 1061.243, 1054.862, 1048.434, 1045.008, 1040.009, 1035.315, 1033.208, 1028.053, 1025.667, 1024.304, 1020.328, 1018.347, 1018.979, 1014.818, 1013.681, 1010.385, 1010.746, 1007.63, 1003.896, 1002.79, 1003.774, 999.89, 1000.89, 999.852, 995.226, 995.48, 994.602, 993.19, 992.117, 990.975, 990.953, 988.011, 987.737, 985.461, 986.017, 985.32, 981.069, 983.338, 981.508, 980.345, 981.838, 978.183, 979.253, 976.643, 977.112, 976.182, 974.374, 975.318, 973.303, 971.879, 975.184, 972.475, 968.585, 969.679, 971.187, 966.831, 967.306, 965.843, 965.837, 965.565, 963.108, 963.744, 964.095, 963.823, 962.968, 960.916, 962.442, 958.52, 958.013, 958.237, 959.728, 955.841, 957.727, 955.275, 953.056, 956.112, 950.963, 951.923, 951.919, 952.597, 952.733, 953.196, 950.63, 951.35, 948.53, 952.956, 948.513, 949.703, 948.052, 949.775], 'val_loss': [1414.864, 1225.562, 1210.422, 1205.907, 1173.392, 1120.366, 1082.628, 1064.681, 1073.392, 1049.55, 1073.924, 1052.354, 1064.07, 1057.965, 1052.521, 1054.385, 1052.403, 1046.433, 1053.856, 1057.176, 1057.354, 1068.034, 1056.478, 1052.89, 1044.302, 1046.364, 1056.453, 1050.221, 1045.172, 1053.071, 1047.158, 1042.494, 1053.239, 1042.283, 1050.628, 1042.458, 1051.16, 1041.355, 1048.146, 1041.286, 1042.869, 1053.745, 1047.932, 1044.259, 1037.916, 1044.593, 1047.19, 1041.597, 1041.782, 1036.287, 1032.753, 1042.437, 1035.087, 1038.627, 1032.191, 1040.547, 1036.536, 1037.324, 1035.735, 1042.686, 1035.002, 1032.394, 1040.936, 1034.953, 1033.81, 1037.217, 1032.539, 1033.883, 1032.843, 1033.177, 1032.605, 1032.105, 1039.283, 1037.94, 1035.173, 1033.453, 1040.359, 1036.486, 1037.549, 1032.59, 1033.341, 1038.177, 1032.685, 1037.713, 1034.863, 1034.779, 1031.782, 1031.47, 1041.692, 1036.987, 1033.997, 1039.429, 1034.146, 1027.741, 1031.912, 1029.213, 1028.717, 1030.848, 1030.64, 1028.066]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:33 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:127 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:94 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:111 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:100 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.004345102422989143 beta1:0.9045811241823547 beta2:0.859084075946502 weight_decay:3.5253704020870216e-05 batch_size:15 epochs:100	100	1000	True	1470.62292		8856255	15	-1	393.88860058784485	{'train_loss': [6381.928, 5462.017, 5470.404, 5399.76, 5344.746, 5293.667, 4805.207, 3339.027, 2010.188, 1625.939, 1457.009, 1402.102, 1393.104, 1390.575, 1389.664, 1384.365, 1379.396, 1374.829, 1372.069, 1369.701, 1367.812, 1366.17, 1365.575, 1367.514, 1363.73, 1358.238, 1358.904, 1355.714, 1357.368, 1342.69, 1308.119, 1274.061, 1248.788, 1240.477, 1226.305, 1220.553, 1204.553, 1194.941, 1189.661, 1183.622, 1180.95, 1175.23, 1168.615, 1161.382, 1157.583, 1154.689, 1148.455, 1148.654, 1144.619, 1138.895, 1138.974, 1130.427, 1128.621, 1125.303, 1122.665, 1121.342, 1116.371, 1116.906, 1114.915, 1107.281, 1107.314, 1104.384, 1103.634, 1097.263, 1101.654, 1100.657, 1102.991, 1096.685, 1095.857, 1090.532, 1091.453, 1095.247, 1092.406, 1083.521, 1082.131, 1077.302, 1082.019, 1079.678, 1082.156, 1083.944, 1081.668, 1086.401, 1079.27, 1081.34, 1081.183, 1079.654, 1085.433, 1079.703, 1076.44, 1070.916, 1069.423, 1071.426, 1068.973, 1066.809, 1070.351, 1072.628, 1069.675, 1070.709, 1073.016, 1073.417], 'val_loss': [13948.479, 5149.502, 5149.502, 5149.502, 5149.502, 5149.502, 4539.392, 2506.762, 1919.794, 1528.755, 1415.224, 1408.128, 1435.567, 1416.812, 1445.113, 1441.921, 1427.654, 1411.491, 1422.4, 1439.636, 1382.933, 1383.919, 1381.766, 1447.975, 1392.239, 1353.006, 1379.519, 1387.314, 1409.967, 1394.719, 1297.97, 1292.92, 1333.029, 1295.349, 1280.707, 1258.96, 1241.937, 1249.98, 1219.232, 1263.232, 1209.174, 1187.176, 1181.308, 1275.615, 1227.685, 1200.129, 1195.358, 1205.963, 1223.767, 1202.804, 1230.137, 1190.252, 1229.538, 1254.811, 1230.907, 1207.848, 1250.964, 1236.105, 1220.373, 1189.579, 1183.052, 1238.11, 1195.697, 1249.592, 1270.061, 1233.432, 1257.993, 1251.921, 1226.62, 1229.161, 1216.821, 1239.259, 1212.879, 1202.695, 1247.075, 1261.313, 1242.934, 1254.145, 1269.965, 1270.675, 1229.374, 1251.808, 1263.324, 1270.297, 1277.293, 1288.023, 1295.26, 1255.203, 1323.106, 1229.75, 1263.849, 1242.719, 1247.124, 1274.844, 1286.005, 1264.869, 1281.866, 1306.458, 1272.898, 1277.425]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:16 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.003964307183853675 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:3.5253704020870216e-05 batch_size:15 epochs:100	100	1000	True	984.0813		395323	15	-1	377.0980575084686	{'train_loss': [1487.58, 1291.828, 1234.477, 1217.308, 1208.003, 1202.939, 1195.158, 1163.978, 1130.321, 1106.424, 1091.35, 1080.256, 1074.073, 1066.454, 1062.467, 1057.713, 1054.704, 1048.523, 1046.35, 1042.285, 1038.378, 1037.996, 1031.827, 1028.785, 1025.953, 1023.845, 1022.716, 1021.089, 1017.976, 1016.751, 1014.484, 1011.534, 1012.494, 1009.915, 1006.839, 1005.767, 1005.406, 1003.601, 1002.175, 1001.694, 1000.7, 1000.038, 997.136, 996.581, 997.97, 994.795, 994.35, 992.07, 991.177, 993.269, 990.436, 991.291, 988.217, 988.862, 988.353, 986.234, 985.652, 985.318, 983.617, 984.053, 982.873, 980.668, 983.094, 979.296, 980.906, 978.916, 978.395, 978.815, 977.658, 977.671, 976.211, 973.588, 976.393, 975.133, 976.561, 974.179, 973.027, 972.829, 974.039, 970.088, 971.949, 972.24, 972.421, 968.088, 970.196, 967.529, 968.216, 969.451, 967.942, 966.156, 965.852, 966.403, 964.196, 965.05, 963.629, 964.949, 964.116, 963.534, 963.711, 964.812], 'val_loss': [1355.908, 1234.0, 1215.008, 1203.929, 1201.331, 1183.24, 1174.632, 1132.583, 1104.31, 1084.722, 1073.726, 1086.302, 1066.802, 1062.355, 1058.133, 1055.486, 1053.902, 1053.354, 1042.894, 1046.225, 1045.09, 1043.214, 1042.423, 1034.188, 1037.801, 1035.049, 1042.136, 1038.629, 1041.925, 1041.161, 1033.546, 1044.348, 1033.085, 1033.338, 1030.549, 1033.163, 1035.026, 1033.191, 1041.42, 1033.486, 1039.257, 1024.349, 1029.141, 1031.887, 1027.247, 1031.372, 1035.783, 1025.51, 1028.798, 1026.612, 1030.475, 1027.52, 1024.284, 1027.063, 1029.386, 1027.866, 1022.466, 1018.49, 1025.56, 1028.004, 1028.928, 1023.369, 1024.985, 1025.329, 1021.035, 1022.723, 1021.372, 1019.057, 1029.435, 1020.839, 1018.24, 1021.927, 1015.041, 1016.677, 1027.31, 1026.854, 1023.996, 1021.437, 1023.808, 1032.265, 1028.309, 1035.175, 1023.557, 1024.853, 1030.264, 1031.02, 1023.332, 1026.301, 1021.024, 1025.941, 1032.355, 1022.764, 1034.025, 1030.147, 1029.575, 1029.866, 1024.011, 1043.661, 1032.265, 1033.016]}	100	100	True
