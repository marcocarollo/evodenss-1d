id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.009632013248587246 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:3.5253704020870216e-05 batch_size:15 epochs:100	100	1000	True	991.97839		452251	14	-1	266.0628333091736	{'train_loss': [1474.174, 1269.828, 1237.95, 1227.222, 1218.923, 1212.444, 1188.689, 1145.674, 1122.204, 1101.685, 1093.591, 1084.098, 1074.547, 1069.976, 1065.515, 1063.727, 1059.249, 1056.627, 1053.416, 1050.902, 1049.219, 1044.066, 1042.135, 1042.244, 1039.705, 1037.172, 1034.12, 1033.552, 1033.143, 1031.761, 1028.202, 1026.696, 1025.873, 1025.6, 1024.091, 1019.414, 1020.423, 1019.198, 1018.43, 1017.193, 1019.152, 1016.904, 1015.587, 1013.389, 1013.18, 1014.601, 1011.793, 1013.295, 1012.019, 1011.802, 1011.675, 1010.546, 1008.212, 1013.344, 1007.121, 1006.458, 1005.779, 1007.583, 1007.015, 1005.945, 1004.711, 1008.482, 1006.461, 1007.037, 1001.962, 1004.525, 1000.538, 1004.313, 1002.596, 998.288, 998.642, 998.335, 996.68, 999.474, 1000.351, 999.416, 999.145, 999.656, 997.332, 998.286, 997.113, 993.914, 995.83, 997.372, 994.977, 994.296, 998.834, 992.651, 996.222, 993.778, 995.927, 989.999, 993.808, 990.002, 992.219, 991.77, 989.737, 991.334, 988.363, 991.883], 'val_loss': [1394.007, 1215.002, 1203.941, 1211.86, 1218.666, 1203.187, 1164.723, 1129.394, 1093.747, 1079.126, 1106.257, 1083.929, 1071.391, 1091.197, 1085.573, 1099.675, 1053.025, 1053.12, 1081.353, 1036.568, 1054.851, 1032.845, 1038.688, 1033.977, 1056.94, 1045.88, 1031.9, 1053.963, 1032.459, 1028.866, 1035.733, 1053.979, 1038.329, 1047.497, 1051.43, 1028.927, 1041.503, 1036.401, 1036.442, 1042.664, 1036.577, 1037.948, 1032.535, 1044.826, 1031.806, 1028.781, 1026.226, 1040.366, 1037.954, 1027.755, 1043.276, 1030.114, 1026.691, 1031.094, 1027.126, 1025.53, 1032.699, 1034.666, 1039.593, 1030.918, 1034.137, 1062.669, 1036.082, 1036.306, 1052.443, 1031.034, 1039.559, 1029.9, 1033.405, 1020.989, 1033.521, 1032.074, 1040.861, 1024.025, 1041.957, 1027.694, 1028.531, 1026.562, 1029.399, 1028.956, 1057.15, 1029.28, 1037.766, 1029.729, 1036.057, 1030.337, 1035.278, 1032.502, 1035.126, 1045.579, 1039.507, 1024.674, 1037.773, 1048.038, 1031.062, 1029.105, 1031.333, 1041.924, 1036.864, 1031.511]}	100	100	True
