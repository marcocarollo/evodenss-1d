id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	3000	True	881.6441		548637	18	-1	384.90183544158936	{'train_loss': [1330.789, 1118.19, 1086.884, 1075.901, 1065.734, 1054.817, 1018.973, 986.822, 972.234, 963.727, 953.171, 945.172, 938.035, 932.371, 929.227, 926.772, 921.139, 920.678, 918.057, 914.586, 914.432, 910.993, 910.316, 910.958, 905.99, 903.086, 902.703, 900.226, 900.64, 897.69, 898.674, 899.462, 896.62, 893.898, 892.882, 893.53, 891.901, 889.647, 888.202, 888.867, 885.636, 886.562, 884.94, 884.747, 883.427, 881.174, 882.143, 885.004, 878.437, 880.135, 881.704, 880.476, 875.967, 879.072, 877.152, 877.089, 876.168, 877.896, 878.307, 876.801, 873.424, 873.977, 873.438, 871.853, 871.377, 872.822, 875.133, 871.046, 871.452, 866.937, 869.287, 869.027, 869.845, 868.204, 868.052, 867.929, 866.519, 868.262, 862.69, 863.033, 861.115, 862.881, 858.242, 862.564, 857.768, 861.073, 863.39, 861.882, 861.139, 860.421, 861.343, 858.746, 856.506, 858.038, 858.43, 856.064, 856.495, 859.475, 854.75, 853.012], 'val_loss': [1200.099, 1071.208, 1058.336, 1037.495, 1033.661, 1024.75, 967.305, 964.118, 968.233, 963.848, 941.693, 912.589, 914.35, 912.418, 906.642, 923.105, 906.539, 909.413, 910.213, 901.308, 899.896, 909.86, 898.299, 903.1, 891.753, 900.819, 893.77, 893.825, 899.074, 896.438, 892.768, 895.044, 893.067, 902.635, 905.889, 904.657, 901.243, 918.896, 901.024, 904.485, 912.93, 903.471, 917.835, 908.998, 912.804, 923.153, 922.36, 915.274, 911.968, 910.385, 910.226, 909.75, 906.774, 909.56, 902.994, 903.767, 900.888, 905.747, 917.832, 903.388, 901.588, 906.553, 906.281, 899.231, 897.361, 900.322, 901.222, 918.283, 896.674, 907.367, 909.593, 912.622, 910.886, 909.385, 909.875, 908.517, 920.526, 912.812, 909.319, 905.436, 915.629, 900.44, 909.747, 924.431, 916.573, 907.886, 915.24, 912.466, 920.372, 910.146, 918.207, 906.838, 912.146, 914.937, 918.086, 918.634, 912.079, 923.11, 918.556, 914.146]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:105 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:deconv1d out_channels:11 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:7.426107854292606e-05 batch_size:13 epochs:100	100	1000	True	1060.4281		2237390	19	-1	402.55012464523315	{'train_loss': [4836.57, 5202.191, 4909.527, 4824.586, 4588.309, 1961.095, 1221.203, 1188.549, 1158.667, 1138.227, 1131.526, 1129.394, 1138.275, 1129.953, 1136.885, 1129.493, 1126.319, 1127.703, 1127.951, 1125.818, 1128.533, 1122.254, 1123.709, 1110.672, 1108.897, 1098.995, 1090.976, 1090.774, 1085.976, 1087.673, 1081.555, 1080.871, 1082.493, 1085.583, 1080.302, 1078.63, 1080.457, 1076.03, 1076.653, 1075.02, 1073.295, 1074.511, 1078.416, 1072.29, 1072.155, 1068.864, 1066.108, 1069.326, 1070.903, 1071.08, 1067.665, 1065.943, 1065.447, 1061.099, 1063.765, 1064.044, 1062.438, 1064.084, 1066.667, 1058.034, 1063.305, 1061.844, 1061.541, 1063.164, 1059.854, 1061.024, 1060.163, 1054.717, 1056.054, 1061.853, 1057.813, 1052.97, 1052.031, 1058.388, 1055.483, 1055.627, 1055.292, 1051.154, 1052.627, 1054.169, 1053.567, 1051.598, 1050.579, 1050.985, 1047.631, 1044.399, 1051.1, 1050.257, 1046.575, 1048.695, 1050.102, 1051.094, 1039.357, 1047.831, 1045.467, 1042.481, 1043.805, 1045.876, 1042.605, 1040.892], 'val_loss': [4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 1183.392, 1162.865, 1144.429, 1072.486, 1099.351, 1081.208, 1070.531, 1077.198, 1071.769, 1075.943, 1058.389, 1065.545, 1070.19, 1068.401, 1056.223, 1073.662, 1067.787, 1059.413, 1075.581, 1068.763, 1056.31, 1051.824, 1064.963, 1048.476, 1037.673, 1048.66, 1056.447, 1060.555, 1054.558, 1028.692, 1033.277, 1051.873, 1047.035, 1053.519, 1034.556, 1019.98, 1034.124, 1033.479, 1020.865, 1035.676, 1035.029, 1033.311, 1038.651, 1040.259, 1055.386, 1028.539, 1047.154, 1045.292, 1026.433, 1021.037, 1036.603, 1083.233, 1035.413, 1016.466, 1056.581, 1025.199, 1030.141, 1046.56, 1023.316, 1029.004, 1039.303, 1051.841, 1052.514, 1030.814, 1062.448, 1071.217, 1027.818, 1023.361, 1047.284, 1045.447, 1031.401, 1024.066, 1020.595, 1055.474, 1046.123, 1050.133, 1049.268, 1030.686, 1043.445, 1019.096, 1059.858, 1071.818, 1042.961, 1040.651, 1064.784, 1050.719, 1029.374, 1044.578, 1032.608, 1043.72, 1031.348, 1046.935, 1033.253, 1033.969, 1026.371]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:71 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:124 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:67 kernel_size:7 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.009097544753807963 batch_size:13 epochs:100	100	3000	True	991.83911		5652903	19	-1	558.7793440818787	{'train_loss': [1724.975, 1590.036, 1538.933, 1501.655, 1473.755, 1434.097, 1391.621, 1368.566, 1346.796, 1326.377, 1303.691, 1284.099, 1262.982, 1241.235, 1214.5, 1189.891, 1165.318, 1143.223, 1130.554, 1115.375, 1103.467, 1091.273, 1082.283, 1073.812, 1062.924, 1055.337, 1049.301, 1043.394, 1037.266, 1033.455, 1026.123, 1023.878, 1020.94, 1016.259, 1011.569, 1009.666, 1005.243, 1004.551, 999.1, 996.646, 997.211, 992.705, 986.908, 986.128, 984.344, 983.504, 977.526, 973.917, 975.595, 973.479, 969.214, 967.247, 967.412, 964.846, 962.336, 961.026, 958.053, 956.961, 955.435, 953.305, 950.736, 944.98, 948.576, 943.442, 944.135, 943.442, 941.008, 939.327, 936.635, 935.643, 935.317, 932.756, 931.572, 928.432, 931.591, 927.436, 925.688, 925.398, 921.154, 922.951, 922.657, 921.862, 921.233, 918.402, 918.988, 916.25, 914.589, 913.116, 914.492, 911.951, 911.392, 910.941, 908.768, 908.43, 908.196, 904.548, 907.244, 905.543, 903.993, 903.551], 'val_loss': [1573.879, 1421.734, 1380.88, 1313.695, 1310.265, 1312.348, 1239.425, 1246.828, 1216.056, 1192.495, 1159.211, 1146.606, 1167.71, 1098.288, 1092.769, 1064.431, 1077.587, 1049.018, 1047.9, 1052.568, 1038.594, 1022.524, 1028.969, 1017.78, 1008.276, 1008.461, 1004.701, 1008.186, 1008.69, 995.818, 992.124, 996.062, 984.64, 983.204, 981.39, 975.318, 977.668, 984.274, 963.372, 972.443, 967.856, 967.185, 964.983, 959.61, 955.292, 949.058, 955.532, 953.15, 944.645, 943.33, 939.263, 943.394, 937.931, 936.886, 937.595, 932.851, 933.759, 931.593, 933.82, 934.978, 932.999, 930.862, 931.054, 932.503, 921.427, 922.747, 922.958, 919.196, 922.44, 924.333, 920.06, 919.97, 924.684, 918.802, 921.078, 916.985, 920.046, 923.565, 920.541, 914.513, 917.307, 917.442, 919.581, 919.364, 912.894, 915.571, 915.681, 920.259, 919.173, 912.287, 912.898, 909.875, 915.796, 910.192, 914.029, 918.227, 909.345, 915.758, 913.1, 911.714]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	3000	True	849.26691		548637	18	-1	387.93105912208557	{'train_loss': [1349.362, 1106.048, 1073.131, 1064.542, 1053.943, 1050.616, 1046.418, 1044.539, 1040.233, 1039.64, 1040.448, 1032.571, 1032.359, 1025.329, 1027.458, 1027.6, 1027.909, 1017.717, 1014.959, 1015.045, 1008.442, 1010.09, 1007.451, 1005.333, 1005.963, 1003.401, 1000.988, 1000.266, 997.018, 995.415, 996.132, 995.508, 990.084, 991.502, 992.08, 986.971, 988.85, 987.604, 985.096, 983.07, 980.897, 971.014, 965.358, 957.087, 956.777, 948.633, 942.619, 939.818, 932.17, 931.475, 926.971, 924.604, 917.817, 918.522, 913.185, 912.928, 911.261, 908.131, 905.31, 906.603, 903.596, 902.534, 899.735, 899.263, 897.769, 893.296, 894.419, 890.382, 892.183, 890.255, 889.503, 890.088, 886.162, 885.121, 884.595, 883.742, 889.507, 882.767, 884.528, 882.332, 881.187, 882.409, 881.837, 878.707, 880.5, 877.185, 878.666, 877.637, 875.649, 874.53, 873.259, 873.413, 871.367, 872.55, 874.89, 869.347, 870.806, 869.306, 872.041, 869.604], 'val_loss': [1114.68, 1063.951, 1046.779, 1032.094, 1027.693, 1014.467, 1023.179, 1006.744, 1013.828, 1009.593, 1010.419, 1014.666, 1004.777, 1009.575, 1005.354, 1005.032, 995.413, 998.304, 1007.832, 993.053, 997.335, 995.272, 992.973, 991.565, 986.72, 990.899, 985.809, 989.29, 986.104, 990.299, 986.187, 980.068, 988.079, 985.945, 976.601, 980.197, 984.029, 976.004, 981.487, 977.167, 965.497, 960.353, 947.385, 936.485, 934.575, 926.927, 931.628, 908.604, 927.822, 909.938, 913.639, 911.829, 912.413, 901.061, 922.974, 907.5, 899.915, 898.685, 896.418, 897.933, 900.33, 903.775, 899.209, 898.209, 899.094, 899.87, 894.808, 897.049, 896.481, 898.714, 912.359, 907.203, 892.831, 905.981, 911.173, 895.363, 898.434, 905.452, 907.102, 904.135, 907.316, 905.32, 895.726, 905.429, 902.008, 894.223, 904.401, 897.326, 900.919, 899.392, 889.907, 901.818, 894.891, 903.736, 897.135, 902.367, 900.083, 894.947, 892.658, 893.441]}	0	100	True
