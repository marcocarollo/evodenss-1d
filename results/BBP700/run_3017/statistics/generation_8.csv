id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.009097544753807963 beta1:0.804697104124598 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	872.01202		455044	17	-1	349.18405270576477	{'train_loss': [1323.923, 1109.322, 1083.526, 1071.959, 1024.073, 992.989, 975.508, 961.735, 953.332, 945.377, 941.114, 938.953, 933.849, 928.572, 925.443, 922.193, 919.215, 919.648, 916.461, 915.043, 912.792, 909.946, 910.037, 905.644, 905.674, 907.718, 903.267, 904.321, 902.91, 900.913, 898.928, 898.024, 899.91, 897.917, 894.245, 893.127, 893.814, 894.349, 893.618, 892.776, 888.092, 890.883, 888.679, 885.227, 886.062, 889.021, 884.137, 885.598, 882.181, 883.194, 880.958, 879.114, 883.408, 879.051, 879.883, 879.069, 875.854, 875.724, 874.776, 874.477, 871.357, 873.107, 869.161, 874.738, 871.118, 869.748, 868.016, 870.292, 869.879, 872.136, 871.549, 867.98, 869.257, 870.436, 868.994, 865.836, 864.359, 867.599, 867.245, 865.188, 866.131, 863.088, 859.494, 863.293, 864.43, 863.952, 861.01, 860.361, 860.371, 863.211, 856.666, 855.545, 855.905, 858.162, 858.674, 856.537, 858.417, 855.406, 857.723, 859.139], 'val_loss': [1144.854, 1071.443, 1069.152, 1018.761, 985.122, 950.826, 948.851, 927.659, 916.038, 932.309, 922.973, 920.905, 911.137, 916.132, 909.637, 910.059, 906.865, 906.907, 915.047, 935.511, 915.65, 917.95, 926.755, 901.452, 907.137, 900.187, 926.27, 910.953, 895.712, 912.425, 928.513, 901.712, 909.403, 921.357, 897.633, 893.338, 917.717, 911.921, 914.042, 905.976, 903.248, 921.646, 905.874, 916.537, 921.41, 917.023, 917.781, 903.812, 908.359, 919.147, 918.998, 909.979, 907.016, 906.984, 903.491, 913.327, 900.811, 915.035, 899.844, 914.725, 905.952, 910.634, 918.797, 911.25, 911.891, 898.766, 898.21, 912.351, 904.847, 898.801, 901.237, 894.673, 894.956, 892.475, 897.991, 905.0, 959.023, 898.751, 893.062, 902.273, 900.009, 894.845, 908.249, 903.484, 897.202, 907.465, 894.534, 896.888, 899.464, 899.271, 897.562, 898.851, 904.447, 891.681, 903.597, 906.197, 903.042, 898.071, 898.875, 900.944]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:49 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:42 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.804697104124598 beta2:0.9353831093483147 weight_decay:0.0008833044831876668 batch_size:13 epochs:100	100	1000	True	935.71783		375377	18	-1	367.6488220691681	{'train_loss': [1202.152, 1114.818, 1076.796, 1067.667, 1060.648, 1055.44, 1051.185, 1047.772, 1047.708, 1040.348, 1037.734, 1033.833, 1025.973, 1026.903, 1023.747, 1017.441, 1016.34, 1017.611, 1014.149, 1015.085, 1008.459, 1007.707, 1005.492, 1005.701, 1002.036, 1001.947, 1001.116, 1002.4, 999.531, 998.957, 997.055, 995.879, 995.053, 994.983, 994.659, 993.099, 993.548, 992.683, 993.94, 993.852, 992.451, 992.197, 988.978, 991.702, 990.011, 989.593, 988.621, 986.524, 986.393, 984.255, 988.384, 984.432, 986.006, 986.012, 983.409, 986.056, 985.58, 984.995, 983.591, 984.951, 981.271, 981.937, 981.895, 982.037, 984.23, 983.963, 981.685, 983.126, 980.682, 979.912, 981.051, 979.804, 975.551, 979.619, 978.304, 979.291, 979.858, 975.209, 976.789, 978.268, 978.195, 976.366, 974.81, 977.482, 976.468, 975.132, 975.238, 974.982, 976.829, 975.659, 975.592, 973.71, 973.0, 972.686, 969.157, 972.039, 974.27, 972.816, 971.452, 971.836], 'val_loss': [1145.555, 1063.193, 1047.882, 1042.052, 1042.403, 1049.907, 1027.957, 1027.354, 1024.028, 1009.989, 1005.855, 1003.402, 1001.412, 1021.781, 1013.529, 1044.353, 999.153, 1005.859, 982.914, 1005.743, 993.114, 990.193, 990.371, 977.076, 984.027, 1020.088, 1008.218, 1010.29, 1013.782, 1004.167, 1033.863, 1000.143, 974.735, 986.536, 983.485, 978.134, 987.361, 977.828, 983.35, 1015.063, 1001.782, 973.104, 979.494, 994.06, 972.718, 986.363, 974.8, 968.962, 970.679, 979.528, 1006.583, 986.021, 1013.737, 1013.893, 986.185, 1024.358, 994.55, 973.389, 981.411, 969.544, 976.259, 985.754, 987.358, 985.64, 991.336, 977.558, 1003.098, 971.847, 990.936, 1021.906, 973.65, 970.004, 970.744, 998.791, 978.921, 968.096, 989.918, 982.108, 979.618, 986.16, 965.216, 1020.137, 979.735, 972.393, 975.407, 978.338, 976.422, 966.292, 959.775, 978.799, 974.933, 964.636, 990.258, 984.287, 1009.029, 1032.954, 972.472, 971.655, 975.329, 966.934]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:51 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:deconv1d out_channels:125 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.9223942116749735 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	6404.37939		80530859	18	-1	702.5989351272583	{'train_loss': [12084.861, 7101.149, 4682.904, 7262.813, 5693.816, 4599.542, 4854.898, 4740.425, 4586.178, 4861.16, 4585.699, 4706.435, 4597.634, 4605.469, 4608.306, 4608.902, 4601.14, 4615.168, 4599.859, 4591.739, 4607.999, 4590.995, 4601.517, 4605.774, 4606.884, 4599.747, 4607.793, 4601.057, 4609.071, 4596.276, 4605.571, 4599.186, 4599.47, 4602.799, 4597.962, 4601.638, 4606.475, 4598.524, 4596.784, 4602.207, 4607.022, 4604.742, 4601.991, 4620.34, 4593.837, 4611.987, 4593.667, 4601.896, 4593.858, 4605.117, 4595.225, 4605.515, 4598.84, 4614.096, 4596.327, 4606.042, 4600.723, 4614.39, 4599.25, 4606.305, 4600.049, 4604.567, 4594.668, 4603.35, 4595.334, 4604.042, 4601.731, 4607.388, 4611.529, 4609.292, 4601.466, 4611.585, 4603.594, 4602.049, 4610.912, 4610.157, 4608.021, 4609.012, 4615.499, 4606.38, 4606.606, 4613.821, 4607.01, 4602.952, 4603.514, 4602.572, 4603.728, 4601.234, 4609.862, 4603.162, 4604.509, 4600.479, 4608.092, 4597.545, 4610.865, 4605.808, 4616.352, 4614.442, 4608.536, 4604.261], 'val_loss': [4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 5392.188, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.623, 4462.902, 4462.902, 4462.902, 4468.602, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4664.927, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4877.143, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.859, 4462.902, 4462.902, 4822.507, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.901, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4606.307, 4462.902, 4462.902, 4462.902, 4436.624, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.814, 4462.902]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:8 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:23 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:95 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:deconv1d out_channels:62 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.009097544753807963 beta1:0.804697104124598 beta2:0.9353831093483147 weight_decay:9.476430987323203e-06 batch_size:13 epochs:100	100	1000	True	4945.98486		20438852	17	-1	397.2858040332794	{'train_loss': [4623.961, 5456.992, 4854.429, 5847.042, 4748.667, 4639.893, 4624.056, 4590.757, 4601.233, 4594.938, 4597.064, 4595.762, 4594.688, 4595.534, 4590.383, 4589.731, 4595.402, 4592.362, 4591.81, 4592.129, 4593.649, 4590.517, 4593.532, 4590.785, 4590.545, 4591.332, 4594.833, 4596.089, 4598.184, 4589.701, 4591.942, 4597.73, 4595.478, 4596.76, 4594.897, 4595.417, 4597.159, 4594.522, 4593.007, 4596.881, 4595.426, 4594.931, 4594.759, 4594.264, 4597.172, 4592.637, 4595.44, 4598.719, 4594.296, 4597.375, 4594.825, 4595.693, 4597.272, 4594.266, 4595.725, 4594.458, 4595.837, 4596.779, 4594.443, 4598.177, 4596.441, 4596.658, 4595.701, 4601.085, 4595.666, 4599.644, 4597.731, 4597.411, 4597.524, 4596.696, 4598.28, 4595.858, 4599.078, 4594.243, 4599.618, 4604.527, 4595.232, 4601.812, 4595.033, 4599.815, 4599.079, 4597.456, 4599.076, 4595.724, 4596.998, 4598.261, 4593.812, 4597.791, 4599.249, 4591.743, 4595.476, 4594.459, 4597.857, 4596.349, 4594.778, 4596.576, 4598.672, 4600.264, 4594.488, 4592.899], 'val_loss': [4462.902, 4462.902, 9600.572, 4462.902, 4462.902, 4462.902, 4462.902, 4457.096, 4462.902, 4478.275, 4462.902, 4462.771, 4462.902, 4462.902, 4462.902, 4462.902, 4462.901, 4462.874, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4461.977, 4462.902, 4462.901, 4688.706, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.901, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4535.122, 4462.901, 4462.902, 4462.902, 4462.902, 4599.348, 4575.183, 4462.902, 4462.902, 4462.895, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4463.276, 4462.902, 4462.902, 4462.902, 4460.591, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4479.51, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.751, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4521.078, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902]}	100	100	True
