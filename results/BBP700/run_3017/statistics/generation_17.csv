id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:65 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.0037291634219010156 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	861.28625		564971	19	-1	412.0366609096527	{'train_loss': [1336.673, 1114.581, 1072.552, 1059.986, 1053.721, 1047.49, 1042.904, 1027.819, 998.012, 976.352, 961.944, 950.93, 944.605, 936.71, 933.203, 928.267, 924.025, 917.037, 915.535, 911.722, 910.165, 909.316, 907.584, 908.112, 900.602, 899.677, 899.388, 897.867, 895.735, 893.825, 892.147, 889.0, 887.864, 885.426, 883.13, 885.569, 881.84, 882.559, 879.686, 879.35, 879.052, 877.616, 876.908, 876.361, 873.382, 871.329, 872.104, 873.18, 869.573, 868.494, 866.865, 866.645, 867.342, 867.159, 864.29, 864.531, 863.602, 863.755, 861.556, 860.011, 860.663, 860.379, 856.991, 857.844, 857.996, 855.842, 854.787, 854.773, 853.877, 853.242, 854.2, 852.175, 852.445, 854.792, 851.025, 852.789, 849.598, 848.88, 848.534, 850.209, 845.663, 845.71, 845.855, 845.136, 846.278, 845.311, 844.819, 842.465, 843.912, 843.456, 843.661, 844.632, 843.333, 844.017, 839.052, 839.692, 839.032, 840.555, 839.854, 840.263], 'val_loss': [1175.619, 1076.735, 1049.512, 1038.791, 1027.894, 1020.433, 1021.037, 985.845, 971.205, 944.047, 944.832, 941.736, 928.922, 920.027, 916.672, 909.463, 910.177, 901.519, 905.285, 907.066, 901.466, 904.737, 905.924, 902.93, 902.686, 900.137, 897.729, 898.243, 903.078, 901.855, 895.539, 888.986, 893.488, 898.881, 895.895, 888.874, 885.525, 890.907, 887.614, 889.652, 885.862, 889.052, 891.284, 886.611, 886.818, 889.564, 890.274, 886.414, 886.446, 890.308, 883.691, 885.069, 893.676, 888.777, 886.697, 885.409, 891.697, 890.039, 888.382, 891.613, 893.277, 891.619, 889.496, 887.652, 886.466, 883.786, 887.86, 889.265, 883.173, 884.209, 885.795, 888.375, 888.705, 883.442, 885.291, 886.597, 885.431, 889.124, 888.707, 889.079, 889.635, 891.32, 890.07, 885.289, 891.0, 890.781, 891.832, 893.346, 888.143, 890.609, 886.674, 886.857, 888.225, 888.825, 882.206, 888.659, 884.409, 893.089, 889.631, 889.088]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:11 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:36 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:67 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:16 layer:conv1d out_channels:65 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.0037291634219010156 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:4 epochs:100	81	1000	True	281.72223		764568	21	-1	1011.6053628921509	{'train_loss': [419.868, 315.951, 305.258, 300.119, 296.21, 293.718, 291.66, 290.394, 289.121, 288.096, 287.39, 286.937, 286.764, 285.005, 285.03, 283.298, 283.243, 283.046, 282.454, 282.069, 282.21, 281.22, 280.759, 280.176, 279.831, 279.951, 279.264, 278.155, 277.684, 277.289, 276.842, 276.717, 276.238, 276.639, 275.717, 274.914, 274.705, 274.043, 274.235, 273.22, 273.79, 273.085, 272.44, 272.233, 272.25, 271.639, 270.915, 270.412, 270.781, 269.757, 270.234, 269.52, 269.243, 268.34, 267.693, 268.433, 266.695, 267.424, 265.916, 266.449, 265.989, 266.769, 266.019, 265.211, 264.928, 265.024, 264.091, 264.376, 263.746, 263.887, 263.452, 263.191, 262.723, 262.678, 261.958, 262.164, 262.233, 261.605, 260.792, 260.853, 261.176], 'val_loss': [327.926, 303.133, 295.954, 292.739, 289.388, 295.213, 292.265, 297.232, 291.632, 286.048, 292.893, 289.512, 289.964, 285.42, 292.279, 284.542, 286.414, 290.049, 289.533, 288.321, 286.88, 282.236, 283.304, 288.035, 284.699, 284.121, 287.621, 285.906, 284.677, 286.159, 282.12, 282.844, 285.805, 290.145, 295.96, 287.4, 290.551, 286.244, 286.36, 285.108, 287.206, 286.779, 283.148, 283.678, 281.471, 284.403, 286.14, 286.566, 283.177, 294.099, 284.025, 285.905, 294.609, 283.187, 287.888, 282.535, 286.059, 283.478, 284.643, 288.164, 285.437, 287.0, 286.906, 283.207, 285.584, 280.861, 283.927, 283.119, 285.401, 285.212, 280.652, 283.366, 285.764, 286.719, 283.378, 280.796, 282.527, 281.151, 279.824, 284.313, 283.568]}	81	81	False
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:56 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:93 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:4 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:deconv1d out_channels:13 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adam lr:0.0037291634219010156 beta1:0.8418140128746955 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	899.90076		811568	20	-1	719.559472322464	{'train_loss': [1702.916, 1146.438, 1045.13, 997.508, 969.842, 951.211, 942.781, 936.714, 930.75, 927.133, 920.732, 919.522, 914.46, 911.122, 908.674, 906.355, 903.727, 898.824, 898.299, 897.621, 894.849, 891.11, 891.089, 888.532, 887.757, 887.673, 886.196, 886.497, 883.958, 882.509, 881.127, 881.352, 877.035, 875.787, 872.498, 874.194, 872.487, 872.836, 869.13, 869.909, 867.391, 864.601, 867.226, 865.255, 863.54, 861.557, 866.077, 862.864, 861.821, 861.778, 861.276, 859.277, 858.946, 861.084, 861.738, 860.069, 856.599, 859.392, 857.983, 856.123, 854.952, 850.483, 851.479, 853.753, 855.153, 853.33, 854.101, 853.592, 851.867, 852.472, 850.801, 850.315, 852.446, 845.315, 846.96, 848.785, 847.137, 846.23, 843.264, 844.06, 843.895, 844.186, 842.797, 840.996, 837.596, 839.076, 838.691, 836.592, 840.324, 837.008, 834.358, 836.142, 835.552, 832.709, 835.533, 836.289, 833.53, 832.017, 831.785, 832.063], 'val_loss': [1187.62, 1059.685, 985.069, 948.455, 930.606, 922.159, 927.628, 915.966, 921.396, 913.156, 911.675, 909.837, 919.362, 910.379, 914.389, 908.198, 910.435, 901.716, 907.05, 905.551, 911.365, 905.809, 910.646, 918.345, 913.522, 911.107, 911.031, 911.52, 918.464, 916.379, 914.356, 923.251, 910.492, 906.857, 907.523, 914.433, 914.314, 910.234, 908.181, 898.298, 908.337, 897.602, 897.151, 901.406, 899.615, 909.099, 906.293, 897.804, 904.929, 907.534, 900.444, 901.395, 905.489, 911.935, 895.59, 900.307, 899.38, 900.32, 900.618, 899.547, 897.599, 910.764, 898.235, 906.843, 906.282, 902.37, 894.502, 902.285, 905.383, 906.253, 924.025, 902.678, 902.878, 901.929, 910.013, 899.486, 900.431, 900.38, 898.734, 905.009, 900.656, 904.031, 899.723, 900.13, 898.29, 909.039, 904.063, 914.13, 915.823, 903.807, 912.407, 904.251, 907.199, 913.147, 911.348, 903.714, 908.901, 910.725, 910.464, 913.175]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:103 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:deconv1d out_channels:52 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:deconv1d out_channels:38 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:65 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:16 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adadelta lr:0.0037291634219010156 batch_size:13 epochs:100	100	1000	True	887.27924		1204991	20	-1	516.6523423194885	{'train_loss': [1606.053, 1231.342, 1199.045, 1189.866, 1185.385, 1176.612, 1140.142, 1107.169, 1083.366, 1065.524, 1057.165, 1047.39, 1042.3, 1035.879, 1030.845, 1027.369, 1024.854, 1022.127, 1017.43, 1013.923, 1012.263, 1009.848, 1007.956, 1006.666, 1004.978, 1000.796, 1000.203, 998.667, 996.888, 995.054, 992.816, 991.841, 989.289, 987.113, 986.037, 983.852, 982.36, 979.895, 979.924, 978.669, 979.436, 976.888, 977.112, 973.425, 971.58, 972.5, 970.284, 967.431, 968.613, 965.765, 965.021, 964.199, 963.036, 961.877, 959.134, 961.167, 959.062, 955.863, 956.112, 956.7, 954.372, 953.588, 953.844, 952.31, 950.894, 950.394, 950.088, 947.316, 948.45, 946.539, 946.365, 945.592, 944.804, 942.74, 941.933, 941.55, 939.78, 941.071, 941.616, 939.295, 941.045, 938.348, 939.44, 934.899, 935.1, 934.766, 935.074, 934.564, 933.578, 932.001, 933.949, 932.08, 931.026, 931.299, 929.644, 930.034, 928.63, 928.815, 925.836, 927.414], 'val_loss': [1421.598, 1221.48, 1183.648, 1169.666, 1161.457, 1140.868, 1110.981, 1080.344, 1056.339, 1037.713, 1029.695, 1020.422, 1013.18, 1008.925, 1002.293, 998.007, 992.336, 990.701, 986.303, 986.42, 985.122, 980.517, 978.821, 977.064, 974.384, 973.388, 970.046, 968.216, 970.673, 968.672, 967.373, 964.983, 961.688, 962.789, 962.734, 962.176, 959.518, 957.475, 957.318, 957.008, 955.639, 956.523, 955.202, 954.3, 952.563, 950.627, 951.062, 951.29, 946.896, 948.685, 950.005, 948.734, 947.476, 946.062, 946.218, 944.895, 943.311, 943.779, 942.621, 940.682, 939.355, 940.179, 940.626, 940.497, 935.657, 936.757, 934.833, 934.512, 933.525, 932.748, 932.188, 930.849, 928.139, 930.313, 930.656, 928.589, 928.208, 925.162, 928.706, 925.194, 925.65, 925.828, 921.309, 921.73, 920.8, 918.035, 921.683, 919.736, 917.502, 915.472, 917.351, 913.945, 916.846, 915.944, 914.155, 915.371, 914.555, 913.631, 914.372, 914.026]}	100	100	True
