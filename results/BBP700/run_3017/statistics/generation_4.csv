id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:15 epochs:100	100	2000	True	976.18646		501251	14	-1	324.0149917602539	{'train_loss': [1550.931, 1290.981, 1182.624, 1136.187, 1108.037, 1091.14, 1080.161, 1068.971, 1062.472, 1057.518, 1052.686, 1047.553, 1045.32, 1040.966, 1037.918, 1036.342, 1032.0, 1030.858, 1028.313, 1025.216, 1024.889, 1020.752, 1020.567, 1017.87, 1013.853, 1014.055, 1008.581, 1009.899, 1008.78, 1007.384, 1006.239, 1005.229, 1001.789, 1005.135, 999.7, 1000.108, 997.532, 997.987, 995.185, 995.051, 994.101, 993.607, 991.406, 989.409, 986.764, 986.247, 986.444, 987.07, 983.92, 982.046, 982.393, 981.72, 980.957, 979.281, 979.416, 976.452, 977.536, 974.995, 975.018, 973.0, 971.893, 972.896, 969.194, 970.696, 966.449, 964.828, 967.681, 965.501, 968.975, 962.845, 965.223, 966.994, 963.129, 962.068, 960.359, 960.422, 963.398, 960.696, 958.298, 957.759, 958.358, 958.581, 956.25, 958.462, 957.514, 953.19, 956.012, 955.244, 953.312, 953.366, 951.881, 953.367, 952.139, 953.574, 952.12, 950.494, 951.612, 948.554, 952.762, 949.674], 'val_loss': [1348.389, 1198.701, 1143.046, 1092.738, 1081.426, 1063.907, 1067.128, 1055.697, 1056.893, 1052.976, 1044.301, 1071.938, 1055.211, 1059.633, 1042.813, 1045.913, 1042.956, 1056.826, 1043.409, 1042.478, 1044.953, 1047.253, 1037.065, 1027.821, 1050.712, 1034.42, 1032.728, 1034.586, 1042.852, 1030.439, 1029.581, 1031.602, 1028.635, 1026.859, 1029.667, 1030.813, 1031.671, 1027.305, 1025.682, 1032.645, 1030.508, 1026.366, 1033.518, 1031.786, 1030.962, 1027.168, 1037.437, 1030.901, 1035.318, 1031.38, 1034.751, 1033.641, 1031.386, 1035.936, 1033.206, 1037.481, 1041.086, 1036.856, 1035.576, 1035.306, 1040.287, 1037.488, 1043.118, 1032.845, 1036.143, 1046.763, 1042.951, 1031.164, 1033.612, 1038.199, 1038.205, 1032.803, 1041.368, 1031.042, 1042.239, 1037.485, 1037.002, 1032.845, 1028.147, 1035.978, 1037.142, 1031.959, 1030.617, 1057.898, 1031.628, 1037.549, 1027.707, 1028.038, 1038.342, 1031.062, 1041.566, 1039.224, 1038.846, 1033.959, 1033.869, 1029.632, 1036.721, 1031.857, 1031.526, 1032.001]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:88 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:94 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:98 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.0032175571575928947 beta1:0.804697104124598 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:15 epochs:100	100	1000	True	1330.24841		8058544	14	-1	333.46124386787415	{'train_loss': [6145.404, 5785.366, 5547.69, 5373.216, 5333.504, 5308.031, 4320.989, 1566.499, 1368.947, 1284.222, 1235.575, 1204.77, 1181.12, 1167.531, 1158.94, 1146.694, 1137.648, 1131.32, 1123.274, 1121.418, 1113.026, 1108.736, 1101.556, 1097.282, 1094.493, 1088.162, 1085.223, 1080.295, 1075.048, 1073.582, 1073.181, 1066.186, 1064.271, 1063.367, 1059.307, 1061.807, 1061.092, 1051.46, 1053.575, 1050.896, 1047.299, 1048.206, 1043.51, 1041.418, 1040.548, 1039.656, 1038.43, 1036.01, 1031.174, 1033.464, 1025.727, 1026.329, 1025.396, 1026.562, 1023.204, 1020.302, 1015.124, 1019.168, 1015.758, 1018.334, 1011.814, 1011.036, 1011.988, 1006.554, 1009.619, 1005.357, 1003.194, 999.304, 999.241, 997.946, 998.554, 990.392, 997.189, 994.223, 992.007, 986.519, 991.017, 989.552, 983.585, 985.56, 982.07, 982.825, 981.091, 982.323, 982.417, 977.156, 977.186, 977.181, 980.86, 979.449, 974.803, 979.466, 976.23, 973.841, 979.291, 979.596, 975.703, 973.451, 977.154, 975.199], 'val_loss': [5149.502, 5149.502, 5149.502, 5149.502, 5149.502, 5477.584, 2333.266, 1461.68, 1328.19, 1294.096, 1256.491, 1192.007, 1177.386, 1149.38, 1134.995, 1125.995, 1151.851, 1119.79, 1152.62, 1133.06, 1133.817, 1138.057, 1133.364, 1191.877, 1141.964, 1201.277, 1161.005, 1160.528, 1168.29, 1202.404, 1163.66, 1176.849, 1188.821, 1174.383, 1173.292, 1218.226, 1198.522, 1178.03, 1237.194, 1165.445, 1147.579, 1155.964, 1159.056, 1159.822, 1136.183, 1167.2, 1195.495, 1163.475, 1159.135, 1115.218, 1131.459, 1130.158, 1142.064, 1156.527, 1150.602, 1151.88, 1153.633, 1139.973, 1220.344, 1164.964, 1192.822, 1188.78, 1152.203, 1259.281, 1174.664, 1197.642, 1195.596, 1175.914, 1179.717, 1187.473, 1159.767, 1209.489, 1171.185, 1167.842, 1168.561, 1177.689, 1172.333, 1172.355, 1207.084, 1178.315, 1185.301, 1187.945, 1175.122, 1190.145, 1220.625, 1184.554, 1225.224, 1183.044, 1179.12, 1168.025, 1156.505, 1193.81, 1163.142, 1232.079, 1235.21, 1151.953, 1231.516, 1177.256, 1140.275, 1181.036]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.004345102422989143 batch_size:15 epochs:100	100	2000	True	1071.04028		501251	14	-1	323.56618428230286	{'train_loss': [1828.456, 1419.339, 1391.109, 1379.31, 1373.719, 1369.087, 1366.836, 1363.653, 1360.451, 1356.132, 1348.589, 1337.41, 1327.129, 1317.819, 1312.516, 1307.897, 1305.076, 1302.63, 1300.89, 1299.804, 1296.503, 1295.165, 1292.355, 1288.11, 1285.555, 1282.904, 1277.75, 1274.908, 1271.068, 1266.21, 1262.762, 1259.889, 1255.399, 1250.951, 1249.897, 1244.74, 1241.78, 1239.417, 1238.354, 1233.699, 1230.665, 1229.769, 1226.71, 1223.137, 1219.204, 1220.268, 1217.333, 1215.455, 1211.55, 1209.501, 1206.708, 1204.617, 1204.441, 1202.137, 1199.604, 1197.412, 1196.616, 1194.517, 1193.581, 1189.72, 1188.187, 1188.281, 1184.685, 1183.26, 1182.168, 1180.392, 1178.109, 1177.276, 1176.833, 1175.763, 1172.274, 1172.69, 1170.663, 1169.336, 1167.747, 1166.28, 1164.722, 1162.502, 1164.925, 1162.912, 1161.074, 1161.08, 1158.801, 1155.734, 1156.828, 1155.882, 1154.118, 1152.789, 1152.731, 1152.492, 1148.508, 1147.597, 1148.097, 1144.604, 1144.78, 1142.223, 1142.613, 1140.831, 1140.328, 1138.418], 'val_loss': [1479.655, 1383.464, 1357.819, 1345.403, 1338.645, 1334.13, 1332.536, 1329.089, 1326.689, 1321.578, 1314.78, 1307.218, 1302.034, 1298.302, 1296.28, 1294.75, 1293.647, 1291.763, 1290.69, 1288.553, 1286.897, 1284.324, 1281.799, 1279.12, 1275.589, 1271.579, 1268.218, 1262.245, 1257.679, 1250.922, 1245.812, 1241.035, 1234.669, 1228.047, 1224.451, 1220.266, 1213.899, 1211.041, 1208.239, 1204.516, 1200.751, 1198.933, 1195.92, 1193.031, 1190.891, 1188.339, 1187.032, 1184.612, 1181.999, 1180.287, 1179.059, 1178.005, 1175.161, 1172.741, 1170.763, 1169.269, 1168.106, 1167.133, 1164.17, 1163.738, 1160.461, 1160.979, 1158.505, 1156.296, 1155.138, 1154.06, 1153.722, 1151.266, 1150.171, 1149.895, 1147.018, 1146.755, 1144.648, 1144.45, 1142.809, 1142.572, 1139.222, 1138.7, 1137.137, 1135.774, 1133.398, 1133.452, 1133.762, 1133.035, 1131.041, 1129.41, 1127.024, 1128.491, 1127.27, 1125.377, 1124.592, 1121.7, 1122.773, 1121.427, 1119.213, 1120.209, 1117.781, 1115.437, 1115.981, 1115.065]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:13 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:12 layer:conv1d out_channels:16 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.004345102422989143 beta1:0.8920499950371901 beta2:0.859084075946502 weight_decay:4.286626192476769e-05 batch_size:15 epochs:100	100	1000	True	3411.70093		1613087	16	-1	378.16802740097046	{'train_loss': [5350.802, 5048.229, 1777.43, 1367.709, 1366.19, 1368.19, 1365.342, 1366.481, 1367.291, 1366.456, 1367.111, 1365.849, 1365.554, 1365.131, 1367.68, 1366.172, 1366.188, 1365.126, 1365.096, 1364.284, 1363.05, 1363.177, 1362.28, 1359.202, 1358.663, 1358.46, 1356.618, 1357.716, 1358.475, 1356.976, 1359.079, 1360.046, 1357.775, 1358.386, 1356.879, 1359.184, 1357.586, 1358.596, 1357.315, 1357.727, 1357.963, 1357.297, 1357.998, 1358.256, 1358.286, 1360.791, 1357.64, 1360.42, 1359.0, 1358.571, 1359.37, 1358.082, 1358.09, 1358.135, 1358.455, 1358.703, 1358.45, 1358.858, 1357.474, 1359.106, 1357.855, 1359.702, 1358.138, 1360.058, 1358.716, 1357.361, 1358.88, 1358.494, 1361.5, 1358.609, 1358.812, 1359.796, 1360.241, 1359.211, 1358.987, 1357.231, 1357.691, 1359.165, 1360.844, 1358.22, 1359.716, 1358.494, 1358.917, 1358.991, 1359.14, 1358.163, 1358.657, 1358.308, 1359.561, 1360.366, 1358.626, 1359.329, 1358.614, 1358.877, 1360.446, 1359.147, 1359.99, 1359.02, 1358.626, 1360.344], 'val_loss': [5175.763, 4217.255, 10901.706, 4173.634, 5264.316, 5999.052, 8293.148, 6445.796, 5529.093, 6756.83, 5613.25, 5571.301, 6289.632, 4479.381, 4595.976, 6303.565, 4194.666, 4195.16, 5139.166, 9546.303, 3280.272, 5531.686, 7604.352, 5866.417, 5300.702, 5520.296, 6839.192, 4692.166, 2882.89, 2850.829, 3982.064, 5130.721, 4450.632, 3473.417, 6275.724, 6195.283, 4539.819, 3345.451, 6347.98, 7550.187, 3139.111, 6698.947, 5144.35, 9610.78, 6893.594, 3637.291, 4204.198, 6668.378, 6991.913, 5396.452, 3818.908, 6415.048, 9448.31, 5165.014, 2909.997, 4845.21, 4559.418, 5526.134, 4305.954, 3893.912, 4325.759, 5449.381, 6625.958, 5646.973, 4548.146, 3699.598, 2721.106, 5747.254, 5012.277, 4482.151, 4361.611, 3729.925, 4975.779, 4560.146, 7687.632, 6697.03, 6894.855, 5501.962, 4823.786, 4761.599, 4378.812, 4177.195, 2672.876, 6079.967, 5677.288, 4385.646, 8004.291, 3667.286, 7799.206, 4400.385, 6174.26, 4248.533, 4013.324, 3575.742, 4598.452, 4164.378, 3715.175, 5404.005, 6461.563, 3379.294]}	100	100	True
