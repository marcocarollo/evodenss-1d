id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	853.3252		548637	18	-1	439.8721842765808	{'train_loss': [1389.746, 1135.46, 1082.564, 1067.776, 1060.932, 1054.46, 1049.234, 1040.92, 1040.287, 1031.759, 1028.741, 1021.508, 1020.226, 1015.737, 1013.522, 1015.301, 1009.542, 1007.008, 1001.321, 995.633, 983.499, 970.339, 961.352, 953.15, 945.802, 942.015, 935.178, 931.881, 924.88, 922.852, 918.431, 916.958, 912.466, 910.542, 905.538, 903.041, 900.95, 899.839, 898.968, 897.414, 896.844, 893.689, 893.89, 891.961, 888.812, 886.354, 886.831, 885.078, 884.813, 881.555, 882.991, 882.941, 879.664, 883.443, 877.047, 875.872, 877.014, 876.205, 872.975, 873.758, 873.774, 873.141, 870.892, 872.822, 869.268, 868.842, 869.391, 865.897, 868.43, 865.802, 867.026, 863.056, 862.791, 863.578, 861.714, 864.261, 864.836, 861.427, 860.176, 863.291, 859.032, 860.188, 859.867, 858.21, 856.589, 855.364, 854.506, 854.823, 856.921, 853.66, 853.945, 851.505, 853.75, 852.634, 852.65, 850.959, 850.482, 852.868, 853.369, 850.434], 'val_loss': [1134.436, 1085.98, 1049.759, 1039.037, 1031.704, 1016.864, 1026.519, 1012.142, 1009.584, 1003.474, 1016.532, 992.297, 1001.247, 1001.205, 1000.217, 997.599, 985.862, 984.368, 989.457, 969.914, 948.237, 941.917, 926.248, 929.693, 925.01, 914.844, 916.936, 913.772, 913.749, 908.069, 917.712, 907.917, 908.499, 908.727, 898.703, 905.536, 897.438, 903.008, 896.401, 904.949, 896.083, 892.768, 896.926, 898.383, 906.506, 898.658, 899.322, 895.511, 901.091, 896.805, 895.001, 895.136, 909.343, 899.093, 901.548, 899.562, 909.943, 901.007, 920.039, 916.408, 899.95, 896.092, 913.81, 902.29, 900.616, 900.607, 903.019, 894.164, 908.666, 918.168, 897.233, 890.096, 910.406, 901.585, 894.871, 912.603, 897.148, 891.581, 910.391, 894.247, 908.459, 892.349, 900.611, 896.935, 906.783, 909.636, 905.261, 897.784, 895.212, 903.444, 917.397, 899.955, 913.796, 896.323, 898.518, 891.81, 898.004, 901.592, 898.654, 893.647]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:64 kernel_size:10 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9422357349918663 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	870.07117		641805	19	-1	606.0188705921173	{'train_loss': [1619.563, 1108.258, 1079.12, 1066.046, 1025.776, 1004.511, 983.612, 971.901, 963.15, 955.556, 946.13, 941.583, 942.509, 934.341, 929.804, 925.051, 924.87, 921.817, 918.598, 915.333, 912.04, 913.279, 909.833, 907.135, 901.771, 902.732, 901.973, 898.855, 895.282, 891.297, 889.877, 891.761, 886.739, 888.661, 883.317, 880.909, 880.336, 875.582, 873.108, 874.307, 873.731, 873.842, 873.242, 869.891, 867.658, 869.892, 868.687, 866.927, 864.411, 863.125, 861.994, 863.655, 860.828, 861.262, 857.086, 856.323, 855.795, 853.857, 856.335, 852.396, 852.656, 849.349, 847.839, 850.688, 848.112, 848.855, 847.055, 847.883, 845.245, 843.884, 844.434, 842.743, 841.305, 844.98, 842.436, 838.898, 836.166, 845.63, 835.573, 841.181, 836.764, 836.569, 831.117, 832.477, 832.787, 833.413, 835.057, 829.485, 831.034, 829.929, 830.969, 831.84, 827.823, 830.588, 822.521, 829.782, 828.63, 827.737, 828.734, 824.685], 'val_loss': [1134.059, 1068.655, 1041.785, 1015.87, 975.237, 974.374, 944.816, 933.19, 928.843, 943.103, 922.883, 912.814, 911.741, 908.802, 911.312, 913.284, 906.959, 914.036, 898.515, 916.808, 895.799, 897.13, 898.549, 895.279, 893.259, 886.998, 903.76, 897.78, 883.293, 899.776, 892.87, 887.105, 895.923, 914.684, 904.388, 909.594, 890.776, 905.534, 898.515, 898.149, 898.482, 906.081, 903.742, 911.362, 903.367, 919.898, 917.644, 909.187, 911.922, 903.959, 917.7, 909.066, 916.654, 902.984, 928.422, 902.328, 899.055, 915.818, 911.49, 921.52, 902.77, 907.071, 908.114, 914.378, 910.432, 912.147, 912.68, 912.932, 925.82, 907.922, 908.721, 907.645, 920.312, 909.813, 926.913, 900.328, 898.392, 910.999, 902.005, 909.799, 905.213, 905.142, 905.134, 911.658, 900.56, 904.114, 899.104, 911.31, 903.158, 914.04, 926.178, 913.404, 920.148, 911.097, 904.354, 911.15, 910.646, 905.2, 911.221, 909.526]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:15 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.08783067164517094 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	1248.7157		547899	19	-1	451.30320858955383	{'train_loss': [1721.378, 1316.513, 1313.53, 1287.672, 1296.252, 1315.07, 1312.632, 1305.018, 1296.328, 1318.849, 1323.23, 1312.737, 1315.638, 1296.271, 1319.583, 1301.901, 1311.172, 1303.177, 1318.25, 1315.511, 1312.645, 1295.667, 1310.252, 1296.707, 1312.46, 1299.941, 1302.822, 1304.753, 1334.749, 1301.381, 1308.844, 1303.572, 1334.577, 1311.182, 1310.242, 1302.726, 1338.475, 1299.47, 1322.531, 1307.766, 1318.379, 1303.177, 1309.761, 1302.784, 1322.327, 1310.301, 1319.867, 1310.079, 1302.729, 1313.835, 1313.869, 1330.287, 1299.467, 1321.451, 1342.543, 1314.981, 1312.551, 1314.621, 1311.385, 1325.659, 1309.673, 1291.21, 1307.063, 1327.084, 1296.299, 1299.219, 1311.385, 1305.678, 1298.655, 1307.444, 1311.841, 1306.053, 1326.942, 1292.208, 1322.713, 1320.427, 1302.962, 1333.894, 1300.011, 1315.133, 1303.774, 1305.767, 1306.774, 1320.519, 1312.111, 1318.148, 1321.514, 1339.54, 1326.736, 1306.853, 1296.938, 1324.57, 1317.755, 1312.686, 1312.859, 1310.255, 1307.217, 1292.072, 1307.553, 1308.408], 'val_loss': [1165.149, 1192.655, 1200.519, 1163.684, 1453.002, 1209.315, 1181.501, 1833.896, 1164.818, 1179.309, 1161.687, 1285.845, 1260.029, 1271.789, 1169.83, 1164.179, 1197.694, 1162.8, 1438.785, 1188.125, 1259.474, 1162.087, 1224.408, 1199.122, 1188.342, 1178.687, 1368.346, 1339.858, 1183.923, 1170.407, 1223.874, 1166.247, 1430.337, 1179.525, 1666.264, 1206.122, 1162.264, 1168.304, 1184.618, 1163.974, 1188.493, 1210.169, 1551.289, 1302.137, 1160.108, 1174.38, 1173.764, 1340.422, 1211.101, 1169.845, 1158.31, 1172.86, 1192.572, 1265.114, 1217.897, 1394.082, 1178.079, 1229.155, 1205.78, 1169.921, 1277.685, 1594.11, 1158.354, 1214.84, 1178.527, 1187.399, 1160.383, 1174.684, 1226.517, 1444.211, 1161.369, 1175.196, 1162.239, 1192.703, 1168.955, 1209.524, 1314.584, 1158.945, 1213.363, 1206.059, 1289.141, 1188.123, 1162.344, 1233.217, 1166.762, 1237.047, 1171.172, 1185.991, 1157.258, 1155.413, 1165.251, 1172.565, 1213.216, 1173.346, 1174.155, 1177.471, 1169.276, 1177.057, 1215.808, 1240.352]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	2000	True	881.91144		548637	18	-1	391.51954555511475	{'train_loss': [1388.815, 1139.214, 1091.411, 1075.088, 1066.37, 1058.946, 1053.814, 1050.496, 1045.977, 1041.353, 1036.837, 1035.692, 1033.851, 1023.873, 997.298, 972.74, 963.327, 952.039, 946.106, 939.352, 937.321, 931.245, 926.826, 920.745, 919.603, 915.29, 912.891, 913.098, 907.809, 905.576, 906.002, 902.185, 902.123, 898.515, 897.038, 893.545, 892.703, 892.45, 890.724, 891.606, 890.049, 888.856, 886.183, 885.614, 884.744, 886.52, 883.89, 885.483, 882.589, 879.676, 878.038, 882.022, 877.441, 876.514, 877.697, 877.851, 876.202, 874.898, 878.207, 874.065, 871.98, 870.169, 869.415, 869.651, 867.718, 869.286, 868.062, 862.703, 863.687, 868.89, 865.177, 863.38, 861.414, 865.325, 859.754, 858.796, 862.972, 857.71, 860.112, 856.148, 855.607, 857.476, 855.161, 853.819, 855.772, 851.352, 852.305, 856.456, 852.039, 853.983, 854.623, 849.717, 850.87, 852.286, 851.037, 844.44, 848.299, 850.675, 851.273, 847.312], 'val_loss': [1163.123, 1100.288, 1061.416, 1040.069, 1028.502, 1019.281, 1013.502, 1022.531, 1013.939, 1026.925, 1013.661, 1020.878, 1011.47, 986.568, 958.925, 931.035, 920.672, 918.685, 923.246, 923.763, 910.173, 913.595, 900.753, 903.771, 897.075, 907.827, 900.315, 900.437, 906.982, 904.748, 904.653, 916.817, 901.032, 903.84, 908.103, 916.082, 903.367, 901.544, 927.567, 898.269, 908.649, 911.193, 902.596, 897.59, 899.902, 909.679, 898.934, 897.895, 901.787, 906.577, 891.742, 892.098, 901.683, 907.357, 923.531, 905.31, 912.044, 898.009, 893.361, 903.015, 912.542, 914.24, 896.53, 898.61, 894.192, 905.719, 898.485, 897.349, 899.821, 905.314, 904.176, 915.659, 894.101, 905.131, 897.046, 894.828, 893.539, 899.934, 903.327, 905.642, 896.424, 908.345, 896.296, 906.33, 917.547, 901.986, 974.242, 897.219, 904.468, 895.299, 899.631, 914.438, 965.302, 909.065, 909.179, 921.38, 918.271, 906.353, 957.769, 908.144]}	0	100	True
