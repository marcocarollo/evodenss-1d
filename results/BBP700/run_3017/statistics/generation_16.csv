id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:65 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.0037291634219010156 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	859.224		564971	19	-1	431.84552454948425	{'train_loss': [1296.096, 1122.118, 1079.384, 1063.774, 1054.521, 1047.715, 1014.126, 976.901, 957.496, 944.655, 936.757, 931.336, 923.37, 918.961, 915.681, 909.565, 907.492, 903.15, 901.91, 897.463, 895.726, 893.542, 890.877, 889.824, 887.567, 885.858, 883.612, 882.672, 883.479, 880.661, 877.7, 879.262, 875.026, 875.734, 873.985, 874.027, 872.074, 870.93, 871.118, 868.749, 867.737, 868.513, 868.262, 865.32, 866.807, 866.004, 864.507, 863.469, 861.21, 862.171, 860.926, 860.964, 857.598, 859.784, 856.119, 856.901, 855.141, 856.046, 853.871, 852.428, 851.391, 852.457, 850.21, 850.596, 849.686, 849.186, 846.856, 847.512, 847.506, 845.02, 846.991, 845.459, 844.765, 844.797, 843.125, 843.957, 843.114, 842.238, 842.273, 840.284, 839.66, 840.274, 838.832, 840.235, 837.868, 839.723, 838.208, 838.392, 837.647, 836.514, 837.764, 835.876, 834.013, 833.019, 834.494, 835.32, 834.022, 835.211, 832.389, 830.579], 'val_loss': [1184.117, 1061.232, 1054.296, 1042.605, 1027.754, 1015.1, 966.825, 947.585, 925.024, 917.945, 906.034, 917.759, 907.328, 903.389, 900.074, 896.416, 902.176, 896.015, 892.605, 893.199, 895.327, 897.154, 894.373, 896.942, 890.458, 891.826, 889.315, 882.278, 890.054, 885.287, 890.21, 897.961, 891.444, 893.057, 893.673, 893.262, 896.739, 894.252, 888.777, 893.546, 894.42, 893.493, 894.112, 890.282, 891.383, 890.176, 891.845, 894.463, 888.523, 894.424, 891.374, 895.49, 886.025, 893.232, 893.614, 895.277, 889.121, 892.888, 892.737, 893.867, 888.805, 897.326, 893.428, 893.451, 896.18, 889.632, 895.954, 895.022, 895.607, 892.481, 893.593, 891.766, 895.833, 897.151, 890.185, 895.61, 893.159, 889.534, 888.767, 893.698, 890.209, 893.258, 891.809, 884.721, 892.815, 891.965, 895.341, 892.128, 890.857, 895.304, 891.985, 894.019, 891.513, 882.119, 888.944, 887.105, 890.069, 889.335, 887.523, 888.112]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:65 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:69 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:65 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:conv1d out_channels:32 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:rmsprop lr:0.0037291634219010156 alpha:0.8437886447280681 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	2490.59595		5655996	19	-1	472.59683060646057	{'train_loss': [10272.098, 6542.591, 5773.134, 4382.819, 1322.299, 1288.174, 1285.709, 1266.275, 1266.191, 1252.271, 1218.828, 1179.445, 1180.738, 1167.207, 1159.589, 1160.127, 1128.24, 1115.107, 1097.412, 1094.306, 1085.403, 1083.051, 1079.135, 1070.233, 1064.607, 1062.014, 1057.792, 1059.32, 1064.818, 1054.917, 1052.341, 1041.634, 1040.302, 1034.859, 1034.867, 1032.355, 1028.91, 1036.396, 1040.86, 1024.826, 1026.88, 1027.335, 1017.318, 1016.942, 1014.18, 1012.238, 1011.484, 1010.399, 1002.449, 1004.865, 998.28, 995.381, 994.399, 987.976, 986.762, 988.17, 986.336, 988.37, 981.924, 975.537, 970.134, 974.055, 972.376, 971.552, 959.554, 976.212, 956.643, 969.749, 960.442, 951.318, 954.204, 948.012, 944.149, 949.672, 945.012, 943.756, 938.072, 942.766, 930.002, 935.513, 939.425, 939.306, 931.779, 929.795, 930.926, 919.875, 926.733, 918.49, 918.24, 908.416, 914.302, 905.604, 904.783, 906.945, 911.354, 908.852, 903.634, 897.889, 893.406, 890.585], 'val_loss': [4462.902, 4462.902, 4462.902, 1475.516, 1330.808, 1169.951, 1282.871, 1208.377, 1229.723, 1094.26, 1107.158, 1075.523, 2843.16, 2496.471, 1852.02, 1894.421, 2092.363, 1549.161, 1296.361, 1844.347, 1714.984, 1660.758, 1607.872, 1449.781, 2041.791, 2202.229, 1741.159, 1822.22, 2131.675, 3250.986, 2228.535, 2563.014, 1305.067, 1251.737, 1664.426, 2580.941, 2993.629, 2135.22, 1482.176, 3155.429, 3102.08, 1785.739, 2902.97, 2394.214, 2812.457, 3114.952, 3786.774, 5707.785, 2737.246, 2602.267, 2728.718, 2602.011, 1716.1, 4177.999, 1795.759, 3541.808, 3063.277, 2686.219, 3506.789, 3679.069, 3682.884, 1891.059, 2359.36, 2401.629, 2491.081, 3325.407, 2723.988, 1868.609, 2894.318, 2800.804, 2457.862, 2364.887, 3236.199, 4193.411, 2521.776, 3078.934, 2343.753, 2385.783, 5117.002, 2685.603, 2170.251, 4683.456, 2189.982, 2936.531, 2250.874, 2426.294, 4084.468, 2374.174, 3753.359, 2767.579, 2527.533, 2783.845, 2782.071, 2142.993, 4078.719, 2703.552, 3348.02, 3513.767, 3432.962, 2355.143]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:27 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:27 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:100 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:deconv1d out_channels:105 kernel_size:10 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:deconv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:15 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:16 layer:conv1d out_channels:4 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:17 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:18 layer:fc act:selu out_features:200 bias:True input:19 learning:adam lr:0.009755819038149555 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	918.17157		996017	21	-1	456.82376074790955	{'train_loss': [4705.042, 2376.343, 1164.648, 1134.975, 1099.471, 1095.084, 1068.846, 1033.115, 1007.955, 992.831, 988.839, 976.362, 971.325, 966.689, 962.505, 956.554, 950.938, 946.771, 945.957, 945.731, 938.397, 934.963, 933.965, 929.941, 928.599, 925.703, 924.782, 924.31, 923.89, 919.998, 920.546, 914.394, 913.519, 915.914, 910.247, 911.071, 912.443, 904.198, 905.647, 900.988, 903.644, 901.451, 902.279, 899.203, 898.307, 898.792, 896.311, 895.408, 892.587, 892.04, 893.891, 887.165, 891.925, 894.15, 886.151, 884.744, 882.796, 884.484, 893.154, 882.389, 877.608, 876.564, 885.673, 876.511, 879.406, 878.541, 876.985, 882.396, 876.833, 869.049, 873.204, 877.802, 878.316, 873.635, 872.638, 867.022, 868.181, 869.295, 868.693, 865.296, 869.6, 870.509, 864.155, 868.566, 867.22, 869.744, 863.802, 866.714, 868.476, 868.999, 867.172, 869.029, 862.885, 862.541, 864.28, 873.675, 855.476, 860.259, 858.254, 861.158], 'val_loss': [4462.839, 1160.44, 1142.223, 1100.074, 1062.39, 1048.738, 1026.705, 1005.818, 958.449, 962.514, 939.088, 933.007, 936.88, 939.854, 925.371, 924.295, 943.193, 927.782, 926.695, 937.61, 930.647, 929.337, 910.307, 915.048, 908.789, 923.492, 900.48, 937.511, 926.08, 908.747, 929.382, 913.839, 922.585, 943.605, 923.342, 922.264, 912.955, 926.875, 909.022, 920.001, 900.702, 910.639, 901.123, 944.933, 938.866, 925.511, 922.698, 928.764, 928.918, 916.258, 928.299, 904.3, 926.629, 920.64, 911.174, 917.442, 922.826, 931.116, 930.894, 917.643, 907.121, 925.399, 923.189, 916.982, 931.863, 911.233, 927.485, 909.924, 906.179, 911.45, 922.334, 907.356, 923.735, 914.29, 918.763, 918.458, 935.298, 940.091, 926.748, 925.056, 921.757, 919.422, 910.407, 934.498, 913.43, 931.189, 916.1, 927.022, 917.874, 947.21, 917.785, 919.879, 929.176, 925.41, 917.545, 926.209, 915.156, 916.294, 955.805, 931.709]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:106 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:65 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:deconv1d out_channels:123 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:17 layer:fc act:selu out_features:200 bias:True input:18 learning:adam lr:0.0037291634219010156 beta1:0.8012292276773302 beta2:0.9353831093483147 weight_decay:9.455536560769911e-05 batch_size:13 epochs:100	100	1000	True	882.3335		709674	20	-1	425.70446610450745	{'train_loss': [1431.812, 1120.661, 1026.211, 987.07, 970.379, 956.929, 947.356, 938.762, 932.466, 926.589, 920.196, 918.601, 913.877, 910.65, 909.96, 907.815, 903.42, 901.679, 899.531, 897.566, 896.64, 895.47, 893.201, 890.795, 889.111, 889.564, 885.418, 884.287, 883.309, 881.332, 880.746, 879.237, 879.225, 875.402, 875.65, 874.032, 873.298, 874.52, 870.225, 870.297, 869.393, 867.387, 867.37, 867.062, 863.988, 862.797, 861.744, 861.84, 861.215, 857.92, 860.859, 859.281, 857.523, 856.49, 856.068, 855.664, 855.492, 855.805, 855.182, 851.54, 852.638, 852.133, 850.605, 848.985, 850.033, 847.384, 848.497, 848.298, 845.77, 846.322, 846.301, 846.409, 843.137, 843.427, 843.166, 843.323, 843.625, 842.827, 841.532, 840.939, 841.354, 840.146, 841.768, 836.471, 838.928, 837.05, 838.326, 836.744, 837.23, 833.707, 833.637, 834.613, 832.072, 832.85, 832.852, 831.674, 830.788, 831.656, 828.677, 829.789], 'val_loss': [1214.869, 1027.711, 962.851, 949.62, 943.448, 934.589, 929.232, 926.458, 929.568, 917.584, 915.422, 912.128, 917.775, 915.639, 918.517, 923.408, 911.961, 915.814, 929.997, 914.918, 929.159, 921.898, 918.031, 906.639, 913.906, 913.981, 909.539, 914.726, 908.962, 913.365, 915.757, 903.825, 912.711, 913.574, 908.377, 901.429, 901.193, 901.539, 902.651, 900.002, 900.82, 901.948, 903.304, 901.276, 891.696, 897.43, 897.523, 894.211, 896.033, 894.698, 902.503, 899.629, 899.169, 896.697, 894.344, 899.923, 892.735, 898.001, 897.441, 901.45, 901.283, 902.344, 889.204, 898.407, 891.575, 895.397, 899.38, 898.015, 904.866, 897.364, 899.396, 899.075, 896.275, 896.109, 896.456, 895.823, 896.987, 901.389, 898.889, 897.695, 897.45, 898.505, 895.482, 901.315, 895.934, 896.927, 902.036, 902.034, 898.583, 896.417, 895.32, 896.382, 900.24, 898.129, 904.153, 900.77, 899.256, 899.37, 897.941, 905.862]}	100	100	True
