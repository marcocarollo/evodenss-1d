id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.009097544753807963 beta1:0.804697104124598 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	878.06006		455044	17	-1	359.5388605594635	{'train_loss': [1342.469, 1100.898, 1011.455, 974.221, 955.143, 945.564, 936.906, 931.547, 927.141, 924.32, 922.861, 918.099, 915.022, 914.559, 912.977, 909.643, 907.009, 905.946, 902.98, 899.476, 900.81, 897.644, 896.543, 893.472, 893.985, 892.391, 890.51, 889.182, 887.363, 886.195, 884.622, 883.373, 882.948, 880.612, 878.498, 876.179, 878.72, 874.328, 875.241, 872.619, 872.431, 874.95, 871.127, 869.434, 872.919, 871.277, 868.755, 868.476, 869.458, 865.83, 866.154, 860.601, 861.493, 861.284, 864.673, 863.452, 863.276, 863.586, 859.961, 862.154, 860.776, 859.822, 857.409, 857.907, 856.476, 857.659, 857.907, 862.306, 857.231, 857.1, 855.143, 853.098, 855.43, 858.51, 853.578, 854.805, 856.427, 852.561, 854.158, 856.638, 860.116, 852.517, 850.655, 851.573, 849.009, 848.25, 848.938, 852.648, 851.274, 850.751, 854.143, 852.297, 849.395, 851.789, 845.11, 846.134, 846.13, 845.163, 847.258, 844.356], 'val_loss': [1158.193, 1016.261, 967.517, 936.462, 924.395, 926.797, 924.544, 927.928, 929.89, 932.385, 931.392, 909.348, 915.024, 909.335, 906.6, 912.55, 900.768, 933.566, 917.005, 917.065, 910.58, 903.262, 902.337, 926.749, 910.701, 920.39, 912.303, 907.268, 910.653, 908.472, 912.502, 909.017, 919.815, 916.435, 913.433, 903.16, 920.122, 906.167, 907.575, 923.641, 900.751, 922.871, 911.986, 906.331, 926.066, 935.179, 899.602, 912.514, 919.723, 911.737, 905.525, 904.979, 916.072, 908.691, 914.136, 924.662, 917.085, 899.179, 928.097, 911.265, 913.888, 910.138, 921.234, 928.448, 908.851, 914.755, 916.751, 911.81, 913.397, 907.851, 919.123, 916.636, 915.068, 910.69, 908.83, 914.447, 907.636, 916.402, 904.654, 919.731, 911.661, 914.423, 909.339, 915.97, 920.55, 911.517, 915.688, 931.067, 917.485, 918.146, 922.546, 920.498, 914.899, 911.748, 912.494, 912.344, 909.833, 910.137, 1042.472, 909.665]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:115 kernel_size:10 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:115 kernel_size:10 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:112 kernel_size:5 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:113 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.804697104124598 beta2:0.9353831093483147 weight_decay:6.105517313734705e-05 batch_size:13 epochs:100	100	1000	True	901.03729		847848	18	-1	712.8716278076172	{'train_loss': [1826.394, 1132.906, 1095.136, 1083.144, 1072.425, 1065.7, 1062.833, 1059.834, 1053.878, 1042.529, 1007.554, 981.089, 965.566, 953.524, 945.169, 936.419, 936.645, 929.693, 928.682, 921.181, 923.083, 918.075, 914.432, 912.544, 908.811, 907.89, 907.156, 908.539, 901.982, 900.118, 901.757, 897.453, 898.624, 894.498, 897.052, 894.712, 896.933, 892.659, 894.85, 889.538, 892.016, 886.04, 892.46, 886.324, 883.809, 884.53, 881.049, 884.086, 882.365, 885.398, 883.141, 879.749, 883.64, 881.893, 881.069, 881.096, 879.683, 877.036, 876.503, 875.522, 879.404, 874.244, 879.559, 875.952, 875.636, 872.138, 865.523, 873.952, 870.881, 869.581, 862.47, 864.704, 869.264, 867.841, 868.052, 862.469, 863.357, 867.493, 871.593, 875.989, 868.953, 870.048, 865.03, 864.931, 859.842, 868.28, 864.238, 862.828, 862.96, 862.344, 865.187, 861.058, 862.539, 859.0, 856.843, 858.171, 857.291, 853.992, 856.13, 852.642], 'val_loss': [1162.071, 1073.427, 1054.724, 1051.704, 1048.4, 1035.076, 1041.183, 1036.264, 1045.385, 1000.255, 970.562, 957.54, 934.313, 932.696, 940.879, 929.673, 925.761, 953.565, 935.245, 934.648, 922.919, 931.046, 923.933, 918.823, 914.787, 923.804, 916.459, 913.304, 909.076, 903.591, 911.378, 904.99, 905.972, 904.161, 904.428, 903.912, 911.027, 904.884, 907.034, 916.482, 902.496, 921.807, 907.696, 905.107, 910.93, 907.095, 914.507, 918.047, 912.619, 920.963, 909.745, 910.929, 904.063, 922.251, 914.92, 917.848, 912.45, 917.558, 936.364, 924.809, 920.769, 911.515, 925.024, 924.414, 924.973, 910.824, 913.393, 918.27, 920.818, 913.263, 909.142, 922.497, 915.338, 930.661, 921.37, 921.457, 912.905, 939.033, 913.434, 936.465, 930.546, 928.091, 931.932, 931.798, 934.485, 923.275, 933.52, 924.849, 935.976, 940.9, 932.186, 935.711, 926.71, 937.855, 964.046, 933.919, 946.735, 937.877, 931.075, 931.885]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:8 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:109 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:13 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:14 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.009097544753807963 beta1:0.804697104124598 beta2:0.9353831093483147 weight_decay:0.0005963093568911525 batch_size:13 epochs:100	100	1000	True	949.69177		563602	18	-1	439.73894715309143	{'train_loss': [1601.272, 1152.554, 1132.223, 1094.821, 1076.299, 1078.446, 1072.237, 1068.217, 1064.871, 1060.653, 1053.996, 1050.927, 1044.111, 1044.34, 1036.648, 1033.614, 1035.289, 1031.524, 1029.876, 1030.677, 1026.538, 1023.87, 1022.189, 1022.769, 1022.973, 1020.57, 1013.609, 1016.874, 1016.178, 1014.653, 1012.563, 1010.246, 1012.02, 1010.961, 1007.657, 1005.918, 1008.204, 1006.234, 1004.705, 1007.217, 1004.56, 1005.29, 1001.298, 1000.744, 999.951, 999.781, 999.894, 995.606, 995.078, 994.717, 994.999, 998.929, 994.717, 994.812, 994.473, 992.321, 994.154, 986.939, 989.961, 991.578, 990.946, 991.599, 990.211, 993.581, 987.886, 988.954, 988.039, 992.094, 992.805, 991.929, 985.941, 986.371, 986.032, 984.659, 984.402, 983.931, 981.589, 985.433, 980.07, 983.134, 980.683, 979.128, 981.058, 977.355, 978.513, 979.247, 975.381, 975.304, 980.491, 973.322, 975.335, 974.623, 975.842, 978.035, 978.261, 975.609, 976.769, 973.884, 973.003, 974.673], 'val_loss': [1164.716, 1132.694, 1080.358, 1052.137, 1035.675, 1051.691, 1050.142, 1015.551, 1023.357, 1015.247, 1026.524, 1021.773, 1038.499, 1030.337, 1023.7, 1040.113, 1012.148, 1007.028, 1032.492, 1007.811, 1003.807, 994.45, 1030.865, 1040.086, 994.568, 1039.736, 1056.291, 1021.576, 1026.286, 993.31, 1025.504, 1004.442, 1033.299, 985.224, 997.826, 1001.56, 998.699, 1006.801, 989.355, 1035.994, 1029.071, 1016.568, 1039.141, 1011.563, 979.813, 995.401, 989.985, 1020.33, 985.713, 989.862, 993.157, 1006.017, 991.25, 1004.193, 981.439, 991.475, 980.687, 990.662, 992.98, 985.139, 989.894, 980.105, 977.794, 990.52, 980.579, 981.087, 988.414, 993.919, 976.326, 976.651, 982.277, 983.616, 988.049, 988.922, 986.203, 988.607, 979.948, 994.85, 990.082, 984.317, 981.721, 992.319, 980.183, 986.612, 999.984, 988.782, 992.854, 990.909, 991.926, 984.964, 987.576, 996.287, 994.388, 1020.105, 995.092, 985.992, 988.745, 978.773, 989.892, 984.253]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:55 kernel_size:9 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:13 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:14 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:15 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adadelta lr:0.009097544753807963 batch_size:13 epochs:100	100	1000	True	910.33337		477346	19	-1	379.3815858364105	{'train_loss': [1456.588, 1205.279, 1195.197, 1190.311, 1186.16, 1183.455, 1180.26, 1167.735, 1148.095, 1135.468, 1128.286, 1121.807, 1114.951, 1106.939, 1097.673, 1088.424, 1081.503, 1073.051, 1066.629, 1061.353, 1058.208, 1052.746, 1048.569, 1045.075, 1042.006, 1039.079, 1037.743, 1033.898, 1031.7, 1029.805, 1026.217, 1025.729, 1023.103, 1021.067, 1021.199, 1017.875, 1017.555, 1014.608, 1014.152, 1013.219, 1012.324, 1011.393, 1009.193, 1008.049, 1006.256, 1005.82, 1004.246, 1003.768, 1002.599, 1000.331, 999.945, 998.336, 998.613, 998.768, 995.891, 994.996, 993.578, 994.584, 991.403, 990.46, 989.629, 989.499, 987.723, 986.988, 988.06, 986.976, 986.461, 984.4, 984.115, 981.785, 982.341, 980.783, 980.872, 980.652, 980.016, 978.35, 977.45, 974.208, 975.222, 973.311, 974.495, 972.039, 974.514, 970.364, 971.039, 968.857, 970.031, 966.492, 967.523, 965.303, 964.795, 964.144, 965.031, 961.869, 963.619, 960.605, 959.64, 959.918, 959.536, 960.082], 'val_loss': [1255.261, 1185.335, 1169.502, 1162.449, 1157.79, 1154.779, 1148.929, 1134.184, 1125.113, 1120.7, 1115.0, 1110.544, 1104.27, 1092.339, 1077.667, 1068.328, 1057.767, 1051.512, 1044.958, 1042.36, 1036.656, 1031.545, 1031.89, 1026.035, 1022.551, 1020.109, 1017.707, 1014.876, 1012.586, 1009.402, 1006.911, 1002.925, 1001.988, 999.539, 996.226, 996.551, 990.829, 992.31, 992.352, 990.261, 989.256, 987.228, 984.828, 985.61, 980.866, 981.011, 980.235, 978.343, 978.054, 977.249, 975.208, 973.367, 972.377, 970.249, 971.331, 968.718, 969.228, 968.0, 965.373, 965.46, 965.341, 963.221, 963.964, 962.153, 961.184, 959.848, 959.881, 960.562, 958.146, 956.1, 957.279, 957.43, 955.47, 955.969, 950.91, 952.061, 953.129, 950.529, 949.659, 949.496, 948.905, 947.701, 945.754, 944.893, 947.053, 944.703, 945.057, 943.087, 941.464, 941.595, 944.768, 938.067, 939.173, 938.929, 936.398, 938.113, 935.955, 935.373, 934.9, 934.56]}	100	100	True
