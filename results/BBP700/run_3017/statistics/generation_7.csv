id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.004345102422989143 beta1:0.804697104124598 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	900.34381		750874	17	-1	458.5856993198395	{'train_loss': [1682.549, 1076.675, 1023.521, 994.529, 974.437, 957.6, 948.081, 940.244, 935.035, 929.494, 923.355, 920.426, 915.87, 913.602, 908.015, 907.043, 901.373, 898.859, 897.847, 893.964, 890.35, 890.22, 886.766, 882.471, 881.406, 880.814, 878.706, 876.56, 874.093, 871.359, 869.327, 869.364, 865.078, 864.138, 864.561, 862.613, 862.311, 856.392, 854.241, 852.667, 854.759, 852.644, 853.867, 850.09, 850.688, 849.116, 847.763, 846.347, 847.16, 844.294, 841.193, 842.978, 839.881, 839.422, 836.963, 838.965, 832.279, 832.414, 837.327, 834.019, 831.9, 829.219, 832.006, 827.57, 828.061, 826.595, 829.345, 827.274, 827.48, 822.955, 822.863, 822.085, 820.592, 819.77, 818.268, 817.964, 822.204, 819.53, 816.291, 815.275, 815.925, 813.431, 814.114, 815.142, 813.352, 815.237, 816.4, 814.378, 815.38, 815.586, 813.575, 817.271, 813.035, 815.403, 811.084, 811.182, 808.425, 810.693, 808.004, 809.84], 'val_loss': [1087.227, 1011.034, 973.145, 955.12, 943.766, 933.558, 923.041, 920.017, 923.852, 910.949, 914.714, 911.336, 912.423, 924.065, 920.151, 921.601, 914.616, 919.138, 915.519, 915.218, 917.441, 916.851, 913.058, 912.406, 919.565, 919.969, 909.275, 917.426, 919.624, 921.355, 918.974, 920.668, 918.746, 922.84, 927.289, 921.123, 914.407, 914.018, 915.757, 915.986, 926.838, 915.729, 914.132, 915.656, 914.818, 916.581, 912.377, 912.117, 918.28, 914.736, 921.325, 917.408, 916.45, 924.708, 918.58, 917.5, 916.931, 919.008, 919.853, 909.403, 920.657, 913.188, 917.986, 918.624, 917.774, 923.924, 909.584, 919.342, 921.434, 922.908, 923.027, 923.844, 917.011, 918.705, 919.251, 920.786, 918.565, 919.503, 923.031, 919.221, 927.799, 923.73, 918.373, 923.724, 922.241, 925.435, 918.418, 914.984, 915.151, 915.312, 922.779, 912.287, 917.795, 915.105, 917.679, 926.648, 922.186, 920.012, 918.435, 919.333]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:85 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:115 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:conv1d out_channels:62 kernel_size:6 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:conv1d out_channels:24 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:16 layer:fc act:selu out_features:200 bias:True input:17 learning:adam lr:0.0857145846939786 beta1:0.804697104124598 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	4553.91406		4266115	19	-1	461.19063353538513	{'train_loss': [5002.46, 6067.413, 5963.309, 5205.015, 4854.876, 4916.249, 5083.101, 4960.134, 5203.628, 4972.569, 5233.402, 4984.039, 4937.411, 5053.956, 4767.694, 5389.576, 4929.242, 4908.728, 4869.189, 4776.028, 5088.091, 5215.111, 4836.604, 4738.604, 4866.741, 4904.342, 4958.301, 4874.082, 4774.385, 5209.818, 4894.626, 4850.27, 4678.42, 4650.285, 4700.815, 4693.314, 4622.641, 4688.826, 4766.056, 4681.377, 4741.291, 4666.275, 4729.388, 4691.025, 4688.223, 4668.535, 4683.529, 4750.262, 4690.405, 4722.021, 4676.067, 4695.78, 4690.756, 4676.767, 4706.167, 4721.01, 4692.538, 4712.816, 4697.414, 4675.861, 4704.007, 4689.333, 4661.322, 4704.278, 4671.447, 4676.757, 4706.376, 4642.249, 4753.062, 4667.812, 4731.823, 4678.956, 4716.196, 4665.693, 4688.468, 4731.979, 4706.169, 4711.379, 4701.976, 4703.308, 4661.097, 4716.746, 4685.249, 4708.586, 4657.281, 4714.492, 4690.505, 4703.011, 4663.981, 4684.803, 4720.368, 4651.892, 4709.986, 4683.264, 4691.041, 4743.222, 4656.854, 4722.704, 4671.896, 4679.461], 'val_loss': [4462.902, 4462.902, 4462.902, 4462.902, 88034.453, 4462.902, 33820.125, 4462.902, 4462.902, 4745.204, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4460.208, 4462.902, 4462.902, 4462.902, 4886.89, 321597.469, 4462.902, 4462.902, 4693.951, 6187.832, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 1322184.5, 15673.698, 4462.902, 5265.015, 12609.521, 37878.914, 95431.906, 4462.902, 5169.784, 4462.902, 4462.902, 722396.688, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 26871.641, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 7284.833, 4462.902, 102765.047, 4462.902, 12233.801, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902, 4462.902]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:51 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.009097544753807963 beta1:0.804697104124598 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	864.83264		455044	17	-1	430.1171364784241	{'train_loss': [1322.5, 1126.084, 1083.833, 1064.055, 1019.498, 996.257, 972.085, 960.83, 951.523, 944.877, 941.853, 936.282, 933.268, 930.193, 925.077, 918.45, 918.207, 913.968, 914.03, 912.296, 912.179, 912.393, 909.266, 904.328, 905.544, 904.529, 902.066, 901.024, 898.901, 898.046, 897.589, 897.357, 893.878, 893.746, 895.212, 891.08, 890.776, 891.338, 888.801, 887.639, 888.813, 884.853, 884.325, 883.513, 883.683, 885.716, 882.554, 882.384, 877.217, 879.965, 881.262, 876.609, 882.105, 874.687, 878.588, 875.515, 875.354, 872.181, 874.775, 872.689, 869.36, 870.53, 870.418, 872.664, 865.678, 865.039, 869.479, 871.479, 866.321, 864.903, 866.327, 863.094, 867.764, 864.203, 865.558, 861.848, 865.379, 860.74, 862.428, 861.529, 862.23, 859.884, 859.098, 857.198, 860.574, 865.741, 855.667, 864.039, 855.135, 855.716, 862.313, 858.594, 863.546, 857.726, 853.12, 857.02, 854.447, 855.85, 854.546, 856.313], 'val_loss': [1138.918, 1067.593, 1054.118, 1011.068, 982.86, 952.578, 927.663, 919.324, 932.539, 918.791, 920.275, 909.94, 909.039, 907.179, 906.735, 907.448, 896.387, 907.915, 903.69, 902.608, 903.038, 902.468, 906.49, 907.871, 904.287, 901.72, 906.256, 895.568, 891.986, 888.738, 894.389, 898.413, 892.858, 890.105, 891.079, 898.934, 899.945, 896.65, 900.124, 899.211, 901.865, 891.367, 900.033, 905.449, 897.792, 900.6, 912.736, 897.878, 893.93, 897.91, 899.697, 906.259, 904.665, 900.919, 899.699, 907.602, 912.437, 915.692, 909.201, 916.532, 908.28, 914.243, 908.351, 903.269, 904.885, 904.661, 917.937, 914.642, 914.983, 908.759, 909.056, 913.442, 930.74, 911.995, 905.737, 922.876, 917.915, 909.738, 906.165, 901.443, 910.113, 919.004, 908.323, 911.333, 911.765, 920.217, 906.644, 910.514, 907.079, 903.452, 908.002, 912.767, 902.706, 908.79, 905.754, 901.712, 904.295, 901.23, 903.364, 896.606]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:73 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:47 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:23 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:93 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adam lr:0.004345102422989143 beta1:0.8414043168818335 beta2:0.9353831093483147 weight_decay:4.286626192476769e-05 batch_size:13 epochs:100	100	1000	True	867.7251		750874	17	-1	446.6670551300049	{'train_loss': [2054.503, 1100.906, 1070.851, 1056.141, 1028.215, 986.761, 962.716, 943.688, 935.301, 926.544, 920.217, 916.481, 910.308, 906.416, 904.257, 900.608, 897.198, 894.341, 891.035, 887.621, 886.801, 885.237, 882.127, 882.496, 876.642, 878.128, 873.097, 873.005, 869.777, 868.334, 867.52, 864.725, 863.125, 862.564, 861.915, 859.588, 864.295, 859.315, 857.784, 852.482, 852.649, 851.98, 850.368, 850.541, 848.577, 846.316, 847.921, 845.698, 843.06, 843.285, 844.496, 841.417, 838.585, 837.992, 836.795, 837.034, 837.496, 835.966, 834.216, 832.422, 831.479, 831.559, 830.899, 832.268, 833.09, 834.431, 827.692, 825.82, 826.258, 825.678, 828.281, 822.998, 823.549, 823.092, 819.923, 820.742, 820.271, 819.379, 819.175, 821.006, 820.338, 820.219, 819.133, 814.491, 816.741, 813.709, 815.327, 814.488, 816.007, 816.046, 812.788, 815.076, 811.095, 810.89, 809.663, 809.859, 809.924, 806.208, 809.89, 804.682], 'val_loss': [1155.99, 1059.741, 1044.503, 1024.52, 986.973, 961.813, 921.856, 925.786, 931.976, 919.23, 919.855, 920.856, 920.651, 902.513, 908.585, 913.22, 910.017, 905.126, 905.499, 903.802, 907.833, 900.637, 896.018, 893.641, 892.543, 897.36, 896.369, 899.804, 897.999, 896.161, 903.682, 898.591, 900.563, 902.487, 901.729, 901.641, 904.009, 897.124, 903.313, 899.167, 900.039, 902.057, 901.398, 907.975, 898.859, 896.507, 899.503, 894.69, 897.836, 893.805, 899.71, 891.325, 893.023, 900.158, 899.502, 904.146, 902.202, 895.812, 894.971, 901.968, 889.631, 890.93, 896.743, 890.284, 897.637, 898.289, 892.91, 898.581, 891.867, 894.87, 900.691, 896.617, 891.036, 899.342, 901.059, 890.844, 890.241, 895.838, 895.714, 901.501, 896.041, 893.199, 892.633, 891.496, 887.494, 896.541, 892.908, 890.964, 889.463, 898.759, 905.511, 889.691, 894.508, 891.66, 893.139, 897.945, 891.896, 893.728, 892.481, 891.67]}	100	100	True
