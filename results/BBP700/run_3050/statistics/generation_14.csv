id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	32454.59961		271701	12	-1	229.43995428085327	{'train_loss': [418067.438, 370600.906, 329326.844, 315997.844, 310222.656, 306411.562, 303074.812, 300878.781, 298399.656, 296793.156, 295382.781, 293743.812, 292205.438, 290805.312, 289647.938, 288274.156, 286557.219, 284590.375, 280395.438, 276600.938, 274311.0, 270935.719, 268196.656, 265942.312, 264536.75, 264077.375, 261381.047, 260895.688, 260085.469, 257436.484, 257450.922, 256044.891, 255076.109, 254227.5, 254146.188, 253077.781, 252243.656, 251154.312, 250779.062, 249906.625, 248589.797, 248506.5, 247463.375, 246281.891, 246444.0, 246302.781, 245563.734, 244696.453, 244841.719, 243097.281, 243921.453, 243436.688, 243096.375, 242883.75, 241591.141, 240566.844, 241041.578, 240137.094, 239545.984, 239255.625, 238831.562, 239095.328, 238413.406, 237493.547, 238048.75, 236395.312, 236218.969, 236890.047, 235603.906, 235742.562, 234524.75, 234374.5, 234903.312, 233669.828, 233961.844, 233447.938, 233552.547, 232603.078, 231865.391, 232278.719, 232215.031, 232075.297, 232205.781, 231499.844, 232014.719, 231157.156, 230962.266, 230977.984, 230990.609, 230825.562, 229685.031, 230558.094, 230080.469, 229237.062, 229450.453, 228443.656, 228369.344, 227028.016, 227585.797, 226927.328], 'val_loss': [3492.196, 3108.23, 2950.376, 2918.32, 2895.226, 2876.425, 2860.897, 2842.941, 2824.301, 2814.343, 2803.275, 2786.421, 2777.58, 2769.729, 2757.581, 2756.654, 2742.861, 2716.667, 2667.461, 2643.013, 2645.396, 2591.937, 2601.323, 2564.549, 2539.89, 2538.379, 2526.79, 2487.387, 2470.504, 2487.167, 2451.564, 2445.599, 2455.716, 2426.705, 2441.749, 2412.533, 2397.77, 2395.177, 2378.156, 2393.528, 2375.188, 2372.471, 2372.482, 2363.729, 2345.097, 2361.417, 2342.729, 2332.834, 2339.971, 2362.829, 2343.247, 2329.48, 2309.309, 2321.92, 2328.276, 2337.651, 2350.028, 2317.234, 2318.519, 2307.927, 2300.561, 2305.77, 2292.821, 2291.626, 2276.941, 2291.574, 2283.25, 2288.58, 2291.658, 2278.841, 2281.523, 2281.651, 2267.435, 2300.239, 2313.881, 2271.062, 2285.542, 2275.662, 2261.219, 2252.773, 2277.74, 2250.225, 2234.078, 2258.63, 2251.136, 2264.466, 2258.731, 2226.288, 2260.94, 2259.012, 2243.741, 2245.61, 2249.645, 2224.486, 2236.648, 2224.351, 2226.638, 2227.283, 2249.454, 2248.921]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:14 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:85 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:14 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	33304.39844		1350709	13	-1	235.11115431785583	{'train_loss': [1136036.0, 1092607.75, 1101626.875, 716411.688, 355240.844, 313846.0, 304032.875, 298742.312, 294651.844, 291046.375, 288461.531, 285605.406, 283121.938, 280248.156, 279822.688, 277658.562, 276371.531, 273819.812, 272877.281, 270178.312, 268984.469, 268611.969, 267654.5, 266863.875, 265038.312, 264439.688, 262819.812, 263195.094, 261262.031, 260154.297, 259465.328, 259283.406, 258138.359, 257188.234, 257441.641, 257336.594, 256440.75, 255915.344, 254594.219, 253571.781, 253782.0, 254145.438, 251837.031, 251667.766, 252049.328, 250174.188, 250450.906, 249661.562, 249221.062, 248422.656, 248926.141, 247176.219, 246881.594, 247966.766, 246774.078, 246098.859, 245911.906, 245152.875, 245452.047, 244220.266, 244817.312, 244436.891, 242971.328, 243992.562, 242312.531, 242652.422, 241365.5, 241295.516, 241692.375, 242707.516, 240507.109, 240832.266, 241105.141, 240073.516, 239970.234, 239559.156, 238778.797, 238491.375, 238850.578, 239346.047, 237878.531, 238628.5, 236629.031, 237490.141, 237662.953, 236982.594, 237499.188, 236797.312, 235786.391, 235227.719, 235050.312, 235425.422, 235344.625, 234680.922, 234226.969, 235391.844, 233324.016, 234221.484, 234002.344, 232372.719], 'val_loss': [10598.306, 10745.844, 11247.701, 3504.213, 2897.746, 2849.102, 2831.718, 2792.174, 2769.322, 2741.958, 2725.219, 2686.177, 2726.222, 2696.141, 2670.794, 2660.155, 2624.922, 2619.968, 2607.928, 2587.218, 2596.628, 2590.172, 2563.171, 2580.2, 2558.817, 2541.757, 2527.541, 2527.709, 2522.018, 2516.234, 2526.964, 2513.252, 2506.836, 2499.535, 2493.931, 2492.98, 2504.406, 2518.143, 2490.011, 2500.348, 2485.799, 2474.969, 2507.635, 2468.237, 2468.264, 2469.813, 2442.717, 2458.383, 2461.553, 2448.137, 2485.277, 2445.15, 2463.454, 2453.93, 2460.207, 2420.764, 2441.815, 2441.484, 2446.19, 2425.928, 2448.111, 2427.912, 2434.01, 2439.164, 2421.695, 2428.731, 2442.394, 2432.677, 2431.522, 2436.529, 2429.269, 2429.163, 2450.884, 2420.707, 2466.562, 2415.67, 2429.535, 2441.397, 2403.497, 2426.181, 2451.272, 2409.735, 2407.612, 2420.364, 2452.721, 2445.652, 2433.004, 2415.917, 2445.39, 2421.701, 2436.056, 2459.111, 2421.95, 2426.698, 2447.207, 2423.383, 2440.625, 2457.925, 2414.602, 2408.821]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:conv1d out_channels:51 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	33730.61719		1226553	12	-1	232.3183410167694	{'train_loss': [1139882.375, 1103962.375, 1088623.625, 981096.562, 402062.094, 327054.188, 312386.0, 305381.562, 302465.625, 299610.938, 297256.375, 296011.062, 293672.438, 291514.625, 289404.562, 287076.969, 285633.406, 283691.125, 281524.562, 280216.875, 278611.094, 276781.438, 276806.719, 275325.406, 273623.438, 272628.938, 271946.625, 270894.75, 270195.688, 268508.031, 267099.75, 266905.969, 266471.156, 266414.031, 264930.469, 264164.969, 263485.562, 262939.625, 260628.547, 261976.094, 259737.047, 259846.531, 259898.953, 259621.703, 258504.672, 258496.641, 257100.156, 257010.406, 255414.219, 255150.609, 256427.625, 255349.062, 255469.016, 254730.609, 254181.188, 253653.641, 253476.516, 253084.203, 253651.219, 251739.781, 251005.891, 251219.234, 250352.703, 250028.375, 249960.766, 249634.672, 249132.234, 248813.031, 248515.75, 248082.25, 249076.625, 248047.938, 245906.312, 248193.547, 247421.781, 246975.234, 246649.219, 246369.578, 246258.797, 245850.234, 246058.875, 245143.734, 246280.906, 245061.359, 244679.703, 243767.359, 243513.375, 243264.203, 243938.859, 243007.812, 243553.734, 243155.016, 242199.719, 241981.375, 243286.766, 241889.875, 242277.812, 241603.922, 242122.172, 240138.438], 'val_loss': [11015.636, 13655.776, 12372.001, 4724.3, 3091.13, 2934.986, 2910.426, 2884.385, 2817.695, 2828.533, 2777.249, 2755.548, 2746.528, 2724.997, 2727.781, 2701.106, 2713.679, 2712.746, 2648.706, 2645.475, 2628.138, 2609.98, 2605.635, 2597.244, 2562.907, 2584.582, 2558.189, 2538.183, 2552.324, 2514.09, 2515.906, 2532.508, 2500.107, 2502.958, 2531.053, 2508.124, 2518.442, 2483.039, 2500.631, 2486.496, 2504.961, 2484.949, 2483.087, 2492.66, 2464.814, 2495.345, 2468.733, 2468.948, 2449.374, 2479.004, 2451.453, 2459.847, 2468.443, 2448.88, 2446.631, 2438.759, 2436.919, 2490.375, 2473.939, 2444.186, 2456.836, 2433.002, 2427.996, 2435.289, 2442.918, 2423.867, 2446.844, 2414.012, 2432.957, 2435.878, 2433.582, 2431.428, 2424.124, 2415.706, 2417.24, 2426.214, 2419.847, 2445.674, 2409.936, 2419.942, 2408.01, 2404.462, 2408.817, 2409.908, 2379.302, 2384.909, 2395.633, 2387.093, 2376.331, 2391.499, 2377.662, 2382.243, 2368.413, 2393.402, 2393.154, 2379.069, 2369.188, 2384.502, 2378.204, 2390.73]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:101 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	31894.26562		360190	11	-1	221.24114727973938	{'train_loss': [491286.406, 369038.562, 327355.656, 314633.406, 306383.375, 297466.938, 290876.562, 285248.031, 280780.812, 276331.969, 273648.75, 270342.031, 268322.531, 266128.344, 263849.031, 262469.781, 260038.5, 257963.906, 255456.234, 252667.891, 252199.234, 250473.344, 249063.672, 247296.625, 247192.016, 245950.359, 244513.312, 244037.016, 242350.234, 240926.438, 240645.516, 240372.953, 239900.984, 238577.828, 238316.656, 236563.375, 236351.484, 236416.484, 235693.422, 235659.5, 234500.312, 233380.203, 232750.953, 231964.422, 232098.234, 231404.438, 230967.0, 230403.938, 230232.75, 229622.375, 229678.031, 229355.609, 228729.141, 228472.406, 227058.281, 227954.656, 227331.719, 226667.688, 226191.531, 226694.016, 225703.547, 225655.516, 225368.109, 224874.25, 224709.531, 224032.234, 224011.078, 223610.438, 222454.094, 222628.328, 222764.234, 222927.266, 222600.938, 222703.703, 221595.875, 221402.188, 221208.484, 221002.266, 221729.328, 219853.344, 221325.984, 221077.922, 219763.109, 220548.953, 219199.469, 219329.109, 218366.812, 218810.781, 218439.406, 217660.656, 218395.516, 217126.875, 218322.125, 217589.828, 218363.734, 218017.875, 217242.609, 217476.172, 216612.938, 216831.641], 'val_loss': [3770.543, 3066.46, 2929.555, 2904.809, 2817.931, 2767.669, 2725.7, 2676.667, 2672.988, 2616.696, 2596.305, 2571.033, 2571.279, 2530.774, 2500.558, 2492.89, 2465.985, 2458.307, 2430.907, 2438.153, 2448.459, 2393.255, 2396.549, 2356.665, 2393.937, 2358.511, 2360.48, 2360.779, 2347.106, 2348.931, 2373.43, 2330.323, 2334.269, 2323.637, 2308.801, 2329.021, 2324.967, 2332.927, 2338.543, 2303.033, 2308.056, 2336.8, 2311.563, 2313.331, 2271.698, 2281.621, 2274.199, 2285.281, 2286.598, 2245.946, 2276.413, 2259.809, 2271.288, 2272.112, 2275.831, 2278.976, 2278.142, 2257.964, 2253.561, 2264.884, 2267.892, 2242.773, 2240.98, 2262.412, 2278.268, 2249.082, 2250.524, 2274.742, 2246.796, 2242.234, 2247.544, 2269.942, 2270.996, 2256.492, 2256.651, 2226.54, 2245.086, 2241.457, 2247.399, 2256.283, 2208.545, 2244.62, 2242.462, 2248.844, 2224.26, 2219.306, 2244.898, 2226.45, 2249.61, 2216.924, 2221.355, 2248.793, 2235.56, 2251.973, 2219.038, 2217.106, 2212.974, 2256.756, 2224.509, 2215.74]}	100	100	True
