id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	31136.42188		576588	11	-1	202.50540947914124	{'train_loss': [794033.375, 351391.469, 311422.062, 297703.562, 290010.375, 284926.125, 280642.781, 276711.344, 272991.906, 269989.969, 267417.406, 263955.312, 261814.031, 259822.438, 257354.562, 257252.594, 253569.828, 253130.828, 251844.031, 249591.594, 248433.719, 247483.969, 246430.984, 245219.797, 243580.281, 243030.578, 243201.297, 239828.469, 240933.406, 239494.359, 239762.906, 238931.906, 237347.859, 237371.266, 236377.031, 236463.203, 235181.594, 235008.016, 234348.062, 234185.406, 233048.797, 232898.031, 233273.578, 231286.203, 231560.562, 232041.125, 230783.734, 229826.078, 230166.641, 230073.219, 229371.484, 227803.406, 228618.438, 227973.859, 228410.031, 226920.188, 227362.281, 227205.469, 226120.406, 225944.844, 224575.609, 225838.297, 225651.109, 225634.766, 224258.297, 225370.984, 224158.406, 224377.844, 224041.672, 224600.578, 223549.484, 223540.328, 222174.688, 222753.641, 222676.109, 221727.562, 222547.828, 222416.094, 221424.078, 222173.672, 221293.938, 220760.203, 220421.094, 221208.203, 220314.062, 220488.375, 221138.766, 219510.281, 220722.078, 219663.359, 219714.391, 220345.891, 219485.828, 218613.734, 218082.891, 218245.688, 218943.297, 218641.781, 218428.672, 217917.453], 'val_loss': [3288.151, 2976.811, 2849.861, 2781.564, 2721.739, 2658.43, 2644.323, 2613.225, 2580.723, 2568.15, 2531.973, 2516.215, 2520.293, 2497.646, 2519.521, 2482.613, 2467.549, 2470.611, 2450.698, 2406.11, 2423.287, 2424.61, 2407.414, 2378.336, 2392.412, 2398.998, 2351.287, 2377.568, 2351.116, 2359.585, 2346.194, 2344.457, 2343.969, 2322.262, 2311.44, 2320.536, 2324.648, 2320.954, 2298.941, 2338.187, 2320.656, 2295.016, 2307.818, 2287.395, 2288.632, 2277.905, 2308.506, 2284.798, 2278.16, 2250.704, 2264.478, 2263.078, 2276.619, 2273.688, 2268.37, 2250.143, 2258.727, 2240.55, 2231.597, 2237.438, 2258.169, 2236.757, 2249.596, 2233.375, 2255.808, 2223.812, 2233.768, 2213.232, 2212.38, 2218.487, 2287.155, 2222.267, 2213.768, 2200.014, 2206.038, 2228.506, 2188.594, 2207.181, 2202.778, 2177.522, 2196.305, 2208.528, 2187.936, 2186.931, 2205.308, 2227.344, 2218.535, 2227.57, 2199.696, 2182.238, 2181.365, 2188.859, 2193.902, 2212.895, 2183.438, 2184.43, 2198.619, 2181.226, 2176.237, 2180.174]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:55 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:17 epochs:100	100	1000	True	31563.76953		429866	11	-1	291.45357489585876	{'train_loss': [456205.281, 314776.25, 296378.625, 286572.469, 279113.719, 273295.719, 268783.938, 265240.938, 261992.797, 259572.969, 257186.0, 254724.734, 252638.969, 250814.531, 250023.609, 247913.016, 247460.219, 246139.406, 245319.438, 244438.828, 242805.297, 242096.297, 240475.547, 239678.984, 238999.75, 238418.734, 236919.172, 235699.609, 235401.906, 234888.812, 233455.234, 233728.672, 232667.578, 232600.234, 231824.578, 231032.234, 229551.844, 229631.531, 228649.734, 228182.062, 228367.938, 226085.578, 226079.281, 226332.922, 225126.656, 224241.547, 224596.547, 223765.953, 223493.516, 222621.078, 221901.094, 222521.375, 222127.797, 222332.453, 221026.125, 220705.234, 220784.047, 219287.578, 218949.188, 219035.844, 219200.75, 217755.703, 218346.297, 218373.453, 217122.844, 217412.328, 217040.094, 215835.625, 215871.641, 215660.562, 214919.359, 215217.062, 213624.719, 214941.453, 213867.609, 214647.906, 213699.703, 213606.109, 212582.578, 212285.641, 213007.797, 211665.531, 212066.531, 211339.219, 211901.609, 210099.906, 210728.469, 210894.797, 210512.797, 210562.906, 209968.625, 210042.703, 209764.375, 209266.625, 209444.625, 208662.266, 207998.703, 207748.844, 207802.938, 207218.391], 'val_loss': [1629.682, 1502.694, 1447.299, 1422.085, 1383.698, 1359.67, 1335.623, 1315.854, 1303.906, 1290.257, 1293.198, 1277.247, 1262.174, 1259.379, 1258.474, 1259.711, 1245.26, 1242.637, 1241.742, 1233.138, 1227.625, 1216.505, 1219.752, 1218.338, 1222.297, 1203.833, 1220.159, 1212.01, 1201.443, 1205.553, 1193.007, 1201.038, 1201.051, 1206.226, 1194.92, 1193.824, 1199.413, 1198.981, 1193.17, 1190.536, 1202.037, 1202.4, 1202.114, 1184.168, 1172.388, 1188.042, 1186.051, 1177.772, 1171.728, 1185.715, 1176.175, 1178.853, 1183.444, 1183.783, 1193.978, 1176.042, 1172.703, 1170.506, 1160.477, 1166.075, 1165.57, 1166.278, 1163.64, 1156.732, 1150.954, 1164.289, 1154.145, 1162.74, 1158.697, 1157.987, 1153.27, 1169.913, 1182.266, 1153.454, 1172.062, 1154.519, 1164.79, 1155.328, 1152.98, 1149.809, 1154.342, 1152.239, 1155.32, 1150.013, 1157.786, 1158.954, 1163.66, 1159.151, 1158.788, 1145.073, 1159.675, 1138.552, 1147.469, 1162.549, 1170.178, 1154.468, 1154.391, 1149.534, 1152.012, 1151.564]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	31254.6543		576588	11	-1	205.38417148590088	{'train_loss': [779699.562, 366530.25, 316180.438, 303254.062, 294749.781, 288233.688, 281835.906, 274689.688, 269556.594, 266505.281, 264000.875, 262310.219, 258699.875, 256530.531, 254506.922, 254136.25, 252413.656, 250567.703, 250443.844, 249069.0, 246503.969, 245459.656, 245226.906, 244742.141, 243258.078, 242178.812, 241510.703, 241084.156, 241814.188, 239590.953, 239666.344, 237782.75, 237931.141, 237262.453, 237154.25, 235584.047, 235483.516, 235198.172, 234835.812, 233781.75, 232556.5, 232299.344, 232146.609, 231951.562, 231629.047, 230717.688, 231783.625, 230426.859, 230770.906, 228911.781, 229469.844, 228639.891, 228431.312, 229441.547, 227795.828, 227326.609, 226948.25, 227462.422, 227105.312, 225913.234, 225840.438, 225860.641, 225874.031, 224349.031, 224099.688, 224886.172, 224715.422, 222539.953, 223367.906, 223251.703, 223423.281, 222960.281, 222165.922, 221985.016, 222954.625, 221443.391, 220495.234, 221200.922, 220292.781, 220438.578, 220358.781, 219810.719, 220154.297, 219626.812, 218425.188, 219092.906, 217882.688, 219457.5, 218813.969, 218375.484, 217020.688, 218083.375, 216808.25, 217739.453, 219293.297, 216993.672, 215679.516, 216877.438, 215087.469, 216410.453], 'val_loss': [3513.151, 3037.3, 2845.544, 2803.757, 2717.966, 2656.687, 2589.088, 2551.861, 2545.717, 2484.566, 2472.319, 2456.939, 2453.343, 2446.647, 2407.874, 2420.819, 2385.695, 2371.329, 2379.345, 2357.33, 2357.413, 2334.479, 2326.433, 2336.805, 2328.881, 2308.566, 2318.7, 2296.921, 2292.268, 2296.899, 2278.358, 2288.3, 2274.294, 2252.574, 2262.674, 2259.023, 2264.151, 2251.991, 2253.254, 2240.483, 2241.433, 2248.291, 2242.903, 2244.946, 2239.421, 2237.283, 2258.576, 2251.464, 2236.055, 2248.404, 2236.228, 2227.036, 2219.202, 2219.137, 2220.819, 2215.408, 2224.005, 2218.042, 2227.133, 2206.47, 2235.332, 2199.731, 2203.382, 2196.49, 2214.374, 2228.359, 2192.71, 2196.85, 2213.148, 2196.583, 2194.812, 2191.72, 2198.879, 2189.41, 2197.21, 2182.953, 2169.73, 2178.523, 2171.146, 2176.117, 2181.266, 2189.808, 2174.905, 2180.76, 2179.716, 2184.665, 2184.23, 2162.195, 2172.907, 2153.518, 2154.136, 2172.129, 2184.298, 2154.832, 2176.869, 2136.809, 2154.957, 2156.702, 2170.822, 2154.085]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:122 kernel_size:5 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.12453287305170418 alpha:0.9829630148080644 weight_decay:0.0009310301220562968 batch_size:32 epochs:100	100	1000	True	137271.15625		1491924	12	-1	273.12887048721313	{'train_loss': [3424313.25, 1066921.0, 1066921.0, 1066921.0, 1066963.5, 1072654.0, 1130116.0, 1066921.0, 20034106.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1990970.625, 1087369.125, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1126564.625, 1067206.75, 23487696.0, 1066921.0, 1066921.0, 1066921.0, 1098686.125, 1085947.125, 1066921.0, 1066921.0, 2795504.5, 1066921.0, 11957788.0, 1066921.0, 1071118.875, 1607156.5, 1066964.5, 1066921.0, 1066921.0, 1066921.0, 5638489.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1068952.5, 1078288.375, 27877224.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 24483918.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1067014.25, 1066921.0, 15961488.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 30489870.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 21929676.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1071071.875, 1066921.0, 1066921.0, 1066921.0, 1152486.625], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 11879.406, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
