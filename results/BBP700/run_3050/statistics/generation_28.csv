id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	30759.18945		577492	11	-1	212.60785174369812	{'train_loss': [698081.438, 368607.531, 319745.375, 304642.219, 295502.344, 287838.062, 280687.938, 275705.531, 271245.562, 266390.531, 263712.812, 260199.375, 257862.766, 255306.109, 252352.594, 250747.562, 249786.781, 248321.953, 247005.047, 245649.844, 243208.891, 243233.0, 242547.406, 240564.516, 240814.125, 239279.5, 238769.219, 238250.703, 236430.812, 236461.812, 235622.594, 235886.906, 234507.156, 233804.609, 232928.594, 232274.547, 232954.594, 231433.047, 231535.047, 230081.828, 231056.094, 229528.281, 229252.656, 229179.562, 228928.188, 228001.219, 228166.531, 227421.609, 226783.703, 226867.938, 226194.719, 227110.672, 224931.5, 225721.453, 224702.062, 223198.906, 224828.016, 224682.734, 223468.953, 223362.375, 224171.453, 224033.328, 222659.312, 221846.891, 222845.312, 221702.719, 222645.969, 220465.125, 221794.078, 220627.406, 220170.797, 220099.062, 221201.453, 219425.875, 219336.375, 219728.594, 218709.25, 219999.719, 220060.734, 218933.125, 218801.359, 218300.719, 219438.469, 217630.828, 217481.422, 217388.625, 217390.453, 217651.422, 216485.938, 216806.281, 216933.719, 216981.484, 216867.609, 216125.594, 216124.844, 215795.922, 215586.594, 215497.734, 215216.125, 215818.953], 'val_loss': [3805.891, 3038.921, 2874.958, 2814.939, 2765.245, 2674.7, 2638.453, 2617.009, 2566.994, 2534.813, 2507.949, 2500.718, 2449.891, 2426.447, 2408.34, 2406.552, 2408.471, 2406.977, 2373.92, 2353.452, 2372.188, 2336.786, 2339.97, 2321.241, 2338.142, 2323.187, 2343.609, 2309.438, 2320.309, 2309.027, 2325.402, 2312.842, 2311.534, 2292.361, 2282.245, 2264.931, 2267.424, 2256.288, 2276.021, 2279.168, 2272.582, 2245.365, 2249.516, 2248.727, 2254.234, 2260.712, 2245.172, 2249.511, 2255.885, 2253.701, 2248.421, 2259.195, 2255.223, 2236.576, 2250.759, 2246.729, 2227.354, 2227.287, 2216.924, 2220.663, 2289.635, 2212.396, 2237.849, 2240.902, 2225.442, 2214.537, 2210.906, 2232.005, 2215.276, 2217.866, 2216.613, 2210.707, 2212.7, 2227.628, 2207.469, 2200.496, 2196.494, 2263.511, 2187.571, 2184.255, 2197.266, 2191.878, 2205.11, 2194.941, 2213.9, 2212.41, 2190.654, 2188.713, 2179.605, 2193.503, 2190.112, 2269.442, 2178.944, 2184.633, 2182.139, 2168.581, 2197.787, 2168.617, 2183.644, 2164.603]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	31264.35938		577492	11	-1	213.83284735679626	{'train_loss': [685472.312, 345642.062, 313149.594, 300921.375, 291316.188, 285285.25, 279064.062, 273126.094, 269416.281, 264362.438, 261542.766, 259421.875, 257616.812, 254736.469, 252862.312, 251861.391, 249317.422, 248208.125, 246640.375, 245460.0, 245485.734, 244054.438, 242441.828, 243132.203, 241429.266, 240811.844, 239288.703, 239190.672, 239159.219, 236632.719, 236806.031, 236451.391, 235651.391, 237407.297, 234697.141, 234054.188, 233697.859, 233201.688, 232416.984, 232284.484, 230937.234, 231544.125, 229830.328, 229364.094, 229559.969, 229442.438, 228449.875, 228501.328, 228534.469, 227793.109, 227074.859, 227046.078, 227478.094, 227117.734, 226131.703, 225795.844, 226390.703, 225523.375, 225356.359, 225835.656, 224113.703, 223687.938, 223910.938, 223026.859, 222774.281, 223165.828, 223291.375, 222815.906, 222799.219, 222943.984, 221886.922, 221582.406, 221797.203, 222171.859, 220880.906, 222024.641, 220478.062, 220025.422, 220267.125, 220226.375, 220028.484, 219881.656, 219440.078, 219600.438, 219438.906, 219209.859, 219210.844, 218442.078, 217646.297, 218572.969, 218714.438, 217733.078, 217672.453, 218054.438, 216555.172, 217301.703, 217102.953, 216538.266, 216819.797, 216745.281], 'val_loss': [3209.244, 2980.035, 2816.641, 2761.705, 2702.03, 2674.305, 2603.707, 2589.29, 2528.613, 2546.928, 2513.345, 2496.613, 2487.157, 2492.142, 2438.089, 2446.114, 2458.929, 2429.786, 2404.946, 2403.444, 2399.566, 2408.639, 2384.795, 2374.885, 2383.903, 2397.863, 2384.94, 2383.258, 2363.364, 2386.857, 2373.694, 2354.621, 2336.962, 2350.273, 2336.999, 2321.382, 2324.541, 2322.852, 2322.581, 2327.913, 2325.46, 2331.623, 2292.875, 2305.461, 2323.823, 2276.72, 2304.436, 2294.629, 2275.795, 2286.049, 2297.993, 2265.283, 2283.657, 2279.744, 2272.802, 2274.353, 2338.107, 2305.305, 2250.924, 2287.495, 2282.905, 2274.786, 2264.145, 2238.615, 2251.141, 2270.361, 2257.906, 2227.254, 2240.049, 2262.838, 2226.5, 2223.372, 2254.151, 2241.834, 2222.73, 2228.095, 2201.283, 2229.449, 2209.648, 2223.412, 2236.76, 2229.378, 2223.154, 2199.076, 2235.845, 2240.895, 2201.479, 2216.971, 2216.927, 2231.42, 2223.409, 2213.661, 2198.701, 2206.577, 2223.335, 2222.194, 2214.274, 2197.679, 2195.639, 2208.477]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:rmsprop lr:0.13236785986802266 alpha:0.912627208011538 weight_decay:0.0005507620711143113 batch_size:32 epochs:100	100	1000	True	137265.79688		348215	9	-1	181.44392132759094	{'train_loss': [2314830.25, 1106401.25, 2666423.5, 1296103.625, 1099830.25, 1406916.375, 1098389.125, 1261887.75, 1138132.875, 1181321.0, 1188368.25, 1090171.5, 1235020.75, 1124539.625, 1120242.0, 1226130.5, 1141732.625, 1131240.125, 1183984.0, 1175840.125, 1147447.875, 1187517.625, 1186309.625, 1132722.625, 1191864.375, 1192275.25, 1144621.375, 1125737.75, 1212398.375, 1141000.75, 1132616.875, 1181059.25, 1190895.0, 1134289.25, 1670688.375, 1262664.875, 1142894.625, 1138632.125, 1292270.875, 1094537.5, 1199966.125, 1189870.25, 1129755.25, 1208183.5, 1161041.25, 1133992.75, 1212935.375, 1191716.375, 1130776.0, 1151824.5, 1299692.5, 1132248.625, 1121925.625, 1236893.0, 1153635.0, 1134275.125, 1160258.875, 1160251.375, 1174200.125, 1331910.375, 1151635.75, 1092419.125, 1158409.5, 1267481.75, 1132325.875, 1140498.125, 1219587.5, 1116912.5, 1128438.5, 1249847.375, 1179765.25, 1135627.625, 1186577.375, 1212856.125, 1138467.25, 1151315.5, 1190253.5, 1202553.875, 1114782.25, 1187042.375, 1144769.25, 1171415.0, 1181687.125, 1139684.125, 1189608.875, 1208036.0, 1128141.875, 1229739.25, 1131152.75, 1184803.125, 1200575.375, 1171268.875, 1183841.75, 1174606.25, 1149890.125, 1209993.875, 1174757.875, 1132062.5, 1188032.625, 1231660.75], 'val_loss': [10586.708, 12060.584, 103127.43, 10587.216, 10587.216, 10834.868, 10587.207, 10587.216, 10587.216, 10486.611, 10587.216, 10587.216, 11903.043, 10562.788, 34161.664, 10587.191, 10587.216, 10586.014, 13106.658, 10623.151, 11517.085, 10574.536, 12982.715, 10587.216, 10583.23, 10584.011, 10587.216, 10587.205, 15146.152, 12816.625, 10585.83, 12331.51, 10587.216, 10784.51, 10587.216, 10587.216, 11311.997, 10586.946, 13877.365, 10587.216, 11794.19, 10587.212, 10587.031, 10587.216, 10587.216, 10628.363, 10587.198, 15330.44, 10587.216, 10587.216, 10587.215, 10587.216, 10587.159, 12162.32, 11219.566, 10587.216, 10587.216, 11016.192, 11501.59, 10587.216, 10725.544, 10930.352, 10801.502, 15402.792, 10882.93, 12293.583, 12984.979, 10609.008, 10586.957, 10583.631, 10586.656, 12671.571, 10586.53, 10583.636, 10587.216, 10587.187, 10587.216, 10587.213, 11605.178, 10587.209, 10587.213, 10587.216, 12185.884, 10587.216, 13374.839, 10587.216, 10586.256, 10587.216, 10587.216, 10564.599, 10587.216, 10587.216, 10587.123, 11798.453, 20863.857, 10587.188, 10587.216, 10587.172, 10728.607, 10587.204]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:114 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:114 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:98 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	137628.23438		125037703	12	-1	597.9376246929169	{'train_loss': [2029293.875, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
