id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	31401.23828		469896	10	-1	205.5839660167694	{'train_loss': [600383.625, 357166.0, 311398.906, 302747.312, 298288.344, 294984.656, 291807.438, 288224.25, 285000.781, 280955.844, 276249.125, 272835.562, 269576.781, 267351.438, 264670.406, 262467.188, 260907.688, 259050.516, 257574.906, 255854.641, 254204.516, 253028.516, 252195.094, 250293.188, 249310.984, 248569.875, 247339.938, 246600.156, 244803.281, 244310.016, 243437.859, 242974.922, 242007.031, 241435.422, 240302.0, 240935.031, 239843.609, 238427.703, 237696.375, 236847.094, 236583.375, 235742.094, 235482.219, 235022.391, 233524.234, 234340.578, 233740.453, 233080.109, 232310.672, 231824.219, 231885.391, 230637.516, 230456.875, 229438.422, 229985.375, 228956.203, 228479.688, 227751.328, 227272.422, 227327.875, 226937.312, 226487.812, 226639.328, 225363.875, 225872.547, 224984.359, 224679.625, 224230.703, 224258.484, 222987.031, 222859.078, 223089.125, 222683.953, 222642.719, 222193.125, 221203.703, 221716.906, 220347.578, 220716.484, 220843.391, 220799.188, 221054.047, 221002.172, 219767.016, 219253.953, 218868.719, 218130.797, 218376.922, 218479.531, 219065.906, 217069.438, 218263.406, 217459.375, 217397.062, 217600.719, 216936.297, 216279.766, 216673.797, 216679.578, 216281.047], 'val_loss': [4128.739, 2944.699, 2865.802, 2835.604, 2809.314, 2798.163, 2769.406, 2747.864, 2710.757, 2664.086, 2603.298, 2583.706, 2557.254, 2540.102, 2515.301, 2499.709, 2486.657, 2478.765, 2462.628, 2455.944, 2460.517, 2456.503, 2437.789, 2436.445, 2409.911, 2418.684, 2420.348, 2392.705, 2398.172, 2398.133, 2391.612, 2391.019, 2370.728, 2380.648, 2379.047, 2358.689, 2353.843, 2345.688, 2352.922, 2344.723, 2335.833, 2324.185, 2333.283, 2316.52, 2311.871, 2319.896, 2297.306, 2313.998, 2307.396, 2296.458, 2311.325, 2302.507, 2300.754, 2286.066, 2271.489, 2269.374, 2299.23, 2273.396, 2259.655, 2265.328, 2264.253, 2262.634, 2259.695, 2230.732, 2245.995, 2265.801, 2254.937, 2238.313, 2251.335, 2238.308, 2226.573, 2242.547, 2234.615, 2238.749, 2228.92, 2216.427, 2235.778, 2237.913, 2211.27, 2222.847, 2212.201, 2226.794, 2221.412, 2240.105, 2215.202, 2213.617, 2202.623, 2217.932, 2197.989, 2205.014, 2256.29, 2217.945, 2197.483, 2192.106, 2193.869, 2214.008, 2191.804, 2207.674, 2191.077, 2203.237]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:114 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:21 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	33951.01172		1970103	10	-1	200.87727856636047	{'train_loss': [1121088.625, 1133144.5, 1078992.875, 1114344.75, 666656.125, 356889.688, 315084.469, 306297.812, 301439.125, 298481.531, 294509.312, 292423.406, 290044.875, 287476.406, 285874.562, 283239.531, 281306.75, 279009.031, 276727.094, 274611.438, 273637.406, 271190.438, 270544.719, 269428.688, 267665.844, 266185.312, 265536.5, 263894.5, 262843.344, 260655.719, 260550.453, 260247.984, 258154.094, 257576.188, 257974.203, 255666.578, 255889.625, 254001.906, 254666.922, 252925.578, 252452.906, 250057.0, 250703.922, 250934.406, 249900.047, 248657.141, 248843.875, 248315.891, 246730.172, 246259.906, 245968.188, 245955.75, 244663.328, 245749.453, 244214.5, 245012.484, 244065.266, 243057.719, 242212.438, 242419.75, 241535.719, 241745.75, 241119.328, 240991.766, 241432.703, 239813.875, 240313.734, 239545.469, 239435.125, 238417.953, 239037.031, 238543.312, 238180.547, 236959.141, 235727.422, 236797.531, 236723.625, 236136.219, 235088.703, 235430.344, 235212.25, 234399.125, 235169.531, 235181.906, 234595.0, 234481.375, 234512.625, 233095.875, 233284.328, 233689.125, 232487.641, 232929.078, 232422.344, 231129.422, 232798.672, 231632.156, 231802.891, 231634.422, 230463.219, 232011.188], 'val_loss': [10586.973, 10587.206, 10563.512, 11481.467, 3293.325, 2878.02, 2857.246, 2803.47, 2787.612, 2755.447, 2718.083, 2697.33, 2681.753, 2668.695, 2654.819, 2641.927, 2620.882, 2583.137, 2585.139, 2603.881, 2585.785, 2562.193, 2551.468, 2553.605, 2526.09, 2517.676, 2518.241, 2495.466, 2459.806, 2469.429, 2473.864, 2456.292, 2474.265, 2448.81, 2428.653, 2422.242, 2416.399, 2421.753, 2427.725, 2406.664, 2410.174, 2414.461, 2408.842, 2412.49, 2407.448, 2383.774, 2380.263, 2394.963, 2384.41, 2392.19, 2404.672, 2388.365, 2420.906, 2389.944, 2376.018, 2368.645, 2354.403, 2402.941, 2353.06, 2370.486, 2355.008, 2401.764, 2367.1, 2393.995, 2363.697, 2380.558, 2365.393, 2355.85, 2335.938, 2359.816, 2363.84, 2335.107, 2350.136, 2363.761, 2357.901, 2349.77, 2366.791, 2360.142, 2328.544, 2364.438, 2333.803, 2353.287, 2303.177, 2326.854, 2336.934, 2322.928, 2355.424, 2336.935, 2329.456, 2330.131, 2323.352, 2324.611, 2298.896, 2310.693, 2327.035, 2339.37, 2336.021, 2330.089, 2350.581, 2340.567]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:118 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:118 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:rmsprop lr:0.09712469306752614 alpha:0.8868019091948152 weight_decay:8.523584741590746e-05 batch_size:32 epochs:100	100	1000	True	137275.57812		1693387	11	-1	300.26759099960327	{'train_loss': [1229881.375, 1824691.125, 2095369.625, 1066921.0, 3332111.0, 1066921.0, 2203542.75, 1066921.0, 2256031.25, 2588741.5, 1113538.375, 1813592.625, 2336510.0, 1782747.375, 3322285.25, 1493353.75, 3828662.25, 1066921.0, 2828016.75, 3251463.5, 1180331.625, 1276344.625, 1463169.0, 1667149.625, 1351355.375, 2284849.0, 1066921.0, 1982725.75, 1066921.0, 2269634.5, 3922996.25, 1161651.75, 1690666.0, 1066921.0, 2878243.75, 1838890.625, 1066921.0, 2812921.5, 1066921.0, 2209195.75, 1066921.0, 1834400.125, 1632625.375, 3055330.25, 2206983.5, 1066921.0, 4168432.0, 1066921.0, 2164659.75, 2813106.5, 1348584.25, 2076546.125, 1776760.375, 1497236.25, 1215569.375, 2617542.75, 1263450.625, 1195518.125, 1569415.875, 1217599.5, 1407213.75, 1531714.875, 1568851.75, 1291854.0, 1402998.125, 1494389.625, 1421499.25, 1248518.125, 1460177.5, 1380922.125, 1242992.25, 1528447.25, 1410435.25, 1274282.75, 1316414.25, 1442166.0, 1353656.875, 1307129.625, 1563262.625, 1298190.375, 1407346.0, 1352420.0, 1293005.875, 1481474.0, 1307757.875, 1324677.125, 1376155.625, 1435115.25, 1385701.125, 1375404.125, 1364788.75, 1464731.25, 1382561.125, 1363742.875, 1433177.75, 1562975.375, 1319419.375, 1407557.0, 1264555.125, 1543520.0], 'val_loss': [593608.812, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10579.585, 10587.216, 10587.216, 10587.216, 10587.216, 10927.729, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.188, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.182, 10587.171, 10587.216, 10587.216, 10587.216, 10587.081, 10587.216, 10587.201, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 35387.961, 10587.195, 10732.746, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 17908.984, 10587.216, 10587.2, 11793.839, 10587.216, 10587.216, 10587.216, 15041.673, 10587.216, 10587.211, 11247.789, 10587.216, 12821.331, 14652.577, 10587.216, 10587.216, 10828.713, 10587.212, 10587.216, 11676.161, 10587.199, 10586.342, 13794.366, 10587.216, 10587.216, 10584.07, 10587.215, 11754.473, 10587.209, 10587.216, 12350.352, 10796.261, 10587.216, 26232.834, 10587.209, 10587.216, 10587.216, 10829.666, 11286.483, 10587.216]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:76 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:104 kernel_size:7 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:deconv1d out_channels:106 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	137314.5		17324468	12	-1	234.58807945251465	{'train_loss': [1147588.125, 1154133.5, 1081611.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 45187.758, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
