id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	31281.50977		576588	11	-1	235.9408254623413	{'train_loss': [736704.812, 363273.344, 318023.125, 304247.656, 296844.594, 290594.281, 284614.219, 280067.219, 276581.594, 273526.219, 272180.781, 269706.188, 269384.469, 265432.312, 263712.875, 262067.391, 260273.531, 258270.5, 258223.922, 255747.188, 254928.5, 254019.438, 252108.453, 251248.438, 249801.562, 249444.516, 247580.0, 247047.0, 245955.312, 244320.438, 245054.062, 243708.062, 242467.578, 242877.625, 241244.031, 240572.484, 239420.875, 239204.062, 238632.703, 238119.75, 237753.656, 237147.781, 236447.531, 236398.188, 235461.656, 236128.406, 234578.984, 234731.5, 234107.562, 234122.828, 233335.266, 231961.938, 232919.797, 232500.656, 232149.141, 231885.234, 231613.391, 230325.594, 229764.906, 229857.609, 230196.906, 229350.422, 228536.062, 228934.719, 227531.641, 228259.594, 227687.547, 226272.062, 226599.156, 226892.047, 226222.438, 225914.984, 225535.734, 225549.031, 225427.359, 224983.359, 224960.531, 224643.938, 225110.359, 224105.812, 223762.125, 223528.078, 223224.438, 224352.562, 222446.031, 222627.734, 221760.828, 222182.625, 221400.562, 221531.188, 220675.281, 221051.125, 221462.484, 220770.078, 220936.797, 220090.297, 220641.453, 220490.156, 220721.156, 220707.812], 'val_loss': [3381.511, 3115.305, 2872.445, 2795.646, 2764.706, 2706.67, 2658.411, 2635.137, 2603.326, 2580.052, 2546.897, 2552.546, 2526.938, 2506.271, 2481.221, 2476.843, 2465.292, 2445.177, 2464.01, 2414.108, 2410.249, 2421.862, 2396.824, 2381.156, 2400.629, 2395.471, 2374.552, 2393.132, 2350.935, 2346.811, 2344.88, 2338.443, 2319.418, 2332.321, 2337.798, 2312.415, 2314.768, 2332.873, 2319.955, 2313.714, 2280.253, 2278.601, 2269.304, 2267.893, 2287.525, 2272.465, 2257.069, 2269.793, 2284.434, 2286.906, 2246.842, 2259.909, 2280.125, 2250.56, 2249.86, 2238.622, 2254.63, 2242.804, 2244.746, 2228.896, 2221.213, 2224.237, 2230.717, 2218.544, 2228.736, 2224.956, 2212.019, 2232.896, 2252.803, 2214.31, 2242.739, 2206.465, 2203.291, 2200.917, 2211.004, 2222.402, 2193.185, 2219.522, 2223.858, 2195.166, 2190.536, 2206.505, 2183.828, 2183.056, 2195.973, 2193.416, 2205.23, 2205.086, 2176.461, 2204.745, 2190.269, 2192.433, 2198.908, 2187.642, 2184.577, 2177.939, 2184.452, 2176.281, 2198.529, 2177.6]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:43 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:74 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adam lr:0.11692661341929608 beta1:0.819992776771768 beta2:0.8537594968370898 weight_decay:5.6748586165234324e-05 batch_size:32 epochs:100	100	1000	True	79320.66406		358707	10	-1	257.4960253238678	{'train_loss': [1124449.375, 1086478.25, 1075531.125, 1082736.5, 1079798.375, 1015909.25, 459897.875, 481235.656, 463378.625, 441073.688, 436720.781, 460836.0, 459330.812, 449257.75, 420672.281, 456506.281, 457256.312, 439024.781, 466986.219, 463244.906, 422659.812, 449029.906, 442541.688, 459315.625, 427737.094, 452452.0, 457089.188, 448090.531, 426784.531, 451398.781, 480306.438, 443087.75, 459490.406, 442513.75, 420003.938, 473877.031, 445297.562, 463250.688, 438029.125, 460535.25, 462044.938, 426064.969, 495474.25, 440749.0, 426340.312, 463683.406, 442160.938, 417917.469, 480468.781, 425171.5, 490699.938, 445936.156, 456204.719, 443068.812, 432145.062, 455813.906, 440182.75, 440315.688, 470944.781, 452190.344, 432553.219, 461902.75, 477047.438, 448309.344, 459459.594, 521892.719, 495997.719, 438766.469, 423223.031, 423788.25, 450658.719, 435579.969, 430407.812, 414236.812, 429969.719, 428566.5, 430096.5, 435521.219, 440608.875, 430369.281, 470473.156, 418505.281, 439814.062, 424358.938, 432921.125, 471586.531, 454060.344, 443025.031, 449714.938, 417968.094, 442473.438, 449459.656, 417046.5, 458670.188, 461126.062, 446624.562, 429659.906, 450543.25, 468619.531, 453504.906], 'val_loss': [10587.216, 11426.029, 10587.216, 11598.002, 10587.216, 5765.367, 4225.916, 4140.029, 7504.144, 4387.546, 5084.789, 8692.578, 4523.671, 6282.278, 4405.362, 9636.052, 5434.112, 9073.903, 3457.654, 3676.44, 4099.174, 4790.189, 3711.657, 3689.986, 3978.326, 4547.903, 3690.334, 5968.563, 3999.379, 6934.265, 3723.617, 4520.999, 4155.206, 7449.173, 4813.536, 11042.977, 3575.044, 4880.221, 4581.891, 5364.205, 9934.66, 3662.585, 4085.381, 4515.743, 4087.263, 4952.941, 3793.444, 3485.934, 8102.702, 3565.575, 4906.222, 4331.595, 4557.477, 6014.097, 4860.012, 5698.894, 5094.581, 3554.155, 3764.252, 4055.088, 4310.731, 4383.609, 4393.919, 3723.374, 3531.884, 6780.028, 4221.876, 5457.759, 3363.332, 5314.973, 4214.25, 4506.723, 3479.656, 3885.965, 5208.459, 3419.601, 4486.352, 3571.054, 3812.19, 4321.003, 3954.245, 3237.832, 5440.403, 3485.44, 3978.176, 6708.395, 4067.617, 5568.139, 4193.375, 5025.351, 3704.917, 3556.958, 3590.677, 6000.166, 4293.697, 3394.403, 3758.707, 5649.437, 6403.592, 6060.638]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:4 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:4 epochs:100	96	1000	True	34295.23438		1514800	10	-1	1006.9665403366089	{'train_loss': [483144.938, 308072.375, 301040.719, 295449.188, 290899.062, 287016.594, 282804.438, 279924.344, 276960.156, 275414.406, 273528.594, 272878.625, 271621.812, 270641.562, 269633.969, 268749.688, 267785.25, 268207.781, 266332.438, 265880.344, 265159.438, 264031.719, 263652.375, 263742.0, 263967.625, 261912.688, 260978.984, 261053.594, 260027.922, 259615.719, 258581.312, 258622.203, 257380.75, 257681.25, 256493.422, 257455.906, 257298.359, 255516.953, 255892.438, 255276.156, 254691.781, 254431.469, 253665.844, 253467.625, 253048.547, 253357.234, 253273.25, 252854.281, 251194.938, 251957.516, 250912.531, 250659.016, 249577.625, 250006.25, 248459.75, 248864.938, 248537.484, 248035.609, 248481.188, 247401.125, 247579.297, 247676.719, 245589.594, 245359.391, 245130.891, 246354.172, 245068.578, 244059.094, 245270.734, 244439.812, 243090.047, 243128.953, 243234.844, 242913.0, 243750.75, 242705.969, 242449.422, 241873.797, 240980.719, 240971.562, 241962.359, 240507.438, 238949.875, 240062.0, 239652.5, 238428.891, 239682.672, 239553.438, 238276.344, 238519.062, 238006.609, 236929.562, 236079.734, 236978.188, 236664.312, 236771.422], 'val_loss': [367.638, 359.536, 361.203, 348.065, 343.857, 341.538, 332.515, 333.524, 325.781, 324.777, 321.885, 321.887, 322.471, 319.335, 326.749, 320.094, 318.83, 315.42, 315.93, 315.912, 310.452, 317.897, 323.327, 312.124, 318.485, 315.191, 320.156, 319.78, 318.482, 311.01, 330.546, 314.249, 309.9, 312.192, 309.47, 311.286, 311.626, 308.858, 310.182, 309.271, 311.668, 307.53, 303.243, 309.979, 300.723, 304.44, 304.368, 301.085, 304.765, 304.381, 309.079, 303.469, 302.545, 302.653, 302.534, 299.483, 303.602, 304.969, 305.018, 303.331, 301.874, 302.052, 304.84, 306.735, 301.382, 306.25, 307.072, 304.254, 303.537, 302.418, 300.275, 302.935, 302.279, 301.065, 311.075, 302.287, 303.845, 310.602, 302.406, 306.523, 301.04, 300.247, 298.728, 308.582, 302.556, 300.526, 296.501, 302.161, 296.287, 299.71, 303.617, 301.574, 307.448, 299.042, 307.239, 301.871]}	96	96	False
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:111 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:rmsprop lr:0.11149453175180231 alpha:0.9856343337463634 weight_decay:8.42324359488845e-05 batch_size:32 epochs:100	100	1000	True	137268.84375		352977	10	-1	219.15535140037537	{'train_loss': [1398903.75, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1067357.875, 1957977.375, 1068862.75, 1066949.0, 1066921.0, 1194549.125, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 3090394.25, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1075554.375, 1066921.0, 1210004.0, 1066921.0, 1066921.0, 1072168.75, 1066921.0, 2122763.75, 1066921.0, 1066921.0, 1066920.125, 1066921.0, 1066921.0, 1066921.0, 2329142.25, 1067339.25, 1066921.0, 1066921.0, 1067337.25, 1066921.0, 1751326.5, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1073169.25, 1098651.375, 1100659.875, 1066921.0, 1066921.0, 1087191.375, 1205233.875, 1081558.375, 1085641.5, 1453510.375, 1077916.25, 1104097.25, 1105229.25, 1087458.625, 1421847.375, 1066921.0, 1083710.375, 1091043.875, 1085136.125, 1071524.25, 1226368.375, 1074521.375, 1091074.75, 1087391.625, 1125259.125, 1103931.5, 1090211.375, 1181656.75, 1087356.75, 1102923.375, 1121173.625, 1067118.625, 1067046.25, 1068670.0, 1080300.625, 1096289.125, 1372241.625, 1067850.25, 1079601.25, 1147944.875, 1076895.125, 1073791.0, 1086670.125, 1084119.75, 1180361.125, 1066976.625, 1094567.25, 1071032.0, 1243743.125, 1066921.0, 1068314.0, 1111196.125], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.186, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.204, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10636.846, 10587.216, 10587.216, 10587.216, 15876.415, 10587.216, 12880.527, 18282.609, 10578.307, 10587.216, 10587.216, 10586.001, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.204, 10587.216, 10587.216, 10587.199, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
