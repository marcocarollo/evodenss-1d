id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	31678.81836		577492	11	-1	209.21057987213135	{'train_loss': [687003.0, 351556.844, 314821.531, 303835.688, 297832.594, 293385.156, 286154.469, 282321.125, 278689.094, 275072.5, 272717.344, 270009.625, 267868.0, 265490.781, 263281.156, 261560.484, 259706.609, 259288.562, 257360.969, 256130.703, 254026.422, 252330.719, 252330.688, 250602.625, 249683.266, 248504.172, 247118.281, 247407.812, 245263.125, 245066.203, 244535.562, 243280.609, 243008.25, 241662.578, 241675.391, 239893.203, 240165.359, 239395.703, 238350.25, 237810.0, 237634.672, 237971.688, 236355.906, 235441.406, 236100.578, 235855.266, 234465.906, 234283.031, 233533.094, 232344.328, 234000.578, 232214.828, 231722.094, 231073.641, 231247.25, 230473.969, 231346.109, 229882.109, 230942.781, 228972.422, 229268.25, 229104.891, 228352.172, 227691.0, 227327.844, 227335.266, 227139.031, 227150.016, 226390.141, 224840.234, 225890.219, 225211.984, 224652.828, 225170.219, 224153.984, 224433.344, 223386.203, 223221.562, 222638.484, 223140.781, 222567.688, 221718.578, 221649.641, 221964.375, 221580.578, 221650.594, 220722.984, 221158.141, 220799.219, 220430.703, 220647.234, 220910.859, 219679.656, 220504.0, 219586.922, 218970.516, 218729.219, 217638.172, 219011.828, 218608.469], 'val_loss': [3404.895, 3025.627, 2930.512, 2826.988, 2751.016, 2691.561, 2671.286, 2624.836, 2597.583, 2579.698, 2549.412, 2520.301, 2520.796, 2536.719, 2527.072, 2510.025, 2484.35, 2460.662, 2490.57, 2510.191, 2461.005, 2477.543, 2423.275, 2428.294, 2433.709, 2421.532, 2406.979, 2388.199, 2387.368, 2405.613, 2382.73, 2382.135, 2354.607, 2339.857, 2356.855, 2349.928, 2333.143, 2348.189, 2318.127, 2332.099, 2318.876, 2309.75, 2307.586, 2297.62, 2300.473, 2322.73, 2295.045, 2311.857, 2280.906, 2276.705, 2286.115, 2271.02, 2269.05, 2274.166, 2253.081, 2257.93, 2255.4, 2277.864, 2290.954, 2254.354, 2260.269, 2286.738, 2260.055, 2260.592, 2281.269, 2263.275, 2256.588, 2286.123, 2247.256, 2265.946, 2261.809, 2266.351, 2257.796, 2250.555, 2254.615, 2265.491, 2242.447, 2226.776, 2268.108, 2229.156, 2252.655, 2282.696, 2259.024, 2249.104, 2249.688, 2240.935, 2239.147, 2239.811, 2233.062, 2214.894, 2236.265, 2250.602, 2253.7, 2232.077, 2214.602, 2246.0, 2229.186, 2223.16, 2253.792, 2231.311]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	137290.64062		9130205	10	-1	212.10430717468262	{'train_loss': [1172659.5, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1067350.25, 1079913.75, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	31632.29688		559964	10	-1	199.40867257118225	{'train_loss': [630232.0, 353535.969, 315661.812, 301016.938, 295119.438, 290219.219, 285701.75, 282045.688, 277946.469, 274252.031, 270193.75, 267594.469, 265655.5, 262951.062, 261637.812, 259442.531, 257137.531, 256097.969, 254287.062, 253022.734, 250876.016, 250411.531, 249812.703, 248327.406, 247099.125, 246463.922, 245837.875, 244965.578, 243920.859, 243123.406, 242826.703, 241192.391, 241509.562, 240889.906, 240168.875, 238967.359, 238585.516, 237308.719, 237544.188, 236662.562, 237107.703, 235901.344, 235858.375, 235594.984, 234698.703, 235984.375, 234434.547, 233475.312, 232835.062, 232756.031, 231736.125, 232453.969, 232693.562, 231320.953, 231638.5, 231163.047, 231149.938, 230427.359, 230230.578, 230388.688, 229361.562, 228748.078, 228489.219, 228924.734, 228277.047, 228397.969, 227737.562, 227115.688, 226756.547, 227394.0, 226057.906, 227120.672, 226954.141, 226105.078, 226998.609, 226139.516, 225924.328, 225660.266, 225765.5, 225672.25, 225100.625, 225204.891, 223883.047, 224730.016, 223673.844, 224824.141, 224524.047, 223477.625, 223932.516, 222577.188, 223196.969, 222994.469, 222795.594, 222116.0, 222307.797, 221839.328, 222378.234, 221045.422, 222093.188, 221004.984], 'val_loss': [3451.79, 2974.495, 2825.728, 2788.366, 2752.74, 2714.881, 2679.929, 2643.362, 2601.856, 2573.793, 2540.185, 2512.396, 2495.689, 2472.459, 2493.37, 2445.846, 2437.434, 2390.53, 2415.171, 2406.031, 2405.841, 2389.353, 2413.982, 2394.673, 2369.003, 2376.314, 2379.164, 2363.084, 2335.398, 2340.957, 2329.801, 2350.742, 2321.242, 2306.476, 2316.553, 2313.919, 2313.707, 2318.707, 2306.961, 2306.161, 2300.871, 2302.091, 2273.67, 2280.251, 2284.519, 2290.428, 2298.398, 2276.03, 2262.331, 2265.724, 2276.875, 2258.448, 2247.532, 2278.377, 2291.79, 2250.833, 2255.081, 2221.314, 2236.686, 2232.509, 2239.223, 2240.736, 2225.817, 2250.25, 2244.711, 2250.23, 2236.802, 2224.472, 2245.186, 2248.967, 2202.569, 2219.75, 2230.547, 2236.409, 2209.331, 2217.104, 2213.117, 2206.826, 2197.314, 2198.416, 2219.033, 2208.218, 2213.147, 2212.53, 2197.739, 2194.409, 2224.479, 2198.024, 2206.096, 2202.598, 2225.76, 2204.193, 2214.816, 2204.658, 2201.667, 2188.118, 2194.949, 2198.68, 2178.924, 2193.52]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:97 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:90 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	32794.45703		346773	11	-1	206.76819372177124	{'train_loss': [392491.25, 329761.938, 313649.781, 303787.594, 297714.594, 293623.094, 290186.812, 285902.031, 282695.5, 279758.5, 275924.5, 273187.438, 271819.969, 268261.188, 267061.344, 265400.062, 264101.094, 262101.797, 261530.953, 260686.438, 260051.359, 259122.688, 257887.844, 256730.344, 256976.484, 256324.109, 255046.312, 254246.125, 253826.031, 252766.828, 252518.031, 251560.953, 252337.422, 251146.484, 250343.25, 249792.391, 249414.359, 248613.266, 248378.141, 247736.938, 247863.547, 246794.359, 246507.766, 245897.078, 245553.141, 244693.172, 244743.766, 244047.641, 243758.656, 243479.344, 242683.469, 242102.75, 242318.344, 241846.094, 241916.641, 241075.797, 239936.812, 239423.281, 239114.125, 240072.797, 239199.438, 238778.625, 238223.141, 237573.016, 237573.281, 237027.562, 237194.828, 236710.703, 236653.516, 235625.438, 235632.172, 236108.109, 235068.688, 234487.453, 234388.531, 234597.734, 233792.859, 233649.266, 233435.438, 233070.188, 233512.094, 233060.125, 232047.141, 232484.797, 231932.188, 231732.703, 231174.391, 231395.984, 229985.172, 230271.453, 230431.141, 230072.016, 229816.172, 228883.828, 228457.531, 229094.188, 229063.125, 228849.109, 228417.016, 227622.641], 'val_loss': [3232.558, 2988.335, 2870.481, 2843.167, 2799.282, 2785.134, 2735.101, 2699.57, 2668.385, 2641.531, 2609.416, 2582.549, 2567.193, 2561.604, 2541.744, 2513.886, 2513.521, 2502.905, 2497.612, 2487.068, 2480.049, 2481.485, 2472.408, 2483.987, 2493.274, 2455.865, 2448.739, 2453.027, 2460.312, 2441.79, 2438.521, 2436.434, 2439.55, 2430.79, 2425.28, 2418.605, 2415.02, 2409.24, 2416.586, 2423.784, 2417.625, 2416.362, 2411.993, 2414.781, 2405.493, 2399.488, 2404.731, 2411.898, 2411.689, 2386.27, 2398.637, 2402.753, 2396.15, 2381.471, 2378.161, 2374.316, 2380.74, 2373.465, 2368.203, 2353.555, 2371.425, 2377.11, 2352.335, 2350.32, 2347.957, 2360.757, 2364.759, 2358.058, 2337.903, 2342.988, 2340.252, 2347.066, 2354.72, 2343.997, 2336.249, 2349.671, 2354.996, 2326.202, 2354.279, 2348.347, 2332.074, 2320.318, 2327.634, 2338.869, 2344.383, 2336.503, 2328.029, 2324.236, 2324.366, 2327.076, 2323.475, 2310.003, 2308.2, 2306.883, 2305.56, 2310.475, 2314.173, 2322.123, 2300.848, 2307.58]}	100	100	True
