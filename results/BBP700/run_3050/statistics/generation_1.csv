id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	32991.74219		452251	14	-1	238.43118453025818	{'train_loss': [427567.938, 364891.469, 322312.531, 306383.281, 297159.438, 290720.969, 284360.281, 279778.312, 275950.812, 272808.969, 271021.25, 268224.125, 266835.438, 265754.281, 264651.938, 263437.781, 262917.438, 261724.219, 260228.672, 259609.734, 258610.859, 257350.109, 257147.234, 256260.656, 255490.484, 254754.812, 254084.469, 253120.469, 252189.359, 251514.812, 251355.812, 250537.281, 249426.922, 250218.578, 249438.625, 248288.531, 248261.812, 246995.234, 246959.297, 246417.188, 246573.344, 245411.672, 245421.797, 245842.219, 244187.609, 244060.109, 243503.625, 243540.078, 242856.594, 242601.188, 241494.328, 241669.359, 240268.672, 240984.359, 240377.031, 239416.562, 240181.844, 238262.453, 238775.031, 237718.516, 238157.516, 237192.656, 237250.75, 236477.562, 236402.688, 235031.109, 235554.062, 235529.328, 235396.734, 236348.625, 234707.297, 233768.234, 233144.578, 234317.688, 234406.359, 232861.719, 232997.094, 232550.078, 232960.844, 231627.156, 230347.656, 231261.828, 231094.734, 231501.844, 230751.234, 230253.109, 230511.828, 230356.75, 230379.906, 230137.859, 229684.891, 229552.922, 229339.234, 228645.344, 228509.641, 228819.312, 228171.656, 228026.203, 228408.609, 227768.297], 'val_loss': [3693.302, 3126.019, 2975.92, 2880.581, 2842.949, 2767.693, 2695.692, 2641.718, 2609.271, 2587.953, 2596.383, 2558.632, 2574.272, 2532.332, 2499.563, 2507.731, 2502.154, 2488.195, 2512.777, 2485.13, 2460.82, 2465.757, 2463.392, 2461.703, 2458.327, 2439.981, 2448.848, 2450.681, 2433.835, 2443.657, 2432.553, 2441.797, 2443.128, 2424.198, 2464.198, 2389.698, 2426.731, 2421.838, 2409.073, 2390.178, 2402.7, 2428.144, 2381.27, 2380.275, 2424.286, 2391.809, 2353.422, 2404.24, 2400.334, 2396.151, 2372.244, 2409.878, 2401.452, 2399.577, 2377.677, 2366.162, 2340.662, 2359.34, 2347.387, 2383.81, 2354.784, 2383.191, 2363.557, 2344.333, 2351.679, 2330.828, 2327.794, 2332.754, 2303.935, 2292.901, 2316.786, 2344.382, 2304.809, 2330.093, 2295.71, 2302.052, 2288.288, 2296.261, 2289.66, 2315.638, 2273.558, 2326.559, 2284.698, 2259.819, 2311.259, 2322.946, 2279.496, 2310.034, 2286.75, 2273.556, 2261.439, 2287.675, 2275.43, 2282.545, 2286.012, 2275.46, 2268.005, 2281.278, 2301.205, 2277.616]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:23 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:64 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:54 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.18574991674851946 alpha:0.8387999593529671 weight_decay:4.620394234215013e-05 batch_size:32 epochs:100	100	1000	True	50261.17969		341634	14	-1	213.19548106193542	{'train_loss': [474885.094, 462319.531, 447120.125, 438749.281, 444828.031, 443353.938, 431536.719, 437136.969, 442905.188, 435550.844, 439934.625, 437069.562, 441101.844, 438918.375, 436555.469, 434788.781, 436273.844, 438989.438, 432105.062, 432091.062, 439582.469, 432952.719, 440762.906, 431767.344, 432369.75, 436989.656, 431734.094, 441492.781, 437291.969, 429501.844, 442529.969, 434234.875, 436019.844, 435933.719, 438134.031, 442631.812, 441252.969, 427681.906, 442312.969, 447729.406, 435886.844, 445751.281, 436473.75, 430601.844, 441738.531, 442621.312, 438848.344, 436021.5, 435073.25, 438060.25, 440078.781, 442468.281, 433462.125, 436401.75, 444847.969, 435100.969, 442579.375, 433746.531, 436266.562, 436657.625, 435382.969, 440990.969, 444279.031, 432374.469, 433536.625, 443266.125, 436280.75, 437993.438, 443882.906, 436418.438, 437530.406, 440494.094, 430963.594, 432367.031, 439429.219, 442213.75, 433859.094, 445542.875, 432865.688, 436496.0, 440758.438, 430848.531, 442460.969, 439056.75, 432296.031, 440587.062, 434130.312, 448277.75, 435198.562, 442395.031, 437836.531, 442586.469, 438419.281, 436847.688, 439854.312, 437729.031, 432934.906, 438679.312, 433076.062, 439447.094], 'val_loss': [6360.634, 5019.19, 4597.952, 3608.49, 3750.726, 3452.009, 5202.848, 3813.621, 3683.175, 4093.9, 3608.26, 3524.354, 3545.71, 3661.937, 3723.41, 3517.238, 3466.772, 3437.361, 3643.149, 3927.346, 3482.757, 4610.996, 3704.771, 3593.162, 8699.068, 3496.059, 3462.278, 3483.525, 3481.808, 4271.69, 3795.776, 3586.938, 3450.773, 7188.614, 3710.163, 4678.64, 3901.862, 9807.144, 3769.337, 3700.954, 6869.414, 3483.058, 3460.427, 3716.177, 3761.786, 3485.392, 3447.999, 3440.578, 3725.074, 3659.344, 3816.851, 4169.396, 3540.611, 3800.894, 6409.207, 4565.815, 3444.67, 3433.267, 4757.723, 3674.793, 3553.881, 6813.92, 3558.942, 6405.235, 7240.541, 3441.104, 3470.556, 3838.473, 3976.696, 3906.864, 3526.405, 4239.0, 3671.444, 3427.743, 7010.123, 3459.165, 3692.304, 7360.185, 9013.042, 5366.455, 4215.119, 4262.989, 4076.966, 7844.45, 4390.536, 4163.321, 3524.631, 3472.317, 3423.306, 3479.206, 3472.288, 3682.143, 3494.934, 3486.758, 4066.775, 7216.452, 3477.393, 3511.902, 3591.143, 3535.204]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.18658556776826268 beta1:0.803845868454246 beta2:0.9480734352117761 weight_decay:7.110145124769666e-05 batch_size:32 epochs:100	100	1000	True	137066.5		450779	14	-1	250.2105255126953	{'train_loss': [579082.062, 510780.688, 856259.0, 1067988.875, 1068930.625, 1179085.25, 1066929.0, 1069522.0, 1075608.25, 1104553.75, 1068104.75, 1068915.5, 1071722.875, 1075780.0, 1090357.625, 1074339.875, 1067848.875, 1071451.625, 1072756.875, 1082600.5, 1092202.25, 1071062.125, 1071936.375, 1075524.5, 1080317.625, 1083648.125, 1082646.625, 1078010.625, 1075382.125, 1080957.625, 1081518.0, 1084598.375, 1082079.875, 1075873.125, 1075120.375, 1073391.125, 1084542.375, 1077024.0, 1085302.5, 1078106.875, 1077042.875, 1074388.375, 1081950.5, 1077481.875, 1082724.75, 1070683.375, 1074080.625, 1078476.125, 1089301.75, 1076643.0, 1077721.25, 1074970.125, 1077959.875, 1077657.25, 1082392.125, 1080265.25, 1076375.5, 1076264.375, 1077641.625, 1077727.125, 1076785.625, 1085206.0, 1070997.0, 1079046.875, 1077400.875, 1083752.875, 1084583.375, 1077260.25, 1077031.25, 1073624.5, 1078452.375, 1079680.0, 1078086.0, 1076048.25, 1077372.5, 1073677.75, 1078520.375, 1079241.625, 1079814.875, 1073972.5, 1076223.0, 1075004.5, 1080505.0, 1078200.875, 1083789.25, 1076228.125, 1080709.125, 1077610.125, 1079627.375, 1080244.25, 1074513.25, 1078327.75, 1074404.875, 1079424.25, 1086929.0, 1077045.125, 1080350.375, 1078353.25, 1077564.25, 1077184.875], 'val_loss': [4775.572, 3500.36, 10587.216, 10587.216, 10587.216, 10587.216, 10586.715, 10587.216, 11155.145, 10587.216, 10587.216, 10587.216, 10587.216, 10597.506, 10904.573, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10967.785, 10582.094, 10587.216, 11850.725, 11031.361, 10833.433, 10587.216, 10719.373, 11180.333, 12965.74, 10587.216, 10587.216, 10587.135, 10587.216, 10587.216, 10750.595, 10587.216, 10967.386, 10587.216, 10587.216, 10587.216, 10819.749, 10587.216, 10587.212, 10587.216, 10586.004, 10997.551, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 11011.289, 10587.216, 10587.216, 10587.216, 10587.216, 10775.866, 10577.724, 10792.683, 10583.213, 10587.216, 10587.216, 10587.216, 10599.082, 10587.216, 10587.216, 10587.216, 10586.9, 10587.216, 10587.216, 10587.209, 10587.216, 10587.216, 10587.216, 10587.216, 10747.343, 10587.216, 11209.959, 10587.216, 12958.792, 10587.216, 10822.552, 10587.216, 10587.212, 11026.584, 10586.97, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10569.656]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:53 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	32633.90625		490746	14	-1	248.94295144081116	{'train_loss': [492826.719, 393612.219, 339919.656, 317182.188, 309692.938, 305634.188, 301756.156, 296075.781, 289513.688, 284348.25, 279151.812, 276636.562, 273085.688, 270232.781, 268527.656, 266569.031, 265442.094, 263950.688, 261436.031, 261081.219, 260225.609, 258643.328, 257436.219, 256941.797, 255875.531, 255452.562, 254076.828, 253873.203, 252144.219, 251810.625, 252159.031, 251447.641, 249496.688, 249891.453, 248923.297, 249487.766, 247982.391, 246255.203, 246488.719, 246508.562, 246521.828, 245117.328, 244501.469, 244724.734, 244671.922, 243688.5, 243317.422, 243183.266, 241455.453, 241990.234, 241718.578, 240751.875, 240525.141, 240752.906, 240300.391, 239431.031, 239370.281, 239376.141, 239592.812, 238188.094, 238181.188, 238170.438, 238442.25, 238206.703, 237511.297, 237008.5, 237308.906, 236784.422, 236517.297, 236886.703, 236034.656, 235806.359, 234745.516, 234900.609, 234176.891, 233837.75, 234376.609, 233912.734, 233797.375, 234274.469, 232934.406, 233072.938, 233017.047, 232674.641, 232577.844, 231575.812, 231536.953, 231698.625, 231235.984, 230919.375, 230680.234, 231008.797, 230285.125, 230232.656, 229903.047, 229378.25, 228716.672, 228764.094, 228473.828, 228417.172], 'val_loss': [3723.564, 3322.004, 2999.864, 2908.591, 2874.685, 2850.91, 2817.779, 2750.436, 2710.897, 2688.402, 2637.447, 2615.039, 2618.194, 2600.118, 2572.708, 2554.781, 2559.341, 2549.224, 2525.54, 2503.11, 2493.51, 2531.226, 2519.105, 2490.955, 2491.737, 2485.247, 2485.475, 2502.093, 2458.96, 2463.038, 2469.661, 2446.619, 2462.885, 2446.363, 2429.782, 2445.04, 2438.667, 2445.438, 2436.535, 2426.385, 2422.941, 2410.34, 2410.471, 2409.108, 2416.391, 2418.003, 2403.685, 2428.948, 2396.901, 2403.99, 2414.593, 2394.757, 2408.108, 2388.118, 2393.846, 2408.889, 2388.801, 2394.013, 2398.824, 2408.144, 2389.049, 2401.111, 2386.879, 2378.514, 2398.508, 2380.854, 2377.675, 2384.09, 2382.499, 2372.363, 2367.413, 2357.901, 2370.475, 2366.52, 2354.949, 2374.154, 2381.256, 2362.273, 2373.379, 2368.979, 2375.415, 2371.547, 2337.469, 2348.968, 2349.205, 2351.565, 2361.634, 2339.817, 2338.996, 2342.632, 2343.562, 2334.535, 2340.918, 2343.838, 2332.011, 2343.035, 2324.771, 2341.248, 2340.504, 2339.129]}	100	100	True
