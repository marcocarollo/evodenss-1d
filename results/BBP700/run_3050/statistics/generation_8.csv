id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:108 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	31978.55664		363673	12	-1	238.0827991962433	{'train_loss': [420263.219, 369208.969, 330229.688, 316932.781, 310335.0, 305922.281, 302442.938, 299540.25, 296941.938, 295529.656, 293626.219, 291954.031, 290489.062, 288377.656, 284705.438, 279883.5, 276840.5, 273292.594, 269537.375, 266362.75, 264910.594, 262724.719, 262119.672, 259941.016, 259387.609, 256638.094, 254707.766, 254918.859, 252766.844, 252519.344, 250831.469, 250793.719, 248664.531, 247330.172, 247886.734, 246406.609, 245564.75, 243658.875, 243623.188, 243470.469, 241207.766, 241075.641, 241049.906, 240492.375, 239088.812, 238514.672, 237964.625, 238265.016, 237309.609, 236383.375, 235261.531, 236274.312, 234357.188, 234250.469, 234177.234, 234597.922, 233138.266, 233047.547, 232536.016, 233290.562, 231865.75, 232269.75, 231683.969, 231328.188, 230183.984, 229834.828, 230046.891, 230403.531, 229211.234, 228257.125, 227373.953, 227690.75, 228886.047, 227937.594, 227530.922, 227621.625, 227276.906, 226867.078, 225520.109, 225934.875, 225908.484, 225852.875, 225600.172, 224763.984, 224848.625, 224306.484, 224571.906, 223428.031, 224068.328, 224155.594, 224047.453, 223519.75, 224441.297, 222688.609, 223634.891, 222781.5, 222358.594, 222272.516, 222103.766, 222466.609], 'val_loss': [4201.147, 3110.03, 2950.905, 2915.497, 2891.22, 2860.051, 2836.831, 2816.851, 2799.396, 2790.886, 2790.976, 2779.913, 2768.368, 2742.738, 2709.258, 2650.354, 2628.125, 2585.678, 2613.015, 2557.52, 2540.494, 2526.634, 2533.126, 2513.472, 2507.277, 2491.887, 2467.545, 2463.233, 2461.229, 2461.708, 2437.811, 2411.737, 2396.211, 2409.202, 2403.823, 2367.062, 2388.679, 2385.562, 2389.512, 2355.166, 2343.451, 2346.45, 2339.176, 2318.998, 2299.606, 2307.854, 2329.499, 2319.438, 2306.736, 2310.348, 2312.765, 2305.341, 2279.982, 2286.792, 2259.89, 2260.656, 2280.381, 2302.932, 2295.033, 2245.785, 2289.234, 2234.829, 2263.879, 2244.944, 2246.504, 2247.335, 2264.481, 2234.102, 2251.866, 2247.015, 2233.853, 2249.347, 2226.943, 2231.924, 2225.106, 2251.647, 2226.962, 2235.163, 2217.091, 2234.429, 2221.298, 2233.853, 2223.263, 2218.362, 2223.151, 2228.918, 2213.836, 2216.361, 2218.528, 2224.465, 2224.339, 2221.04, 2200.161, 2225.585, 2227.35, 2222.175, 2221.827, 2212.014, 2217.365, 2216.923]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:60 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:108 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:111 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:7 epochs:100	100	1000	True	32530.02539		367367	13	-1	711.9116082191467	{'train_loss': [369732.469, 317794.438, 308512.969, 304660.562, 301770.656, 299827.094, 298388.594, 296854.625, 295003.812, 293117.031, 289687.438, 286951.344, 283481.594, 281763.5, 279435.75, 277794.469, 275216.0, 272958.938, 271132.75, 269894.656, 268106.781, 266571.625, 266391.344, 264940.781, 263694.0, 262535.875, 260964.109, 262167.969, 259868.703, 259397.734, 259465.703, 257855.453, 257686.281, 257570.781, 256095.844, 255298.391, 254214.719, 253668.969, 252915.047, 253013.688, 252348.547, 251756.688, 250437.797, 251285.516, 250125.969, 249910.953, 249148.594, 248901.438, 248017.219, 247451.594, 247466.125, 246821.234, 246564.219, 245812.141, 246271.344, 245270.391, 243921.531, 244722.797, 244035.078, 243807.188, 244140.25, 243401.531, 242034.625, 241968.516, 241842.938, 240790.266, 241868.562, 240362.438, 240232.625, 239927.031, 239503.719, 239026.797, 239741.734, 238910.016, 239297.906, 238230.047, 237474.297, 237691.656, 237919.938, 236073.406, 237409.203, 235533.625, 236599.453, 236361.297, 235679.062, 235314.047, 234187.219, 234416.109, 234159.969, 234302.766, 234147.906, 232550.969, 232292.125, 232474.062, 232539.141, 233119.531, 231957.406, 231996.375, 232062.219, 230671.953], 'val_loss': [665.419, 633.763, 626.613, 622.755, 618.067, 617.455, 617.116, 615.31, 611.469, 603.729, 597.29, 586.435, 581.33, 573.918, 569.922, 568.564, 567.11, 565.742, 555.569, 547.319, 543.654, 546.779, 545.025, 540.869, 538.648, 537.567, 537.843, 532.532, 534.893, 530.199, 528.996, 528.871, 528.477, 525.98, 530.548, 531.96, 526.222, 527.661, 526.28, 525.617, 526.808, 527.378, 521.425, 517.198, 517.297, 519.529, 517.981, 517.144, 523.062, 514.124, 506.706, 517.203, 512.48, 514.119, 511.421, 507.151, 511.704, 518.097, 524.239, 515.709, 508.09, 513.666, 506.495, 510.408, 503.52, 505.349, 508.508, 503.183, 503.397, 508.58, 515.534, 507.732, 500.367, 503.89, 511.512, 504.548, 501.226, 506.942, 494.294, 496.933, 503.993, 501.025, 500.499, 496.049, 500.311, 502.507, 497.2, 498.534, 491.804, 493.479, 502.634, 495.039, 499.066, 495.925, 490.046, 499.536, 499.983, 497.24, 495.822, 493.082]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:108 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	2000	True	32038.90234		363673	12	-1	242.6612937450409	{'train_loss': [415676.562, 357830.844, 326753.469, 316356.5, 309963.125, 301310.594, 294853.594, 289205.0, 284153.812, 279939.5, 275714.375, 272245.844, 269729.406, 266884.094, 265214.75, 262436.188, 261358.359, 259410.078, 257702.578, 257091.391, 255265.359, 254537.078, 254033.281, 252220.797, 251120.953, 249703.734, 249385.219, 249016.938, 248819.219, 246008.812, 245607.203, 244554.141, 244717.234, 243498.656, 242416.438, 241719.75, 241634.344, 240648.062, 239694.5, 240246.625, 239576.328, 237616.281, 238461.047, 237894.422, 237696.797, 237306.25, 236324.469, 236345.719, 235353.859, 234659.922, 234197.469, 234784.438, 233445.219, 233667.094, 232644.219, 232234.703, 231521.188, 232224.625, 231850.484, 231618.891, 230466.297, 230360.656, 230404.281, 230097.172, 230020.984, 229851.531, 229700.859, 228879.484, 228472.469, 228465.297, 228042.594, 227305.766, 227252.828, 227040.297, 227253.156, 227445.422, 226219.172, 226844.016, 225592.328, 225886.016, 226073.875, 225263.547, 224272.609, 225170.844, 224543.109, 224584.094, 224323.0, 223542.266, 223887.156, 223009.766, 223463.938, 223634.406, 223114.562, 222737.891, 221738.156, 222203.375, 222080.578, 222042.156, 222713.766, 221774.516], 'val_loss': [3485.834, 3053.068, 2927.094, 2905.964, 2838.528, 2792.611, 2754.166, 2713.947, 2677.199, 2644.241, 2610.292, 2591.805, 2574.15, 2569.302, 2514.502, 2482.575, 2495.572, 2470.806, 2464.655, 2459.704, 2477.466, 2460.406, 2430.469, 2421.841, 2459.612, 2427.333, 2393.479, 2438.057, 2422.928, 2391.846, 2393.609, 2388.335, 2390.828, 2392.863, 2377.909, 2365.412, 2388.35, 2350.966, 2339.367, 2343.825, 2335.799, 2354.186, 2329.946, 2314.445, 2333.198, 2307.892, 2297.266, 2301.241, 2302.12, 2298.426, 2289.074, 2304.478, 2297.478, 2299.279, 2303.421, 2275.496, 2293.613, 2287.797, 2295.684, 2279.212, 2265.615, 2288.24, 2294.222, 2281.524, 2273.256, 2287.424, 2294.367, 2288.423, 2275.143, 2282.117, 2275.663, 2259.419, 2266.062, 2245.798, 2276.367, 2257.051, 2272.355, 2278.527, 2257.181, 2255.689, 2245.948, 2253.62, 2247.884, 2260.586, 2271.418, 2250.833, 2236.246, 2241.24, 2220.192, 2237.919, 2260.53, 2260.219, 2236.962, 2250.258, 2242.931, 2229.493, 2239.782, 2217.182, 2242.393, 2245.574]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:108 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:84 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:35 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:conv1d out_channels:118 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	33282.38672		1503381	14	-1	276.00301790237427	{'train_loss': [1300610.125, 1100608.125, 1093467.0, 1095511.0, 1097031.375, 1090998.0, 1077915.75, 458070.719, 325059.5, 313064.094, 306927.0, 301873.844, 297311.281, 293553.094, 290428.0, 288403.156, 284212.469, 281943.625, 279658.812, 277190.062, 275760.719, 275687.188, 274008.094, 272154.688, 271357.406, 269858.562, 268713.062, 268908.688, 267954.531, 265954.969, 264828.531, 264641.0, 262710.219, 262311.75, 261982.203, 260783.828, 259177.484, 259272.547, 257543.047, 257861.344, 256343.75, 256090.453, 254421.141, 254948.188, 253409.5, 251985.844, 252130.25, 252536.609, 250704.719, 251277.312, 250387.766, 250317.0, 248893.062, 249468.656, 248731.938, 246918.734, 246222.219, 247922.953, 246322.156, 246763.688, 245084.594, 245370.062, 244631.016, 244113.641, 243616.812, 243855.297, 243374.656, 242911.031, 244092.25, 241028.141, 242307.625, 242974.375, 241432.344, 239692.484, 242405.453, 240862.031, 239513.484, 239932.547, 239017.078, 241298.031, 241130.547, 239275.656, 240994.297, 237502.547, 237431.922, 238021.781, 237843.875, 238448.828, 238317.625, 237850.281, 237479.922, 237947.344, 237300.562, 237519.531, 235465.625, 236252.594, 236836.781, 235864.859, 235073.078, 235903.109], 'val_loss': [10586.536, 10586.644, 10587.15, 10890.351, 11531.73, 10660.293, 7897.755, 3007.189, 2903.463, 2845.831, 2831.88, 2806.478, 2767.548, 2734.648, 2706.858, 2682.244, 2657.238, 2630.24, 2615.86, 2598.898, 2609.204, 2590.824, 2579.279, 2559.717, 2566.773, 2568.3, 2554.407, 2553.488, 2536.223, 2558.326, 2528.63, 2538.095, 2521.52, 2514.046, 2485.228, 2494.3, 2477.752, 2454.367, 2472.596, 2472.97, 2462.982, 2440.801, 2460.694, 2433.069, 2429.107, 2442.501, 2403.688, 2435.584, 2444.896, 2424.814, 2418.295, 2429.211, 2428.422, 2451.146, 2383.632, 2468.324, 2429.164, 2398.118, 2458.451, 2410.723, 2415.948, 2409.013, 2415.74, 2454.331, 2408.809, 2453.076, 2387.549, 2428.635, 2412.736, 2452.198, 2438.313, 2421.707, 2436.023, 2427.972, 2403.75, 2406.692, 2413.308, 2400.197, 2410.262, 2418.502, 2384.521, 2363.573, 2402.635, 2377.706, 2421.596, 2426.148, 2333.185, 2376.915, 2375.494, 2356.747, 2352.015, 2372.661, 2394.177, 2376.942, 2367.309, 2338.892, 2380.258, 2435.606, 2331.582, 2399.898]}	100	100	True
