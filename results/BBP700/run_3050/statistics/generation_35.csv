id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	31417.20898		469896	10	-1	207.08464121818542	{'train_loss': [597079.562, 361972.219, 319825.5, 305783.312, 300367.219, 295997.406, 291521.719, 287002.781, 281912.594, 276965.656, 271981.469, 268203.062, 265084.781, 262525.25, 260829.453, 259381.828, 256596.094, 254864.984, 254387.578, 253527.641, 252381.547, 250913.969, 249991.75, 248944.859, 248508.062, 247587.094, 246499.641, 244623.234, 245167.25, 243588.812, 243396.953, 242635.719, 241015.812, 241013.078, 240739.938, 240253.109, 238250.203, 237621.969, 237813.078, 236877.422, 236116.859, 236362.469, 235367.844, 234554.891, 233982.344, 233456.203, 233634.391, 232610.906, 231996.453, 232347.375, 231937.281, 231187.719, 229460.141, 229389.875, 229725.25, 228524.984, 229698.844, 228408.797, 228019.797, 227534.953, 227052.922, 226985.219, 226214.922, 225738.406, 225486.984, 224820.203, 224928.781, 223854.562, 224036.766, 224034.0, 222802.922, 222078.609, 222318.281, 222817.766, 221965.719, 220990.859, 221347.641, 220803.375, 221346.266, 220024.703, 220832.438, 219373.359, 218953.062, 219646.969, 219287.875, 218864.672, 218167.078, 218462.75, 217503.0, 216806.672, 217308.297, 216826.734, 216884.0, 216238.938, 215763.203, 215501.391, 215938.125, 215308.953, 215465.703, 215230.25], 'val_loss': [3802.98, 3034.485, 2903.911, 2852.057, 2826.208, 2798.56, 2757.567, 2705.771, 2648.659, 2611.698, 2576.804, 2546.136, 2528.892, 2519.587, 2509.033, 2489.713, 2472.183, 2474.603, 2444.352, 2436.546, 2420.903, 2413.822, 2429.572, 2412.258, 2427.696, 2392.635, 2382.489, 2381.189, 2364.691, 2365.332, 2362.957, 2360.236, 2346.143, 2350.642, 2333.069, 2336.867, 2339.354, 2334.02, 2333.502, 2317.194, 2309.511, 2293.841, 2315.01, 2291.35, 2313.883, 2311.979, 2294.115, 2280.749, 2269.695, 2277.069, 2257.332, 2265.92, 2260.121, 2242.647, 2257.342, 2246.291, 2226.568, 2261.656, 2226.9, 2234.953, 2246.898, 2239.369, 2221.148, 2229.49, 2219.726, 2216.554, 2221.664, 2209.232, 2235.052, 2211.401, 2206.719, 2221.043, 2214.157, 2190.045, 2192.846, 2198.984, 2185.587, 2186.312, 2182.62, 2181.736, 2181.241, 2173.519, 2184.095, 2169.975, 2176.395, 2179.12, 2155.219, 2172.6, 2164.69, 2166.895, 2165.372, 2155.768, 2161.545, 2155.192, 2141.969, 2153.194, 2170.204, 2149.627, 2156.817, 2146.426]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:32 epochs:100	100	1000	True	32692.75391		301568	9	-1	189.8261091709137	{'train_loss': [418752.656, 328324.031, 308214.031, 300807.188, 296459.844, 293430.812, 288976.531, 284831.75, 280831.844, 277475.906, 275018.469, 272732.188, 270325.531, 269164.562, 266038.75, 263648.188, 263874.25, 261965.312, 261001.656, 259890.422, 258331.891, 258484.188, 257235.812, 255592.891, 255339.094, 255572.594, 254255.203, 253389.797, 252926.156, 252480.531, 252166.703, 250585.797, 250247.281, 250418.422, 250129.5, 249208.969, 248013.609, 248614.812, 247638.922, 247758.891, 247393.172, 246526.516, 246543.875, 245725.859, 245374.734, 246504.062, 244240.859, 245069.891, 244247.312, 243830.844, 244229.0, 243353.25, 243908.844, 242419.453, 242303.031, 242934.609, 242243.047, 241922.031, 241522.188, 241250.734, 241472.094, 241583.141, 240067.453, 240121.547, 239528.141, 239324.391, 239985.281, 239917.391, 239419.891, 239567.578, 239978.375, 238536.547, 238910.234, 238469.922, 237891.156, 238472.938, 237124.938, 238390.859, 237595.125, 237547.328, 236743.062, 237520.391, 237054.297, 236098.312, 236105.031, 236170.438, 236504.172, 235688.828, 235925.562, 236137.984, 236150.0, 236156.984, 235057.609, 235298.375, 235328.438, 234894.266, 233959.125, 234323.328, 234357.875, 234535.219], 'val_loss': [3180.028, 2942.999, 2854.053, 2828.835, 2798.005, 2743.666, 2706.494, 2660.675, 2641.646, 2616.029, 2576.463, 2564.485, 2537.752, 2521.006, 2500.112, 2491.046, 2470.934, 2478.126, 2470.643, 2466.528, 2452.901, 2452.113, 2437.035, 2445.814, 2437.734, 2446.424, 2423.778, 2431.531, 2424.016, 2429.419, 2415.387, 2428.001, 2417.529, 2419.594, 2396.99, 2416.065, 2388.633, 2383.761, 2384.614, 2379.925, 2398.383, 2391.955, 2388.186, 2374.65, 2360.658, 2357.817, 2354.196, 2352.154, 2339.976, 2341.667, 2339.804, 2335.785, 2324.008, 2329.126, 2327.612, 2323.977, 2335.772, 2334.878, 2332.004, 2322.142, 2305.482, 2338.122, 2312.693, 2305.628, 2314.197, 2306.755, 2300.064, 2293.736, 2303.831, 2319.44, 2300.872, 2295.615, 2293.111, 2284.872, 2289.532, 2285.926, 2295.91, 2281.409, 2293.368, 2292.605, 2297.473, 2314.469, 2273.418, 2289.481, 2286.784, 2287.674, 2281.489, 2284.031, 2269.208, 2295.29, 2285.157, 2254.131, 2283.727, 2270.076, 2284.052, 2269.631, 2256.733, 2263.495, 2266.739, 2268.693]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	2000	True	31822.22266		469896	10	-1	199.77088236808777	{'train_loss': [523365.844, 345988.875, 318065.781, 304533.125, 296805.0, 289312.906, 283294.594, 277834.781, 274795.312, 271486.469, 269495.562, 267287.875, 265477.0, 263565.938, 262445.969, 260756.438, 259941.641, 258420.344, 257657.078, 255972.312, 254848.625, 254710.188, 253735.219, 253213.5, 251712.375, 250762.031, 250574.781, 250073.516, 249384.469, 247604.781, 247351.938, 246817.016, 244976.672, 245443.453, 244555.5, 244719.688, 243825.406, 243286.047, 243509.062, 242345.922, 242213.328, 242417.734, 241763.719, 240434.484, 240736.016, 240139.422, 239960.0, 239349.406, 239553.938, 238967.891, 238438.391, 238492.594, 238997.062, 237447.766, 237885.141, 237473.938, 235399.953, 235697.109, 235853.734, 234238.078, 234243.906, 235652.781, 234376.938, 234920.703, 234148.266, 234034.672, 233704.297, 233548.688, 232792.25, 232318.641, 231969.812, 232069.922, 232016.781, 232041.141, 232146.5, 231729.562, 232234.062, 231010.266, 231579.047, 231722.531, 231092.047, 230468.25, 229971.469, 229858.672, 229638.672, 229906.172, 229883.156, 228910.141, 229597.594, 229430.141, 228451.234, 227566.703, 228470.438, 228521.688, 228179.656, 228216.297, 228073.031, 227800.859, 227247.547, 227552.719], 'val_loss': [3252.493, 2956.437, 2838.758, 2789.199, 2725.408, 2670.364, 2631.293, 2590.025, 2569.851, 2523.282, 2528.72, 2494.677, 2464.102, 2478.313, 2479.7, 2456.774, 2446.169, 2439.791, 2428.781, 2423.724, 2403.513, 2415.615, 2390.617, 2398.637, 2396.646, 2401.506, 2368.79, 2371.811, 2350.396, 2347.263, 2361.061, 2352.328, 2325.331, 2348.877, 2314.685, 2329.8, 2324.26, 2319.742, 2319.792, 2326.914, 2331.921, 2301.485, 2342.972, 2319.665, 2323.849, 2293.857, 2291.725, 2299.736, 2330.817, 2296.319, 2298.334, 2288.316, 2308.479, 2284.909, 2292.466, 2269.762, 2269.871, 2281.79, 2266.066, 2265.676, 2258.16, 2270.258, 2259.326, 2247.969, 2258.952, 2276.354, 2247.096, 2279.447, 2241.382, 2259.667, 2245.569, 2249.952, 2246.362, 2247.408, 2238.511, 2240.149, 2245.993, 2241.545, 2228.766, 2238.611, 2237.951, 2222.567, 2236.73, 2254.982, 2226.156, 2218.318, 2236.05, 2210.801, 2228.782, 2231.074, 2229.392, 2220.975, 2220.364, 2208.179, 2220.511, 2200.187, 2220.453, 2200.399, 2214.741, 2210.224]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:109 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:55 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:43 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	137281.82812		6099986	11	-1	220.89334750175476	{'train_loss': [1158261.625, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1300188.75, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1428373.875, 1068495.625, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10586.952, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
