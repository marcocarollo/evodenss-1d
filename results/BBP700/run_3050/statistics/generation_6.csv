id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	31979.93164		463085	13	-1	250.69010257720947	{'train_loss': [421856.625, 384736.219, 346138.719, 321966.875, 312279.625, 306785.125, 303696.75, 301181.0, 299052.719, 296890.094, 294741.656, 292921.406, 291741.906, 290243.781, 288898.688, 287792.438, 286184.0, 283301.281, 278245.938, 273951.844, 269400.438, 267218.719, 264933.469, 262570.25, 260790.516, 258502.594, 257206.328, 256774.484, 254487.828, 252661.797, 252047.656, 250705.438, 249569.516, 247547.688, 247045.094, 245634.312, 245037.5, 245149.844, 243873.359, 243278.094, 242832.172, 242274.188, 241814.109, 240672.781, 240976.078, 239729.797, 238798.906, 238636.766, 237629.156, 236897.781, 236435.844, 236523.281, 235859.312, 235749.703, 235986.984, 235214.047, 234675.906, 234645.031, 233721.172, 233512.469, 232064.531, 233122.547, 232612.828, 232530.469, 232510.719, 231291.625, 231544.625, 230962.594, 230400.453, 229258.719, 230390.422, 230141.469, 229623.953, 229324.359, 229438.469, 228529.391, 228864.0, 228277.25, 228819.469, 227945.125, 228415.312, 227825.391, 227481.812, 226430.703, 227046.797, 226058.266, 226537.484, 225849.125, 225637.609, 226480.891, 225014.469, 225552.953, 225643.656, 225533.688, 224350.719, 224045.516, 224083.188, 223880.438, 223975.75, 224467.281], 'val_loss': [3560.581, 3602.069, 3010.917, 2917.54, 2887.493, 2867.169, 2846.357, 2838.658, 2826.096, 2832.755, 2809.25, 2796.344, 2788.878, 2777.38, 2758.76, 2774.652, 2755.112, 2717.499, 2658.936, 2627.917, 2566.035, 2528.466, 2520.758, 2495.708, 2482.427, 2471.302, 2469.947, 2447.546, 2421.147, 2429.438, 2414.307, 2393.603, 2389.99, 2392.125, 2370.928, 2359.194, 2382.262, 2367.523, 2361.449, 2357.236, 2340.907, 2367.471, 2371.318, 2369.613, 2333.221, 2340.473, 2352.665, 2342.424, 2326.305, 2324.093, 2333.068, 2343.979, 2307.312, 2305.81, 2319.31, 2308.689, 2314.592, 2323.463, 2330.998, 2306.856, 2313.129, 2328.087, 2290.765, 2297.838, 2313.71, 2307.59, 2321.231, 2299.832, 2300.546, 2317.173, 2295.256, 2290.746, 2290.955, 2279.087, 2291.054, 2282.012, 2307.152, 2294.669, 2279.582, 2286.64, 2278.697, 2292.336, 2286.806, 2278.605, 2278.996, 2289.47, 2294.533, 2281.941, 2265.502, 2269.101, 2258.479, 2254.702, 2267.548, 2247.321, 2272.615, 2252.57, 2249.411, 2243.171, 2268.479, 2247.612]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:57 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:80 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:99 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	34145.32422		2388641	13	-1	245.339941740036	{'train_loss': [1135901.125, 1084032.75, 1097131.25, 1067080.375, 1132137.375, 1105820.75, 1068258.875, 1069540.25, 1069395.5, 1070637.0, 1080817.625, 1071711.0, 1081033.875, 1077934.25, 1009603.938, 382491.094, 319846.312, 311040.156, 306361.688, 303028.594, 299366.25, 295667.938, 292675.906, 289703.5, 286372.844, 284868.844, 282186.406, 280201.062, 278023.25, 276308.188, 274851.406, 274263.5, 271522.656, 272791.719, 270189.031, 269044.375, 269165.75, 268024.844, 266077.062, 265643.062, 264742.625, 262266.781, 263171.531, 261698.906, 261624.5, 261643.938, 259343.438, 259688.969, 258957.781, 257874.844, 258861.672, 257596.203, 257704.625, 255559.594, 255668.672, 255481.016, 255266.516, 253747.359, 253375.234, 253458.719, 252817.234, 252154.094, 251929.547, 251227.641, 251409.0, 250290.781, 250163.578, 249443.188, 250037.641, 249680.516, 247264.453, 248080.641, 248474.047, 247080.875, 248637.938, 246676.859, 246428.891, 246084.031, 246134.922, 245744.344, 245258.656, 244595.688, 243425.922, 243480.078, 243107.5, 243537.969, 242138.406, 242710.875, 241953.859, 241752.766, 240913.484, 241466.906, 242029.172, 240058.656, 239274.078, 239021.219, 238466.062, 239503.891, 238859.562, 239064.516], 'val_loss': [12540.295, 11158.768, 10587.212, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.215, 11450.688, 10910.394, 10961.852, 10587.204, 10968.855, 4180.58, 2999.796, 2848.99, 2830.965, 2811.676, 2789.665, 2761.245, 2736.638, 2712.033, 2690.616, 2666.454, 2666.559, 2644.074, 2655.272, 2629.028, 2653.063, 2614.034, 2617.169, 2636.623, 2609.706, 2624.775, 2608.555, 2589.81, 2597.286, 2584.584, 2560.881, 2560.338, 2579.458, 2572.815, 2573.55, 2569.149, 2592.132, 2555.619, 2605.727, 2534.12, 2530.644, 2551.24, 2539.038, 2522.312, 2520.654, 2523.135, 2563.142, 2544.78, 2561.754, 2557.066, 2537.856, 2543.249, 2506.148, 2502.567, 2515.206, 2521.165, 2514.511, 2500.347, 2524.509, 2499.079, 2512.988, 2534.7, 2503.536, 2494.173, 2507.788, 2496.164, 2532.078, 2512.324, 2492.318, 2515.843, 2489.485, 2526.604, 2478.978, 2484.093, 2517.183, 2532.227, 2499.752, 2505.584, 2509.484, 2466.692, 2481.328, 2517.394, 2530.347, 2494.435, 2474.035, 2513.175, 2481.597, 2525.216, 2510.463, 2493.429, 2498.831]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adam lr:0.10217760825773266 beta1:0.9521745410342363 beta2:0.8573155888145998 weight_decay:8.032843957768642e-05 batch_size:32 epochs:100	100	1000	True	51761.35938		407261	12	-1	230.57676434516907	{'train_loss': [437856.438, 367908.75, 372079.0, 377271.062, 393137.688, 373627.0, 372431.188, 375305.344, 375524.438, 373938.156, 374298.219, 377259.312, 375604.719, 373891.688, 370546.156, 367899.406, 375715.594, 374606.094, 379221.125, 371319.719, 372371.75, 370025.094, 373763.438, 367307.781, 366625.031, 377535.062, 368214.438, 375169.656, 377188.906, 367316.594, 373207.312, 371069.25, 369519.531, 374442.469, 379136.531, 371962.0, 374426.219, 370680.375, 379143.125, 372085.25, 371951.312, 368810.938, 369082.219, 378995.375, 374809.375, 374195.062, 376723.406, 378745.125, 372152.156, 369907.938, 368888.875, 372119.969, 372797.594, 371218.906, 370372.188, 368767.125, 372841.0, 371789.562, 372136.562, 372193.719, 368961.969, 379598.75, 383458.531, 368432.438, 370692.281, 369717.375, 364881.469, 372835.375, 375369.875, 371999.75, 368664.062, 368526.0, 374372.156, 369780.75, 368782.0, 370956.969, 370897.031, 369259.0, 369757.312, 371119.5, 374661.656, 373610.125, 371124.344, 369132.75, 372182.562, 368799.688, 374975.031, 369593.031, 366678.531, 368691.562, 366126.906, 371622.281, 379637.094, 374083.156, 373042.312, 375568.562, 371012.875, 373787.312, 367356.938, 374229.594], 'val_loss': [3717.406, 3465.49, 3417.0, 4335.222, 3496.899, 3438.076, 3906.534, 3446.65, 3782.58, 3579.151, 3845.805, 3583.178, 3635.726, 3561.602, 3397.263, 3755.173, 3626.199, 4214.095, 3489.985, 3468.13, 3627.25, 3515.218, 3468.968, 3411.485, 3675.311, 3486.243, 3725.102, 4105.19, 3542.765, 3706.806, 3468.727, 3456.205, 3519.771, 3493.496, 3507.35, 3598.239, 3530.606, 3722.273, 3456.966, 3444.393, 3418.331, 3540.38, 3753.128, 3490.736, 3613.607, 3671.735, 3463.517, 3776.029, 3643.029, 3428.845, 3484.296, 3723.729, 3503.92, 3701.693, 3473.158, 3556.836, 3588.858, 3484.751, 3424.011, 3505.945, 3540.29, 3705.438, 3460.66, 3447.436, 3423.427, 3506.093, 3498.604, 3904.701, 3581.318, 3431.451, 3658.382, 3461.544, 3416.325, 3495.839, 3457.481, 3576.072, 3553.343, 3429.907, 3558.488, 3553.705, 3829.728, 3565.96, 3424.099, 3500.557, 3442.191, 3551.157, 3742.439, 3455.534, 3458.523, 3476.955, 3446.283, 3473.421, 3755.237, 3659.73, 3803.569, 3416.934, 3562.178, 3469.56, 3760.574, 3665.695]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:108 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	31694.01172		363673	12	-1	232.90661478042603	{'train_loss': [416510.5, 363667.438, 326902.531, 315740.594, 310483.312, 306827.75, 303803.031, 300274.5, 293112.375, 285463.219, 279541.75, 275781.219, 272338.562, 269448.438, 267499.375, 265057.781, 264410.375, 261978.672, 260928.016, 259055.188, 257816.672, 257375.797, 255896.797, 254728.25, 254541.125, 252720.844, 251913.375, 250759.047, 250449.625, 249519.828, 249462.203, 248521.188, 248051.359, 245697.203, 245177.484, 244320.031, 243075.188, 242830.938, 241307.594, 241520.312, 240640.25, 239381.469, 238445.094, 238018.703, 237346.828, 237660.094, 236217.672, 235926.703, 235797.422, 235553.797, 234636.766, 233756.047, 234310.281, 233675.547, 233249.391, 232120.562, 233233.359, 231161.656, 231698.641, 230836.297, 229857.391, 229517.469, 230229.578, 229684.141, 229410.281, 228576.406, 228610.469, 227892.109, 227806.891, 227868.719, 227518.516, 227345.906, 226401.25, 226608.141, 226267.234, 226218.719, 225481.672, 225904.469, 225717.141, 224689.781, 225006.328, 224433.188, 225502.859, 224024.0, 224477.781, 223789.594, 223713.844, 223104.562, 223257.312, 222383.25, 222892.188, 222139.75, 222985.703, 222581.625, 222152.234, 221787.344, 221976.25, 221671.109, 221548.578, 221255.438], 'val_loss': [3574.071, 3079.693, 2936.844, 2912.75, 2893.606, 2869.223, 2852.026, 2807.926, 2758.313, 2693.064, 2682.264, 2622.588, 2622.233, 2617.84, 2634.351, 2597.028, 2537.11, 2519.448, 2525.07, 2497.413, 2506.796, 2459.401, 2470.989, 2449.806, 2469.651, 2459.824, 2400.129, 2422.12, 2412.159, 2418.901, 2444.937, 2413.188, 2409.602, 2391.839, 2379.697, 2367.292, 2377.462, 2346.001, 2360.414, 2341.705, 2342.246, 2333.165, 2337.869, 2325.042, 2352.722, 2328.637, 2322.281, 2311.615, 2326.723, 2316.635, 2291.328, 2302.072, 2293.617, 2309.351, 2289.88, 2295.852, 2285.773, 2288.734, 2294.223, 2293.113, 2313.129, 2284.247, 2273.698, 2278.839, 2277.428, 2316.432, 2276.777, 2268.575, 2286.235, 2296.798, 2279.317, 2281.585, 2255.487, 2262.572, 2262.2, 2261.548, 2268.143, 2240.731, 2267.616, 2247.543, 2245.797, 2253.79, 2241.854, 2247.644, 2266.236, 2252.771, 2240.252, 2244.612, 2251.893, 2257.218, 2255.787, 2249.384, 2248.908, 2233.945, 2235.7, 2242.738, 2234.545, 2240.339, 2225.697, 2233.488]}	100	100	True
