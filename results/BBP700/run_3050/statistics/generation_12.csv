id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	2000	True	33025.98828		350017	12	-1	267.34584045410156	{'train_loss': [479051.438, 371962.562, 328209.438, 314669.438, 304385.156, 299546.281, 294596.094, 289383.844, 285490.562, 281139.156, 280293.438, 276891.531, 273294.594, 271512.531, 270378.469, 268361.375, 266653.469, 265569.562, 264595.312, 264351.781, 261751.906, 260810.781, 259380.578, 258747.188, 258159.469, 257443.188, 256273.828, 255802.609, 255375.0, 253611.531, 253743.547, 253071.281, 252913.438, 251088.328, 251215.5, 251019.016, 249736.312, 249679.625, 248543.0, 248889.688, 247241.641, 246084.172, 246217.906, 244746.328, 244966.391, 243417.844, 242447.891, 241974.375, 242250.531, 242146.219, 240068.891, 240708.844, 238898.406, 238911.203, 239338.469, 237774.391, 237775.219, 237791.641, 237113.141, 236204.484, 237292.641, 235226.578, 236295.734, 234503.812, 234072.234, 234693.328, 233525.641, 233912.078, 233447.75, 232530.703, 232265.812, 232955.25, 232143.641, 231682.672, 231656.734, 230333.484, 231029.797, 230024.172, 229927.141, 230018.141, 229176.688, 228801.875, 228640.203, 228988.219, 228529.781, 227916.625, 228307.047, 227289.797, 227834.875, 227080.359, 227016.656, 226055.406, 227716.641, 227418.75, 227190.25, 225271.719, 225632.922, 225582.422, 225739.734, 225491.516], 'val_loss': [3816.589, 3038.237, 2947.267, 2932.997, 2829.32, 2819.055, 2751.49, 2724.015, 2683.68, 2684.949, 2648.402, 2640.908, 2605.509, 2606.265, 2576.757, 2552.181, 2575.691, 2550.99, 2568.33, 2542.47, 2513.14, 2513.21, 2507.196, 2523.307, 2493.435, 2502.385, 2530.548, 2462.906, 2479.848, 2477.059, 2468.535, 2441.648, 2453.463, 2457.825, 2459.611, 2433.959, 2444.35, 2468.948, 2475.765, 2423.08, 2401.324, 2460.289, 2430.483, 2428.401, 2370.874, 2404.79, 2378.386, 2388.949, 2408.636, 2384.653, 2383.604, 2361.693, 2366.816, 2360.491, 2337.095, 2355.565, 2352.789, 2355.435, 2349.421, 2356.787, 2343.864, 2339.811, 2350.717, 2337.3, 2321.34, 2321.957, 2330.207, 2303.473, 2346.645, 2315.802, 2302.662, 2327.986, 2304.204, 2303.324, 2327.248, 2311.644, 2315.267, 2318.23, 2308.752, 2317.663, 2319.239, 2307.299, 2310.756, 2309.809, 2294.771, 2321.409, 2316.259, 2310.083, 2289.503, 2308.763, 2287.861, 2302.162, 2297.538, 2303.916, 2287.675, 2282.148, 2288.741, 2281.097, 2283.201, 2301.719]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:29 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:conv1d out_channels:79 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	137273.9375		3390294	12	-1	245.58160591125488	{'train_loss': [1134630.0, 1117272.5, 1069425.5, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1069770.375, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1067094.375, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.195, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	32000.78516		271701	12	-1	228.91560912132263	{'train_loss': [415050.125, 353293.812, 326392.438, 315656.781, 309544.781, 301304.875, 295705.188, 290431.938, 285919.25, 281833.125, 278588.188, 275607.219, 272819.062, 270233.188, 267519.094, 265847.906, 264072.5, 262609.875, 261118.719, 258574.672, 257280.344, 255925.562, 254238.828, 252901.734, 252085.859, 250923.594, 249215.016, 248396.422, 247175.25, 245868.156, 244804.438, 243859.922, 243540.547, 242052.344, 241811.656, 241934.875, 240389.984, 239602.859, 239025.156, 238500.656, 237440.859, 237839.609, 237686.703, 236432.016, 236725.828, 234984.078, 234927.297, 235081.531, 235453.531, 234410.359, 234072.672, 233593.953, 233064.359, 233191.328, 232739.422, 232044.391, 231979.031, 231815.062, 231080.453, 230237.438, 230924.719, 230517.078, 230408.75, 230809.578, 229520.766, 229344.844, 228845.812, 228552.828, 228457.203, 228308.266, 227516.562, 227531.484, 227924.125, 226740.672, 227137.25, 226741.359, 227305.953, 226188.766, 227105.453, 225531.75, 224680.75, 225656.281, 226027.828, 224425.016, 224286.828, 224124.031, 224408.922, 224272.641, 223397.906, 223734.656, 223912.5, 223136.406, 222928.781, 222267.484, 222719.047, 222464.516, 221997.5, 222365.469, 222692.031, 222282.094], 'val_loss': [3376.298, 3059.038, 2947.398, 2918.667, 2850.0, 2802.204, 2776.402, 2740.946, 2700.368, 2673.88, 2637.766, 2600.716, 2583.708, 2566.582, 2554.156, 2524.455, 2535.745, 2513.065, 2493.35, 2491.559, 2486.077, 2465.635, 2445.47, 2453.4, 2431.277, 2428.447, 2434.426, 2403.896, 2396.904, 2379.49, 2372.21, 2411.489, 2344.95, 2360.65, 2380.443, 2336.278, 2361.346, 2321.647, 2322.312, 2328.561, 2311.046, 2323.559, 2302.886, 2292.594, 2299.144, 2322.33, 2293.802, 2281.028, 2266.782, 2254.73, 2261.873, 2271.347, 2268.163, 2266.507, 2254.585, 2277.31, 2261.737, 2258.935, 2238.895, 2251.143, 2253.161, 2239.826, 2263.653, 2252.734, 2244.554, 2248.023, 2236.069, 2253.216, 2267.227, 2257.029, 2244.194, 2232.448, 2232.467, 2232.443, 2213.216, 2219.307, 2236.203, 2236.549, 2213.619, 2228.18, 2235.763, 2249.219, 2230.647, 2221.328, 2226.635, 2204.598, 2205.405, 2213.708, 2232.013, 2198.736, 2200.337, 2225.155, 2203.636, 2203.605, 2209.998, 2217.682, 2215.045, 2204.106, 2212.204, 2198.707]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:18 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:45 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:4 epochs:100	84	1000	True	34367.42578		632707	13	-1	1012.7221865653992	{'train_loss': [427510.719, 309656.469, 295536.875, 286854.344, 281623.656, 278151.156, 274520.844, 273415.281, 271207.281, 268668.062, 266803.531, 264885.75, 263292.719, 263056.688, 260989.344, 259689.203, 258997.938, 257761.828, 256724.547, 257710.656, 256903.125, 255487.891, 255272.312, 254166.328, 251938.203, 251081.484, 249695.531, 249343.219, 248955.672, 247797.578, 246877.656, 247677.797, 245466.625, 246198.047, 244812.156, 243224.125, 243997.969, 241799.938, 241082.078, 241043.547, 239421.688, 240369.703, 238234.938, 238889.031, 237687.531, 236942.109, 237316.5, 237581.125, 237233.203, 236718.188, 236033.234, 234466.109, 234178.828, 233974.5, 233631.484, 233864.844, 233239.094, 231728.406, 232027.016, 231951.156, 231081.438, 231511.188, 230430.031, 230145.891, 228721.078, 228971.078, 229008.547, 229563.109, 228279.219, 227460.375, 227757.688, 227127.328, 226687.484, 225971.375, 225186.375, 225127.969, 225528.094, 225113.219, 223828.844, 224450.797, 224058.328, 223035.922, 222113.062, 223190.609], 'val_loss': [367.642, 352.931, 345.573, 339.489, 339.548, 332.13, 335.769, 331.417, 329.763, 321.153, 323.579, 318.557, 319.964, 317.19, 313.262, 313.229, 312.748, 314.628, 315.349, 312.586, 313.94, 311.631, 314.651, 309.221, 312.31, 310.328, 307.719, 309.648, 309.022, 309.868, 307.933, 311.34, 307.744, 307.629, 306.522, 308.49, 307.624, 304.012, 304.728, 306.08, 309.513, 302.859, 304.897, 308.504, 306.973, 304.237, 308.96, 304.606, 306.838, 310.017, 303.389, 301.865, 303.04, 311.457, 307.939, 303.584, 309.941, 305.34, 306.118, 305.998, 310.721, 306.361, 317.501, 308.732, 310.874, 305.941, 309.964, 308.296, 308.286, 313.307, 312.647, 309.702, 305.818, 312.199, 318.881, 307.999, 312.259, 307.084, 312.749, 307.289, 315.95, 311.896, 311.812, 306.511]}	84	84	False
