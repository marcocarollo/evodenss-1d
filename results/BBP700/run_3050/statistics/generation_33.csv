id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	31886.34375		469896	10	-1	205.38381505012512	{'train_loss': [616480.188, 366624.719, 322988.125, 309783.312, 301054.688, 293219.062, 286379.656, 282161.594, 277939.531, 275647.25, 273430.844, 271632.312, 269441.938, 267689.75, 266566.688, 264942.031, 264200.5, 262270.188, 260880.938, 259781.391, 259217.047, 257259.922, 257036.266, 256213.906, 256936.859, 254884.875, 254446.406, 254253.766, 253187.219, 252615.891, 251002.531, 250012.453, 250679.828, 249846.812, 248836.734, 248145.469, 247742.203, 247842.859, 246876.25, 246178.047, 245769.234, 245019.953, 245682.531, 244461.062, 243806.203, 243525.844, 243842.141, 242868.516, 242742.812, 242343.531, 241393.719, 241736.375, 241108.719, 241314.672, 240879.578, 240173.156, 239343.922, 239456.609, 239245.938, 239286.266, 239197.078, 238799.703, 237965.797, 237565.422, 237705.938, 237532.578, 237332.094, 235899.188, 236686.453, 236259.188, 235077.469, 235758.578, 234650.172, 234889.891, 235136.141, 234223.828, 233780.0, 234552.672, 233734.875, 233886.266, 233794.094, 233354.062, 233050.125, 233206.844, 232773.375, 233027.422, 231796.75, 231610.688, 231664.047, 231766.969, 231028.812, 231234.891, 231448.156, 231067.594, 230687.828, 230154.219, 230271.844, 230046.844, 229928.703, 230072.938], 'val_loss': [3867.248, 3033.554, 2900.32, 2847.838, 2758.76, 2703.403, 2674.481, 2631.468, 2594.992, 2569.535, 2587.824, 2551.726, 2543.396, 2508.95, 2496.099, 2485.25, 2467.372, 2464.905, 2435.478, 2462.101, 2435.584, 2422.448, 2409.037, 2414.247, 2404.579, 2418.191, 2413.605, 2391.287, 2384.179, 2377.969, 2352.837, 2363.559, 2380.037, 2359.967, 2359.607, 2350.436, 2338.525, 2350.244, 2318.309, 2328.955, 2351.026, 2329.2, 2319.092, 2327.458, 2317.458, 2306.469, 2310.442, 2307.297, 2307.176, 2283.089, 2283.818, 2284.863, 2303.169, 2282.664, 2284.715, 2288.282, 2298.218, 2291.247, 2286.998, 2292.706, 2271.441, 2286.935, 2286.464, 2290.489, 2272.152, 2274.5, 2264.759, 2274.182, 2259.491, 2275.561, 2259.005, 2260.03, 2277.792, 2248.313, 2260.871, 2256.492, 2249.835, 2252.203, 2246.895, 2261.077, 2249.501, 2241.373, 2244.406, 2245.129, 2246.054, 2254.517, 2247.92, 2244.596, 2241.613, 2230.603, 2227.632, 2236.626, 2236.942, 2209.425, 2228.107, 2228.588, 2223.743, 2244.512, 2224.698, 2218.783]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:85 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:39 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adam lr:0.19006852740056368 beta1:0.8461912586201831 beta2:0.8477388054480077 weight_decay:6.837634173589606e-05 batch_size:32 epochs:100	100	1000	True	49697.53906		239396	9	-1	196.18072843551636	{'train_loss': [469950.562, 390673.344, 389866.312, 397545.125, 413834.469, 399044.625, 393260.5, 397569.562, 389422.75, 400481.938, 392515.438, 408155.625, 403707.438, 400183.281, 409380.781, 393552.062, 399756.875, 398842.375, 395031.969, 394890.781, 409778.0, 391662.688, 390494.0, 400139.344, 393622.906, 396316.188, 386936.656, 411381.0, 393062.969, 403065.0, 390211.062, 388525.125, 391395.438, 386611.031, 396166.312, 390461.25, 389234.625, 391426.344, 393056.625, 396692.188, 387026.344, 390789.312, 388096.062, 392433.438, 397641.125, 395901.969, 397389.156, 398315.562, 407000.25, 399011.062, 390074.875, 408587.344, 388546.438, 396625.125, 404847.188, 393860.75, 418009.062, 392198.594, 410328.812, 401265.344, 388734.875, 400655.062, 429218.062, 396164.125, 395417.969, 390202.094, 394881.406, 401647.906, 395289.188, 399695.844, 392917.906, 390897.406, 401093.969, 392506.125, 394833.812, 412639.906, 390392.344, 399698.719, 401313.562, 390406.094, 387912.625, 389565.969, 391884.25, 402456.375, 401224.75, 400209.469, 390296.375, 403534.375, 395670.188, 401308.5, 395027.094, 406627.938, 387778.75, 393939.594, 394006.375, 396031.5, 392037.625, 391735.125, 392814.188, 403590.031], 'val_loss': [3560.08, 3747.77, 5662.591, 3883.744, 3793.955, 3799.63, 3804.363, 3712.553, 3726.818, 3627.221, 3568.152, 3657.088, 4597.248, 4677.895, 3572.32, 4012.267, 3493.916, 4308.814, 3537.517, 3889.528, 3638.301, 3522.149, 3737.684, 4488.094, 3664.039, 4243.506, 3712.459, 3523.588, 4032.704, 3609.781, 3639.264, 3567.74, 3403.368, 3714.489, 3527.467, 3653.942, 3439.085, 3635.994, 3655.033, 3512.64, 3581.037, 3499.996, 3708.488, 3713.488, 3519.802, 3714.017, 3498.539, 3558.558, 3487.887, 3808.933, 3829.235, 3787.95, 3465.19, 3747.766, 3862.843, 3691.473, 3787.75, 3457.653, 3617.998, 4692.699, 3504.609, 3584.985, 3713.214, 3566.764, 3547.378, 3699.958, 3693.341, 3820.647, 3439.185, 3754.645, 3863.798, 3575.247, 3798.89, 3599.699, 3565.892, 3681.309, 3663.712, 3837.597, 5886.433, 3433.456, 3962.034, 3824.331, 3490.496, 3841.951, 5360.713, 3635.985, 3468.605, 3477.027, 3603.251, 4045.504, 5797.832, 4015.419, 3647.2, 3481.517, 3540.456, 3456.509, 3775.911, 3501.059, 3898.032, 3488.776]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:58 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:32 epochs:100	100	1000	True	137282.48438		6326873	9	-1	197.88842034339905	{'train_loss': [1128588.125, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:47 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:78 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:39 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	137283.89062		6807057	11	-1	214.35012984275818	{'train_loss': [1149307.125, 1066920.875, 1398478.625, 1069047.5, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
