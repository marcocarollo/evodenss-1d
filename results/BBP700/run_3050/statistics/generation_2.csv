id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:53 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	32970.25781		490746	14	-1	262.02101492881775	{'train_loss': [493228.156, 397465.094, 340905.625, 317079.156, 308681.875, 304321.438, 299743.469, 295339.0, 288181.844, 282343.062, 277868.562, 273829.312, 271272.25, 269497.625, 266287.75, 264787.844, 262134.953, 261510.719, 260923.531, 259141.203, 258029.188, 257131.094, 255318.375, 254646.812, 253757.125, 252362.828, 250916.766, 250803.266, 250365.812, 249794.562, 248690.266, 248060.391, 247693.922, 245869.922, 246396.062, 245336.891, 245232.234, 244949.438, 243723.0, 243462.203, 243437.953, 241722.234, 242424.641, 241879.844, 241613.891, 239984.234, 238720.297, 239992.484, 238580.281, 238445.047, 238440.828, 237363.234, 236759.422, 237348.0, 236950.344, 236836.297, 235472.688, 235448.703, 234929.25, 234392.031, 234944.281, 233623.797, 234227.812, 232726.891, 233096.906, 233417.344, 232117.781, 233028.516, 231865.672, 230944.969, 231024.359, 231279.75, 231673.734, 230745.359, 229378.875, 229719.359, 230349.719, 229289.141, 229700.188, 229269.562, 227870.875, 228746.547, 227890.359, 227915.703, 229045.062, 227786.359, 227905.469, 227165.203, 227297.328, 227274.672, 226077.156, 226492.109, 226139.703, 226916.797, 227017.719, 225919.359, 226464.766, 225625.594, 225311.094, 224326.203], 'val_loss': [3785.791, 3412.568, 2978.312, 2890.418, 2860.117, 2834.324, 2805.234, 2761.622, 2702.312, 2658.447, 2647.76, 2601.967, 2591.92, 2588.846, 2569.417, 2524.771, 2523.058, 2536.334, 2496.761, 2501.161, 2476.322, 2467.308, 2483.478, 2482.94, 2471.834, 2482.596, 2468.564, 2467.48, 2429.751, 2446.815, 2460.429, 2451.891, 2431.343, 2428.348, 2468.563, 2454.889, 2467.869, 2448.309, 2427.109, 2416.586, 2415.344, 2436.444, 2416.607, 2419.775, 2414.11, 2406.692, 2404.22, 2405.693, 2414.463, 2414.99, 2404.899, 2393.337, 2390.605, 2406.786, 2403.141, 2403.727, 2404.133, 2408.082, 2420.641, 2411.991, 2396.503, 2447.119, 2423.533, 2399.805, 2383.116, 2375.982, 2387.261, 2416.966, 2387.737, 2360.261, 2380.27, 2396.061, 2395.039, 2399.966, 2387.518, 2376.567, 2388.291, 2394.6, 2377.407, 2372.943, 2362.961, 2399.657, 2369.049, 2335.946, 2353.845, 2359.828, 2344.547, 2378.172, 2325.063, 2355.563, 2382.678, 2351.319, 2369.754, 2338.494, 2358.64, 2338.256, 2357.406, 2360.273, 2342.402, 2353.687]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:6 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta batch_size:32 epochs:100	100	1000	True	33238.73047		602053	15	-1	303.43387031555176	{'train_loss': [694259.625, 420656.531, 333162.031, 313064.594, 304868.594, 298876.031, 293633.531, 288683.469, 283566.656, 279933.906, 277229.938, 274104.094, 272170.875, 269860.719, 267903.375, 266878.375, 264436.5, 262700.969, 262226.219, 260708.906, 260351.812, 258762.672, 258102.297, 256914.766, 256206.344, 255624.594, 255097.875, 254733.359, 254278.875, 253224.922, 252168.453, 252181.484, 251300.922, 250175.797, 249452.0, 249957.297, 248951.203, 248574.094, 248233.953, 248180.484, 247647.062, 246564.156, 246332.609, 245068.031, 245494.062, 245268.609, 244369.188, 244520.891, 243600.219, 243926.219, 243100.266, 242247.094, 242534.016, 242050.875, 241659.703, 241759.609, 240781.906, 239972.266, 239770.922, 239815.547, 239402.078, 239363.359, 238584.047, 238975.641, 237905.781, 237968.625, 238184.406, 237189.516, 236889.344, 235829.859, 236451.062, 236293.047, 236002.531, 235779.031, 234976.984, 235570.016, 234605.281, 235344.047, 234467.219, 233258.156, 232578.328, 233821.0, 233859.797, 233169.438, 232370.984, 231985.844, 231161.406, 231481.156, 231535.641, 231564.797, 230550.062, 231221.281, 230578.266, 229594.906, 229464.312, 229279.453, 229602.438, 229247.031, 228763.672, 228945.5], 'val_loss': [6287.896, 3303.901, 2997.392, 2885.251, 2842.433, 2804.369, 2731.858, 2713.639, 2656.093, 2626.912, 2586.263, 2578.729, 2561.956, 2542.579, 2562.578, 2575.897, 2532.564, 2543.691, 2567.502, 2557.257, 2562.023, 2540.622, 2499.306, 2493.29, 2517.599, 2490.086, 2472.798, 2473.55, 2491.831, 2492.093, 2482.058, 2466.934, 2471.218, 2453.246, 2450.279, 2446.441, 2436.997, 2446.131, 2442.972, 2437.439, 2460.09, 2433.003, 2422.998, 2435.359, 2442.421, 2437.4, 2452.916, 2425.076, 2432.58, 2446.981, 2438.139, 2453.127, 2422.752, 2415.688, 2434.806, 2422.648, 2430.978, 2417.68, 2412.252, 2398.839, 2392.122, 2394.066, 2412.653, 2386.92, 2392.738, 2409.835, 2396.923, 2397.636, 2393.215, 2401.969, 2376.02, 2367.769, 2395.823, 2357.099, 2384.242, 2343.613, 2362.324, 2357.9, 2347.661, 2335.993, 2381.979, 2349.975, 2361.24, 2345.943, 2329.508, 2333.355, 2327.31, 2345.498, 2318.873, 2356.386, 2324.086, 2350.141, 2311.125, 2316.11, 2340.379, 2307.106, 2323.245, 2327.781, 2323.896, 2319.742]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:7 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:53 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:75 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:66 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.1486052211854908 beta1:0.9876799287460968 beta2:0.8034705359850989 weight_decay:6.981012243081043e-05 batch_size:32 epochs:100	100	1000	True	198426192.0		479071	15	-1	324.3914067745209	{'train_loss': [726612.562, 1131606.25, 2661280.75, 1457721.0, 1429639.75, 1301173.25, 28135750.0, 22029216.0, 24513424.0, 136145456.0, 128895288.0, 206923888.0, 409736416.0, 62577164.0, 8189615.0, 17252556.0, 1066921.0, 8349223.0, 5862831.5, 27461816.0, 22258640.0, 27428118.0, 6741669.5, 6027883.5, 42144924.0, 17220988.0, 9843995.0, 4797298.5, 148948560.0, 62940788.0, 1066921.0, 25215800.0, 3088342.5, 22590950.0, 13679862.0, 7573963.5, 1066921.0, 1066921.0, 47047140.0, 3124207.5, 1066921.0, 1066921.0, 1199860.125, 8105483.0, 3780849.75, 5661238.0, 1066921.0, 1066921.0, 1066921.0, 8670104.0, 17422964.0, 2665381.0, 19100770.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 2124943.5, 1066921.0, 3087491.5, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 39398364.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 11759849.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [15979.794, 17094.014, 35046.414, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 125872.227, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:53 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:10 layer:deconv1d out_channels:117 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	137292.35938		9720918	14	-1	279.53480887413025	{'train_loss': [1106469.5, 1116352.375, 1066921.0, 1066921.0, 1112753.75, 1184543.75, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1069720.25, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
