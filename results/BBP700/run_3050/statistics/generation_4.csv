id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	32729.58398		463085	13	-1	250.39318108558655	{'train_loss': [413776.156, 330824.812, 317019.406, 305642.312, 297826.781, 292905.625, 287759.156, 284107.094, 279856.375, 277504.594, 275173.094, 271586.0, 269958.0, 267415.0, 266288.594, 264689.219, 263785.781, 262208.062, 261046.094, 259009.859, 258438.266, 256722.172, 256144.125, 256129.047, 254088.656, 253289.375, 252976.953, 251861.922, 251434.984, 251090.719, 250605.547, 248932.328, 248604.719, 247799.328, 247359.156, 246540.625, 245667.156, 244949.016, 244820.938, 244563.766, 244212.094, 243214.312, 243144.453, 241887.844, 241768.375, 240904.359, 240900.422, 240094.781, 241212.297, 240013.844, 238585.656, 238538.672, 238726.141, 238016.281, 238390.312, 237869.625, 237283.672, 236950.891, 237425.828, 237167.031, 236737.453, 236624.625, 235553.734, 235340.875, 235948.156, 235123.531, 234616.906, 233837.688, 235529.141, 235194.438, 234483.078, 234260.344, 234139.141, 233180.484, 233524.969, 233325.25, 233107.812, 232723.016, 232792.734, 231946.375, 231107.016, 231653.75, 231868.141, 230717.094, 230641.25, 229405.047, 230838.422, 230170.359, 229641.812, 230198.203, 229275.719, 229090.859, 229005.875, 229981.297, 228879.641, 229703.484, 228622.406, 229067.906, 227412.047, 228092.219], 'val_loss': [3396.68, 3066.43, 2878.421, 2829.848, 2820.355, 2796.922, 2742.356, 2678.264, 2646.458, 2650.967, 2624.456, 2648.104, 2564.414, 2560.547, 2566.26, 2538.09, 2519.831, 2519.701, 2501.74, 2519.82, 2488.516, 2479.478, 2489.645, 2504.137, 2483.683, 2472.835, 2474.84, 2470.974, 2466.104, 2454.145, 2456.543, 2483.334, 2440.722, 2467.031, 2460.085, 2450.318, 2447.559, 2446.302, 2425.217, 2444.373, 2420.713, 2433.496, 2424.554, 2423.053, 2424.286, 2413.701, 2422.245, 2408.397, 2423.825, 2436.684, 2379.799, 2395.079, 2432.419, 2407.866, 2410.615, 2371.802, 2392.34, 2389.507, 2389.536, 2381.642, 2371.926, 2374.277, 2357.313, 2376.938, 2373.873, 2377.872, 2356.277, 2351.675, 2369.088, 2354.12, 2349.005, 2366.51, 2354.504, 2346.567, 2344.218, 2358.011, 2352.811, 2363.936, 2338.071, 2327.186, 2353.964, 2367.345, 2342.646, 2337.612, 2345.215, 2321.546, 2334.503, 2323.386, 2340.432, 2340.744, 2331.373, 2332.711, 2325.085, 2304.619, 2307.677, 2327.044, 2309.387, 2341.123, 2307.744, 2321.076]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:95 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:59 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.12600846558473194 alpha:0.9373197385601533 weight_decay:2.575338687496374e-05 batch_size:32 epochs:100	100	1000	True	48819.01562		475402	14	-1	244.8126449584961	{'train_loss': [1260252.875, 1009080.062, 1132880.0, 1018021.625, 914851.562, 1134915.375, 1014211.375, 1083103.0, 805376.688, 657196.938, 763408.938, 607984.812, 1178722.5, 1094965.125, 942543.875, 708824.062, 855605.25, 1101333.875, 628877.438, 764469.312, 872893.75, 1019137.812, 880667.188, 1184260.75, 1090093.25, 1066921.0, 1141519.375, 914947.938, 939701.688, 1119913.5, 1000780.938, 971178.0, 1152192.875, 1104261.5, 836275.5, 634900.0, 1181960.0, 964348.188, 968138.562, 573492.812, 809135.375, 877976.188, 562422.688, 896408.0, 1067694.875, 909750.938, 722334.25, 608992.188, 1079694.875, 1249187.25, 778454.375, 964416.438, 676628.812, 787359.5, 917865.062, 869528.938, 862188.562, 1186956.5, 1174334.875, 1159667.375, 984510.938, 625761.25, 1118749.875, 1041267.812, 844546.25, 1147688.25, 1022307.438, 965177.75, 969232.188, 802710.562, 1094967.0, 599167.938, 1214987.375, 1072464.75, 1078523.75, 855631.25, 709426.375, 846561.75, 860966.75, 668575.75, 851396.25, 860405.25, 753720.0, 779764.125, 890038.0, 752173.562, 592986.812, 925064.312, 1034970.688, 1080967.5, 634414.875, 757930.125, 909015.875, 1162051.0, 872524.438, 978703.0, 813286.25, 890943.562, 771531.188, 573107.375], 'val_loss': [9418.362, 8887.351, 9939.539, 6416.057, 3866.768, 6224.209, 5213.687, 8615.249, 5556.01, 3463.759, 3443.583, 3463.549, 10969.655, 7725.736, 5623.173, 3603.968, 10578.043, 3459.826, 3456.611, 4425.243, 3518.594, 4572.85, 8391.085, 10587.216, 10587.216, 10587.216, 9196.949, 9614.689, 10344.429, 3626.408, 10361.977, 10559.234, 10472.029, 9288.646, 3476.14, 3412.662, 3794.307, 6414.87, 3886.944, 14197.679, 3555.127, 3533.0, 3585.684, 10587.154, 11139.655, 4013.65, 3443.21, 3599.396, 10940.37, 5697.092, 10545.776, 3500.099, 55570.125, 3471.118, 10634.161, 16144.938, 10586.191, 10587.181, 10692.198, 10533.424, 3528.47, 3536.108, 6378.992, 3679.163, 10772.066, 8027.353, 4218.24, 10587.188, 3545.436, 10570.807, 5905.778, 6550.739, 9167.332, 10787.263, 18557.699, 3443.165, 29551.252, 10253.598, 3522.079, 8404.783, 3632.947, 7005.176, 10595.714, 3472.959, 10554.422, 3649.203, 3663.34, 11073.253, 5744.973, 3412.774, 3555.648, 3417.978, 10669.4, 10846.066, 10585.376, 3459.916, 10512.27, 3673.839, 3441.478, 3454.507]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:89 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:23 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:115 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:14 epochs:100	100	1000	True	137316.25		7910860	13	-1	455.8148686885834	{'train_loss': [1206063.875, 1066921.25, 1066921.25, 1066921.25, 1069195.625, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25], 'val_loss': [4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.16130737792802288 beta1:0.8057154411320803 beta2:0.8003681964500524 weight_decay:0.00035422153815251923 batch_size:32 epochs:100	100	1000	True	51564.31641		538912	13	-1	252.05825352668762	{'train_loss': [559119.938, 466224.094, 473341.719, 441927.625, 461954.938, 477560.188, 483409.75, 502044.719, 461798.312, 457289.469, 485028.812, 488757.688, 488850.656, 470554.062, 450555.094, 493966.156, 482592.688, 454888.25, 474570.812, 449539.344, 474637.0, 448629.281, 472752.531, 433700.875, 456399.656, 469888.625, 512785.812, 456374.875, 438180.531, 482420.625, 459353.656, 494894.25, 487326.25, 446369.812, 474559.312, 481652.438, 463849.0, 452376.625, 450976.281, 447999.969, 463148.906, 431777.062, 456936.344, 454713.656, 454314.531, 472639.719, 436729.219, 478983.25, 429156.688, 451707.031, 487934.969, 500325.5, 452413.281, 502279.0, 455324.812, 450907.344, 447394.156, 453813.031, 480256.062, 431490.0, 463842.0, 464635.594, 484517.188, 479073.438, 469466.531, 459636.062, 447964.656, 469984.281, 457498.0, 439440.562, 478248.562, 472132.688, 454390.312, 457862.75, 485351.531, 470713.531, 443403.188, 455930.0, 473607.219, 441840.344, 460525.094, 461406.062, 485130.75, 491764.125, 465552.344, 461793.719, 444963.531, 457762.75, 471069.906, 448555.438, 460543.531, 488550.25, 447008.75, 497341.031, 456685.562, 441401.688, 483800.5, 463697.281, 467386.5, 450579.688], 'val_loss': [5315.059, 4878.936, 4233.333, 3630.207, 4678.038, 5238.338, 4025.566, 3739.725, 4574.228, 3670.919, 5074.653, 5497.55, 7136.511, 4988.717, 3535.211, 3855.767, 4075.877, 3472.917, 3965.716, 6382.383, 6132.198, 3528.573, 3866.963, 7160.933, 3777.645, 4721.193, 4067.615, 4219.705, 5092.047, 4299.463, 4234.83, 5960.862, 4795.773, 4106.255, 10757.745, 5389.513, 3826.692, 6725.635, 4126.454, 5132.685, 3625.57, 4759.194, 3893.202, 3638.144, 3602.994, 3960.09, 7863.326, 3742.622, 5838.257, 4246.76, 4451.272, 5365.283, 4047.927, 3555.646, 3521.662, 4130.504, 4958.653, 3771.393, 3946.837, 4733.316, 4726.684, 7479.345, 5459.908, 8852.969, 3725.735, 4979.171, 3984.202, 4074.01, 3832.546, 4007.704, 4314.999, 4226.253, 3552.934, 4231.804, 4082.842, 6290.399, 4700.742, 3910.552, 7327.532, 4149.181, 4003.599, 6732.279, 3553.004, 3629.318, 5716.545, 4849.158, 4855.976, 3795.573, 4204.697, 5092.132, 3505.942, 3803.345, 4297.828, 5707.013, 5096.699, 4324.345, 4358.426, 5614.309, 5857.538, 3665.806]}	100	100	True
