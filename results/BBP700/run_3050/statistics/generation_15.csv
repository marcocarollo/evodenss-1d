id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:101 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	32320.875		360190	11	-1	227.14122223854065	{'train_loss': [495212.406, 369772.969, 326306.719, 312153.469, 303490.156, 299071.375, 295312.469, 291547.344, 288358.625, 284366.656, 282112.844, 279811.625, 276486.156, 274604.125, 273135.25, 271540.531, 269983.5, 268756.0, 267138.344, 266173.5, 263885.312, 263452.938, 261181.172, 260963.438, 259346.359, 257809.219, 257899.375, 257138.188, 254498.188, 254104.234, 254038.25, 252654.938, 252508.875, 251340.688, 251208.375, 249450.031, 248792.672, 249137.891, 247758.703, 246889.75, 246941.734, 245966.297, 246427.828, 245176.062, 244671.078, 244304.297, 243140.969, 243473.219, 242867.859, 242243.828, 241155.719, 241221.938, 240316.156, 240129.406, 239454.906, 239938.172, 239290.766, 238827.516, 238374.031, 238324.953, 237893.234, 237080.703, 235895.109, 235813.5, 235171.422, 235424.734, 234095.469, 234710.688, 234005.781, 233340.984, 233614.734, 233766.75, 232406.047, 232679.234, 231875.188, 231549.859, 230693.375, 230311.641, 231352.297, 230983.391, 230241.047, 230028.625, 229886.203, 228904.234, 229270.344, 229042.766, 228733.25, 228074.109, 228083.5, 227978.078, 228096.078, 228356.781, 226326.094, 226814.297, 227746.25, 226633.172, 225636.453, 225892.953, 225497.297, 224344.406], 'val_loss': [3705.009, 3061.868, 2917.859, 2834.646, 2822.116, 2804.912, 2784.809, 2756.558, 2732.768, 2701.108, 2690.504, 2659.549, 2652.514, 2630.015, 2612.877, 2596.89, 2571.264, 2577.304, 2558.665, 2550.032, 2546.759, 2530.74, 2510.376, 2508.113, 2467.639, 2487.043, 2461.711, 2443.253, 2423.934, 2435.54, 2434.169, 2427.358, 2419.719, 2409.588, 2392.469, 2382.661, 2379.788, 2367.888, 2380.572, 2369.828, 2374.504, 2394.384, 2367.595, 2377.772, 2361.969, 2359.441, 2351.346, 2361.66, 2381.99, 2354.315, 2362.635, 2341.049, 2347.397, 2362.473, 2344.892, 2335.959, 2336.333, 2341.15, 2335.016, 2328.433, 2329.043, 2312.466, 2307.164, 2304.545, 2308.436, 2303.253, 2312.323, 2282.64, 2308.962, 2298.463, 2286.334, 2281.449, 2268.168, 2278.473, 2272.054, 2270.382, 2265.823, 2276.687, 2279.326, 2265.722, 2268.327, 2261.069, 2252.052, 2244.118, 2260.405, 2246.864, 2233.151, 2249.506, 2256.0, 2250.027, 2253.902, 2247.049, 2239.1, 2258.394, 2233.637, 2261.059, 2253.854, 2244.9, 2268.861, 2259.147]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:59 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:77 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:37 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:13 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	32903.82812		1258916	11	-1	224.0312213897705	{'train_loss': [1087637.125, 1104767.0, 1103319.875, 579907.938, 353115.25, 311847.156, 301509.938, 296353.438, 291391.062, 287788.031, 283570.875, 279816.062, 276822.062, 274692.875, 272858.0, 271687.5, 271211.312, 268840.125, 268110.312, 265179.719, 264732.625, 263743.312, 264599.5, 263677.719, 263755.906, 260433.797, 261100.078, 260141.875, 259291.641, 258450.875, 258497.922, 257911.703, 256098.547, 256595.953, 256643.406, 256151.688, 254648.156, 254361.484, 254360.031, 254695.016, 253695.359, 252945.906, 252808.141, 251590.219, 252055.75, 251241.688, 249837.703, 249795.438, 249507.641, 249592.875, 249836.938, 249052.625, 249310.75, 247935.234, 247569.391, 247226.922, 247535.438, 247667.984, 246987.906, 247184.797, 246149.156, 246595.828, 245433.578, 245169.719, 245324.797, 244258.125, 243306.266, 244316.531, 243568.016, 243231.141, 244294.625, 244106.594, 243043.172, 242399.953, 242904.344, 242535.75, 242997.656, 242788.906, 242234.328, 242219.75, 240430.812, 240815.562, 240684.328, 240666.75, 240541.5, 239909.688, 239885.672, 240182.906, 238816.406, 239618.391, 239346.359, 238531.609, 238814.062, 239526.422, 238674.016, 237576.156, 237548.781, 237778.25, 236388.781, 237555.531], 'val_loss': [10586.673, 10751.657, 10226.994, 3521.962, 2924.646, 2846.25, 2839.183, 2769.053, 2761.337, 2687.865, 2672.689, 2647.509, 2612.091, 2593.379, 2568.143, 2571.893, 2549.614, 2538.031, 2508.342, 2499.292, 2489.687, 2489.948, 2479.767, 2481.503, 2465.468, 2453.066, 2465.9, 2460.416, 2457.761, 2451.502, 2473.752, 2460.449, 2435.386, 2450.478, 2436.434, 2433.141, 2444.953, 2401.021, 2425.176, 2402.079, 2412.564, 2396.96, 2400.861, 2404.844, 2396.088, 2396.461, 2392.172, 2398.602, 2389.765, 2390.313, 2377.118, 2395.5, 2396.097, 2381.097, 2377.568, 2398.117, 2383.374, 2400.823, 2391.084, 2384.628, 2367.959, 2386.781, 2379.787, 2373.331, 2385.29, 2373.059, 2360.12, 2376.596, 2382.687, 2381.942, 2367.096, 2395.472, 2377.171, 2367.404, 2373.651, 2355.739, 2341.335, 2367.176, 2353.301, 2371.15, 2354.637, 2352.288, 2352.603, 2368.776, 2359.759, 2369.312, 2352.898, 2359.855, 2350.714, 2355.939, 2363.126, 2352.904, 2361.752, 2348.276, 2333.646, 2354.204, 2352.018, 2342.999, 2348.784, 2345.943]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:101 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	31894.26562		360190	11	-1	221.9166407585144	{'train_loss': [491286.406, 369038.562, 327355.656, 314633.406, 306383.375, 297466.938, 290876.562, 285248.031, 280780.812, 276331.969, 273648.75, 270342.031, 268322.531, 266128.344, 263849.031, 262469.781, 260038.5, 257963.906, 255456.234, 252667.891, 252199.234, 250473.344, 249063.672, 247296.625, 247192.016, 245950.359, 244513.312, 244037.016, 242350.234, 240926.438, 240645.516, 240372.953, 239900.984, 238577.828, 238316.656, 236563.375, 236351.484, 236416.484, 235693.422, 235659.5, 234500.312, 233380.203, 232750.953, 231964.422, 232098.234, 231404.438, 230967.0, 230403.938, 230232.75, 229622.375, 229678.031, 229355.609, 228729.141, 228472.406, 227058.281, 227954.656, 227331.719, 226667.688, 226191.531, 226694.016, 225703.547, 225655.516, 225368.109, 224874.25, 224709.531, 224032.234, 224011.078, 223610.438, 222454.094, 222628.328, 222764.234, 222927.266, 222600.938, 222703.703, 221595.875, 221402.188, 221208.484, 221002.266, 221729.328, 219853.344, 221325.984, 221077.922, 219763.109, 220548.953, 219199.469, 219329.109, 218366.812, 218810.781, 218439.406, 217660.656, 218395.516, 217126.875, 218322.125, 217589.828, 218363.734, 218017.875, 217242.609, 217476.172, 216612.938, 216831.641], 'val_loss': [3770.543, 3066.46, 2929.555, 2904.809, 2817.931, 2767.669, 2725.7, 2676.667, 2672.988, 2616.696, 2596.305, 2571.033, 2571.279, 2530.774, 2500.558, 2492.89, 2465.985, 2458.307, 2430.907, 2438.153, 2448.459, 2393.255, 2396.549, 2356.665, 2393.937, 2358.511, 2360.48, 2360.779, 2347.106, 2348.931, 2373.43, 2330.323, 2334.269, 2323.637, 2308.801, 2329.021, 2324.967, 2332.927, 2338.543, 2303.033, 2308.056, 2336.8, 2311.563, 2313.331, 2271.698, 2281.621, 2274.199, 2285.281, 2286.598, 2245.946, 2276.413, 2259.809, 2271.288, 2272.112, 2275.831, 2278.976, 2278.142, 2257.964, 2253.561, 2264.884, 2267.892, 2242.773, 2240.98, 2262.412, 2278.268, 2249.082, 2250.524, 2274.742, 2246.796, 2242.234, 2247.544, 2269.942, 2270.996, 2256.492, 2256.651, 2226.54, 2245.086, 2241.457, 2247.399, 2256.283, 2208.545, 2244.62, 2242.462, 2248.844, 2224.26, 2219.306, 2244.898, 2226.45, 2249.61, 2216.924, 2221.355, 2248.793, 2235.56, 2251.973, 2219.038, 2217.106, 2212.974, 2256.756, 2224.509, 2215.74]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:101 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:32 epochs:100	100	1000	True	33497.32812		2487631	9	-1	213.74244284629822	{'train_loss': [1236374.5, 1111567.0, 729397.75, 566128.188, 519634.375, 508321.062, 487133.75, 492471.188, 465169.844, 457396.562, 492866.5, 437455.312, 434513.406, 416461.562, 414322.469, 407444.188, 363822.219, 320681.688, 300504.438, 291265.062, 287049.688, 283931.969, 281433.656, 279537.188, 277385.5, 275276.156, 275356.719, 270739.125, 272174.531, 270312.719, 268970.344, 267455.156, 266696.688, 266014.625, 265180.25, 264834.406, 264473.125, 262969.5, 262797.188, 260999.734, 261146.812, 260791.438, 260173.984, 258882.844, 257838.344, 258130.672, 257194.453, 256982.047, 256612.531, 254039.0, 254114.344, 255491.109, 253150.203, 253736.641, 253509.125, 252315.0, 252076.828, 253048.422, 252040.406, 251316.516, 251045.969, 250327.391, 249985.938, 249708.703, 248366.375, 247970.062, 247872.625, 246851.609, 247311.812, 246036.016, 246253.312, 246201.703, 245078.875, 244710.656, 243753.797, 243986.094, 243031.547, 242890.062, 241986.812, 242510.891, 241074.938, 240692.453, 240179.156, 241397.328, 240691.641, 238999.156, 240181.422, 238766.578, 237921.703, 238602.531, 237260.266, 237786.891, 237044.938, 235586.703, 235900.5, 236539.281, 236147.125, 235854.203, 234876.625, 234476.938], 'val_loss': [10587.059, 10815.627, 4263.151, 3427.267, 5364.947, 3353.445, 4332.763, 3782.764, 3541.149, 3972.595, 3306.808, 3386.135, 3849.822, 3361.35, 3049.61, 2952.449, 2828.734, 2725.976, 2705.218, 2639.025, 2641.28, 2607.749, 2584.186, 2546.775, 2544.28, 2527.924, 2521.656, 2492.791, 2503.144, 2480.116, 2515.98, 2484.938, 2486.392, 2480.601, 2481.899, 2479.502, 2446.529, 2439.489, 2433.492, 2435.298, 2432.424, 2430.99, 2416.219, 2421.268, 2407.851, 2401.494, 2412.75, 2425.681, 2407.296, 2426.822, 2414.676, 2421.894, 2409.039, 2394.142, 2417.689, 2414.934, 2418.768, 2401.792, 2421.059, 2415.709, 2403.652, 2398.417, 2417.212, 2384.059, 2409.556, 2375.735, 2397.908, 2404.377, 2385.128, 2387.634, 2391.351, 2390.108, 2385.027, 2428.634, 2415.798, 2392.649, 2425.304, 2393.671, 2373.115, 2417.937, 2401.902, 2398.004, 2412.447, 2402.968, 2411.477, 2417.136, 2401.009, 2376.856, 2430.115, 2400.421, 2374.24, 2412.945, 2386.323, 2395.431, 2396.299, 2423.714, 2403.937, 2393.376, 2413.874, 2393.735]}	100	100	True
