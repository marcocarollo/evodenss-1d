id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:101 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	31650.91211		360190	11	-1	231.24102115631104	{'train_loss': [486769.219, 363751.312, 327352.812, 312406.812, 304438.781, 300059.375, 295613.156, 290654.344, 285916.438, 281894.625, 277989.188, 275378.406, 273383.438, 270869.281, 269282.344, 266961.5, 265746.375, 263611.219, 262604.406, 260663.453, 259481.375, 259370.781, 257031.016, 255715.719, 255882.984, 253531.125, 253131.5, 252989.531, 251074.359, 249883.141, 249689.781, 248501.125, 248447.828, 247783.391, 246611.953, 245358.062, 243987.703, 243227.531, 242838.5, 241728.344, 241340.562, 240814.641, 239471.078, 238946.328, 238104.422, 237906.781, 237719.344, 236794.734, 237334.859, 236126.75, 235564.844, 235215.875, 234484.141, 234396.547, 234264.922, 233899.047, 233437.969, 233122.531, 231983.422, 232206.328, 231414.891, 231717.906, 230809.922, 230587.125, 230351.672, 229536.625, 229431.5, 227847.344, 229173.188, 228756.469, 228611.469, 228492.422, 227777.484, 226857.766, 227050.828, 226580.547, 226581.109, 226884.688, 226762.172, 225335.922, 225312.531, 224583.844, 224515.594, 224080.141, 224412.406, 223424.891, 224276.641, 223844.391, 223196.875, 224015.438, 223522.812, 222728.0, 222332.531, 222957.969, 222672.578, 222330.156, 222048.0, 222309.156, 222194.172, 221231.531], 'val_loss': [4060.276, 3065.669, 2931.548, 2850.426, 2827.518, 2790.002, 2756.414, 2726.518, 2690.292, 2666.56, 2644.741, 2624.644, 2592.047, 2584.239, 2580.429, 2581.029, 2543.838, 2546.964, 2539.682, 2504.939, 2500.892, 2477.689, 2480.829, 2465.676, 2461.124, 2448.966, 2438.829, 2441.27, 2433.965, 2421.639, 2407.108, 2411.985, 2400.117, 2401.926, 2392.54, 2391.799, 2383.172, 2368.972, 2379.985, 2367.881, 2345.353, 2361.312, 2353.683, 2361.978, 2344.321, 2356.025, 2342.895, 2350.794, 2354.831, 2313.089, 2311.399, 2327.96, 2324.292, 2328.589, 2342.687, 2327.813, 2314.159, 2313.964, 2316.049, 2285.921, 2323.111, 2294.719, 2297.07, 2295.165, 2284.491, 2277.324, 2282.394, 2292.384, 2266.418, 2271.57, 2291.732, 2291.144, 2268.531, 2272.275, 2267.551, 2257.762, 2272.346, 2257.2, 2261.362, 2272.18, 2248.305, 2276.474, 2278.645, 2267.471, 2272.904, 2248.577, 2249.972, 2259.878, 2256.731, 2254.636, 2252.527, 2257.621, 2251.118, 2231.721, 2242.765, 2241.011, 2232.867, 2216.67, 2238.628, 2263.985]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:101 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	3000	True	31894.26562		360190	11	-1	222.58396577835083	{'train_loss': [491286.406, 369038.562, 327355.656, 314633.406, 306383.375, 297466.938, 290876.562, 285248.031, 280780.812, 276331.969, 273648.75, 270342.031, 268322.531, 266128.344, 263849.031, 262469.781, 260038.5, 257963.906, 255456.234, 252667.891, 252199.234, 250473.344, 249063.672, 247296.625, 247192.016, 245950.359, 244513.312, 244037.016, 242350.234, 240926.438, 240645.516, 240372.953, 239900.984, 238577.828, 238316.656, 236563.375, 236351.484, 236416.484, 235693.422, 235659.5, 234500.312, 233380.203, 232750.953, 231964.422, 232098.234, 231404.438, 230967.0, 230403.938, 230232.75, 229622.375, 229678.031, 229355.609, 228729.141, 228472.406, 227058.281, 227954.656, 227331.719, 226667.688, 226191.531, 226694.016, 225703.547, 225655.516, 225368.109, 224874.25, 224709.531, 224032.234, 224011.078, 223610.438, 222454.094, 222628.328, 222764.234, 222927.266, 222600.938, 222703.703, 221595.875, 221402.188, 221208.484, 221002.266, 221729.328, 219853.344, 221325.984, 221077.922, 219763.109, 220548.953, 219199.469, 219329.109, 218366.812, 218810.781, 218439.406, 217660.656, 218395.516, 217126.875, 218322.125, 217589.828, 218363.734, 218017.875, 217242.609, 217476.172, 216612.938, 216831.641], 'val_loss': [3770.543, 3066.46, 2929.555, 2904.809, 2817.931, 2767.669, 2725.7, 2676.667, 2672.988, 2616.696, 2596.305, 2571.033, 2571.279, 2530.774, 2500.558, 2492.89, 2465.985, 2458.307, 2430.907, 2438.153, 2448.459, 2393.255, 2396.549, 2356.665, 2393.937, 2358.511, 2360.48, 2360.779, 2347.106, 2348.931, 2373.43, 2330.323, 2334.269, 2323.637, 2308.801, 2329.021, 2324.967, 2332.927, 2338.543, 2303.033, 2308.056, 2336.8, 2311.563, 2313.331, 2271.698, 2281.621, 2274.199, 2285.281, 2286.598, 2245.946, 2276.413, 2259.809, 2271.288, 2272.112, 2275.831, 2278.976, 2278.142, 2257.964, 2253.561, 2264.884, 2267.892, 2242.773, 2240.98, 2262.412, 2278.268, 2249.082, 2250.524, 2274.742, 2246.796, 2242.234, 2247.544, 2269.942, 2270.996, 2256.492, 2256.651, 2226.54, 2245.086, 2241.457, 2247.399, 2256.283, 2208.545, 2244.62, 2242.462, 2248.844, 2224.26, 2219.306, 2244.898, 2226.45, 2249.61, 2216.924, 2221.355, 2248.793, 2235.56, 2251.973, 2219.038, 2217.106, 2212.974, 2256.756, 2224.509, 2215.74]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:101 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:rmsprop lr:0.21054279260523712 alpha:0.9832216546640998 weight_decay:9.412890287103762e-05 batch_size:32 epochs:100	100	1000	True	137268.90625		355894	10	-1	211.19852328300476	{'train_loss': [1276950.75, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 2444857.5, 1066921.0, 1066921.0, 1066921.0, 1083442.375, 1066921.0, 1083623.375, 1069951.375, 1190040.125, 1098193.375, 1097283.875, 5179520.0, 1066921.0, 1082171.25, 1066921.0, 1066921.0, 1066921.0, 1432813.75, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1418943.25, 1162292.375, 1068729.75, 1069016.875, 2808957.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 2562108.25, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1164882.0, 1067249.5, 1066921.0, 1403995.625, 1087101.5, 1090170.375, 1066921.0, 1066921.0, 1066921.0, 1100470.75, 1066921.0, 1066921.0, 1092174.125, 4402039.0, 1067245.25, 1066921.0, 1066921.0, 1074155.5, 2974358.5, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 2678295.5, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 2136200.75, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1926790.5, 1066928.25, 1066921.0, 1066921.0, 1066963.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.212, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:101 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:rmsprop lr:0.14577577878179787 alpha:0.9202785346536102 weight_decay:0.0004819135334123488 batch_size:32 epochs:100	100	2000	True	137265.75		306118	10	-1	228.9752004146576	{'train_loss': [1605994.375, 1309943.375, 1545822.25, 1248543.375, 1317596.375, 1113480.0, 1343655.375, 1066921.0, 1242038.75, 1110347.25, 1202122.0, 1127663.375, 1227799.0, 1136671.25, 1123415.5, 1177517.75, 1125314.625, 1145415.75, 1084518.875, 1132367.75, 1207811.125, 1113242.25, 1109443.375, 1154685.625, 1195721.625, 1080023.375, 1143154.5, 1208690.625, 1106249.875, 1131986.375, 1229357.0, 1120336.125, 1121034.375, 1148362.125, 1190665.375, 1280645.125, 1120451.375, 1193046.5, 1091983.125, 1169151.125, 1127206.125, 1109441.875, 1159014.75, 1132488.0, 1141683.875, 1121176.5, 1116065.875, 1176690.375, 1134876.375, 1106040.5, 1145267.25, 1154645.625, 1118836.5, 1170084.25, 1117957.625, 1126211.625, 1139998.625, 1169434.875, 1117448.625, 1115660.375, 1171032.875, 1130960.625, 1147930.375, 1132320.5, 1134199.625, 1149792.5, 1122421.625, 1150143.75, 1111964.875, 1140748.25, 1133029.125, 1123283.875, 1144290.0, 1142414.75, 1112319.25, 1141619.875, 1164023.0, 1118775.0, 1127952.625, 1145047.625, 1152807.5, 1118213.875, 1121431.625, 1150027.25, 1140363.625, 1143243.0, 1136104.25, 1117671.375, 1134183.5, 1112829.625, 1156268.25, 1137507.875, 1120013.25, 1139869.5, 1142107.375, 1135491.25, 1117952.25, 1144195.75, 1349644.625, 1075698.875], 'val_loss': [10587.076, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10586.592, 10587.216, 10725.915, 11157.137, 10585.714, 10587.191, 11112.34, 11705.353, 10587.213, 10587.216, 10587.188, 10727.869, 10587.216, 10587.193, 11219.142, 12927.682, 10587.216, 10587.216, 10581.377, 13296.882, 10587.215, 11765.396, 10587.216, 10664.021, 10587.129, 12096.48, 122907.008, 10587.216, 10587.216, 10586.348, 10587.216, 10582.645, 10587.118, 10561.933, 10637.86, 10720.078, 10567.032, 10587.171, 11858.104, 15267.471, 10587.216, 11386.892, 11496.826, 13198.18, 10587.214, 10584.067, 10570.035, 12333.843, 10795.442, 11294.221, 10581.157, 10587.216, 10587.216, 10633.784, 10587.118, 11541.297, 10880.467, 10585.041, 10587.216, 10587.216, 10633.767, 11178.26, 10912.531, 11686.048, 10587.165, 10601.069, 13333.186, 13949.173, 11491.602, 10587.216, 10554.757, 11127.641, 10940.23, 10587.216, 10859.32, 10755.688, 13047.052, 12223.906, 11987.935, 10586.87, 10583.789, 10849.661, 10587.13, 10587.043, 10587.16, 11401.183, 10771.947, 11746.902, 10939.615, 10587.216, 10587.216, 10587.216]}	0	100	True
