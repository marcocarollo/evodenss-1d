id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	3000	True	31875.79102		576588	11	-1	208.26318049430847	{'train_loss': [655321.062, 352747.75, 315061.844, 301159.719, 294863.938, 289183.5, 284516.75, 280711.969, 277321.688, 272646.188, 269969.219, 267454.094, 265544.844, 263175.031, 261795.188, 259610.094, 258460.75, 256693.406, 255530.406, 254506.422, 253208.297, 252393.797, 251280.828, 251086.5, 249721.406, 248171.578, 247734.906, 246951.109, 246679.547, 245529.844, 245181.875, 244276.578, 243726.359, 242419.922, 241944.328, 242498.453, 241019.703, 240755.5, 240521.859, 239737.109, 240044.25, 237695.0, 237663.828, 238245.156, 238600.25, 237615.203, 236893.547, 235625.188, 236225.312, 235482.031, 235242.359, 234923.297, 234569.219, 234033.891, 233858.578, 233500.75, 233428.453, 233188.406, 232829.203, 233013.672, 232053.719, 231953.766, 232434.828, 232207.938, 232294.906, 230701.578, 231634.609, 230329.969, 230054.266, 228741.297, 229344.0, 229630.641, 230131.688, 229316.766, 228184.344, 228425.062, 227867.453, 229128.625, 227769.703, 227440.797, 227308.094, 227486.234, 227459.016, 226444.156, 225358.891, 226072.594, 227369.125, 226660.172, 225786.031, 225951.938, 225220.281, 226452.562, 225859.234, 224795.125, 224309.344, 225257.688, 224596.234, 225118.922, 223732.797, 223856.031], 'val_loss': [3462.365, 2977.268, 2848.511, 2791.872, 2752.542, 2723.615, 2672.587, 2648.883, 2608.802, 2585.648, 2550.711, 2529.397, 2504.682, 2497.509, 2491.333, 2472.551, 2458.816, 2449.666, 2441.827, 2429.945, 2411.291, 2427.51, 2411.565, 2424.277, 2410.457, 2392.969, 2381.784, 2393.811, 2364.75, 2377.321, 2359.675, 2374.438, 2359.258, 2347.491, 2354.97, 2348.203, 2335.021, 2349.783, 2346.876, 2346.596, 2327.971, 2339.621, 2324.06, 2324.92, 2317.667, 2314.297, 2320.103, 2297.785, 2328.107, 2318.24, 2301.037, 2314.381, 2327.142, 2296.641, 2300.198, 2295.635, 2306.091, 2297.721, 2294.91, 2292.415, 2279.74, 2294.273, 2282.289, 2285.684, 2279.638, 2286.104, 2274.093, 2272.053, 2268.979, 2264.257, 2280.504, 2272.283, 2253.267, 2258.89, 2259.184, 2267.181, 2258.096, 2256.451, 2237.624, 2242.498, 2239.041, 2258.384, 2242.413, 2226.955, 2245.639, 2234.398, 2235.443, 2243.326, 2237.114, 2234.648, 2240.125, 2236.179, 2239.538, 2236.01, 2229.846, 2217.562, 2225.368, 2241.831, 2209.233, 2252.541]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	31442.73438		559871	11	-1	211.6165909767151	{'train_loss': [847778.312, 397294.938, 321487.875, 306736.75, 301049.5, 296618.344, 291343.594, 286420.531, 281819.781, 276481.812, 273200.75, 271212.688, 266447.625, 264051.281, 262210.625, 259809.047, 258487.594, 256072.297, 254434.078, 253447.969, 251308.609, 249786.312, 249896.703, 247847.188, 245491.469, 246335.609, 244516.938, 243264.188, 242564.688, 241774.062, 239903.938, 240666.875, 239762.062, 239426.844, 236899.469, 237216.516, 236013.844, 234985.375, 235966.75, 234193.297, 234402.203, 233248.188, 232185.078, 232506.781, 230894.797, 230246.234, 229616.938, 229895.688, 228795.922, 227819.312, 227338.953, 226598.766, 227111.0, 224986.781, 224262.672, 224416.641, 224606.281, 223252.531, 223156.688, 222505.188, 222547.203, 221062.516, 221773.438, 220661.891, 220034.953, 220396.094, 220036.312, 219410.141, 219404.344, 219230.438, 219206.766, 217543.625, 217750.547, 216474.406, 217050.281, 216597.219, 215829.156, 216673.703, 215402.938, 215097.234, 215615.828, 214704.359, 214061.312, 213401.922, 214489.609, 213438.016, 211889.625, 212188.406, 212579.578, 212005.469, 210868.656, 211354.219, 210759.281, 210284.078, 210020.344, 209878.328, 209499.141, 209438.891, 209126.156, 209402.781], 'val_loss': [4354.95, 3042.428, 2888.452, 2864.561, 2844.604, 2795.925, 2759.966, 2678.717, 2638.298, 2564.887, 2614.061, 2567.121, 2528.806, 2513.943, 2509.571, 2467.874, 2430.499, 2448.363, 2456.628, 2424.14, 2430.329, 2398.208, 2392.973, 2411.232, 2403.855, 2390.434, 2390.653, 2350.293, 2367.938, 2380.437, 2377.673, 2348.246, 2370.309, 2336.947, 2339.27, 2341.912, 2363.611, 2355.333, 2350.735, 2329.193, 2328.163, 2315.597, 2346.172, 2324.412, 2305.251, 2316.26, 2305.613, 2293.552, 2310.01, 2335.24, 2301.812, 2306.295, 2291.19, 2305.198, 2270.459, 2301.473, 2272.121, 2282.377, 2273.033, 2262.708, 2258.401, 2320.55, 2287.723, 2270.855, 2264.901, 2260.277, 2278.706, 2259.975, 2227.572, 2243.031, 2235.286, 2250.034, 2256.688, 2234.179, 2237.457, 2260.175, 2257.293, 2236.594, 2249.28, 2235.825, 2263.841, 2226.362, 2233.976, 2249.193, 2254.102, 2222.49, 2270.786, 2233.522, 2238.75, 2229.915, 2254.295, 2229.438, 2238.965, 2255.802, 2231.882, 2251.91, 2250.861, 2228.749, 2240.344, 2220.77]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:104 kernel_size:5 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:73 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.2479663074513816 beta1:0.8371920109603805 beta2:0.8884255000142969 weight_decay:0.00032479062093170377 batch_size:32 epochs:100	100	1000	True	137416.39062		23304109	11	-1	247.51487183570862	{'train_loss': [2257147.5, 4242304.0, 1089484.875, 7486800.5, 1088205.25, 9873210.0, 2762988.75, 13679602.0, 8424090.0, 6824121.5, 4919303.0, 5574583.5, 5868750.5, 4514757.5, 7970783.5, 6912948.0, 1342405.875, 3356301.25, 4029221.75, 1500991.875, 4354842.0, 3212011.25, 2463963.0, 3108754.75, 4462559.0, 2820909.5, 2388783.75, 1777648.0, 1841855.375, 1612346.0, 2726855.0, 4299935.0, 1301070.0, 3111093.5, 2279937.75, 3023946.25, 1391418.875, 1092384.125, 1653733.375, 1873300.25, 1449865.875, 1807599.0, 1511775.625, 2142474.5, 1162067.75, 1743315.125, 1644806.75, 1552950.625, 1827378.375, 2047212.5, 1480368.875, 1269902.75, 1583754.5, 1523912.125, 1826587.5, 1454247.125, 1371746.75, 1390103.125, 1846760.75, 1814074.0, 1920204.375, 1328572.5, 1354971.375, 1909276.375, 1776722.0, 1525128.375, 1486535.75, 1338489.5, 1433114.25, 1445676.375, 2120065.75, 1681252.5, 1200195.25, 1884259.0, 1772855.625, 1538828.75, 1344589.0, 1710104.875, 1789355.5, 1589940.0, 1767437.75, 1433214.25, 1702037.0, 1353336.125, 1502702.0, 1951241.75, 1967351.375, 1257111.375, 1312552.25, 1544022.25, 1283990.625, 1519473.0, 1559637.0, 1543128.375, 1219708.125, 1428982.875, 2075896.375, 1702227.75, 1722679.125, 1324178.25], 'val_loss': [10587.216, 10587.216, 815196.562, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 66964.336, 10587.216, 10587.216, 10587.216, 10587.216, 90394.812, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 126809.734, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.16794919905472647 beta1:0.8274242095481892 beta2:0.8006042594673313 weight_decay:2.1139489352586978e-05 batch_size:32 epochs:100	100	1000	True	137288.01562		574988	11	-1	214.89741778373718	{'train_loss': [1129009.0, 1451386.125, 1250127.0, 1103347.625, 1121576.75, 1103990.625, 1071580.75, 1118925.5, 1102537.625, 1099352.5, 1113925.25, 1091515.0, 1142187.625, 1091222.375, 1118800.25, 1105706.0, 1119823.125, 1099185.875, 1125985.25, 1105625.25, 1101387.0, 1107703.25, 1109753.125, 1113263.875, 1133570.0, 1117890.5, 1104211.25, 1123950.625, 1104252.5, 1118591.25, 1124301.625, 1125187.625, 1102914.75, 1122689.0, 1111080.75, 1115665.625, 1103161.5, 1108693.625, 1134037.75, 1110082.75, 1127235.0, 1103282.0, 1122776.875, 1119187.875, 1127980.375, 1111387.25, 1099498.125, 1105028.25, 1123018.375, 1107595.375, 1123705.25, 1117174.5, 1118899.875, 1104826.125, 1115747.625, 1116716.625, 1106808.875, 1128573.0, 1111272.375, 1118984.125, 1096788.5, 1125247.75, 1095723.125, 1124880.625, 1095894.0, 1121820.75, 1102353.5, 1124483.625, 1136640.5, 1121449.625, 1107844.625, 1119534.25, 1108124.375, 1103974.625, 1115617.75, 1102779.25, 1114994.0, 1110515.625, 1106292.625, 1107623.875, 1109390.625, 1115580.25, 1114464.375, 1108847.0, 1115652.75, 1105769.125, 1127486.25, 1112779.875, 1110715.25, 1114476.125, 1112161.25, 1120108.0, 1124081.625, 1115600.125, 1105413.875, 1110128.125, 1107051.25, 1126414.625, 1107030.5, 1117126.0], 'val_loss': [10587.216, 10587.216, 10587.216, 11560.297, 10587.216, 10587.216, 10586.962, 10587.216, 10563.603, 10587.216, 10581.336, 10587.216, 10587.105, 10587.216, 11970.295, 10586.922, 10914.809, 11785.596, 11497.555, 10627.215, 11675.307, 10586.679, 10554.194, 10587.216, 10587.216, 10712.399, 10587.216, 10587.199, 10587.17, 10574.768, 10581.488, 10587.216, 11013.571, 10587.216, 10587.216, 10587.216, 10587.216, 10653.829, 10760.623, 11509.14, 11338.326, 10587.191, 10587.216, 10648.414, 11939.165, 10586.447, 10732.87, 10587.216, 10587.215, 11394.763, 11691.729, 11141.42, 10587.216, 10875.059, 10758.25, 12128.728, 11279.916, 10837.211, 10723.543, 11168.307, 10574.085, 11251.983, 11159.613, 10587.216, 10711.642, 11255.536, 10568.33, 10587.216, 11776.929, 11311.083, 10587.188, 10701.556, 10580.33, 10587.216, 12172.262, 10587.216, 10983.38, 11095.571, 11293.724, 10587.216, 10587.216, 11316.826, 10587.216, 10987.221, 10587.216, 10587.201, 12016.418, 10587.216, 10587.216, 10852.91, 10587.216, 11508.89, 10587.216, 10974.55, 10587.216, 11154.428, 10586.801, 10587.216, 10563.599, 10587.216]}	100	100	True
