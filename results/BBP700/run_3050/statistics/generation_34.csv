id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	31822.22266		469896	10	-1	199.1484396457672	{'train_loss': [523365.844, 345988.875, 318065.781, 304533.125, 296805.0, 289312.906, 283294.594, 277834.781, 274795.312, 271486.469, 269495.562, 267287.875, 265477.0, 263565.938, 262445.969, 260756.438, 259941.641, 258420.344, 257657.078, 255972.312, 254848.625, 254710.188, 253735.219, 253213.5, 251712.375, 250762.031, 250574.781, 250073.516, 249384.469, 247604.781, 247351.938, 246817.016, 244976.672, 245443.453, 244555.5, 244719.688, 243825.406, 243286.047, 243509.062, 242345.922, 242213.328, 242417.734, 241763.719, 240434.484, 240736.016, 240139.422, 239960.0, 239349.406, 239553.938, 238967.891, 238438.391, 238492.594, 238997.062, 237447.766, 237885.141, 237473.938, 235399.953, 235697.109, 235853.734, 234238.078, 234243.906, 235652.781, 234376.938, 234920.703, 234148.266, 234034.672, 233704.297, 233548.688, 232792.25, 232318.641, 231969.812, 232069.922, 232016.781, 232041.141, 232146.5, 231729.562, 232234.062, 231010.266, 231579.047, 231722.531, 231092.047, 230468.25, 229971.469, 229858.672, 229638.672, 229906.172, 229883.156, 228910.141, 229597.594, 229430.141, 228451.234, 227566.703, 228470.438, 228521.688, 228179.656, 228216.297, 228073.031, 227800.859, 227247.547, 227552.719], 'val_loss': [3252.493, 2956.437, 2838.758, 2789.199, 2725.408, 2670.364, 2631.293, 2590.025, 2569.851, 2523.282, 2528.72, 2494.677, 2464.102, 2478.313, 2479.7, 2456.774, 2446.169, 2439.791, 2428.781, 2423.724, 2403.513, 2415.615, 2390.617, 2398.637, 2396.646, 2401.506, 2368.79, 2371.811, 2350.396, 2347.263, 2361.061, 2352.328, 2325.331, 2348.877, 2314.685, 2329.8, 2324.26, 2319.742, 2319.792, 2326.914, 2331.921, 2301.485, 2342.972, 2319.665, 2323.849, 2293.857, 2291.725, 2299.736, 2330.817, 2296.319, 2298.334, 2288.316, 2308.479, 2284.909, 2292.466, 2269.762, 2269.871, 2281.79, 2266.066, 2265.676, 2258.16, 2270.258, 2259.326, 2247.969, 2258.952, 2276.354, 2247.096, 2279.447, 2241.382, 2259.667, 2245.569, 2249.952, 2246.362, 2247.408, 2238.511, 2240.149, 2245.993, 2241.545, 2228.766, 2238.611, 2237.951, 2222.567, 2236.73, 2254.982, 2226.156, 2218.318, 2236.05, 2210.801, 2228.782, 2231.074, 2229.392, 2220.975, 2220.364, 2208.179, 2220.511, 2200.187, 2220.453, 2200.399, 2214.741, 2210.224]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:119 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adam lr:0.10016343687631267 beta1:0.9826266485187283 beta2:0.8778323443877958 weight_decay:3.744489719801547e-05 batch_size:32 epochs:100	100	1000	True	2807198.5		12461871	10	-1	216.80523896217346	{'train_loss': [1145924.5, 1111596.875, 3808128.75, 1592241.125, 5627655.5, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 4616451.5, 1066921.0, 1066921.0, 2077196.375, 2455983.25, 2878600.25, 3396468.75, 5982509.0, 4204432.0, 3318527.5, 3011373.5, 2585300.5, 1418200.75, 1402029.5, 1066921.0, 1150244.125, 1735085.625, 1467630.375, 1210967.5, 1164692.125, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1078696.5, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1107894.125, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [1189601.5, 71225824.0, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adam lr:0.1563146211349351 beta1:0.8669850805050339 beta2:0.9879027543891726 weight_decay:1.995750797734579e-05 batch_size:32 epochs:100	100	1000	True	150477.78125		471496	10	-1	204.9010055065155	{'train_loss': [1128936.5, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1113155.875, 1075164.375, 1066921.0, 1066921.0, 1209532.5, 1072840.875, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1088181.625, 1066921.0, 1067305.25, 1067234.25, 1079402.0, 1087446.75, 1067681.5, 1147498.125, 1066921.0, 1202150.125, 1131512.125, 1095633.75, 1069387.75, 1084933.375, 1071492.125, 1066921.0, 1102209.625, 1066919.875, 1120091.375, 1066921.0, 1066921.0, 1066921.0, 1068583.125, 1212514.25, 1066939.375, 1070764.375, 1066939.25, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1127483.375, 1091659.375, 1148039.625, 1096301.125, 1098275.875, 1071283.0, 1070104.625, 1072397.75, 1067428.875, 1083511.75, 1067235.125, 1081033.375, 1066921.0, 1074484.875, 1072731.25, 1073682.375, 1079735.0, 1412398.875, 1067496.125, 1072720.875, 1066941.625, 1066921.0, 1066921.0, 1073396.0, 1066935.875, 1066923.75, 1067094.0, 1083148.25, 1071489.125, 1068834.5, 1074899.75, 1081075.75, 1071074.75, 1227282.5, 1070147.125, 1068157.25, 1149359.625, 1067379.0, 1075155.75, 1066921.0, 1072805.125, 1072496.25, 1066921.0, 1066921.0, 1075428.5, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 25897.406, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 11311.076, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 23271.238, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10577.413, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10584.446, 10587.216, 10587.216, 10587.216, 12536.435, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10786.814, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 11668.568]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:32 epochs:100	100	1000	True	32297.20703		366638	9	-1	188.1795027256012	{'train_loss': [449000.125, 343078.219, 318885.688, 310676.75, 303828.844, 299900.219, 296763.906, 292177.094, 286509.406, 282169.875, 278076.656, 276149.812, 273713.938, 272927.094, 271464.438, 269899.688, 267299.344, 267173.406, 265867.625, 264980.562, 264607.5, 263460.406, 262462.531, 261522.531, 260774.25, 259906.0, 258770.797, 258546.047, 258157.125, 256639.438, 256849.688, 256719.766, 255649.797, 254831.641, 254337.766, 254051.047, 253650.844, 252488.141, 252477.734, 251836.531, 253104.562, 250897.047, 250701.922, 249393.594, 250330.266, 249372.328, 249404.125, 249067.031, 248326.672, 248359.906, 248078.953, 247518.203, 247188.047, 246545.953, 246719.719, 246224.156, 245772.719, 245699.141, 245576.406, 244056.562, 244171.562, 244225.719, 245373.109, 243213.266, 244030.047, 243697.906, 243647.688, 241944.422, 242656.891, 242369.594, 243169.594, 241926.922, 241386.047, 241725.719, 242151.703, 241306.797, 240965.844, 240560.438, 239539.547, 240404.594, 241433.219, 240330.141, 240108.5, 239956.469, 239380.0, 239068.156, 238356.469, 238062.156, 238662.297, 238380.641, 238476.453, 239187.719, 238409.328, 237525.25, 237894.25, 237702.766, 237241.062, 237301.328, 237593.969, 237026.094], 'val_loss': [3410.938, 2971.859, 2900.996, 2852.715, 2816.489, 2796.38, 2758.678, 2700.282, 2649.698, 2615.85, 2605.243, 2579.511, 2563.142, 2559.069, 2550.685, 2539.597, 2529.529, 2525.626, 2522.645, 2502.278, 2504.882, 2517.959, 2497.493, 2480.467, 2462.941, 2477.268, 2480.252, 2470.98, 2456.02, 2440.924, 2449.018, 2442.091, 2430.457, 2422.845, 2441.844, 2437.235, 2423.869, 2421.488, 2419.344, 2401.898, 2411.052, 2407.473, 2394.672, 2385.186, 2399.614, 2390.925, 2404.164, 2387.921, 2411.981, 2373.861, 2368.353, 2390.93, 2351.952, 2364.344, 2367.677, 2348.926, 2358.81, 2358.916, 2349.847, 2361.57, 2365.438, 2370.679, 2356.243, 2339.652, 2336.917, 2334.565, 2332.062, 2338.668, 2323.586, 2318.772, 2326.715, 2334.067, 2321.936, 2316.65, 2329.648, 2326.934, 2330.642, 2339.466, 2315.88, 2329.953, 2316.475, 2323.218, 2305.7, 2311.805, 2325.363, 2314.177, 2292.014, 2314.844, 2306.07, 2300.517, 2300.415, 2301.401, 2291.871, 2292.573, 2276.941, 2302.77, 2286.675, 2288.355, 2313.397, 2274.748]}	100	100	True
