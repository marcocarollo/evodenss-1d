id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	32442.25586		576588	11	-1	238.83681416511536	{'train_loss': [707195.0, 354056.906, 314422.719, 301872.0, 295008.344, 290955.688, 286668.375, 283641.531, 280481.312, 277084.406, 274075.469, 270633.969, 268117.906, 266667.625, 263748.469, 261927.0, 260909.656, 259470.656, 257082.953, 256505.125, 254561.281, 253490.312, 252675.891, 251517.734, 250063.766, 249931.453, 249049.547, 248143.906, 247623.156, 246951.172, 245834.969, 245265.469, 244641.359, 244123.859, 243882.297, 242476.953, 242030.688, 241759.203, 242247.766, 240504.109, 240280.797, 240145.688, 240144.172, 238573.438, 238767.844, 237719.609, 238303.406, 238174.781, 237838.922, 236916.438, 235987.797, 236800.297, 236498.688, 235450.25, 235284.656, 235070.125, 234710.391, 234486.344, 234763.406, 233893.297, 232942.828, 233169.891, 232347.578, 233611.438, 231615.516, 232635.438, 232383.469, 232181.531, 231394.547, 231487.031, 230033.875, 231042.188, 229961.25, 230572.797, 230513.062, 230914.156, 229876.469, 229689.406, 230012.297, 229346.859, 228471.672, 229360.281, 227854.641, 228016.703, 228164.391, 228007.312, 227207.125, 227162.922, 227992.25, 226914.859, 227205.172, 226218.203, 225591.703, 226573.672, 225920.422, 227117.594, 226143.047, 225142.094, 225060.734, 225023.453], 'val_loss': [3247.214, 3443.05, 2841.342, 2805.963, 2750.419, 2723.094, 2688.687, 2674.936, 2656.361, 2619.207, 2578.292, 2551.881, 2529.601, 2538.281, 2518.98, 2505.696, 2505.37, 2501.347, 2496.136, 2461.116, 2466.922, 2443.96, 2442.419, 2441.661, 2449.278, 2401.382, 2409.098, 2411.463, 2406.969, 2392.866, 2399.5, 2370.038, 2412.179, 2380.415, 2387.854, 2397.782, 2365.874, 2378.436, 2369.253, 2366.639, 2361.457, 2337.296, 2355.204, 2364.715, 2345.265, 2344.777, 2331.095, 2347.996, 2332.47, 2311.549, 2347.878, 2330.966, 2327.392, 2324.588, 2366.372, 2319.55, 2321.788, 2316.086, 2315.008, 2303.774, 2317.081, 2303.081, 2305.545, 2332.195, 2307.336, 2292.891, 2300.127, 2292.644, 2310.938, 2295.616, 2300.455, 2312.431, 2312.737, 2301.473, 2292.959, 2295.047, 2276.866, 2265.844, 2264.331, 2286.602, 2275.045, 2265.047, 2272.322, 2270.829, 2292.292, 2274.034, 2283.608, 2292.075, 2268.274, 2269.925, 2287.692, 2265.405, 2312.557, 2258.485, 2275.328, 2267.714, 2249.104, 2266.76, 2275.017, 2272.496]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	137316.59375		18036521	11	-1	287.79129123687744	{'train_loss': [1223653.5, 1066921.0, 1066921.0, 1066921.0, 1066899.625, 1151876.625, 1067139.25, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	31281.50977		576588	11	-1	236.6887331008911	{'train_loss': [736704.812, 363273.344, 318023.125, 304247.656, 296844.594, 290594.281, 284614.219, 280067.219, 276581.594, 273526.219, 272180.781, 269706.188, 269384.469, 265432.312, 263712.875, 262067.391, 260273.531, 258270.5, 258223.922, 255747.188, 254928.5, 254019.438, 252108.453, 251248.438, 249801.562, 249444.516, 247580.0, 247047.0, 245955.312, 244320.438, 245054.062, 243708.062, 242467.578, 242877.625, 241244.031, 240572.484, 239420.875, 239204.062, 238632.703, 238119.75, 237753.656, 237147.781, 236447.531, 236398.188, 235461.656, 236128.406, 234578.984, 234731.5, 234107.562, 234122.828, 233335.266, 231961.938, 232919.797, 232500.656, 232149.141, 231885.234, 231613.391, 230325.594, 229764.906, 229857.609, 230196.906, 229350.422, 228536.062, 228934.719, 227531.641, 228259.594, 227687.547, 226272.062, 226599.156, 226892.047, 226222.438, 225914.984, 225535.734, 225549.031, 225427.359, 224983.359, 224960.531, 224643.938, 225110.359, 224105.812, 223762.125, 223528.078, 223224.438, 224352.562, 222446.031, 222627.734, 221760.828, 222182.625, 221400.562, 221531.188, 220675.281, 221051.125, 221462.484, 220770.078, 220936.797, 220090.297, 220641.453, 220490.156, 220721.156, 220707.812], 'val_loss': [3381.511, 3115.305, 2872.445, 2795.646, 2764.706, 2706.67, 2658.411, 2635.137, 2603.326, 2580.052, 2546.897, 2552.546, 2526.938, 2506.271, 2481.221, 2476.843, 2465.292, 2445.177, 2464.01, 2414.108, 2410.249, 2421.862, 2396.824, 2381.156, 2400.629, 2395.471, 2374.552, 2393.132, 2350.935, 2346.811, 2344.88, 2338.443, 2319.418, 2332.321, 2337.798, 2312.415, 2314.768, 2332.873, 2319.955, 2313.714, 2280.253, 2278.601, 2269.304, 2267.893, 2287.525, 2272.465, 2257.069, 2269.793, 2284.434, 2286.906, 2246.842, 2259.909, 2280.125, 2250.56, 2249.86, 2238.622, 2254.63, 2242.804, 2244.746, 2228.896, 2221.213, 2224.237, 2230.717, 2218.544, 2228.736, 2224.956, 2212.019, 2232.896, 2252.803, 2214.31, 2242.739, 2206.465, 2203.291, 2200.917, 2211.004, 2222.402, 2193.185, 2219.522, 2223.858, 2195.166, 2190.536, 2206.505, 2183.828, 2183.056, 2195.973, 2193.416, 2205.23, 2205.086, 2176.461, 2204.745, 2190.269, 2192.433, 2198.908, 2187.642, 2184.577, 2177.939, 2184.452, 2176.281, 2198.529, 2177.6]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:5 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	137290.57812		9107701	10	-1	268.0512766838074	{'train_loss': [1149367.375, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1090478.375, 1066921.0, 1066921.0, 1066921.0, 1109174.625, 1066921.0, 1075221.25, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1079447.25, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
