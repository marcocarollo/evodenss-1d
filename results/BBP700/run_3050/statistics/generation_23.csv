id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	31101.73438		576588	11	-1	204.51227807998657	{'train_loss': [881185.688, 432397.094, 332875.75, 307353.719, 299010.062, 293969.656, 288706.5, 283090.625, 276463.375, 271218.719, 267841.406, 262936.969, 260920.234, 257313.859, 255835.719, 252362.812, 252059.797, 249742.828, 247930.922, 247343.844, 245101.359, 244260.719, 242856.312, 242411.969, 240959.016, 240111.109, 238524.578, 238020.672, 237368.688, 236637.234, 235640.734, 234573.234, 232902.531, 233165.688, 231885.094, 232170.766, 230633.859, 229660.609, 229314.875, 228948.719, 228489.172, 226367.812, 227491.234, 225147.922, 225297.312, 224407.453, 225370.688, 223788.438, 223126.578, 222435.641, 222528.625, 221549.609, 220307.156, 219886.734, 219978.891, 219275.562, 217969.578, 218169.328, 217776.656, 217031.984, 217074.969, 216328.266, 215979.406, 215432.453, 215565.438, 214843.156, 213721.016, 213283.703, 213393.172, 212776.094, 211489.281, 212545.875, 212172.422, 211872.438, 210699.062, 211017.109, 210088.594, 210318.625, 208881.469, 209517.281, 209363.766, 209712.938, 207433.438, 207863.297, 207587.906, 206885.547, 206880.344, 206897.562, 206970.531, 206171.516, 206258.422, 205577.219, 205463.719, 204132.453, 204202.797, 204270.094, 202825.656, 203808.094, 203937.625, 203146.531], 'val_loss': [5931.036, 3105.407, 2908.38, 2836.624, 2815.61, 2761.253, 2709.008, 2646.308, 2601.56, 2576.925, 2503.628, 2496.173, 2467.539, 2448.567, 2409.846, 2411.269, 2387.609, 2375.4, 2354.994, 2378.586, 2365.415, 2345.438, 2390.552, 2331.991, 2338.939, 2314.386, 2308.125, 2308.831, 2302.439, 2316.829, 2286.753, 2285.599, 2306.305, 2293.959, 2307.171, 2293.45, 2257.743, 2270.318, 2276.113, 2253.704, 2259.48, 2260.014, 2262.499, 2245.45, 2231.098, 2241.419, 2221.379, 2248.291, 2247.722, 2227.528, 2226.031, 2256.672, 2255.723, 2235.131, 2248.282, 2253.226, 2240.766, 2217.191, 2210.621, 2205.681, 2211.387, 2262.919, 2209.827, 2201.543, 2238.036, 2212.313, 2211.251, 2229.134, 2206.6, 2198.364, 2225.809, 2237.501, 2214.412, 2206.581, 2201.471, 2223.23, 2210.885, 2219.217, 2219.563, 2208.5, 2224.628, 2205.987, 2200.28, 2213.679, 2207.428, 2189.389, 2180.264, 2197.294, 2205.255, 2194.183, 2192.938, 2208.46, 2214.641, 2203.194, 2216.4, 2217.544, 2211.213, 2189.448, 2201.789, 2205.358]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:46 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	2000	True	32512.93164		444150	12	-1	216.64584398269653	{'train_loss': [621667.0, 387734.125, 314463.969, 301828.094, 295930.844, 290834.25, 287060.594, 284035.031, 280402.781, 277243.844, 274310.188, 270597.25, 267818.75, 265279.375, 263641.0, 260868.812, 259388.094, 257568.422, 256528.281, 254895.047, 253491.219, 252405.016, 251168.875, 250198.469, 248770.359, 247500.656, 247011.469, 246235.062, 245732.594, 245130.344, 244549.375, 243282.938, 243395.125, 242079.141, 242055.719, 240853.562, 240472.25, 239729.516, 239056.906, 238911.547, 238523.453, 238427.391, 236959.281, 237009.516, 236634.906, 235802.844, 235640.922, 235216.875, 234123.266, 233848.281, 234869.719, 233432.672, 234225.641, 233156.828, 232486.625, 232692.953, 231046.328, 231566.531, 230304.109, 229965.844, 229601.109, 229239.625, 229274.469, 227556.562, 228832.219, 228070.25, 228224.25, 228049.141, 227354.875, 226939.906, 226480.797, 225879.375, 226173.25, 226030.531, 224885.703, 225188.812, 224202.422, 224743.062, 224381.734, 224235.812, 224157.828, 223810.016, 222929.594, 223625.453, 223726.766, 222212.812, 222568.312, 222293.578, 221369.25, 221599.562, 221216.016, 220978.906, 221359.938, 220396.641, 221192.406, 220517.859, 219802.469, 219552.359, 219072.203, 218646.203], 'val_loss': [4044.969, 2952.663, 2862.476, 2836.203, 2783.101, 2759.159, 2723.367, 2690.199, 2663.833, 2648.911, 2605.762, 2603.361, 2555.195, 2542.386, 2518.055, 2495.979, 2475.126, 2486.006, 2456.283, 2467.928, 2437.365, 2424.443, 2433.936, 2426.319, 2412.638, 2403.095, 2394.762, 2403.096, 2403.672, 2388.954, 2384.365, 2375.292, 2382.76, 2380.634, 2356.533, 2371.364, 2364.428, 2373.572, 2349.068, 2372.299, 2340.708, 2336.329, 2338.744, 2374.074, 2344.068, 2329.642, 2344.015, 2337.808, 2344.178, 2326.501, 2352.971, 2348.51, 2325.902, 2343.694, 2317.01, 2301.6, 2337.251, 2321.926, 2328.837, 2320.548, 2331.015, 2318.306, 2334.563, 2346.148, 2336.546, 2316.948, 2295.676, 2282.715, 2329.382, 2298.557, 2294.897, 2268.465, 2283.544, 2320.242, 2287.457, 2286.165, 2311.026, 2283.064, 2279.967, 2312.873, 2273.094, 2312.972, 2284.527, 2284.046, 2291.118, 2305.18, 2269.087, 2264.092, 2293.174, 2290.646, 2294.87, 2279.627, 2278.985, 2256.671, 2316.219, 2288.502, 2295.071, 2256.566, 2314.433, 2292.924]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adam lr:0.2099423155505395 beta1:0.8806081339653778 beta2:0.8148139930685536 weight_decay:4.347268099951661e-05 batch_size:32 epochs:100	100	1000	True	137284.15625		409857	10	-1	198.90014553070068	{'train_loss': [1223264.625, 1325040.125, 1296722.625, 1101571.5, 1102192.5, 1107328.875, 1077523.625, 1102551.0, 1094160.875, 1077731.5, 1102215.75, 1072593.625, 1110204.75, 1077100.25, 1086913.625, 1077284.875, 1081474.5, 1079600.5, 1076559.875, 1074576.875, 1074396.0, 1085036.375, 1078659.625, 1080778.125, 1090132.125, 1078362.0, 1091727.625, 1082010.5, 1080760.125, 1081274.5, 1081787.0, 1125258.125, 1071319.75, 1085961.0, 1082602.125, 1097750.125, 1082965.125, 1093580.625, 1083881.5, 1073005.125, 1085145.125, 1085655.125, 1085941.75, 1080478.375, 1074034.125, 1079514.25, 1084672.0, 1077641.75, 1082130.875, 1071874.625, 1078028.625, 1081753.5, 1090264.75, 1107692.625, 1074261.125, 1082928.0, 1073557.375, 1074622.75, 1084249.0, 1079838.875, 1091954.75, 1083194.0, 1092448.75, 1086213.875, 1088527.0, 1076950.375, 1080133.125, 1081359.375, 1087582.75, 1086746.375, 1092338.625, 1100636.0, 1083770.75, 1088312.25, 1076320.0, 1078946.125, 1083298.75, 1080084.625, 1085559.0, 1079315.125, 1084024.875, 1080653.75, 1083492.125, 1094338.125, 1081424.25, 1077558.0, 1084091.75, 1077848.0, 1085703.75, 1078530.5, 1074812.25, 1081218.5, 1080442.625, 1080423.75, 1073280.125, 1089495.25, 1088224.5, 1085702.75, 1078016.125, 1071921.75], 'val_loss': [16044.826, 10587.216, 10587.216, 10579.364, 10587.046, 31721.242, 10607.954, 10587.216, 10555.114, 10587.216, 10615.661, 10587.216, 11009.27, 10587.216, 10587.216, 10587.007, 10820.844, 10683.852, 10736.692, 10531.561, 10587.216, 10766.61, 10587.054, 10587.216, 10567.337, 10696.868, 10808.288, 10587.216, 10587.216, 11316.024, 10584.062, 10954.844, 10587.216, 10587.216, 10587.216, 11067.194, 10733.236, 10587.216, 10587.077, 10587.216, 10587.216, 11200.59, 10587.216, 10570.568, 10866.024, 10784.252, 11242.365, 10586.91, 10557.023, 12067.233, 10587.216, 11935.115, 10587.208, 10586.556, 10587.216, 11045.946, 10587.212, 10587.216, 10953.634, 10909.049, 10587.216, 10587.212, 11066.389, 10586.277, 10587.161, 10587.076, 10587.216, 11582.84, 10587.216, 10587.055, 10586.926, 10587.216, 10587.216, 10563.804, 10587.216, 10587.122, 12153.234, 10552.618, 10793.53, 10587.216, 10587.216, 10587.216, 10587.216, 11022.345, 10943.683, 10586.85, 10587.216, 10675.36, 10630.51, 10586.667, 10891.061, 10587.197, 10942.667, 10587.216, 11192.986, 10813.529, 10899.971, 10587.216, 10662.085, 10587.216]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:rmsprop lr:0.1422403232725541 alpha:0.8067126920322805 weight_decay:8.006539227412938e-05 batch_size:32 epochs:100	100	1000	True	152286.6875		409857	10	-1	201.92410254478455	{'train_loss': [1550840.75, 1208078.5, 1238532.125, 1276648.875, 1241343.125, 1170579.0, 1185665.75, 1156621.125, 1215358.125, 1188872.125, 1160871.875, 1217752.25, 1208261.75, 1191983.5, 1187449.25, 1224353.0, 1182553.75, 1193488.625, 1247266.625, 1176725.75, 1269165.75, 1236198.25, 1220894.5, 1181295.75, 1206106.0, 1204264.375, 1189667.375, 1227116.0, 1205319.5, 1177103.125, 1224726.125, 1190629.625, 1225938.75, 1217888.25, 1236660.375, 1213603.625, 1177148.875, 1262188.625, 1166515.375, 1234476.0, 1198816.625, 1234951.625, 1219011.25, 1309511.25, 1107235.875, 1251552.25, 1170471.75, 1251482.25, 1250600.875, 1199814.0, 1242303.125, 1182850.0, 1233643.875, 1204960.0, 1234239.375, 1218417.25, 1216340.625, 1189915.75, 1218985.375, 1204137.375, 1204260.625, 1197376.5, 1195929.375, 1230719.375, 1221486.25, 1225295.75, 1234706.375, 1141642.5, 1216328.25, 1237090.5, 1164503.25, 1262815.75, 1205101.875, 1215482.0, 1211669.75, 1236761.375, 1203153.375, 1201344.875, 1212076.25, 1207352.5, 1211396.25, 1224220.5, 1218291.625, 1218182.25, 1227304.0, 1187439.375, 1234873.0, 1177773.875, 1233153.375, 1198037.0, 1227035.5, 1213309.125, 1213317.0, 1221335.875, 1265264.5, 1160285.75, 1249389.0, 1187764.125, 1236659.625, 1210291.5], 'val_loss': [13375.891, 10587.216, 11530.255, 10770.598, 10587.216, 11328.438, 11468.434, 12824.376, 10964.336, 10820.814, 11103.899, 11308.585, 12438.816, 10580.983, 12674.272, 10721.705, 10568.407, 15017.265, 36701.438, 10928.405, 11395.998, 12780.024, 14190.748, 10728.272, 11679.858, 10704.076, 10566.275, 10761.438, 11411.488, 11189.165, 12731.535, 10879.36, 10944.634, 17800.525, 11200.136, 12564.015, 10587.215, 10576.962, 10587.216, 10587.159, 10856.098, 10937.219, 12624.551, 10585.945, 10587.209, 10553.686, 11103.499, 12341.261, 11404.73, 11192.194, 10586.926, 14741.683, 10586.729, 16168.634, 12188.926, 10707.956, 11151.693, 11182.48, 13999.494, 15282.004, 10587.208, 11342.938, 11336.938, 11332.986, 12241.172, 11225.156, 10834.16, 10847.509, 10919.189, 12398.683, 12540.942, 11811.525, 11401.988, 10585.089, 11371.5, 11521.953, 10587.216, 11537.639, 12907.496, 11296.871, 11920.697, 10584.371, 11265.087, 11208.206, 10687.021, 10583.398, 11730.441, 11205.692, 12399.204, 10587.171, 12317.836, 10584.324, 10578.601, 10583.26, 10587.212, 14839.158, 12514.545, 16820.717, 10790.495, 11837.824]}	100	100	True
