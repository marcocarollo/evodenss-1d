id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	31002.18359		559964	10	-1	200.62438488006592	{'train_loss': [697278.75, 348311.25, 314687.875, 300553.531, 291169.562, 284143.312, 277710.688, 273593.219, 269706.812, 266434.875, 263461.25, 260225.328, 257460.469, 255856.969, 253488.781, 252155.172, 250931.938, 248690.688, 247717.547, 246615.047, 246031.984, 244903.422, 242806.312, 242365.234, 241727.656, 239715.484, 240143.516, 239335.969, 237951.625, 239050.0, 237273.875, 235149.406, 235707.547, 235364.344, 234193.797, 233819.312, 234013.969, 233397.156, 233035.922, 232179.188, 231022.516, 231365.031, 231432.422, 230412.609, 229819.047, 229845.594, 228498.906, 230269.812, 228471.375, 227985.141, 228355.75, 226581.906, 226919.438, 226592.922, 226850.219, 225044.828, 226230.219, 224838.75, 225552.703, 225371.25, 224436.797, 224054.516, 223364.297, 222830.547, 223473.656, 221519.562, 222370.906, 222405.875, 222308.438, 223157.094, 222373.406, 221629.391, 221469.203, 221111.547, 221304.969, 221045.234, 219951.234, 220733.969, 219926.062, 219856.969, 219827.078, 220056.891, 219663.344, 219157.562, 218669.25, 219034.422, 218528.766, 219570.297, 219370.531, 218852.844, 217625.25, 218176.922, 216543.781, 217561.125, 217721.062, 217126.375, 216668.125, 215713.016, 215782.797, 217259.062], 'val_loss': [3277.7, 2965.564, 2825.589, 2775.197, 2675.831, 2627.466, 2597.042, 2616.873, 2532.101, 2502.123, 2467.28, 2456.563, 2449.177, 2424.343, 2420.663, 2415.076, 2414.919, 2402.119, 2390.316, 2360.114, 2385.192, 2351.377, 2345.516, 2338.427, 2333.725, 2340.897, 2319.279, 2316.38, 2305.918, 2296.677, 2295.489, 2317.789, 2291.898, 2307.043, 2280.162, 2281.041, 2292.856, 2285.525, 2289.282, 2265.706, 2268.889, 2267.054, 2259.574, 2260.994, 2256.039, 2256.018, 2271.198, 2236.976, 2246.714, 2234.173, 2243.552, 2238.167, 2235.492, 2231.691, 2224.521, 2234.826, 2243.916, 2254.176, 2227.575, 2206.448, 2241.946, 2217.358, 2217.821, 2240.497, 2230.817, 2224.975, 2223.641, 2213.859, 2220.022, 2236.503, 2220.47, 2208.684, 2209.15, 2214.414, 2207.992, 2202.531, 2210.843, 2207.188, 2212.861, 2214.819, 2197.265, 2181.611, 2198.295, 2186.516, 2196.186, 2188.801, 2191.523, 2193.124, 2188.002, 2183.26, 2179.659, 2183.438, 2179.66, 2199.008, 2198.101, 2194.281, 2183.918, 2182.005, 2173.731, 2172.219]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	30965.86914		469896	10	-1	204.79195499420166	{'train_loss': [583843.562, 352717.719, 312220.125, 300112.25, 292915.5, 286697.125, 280465.906, 275425.594, 271354.812, 267973.125, 265334.312, 262660.781, 260614.547, 258914.531, 257074.609, 255263.297, 254394.297, 253545.609, 251808.078, 250495.078, 248765.672, 248389.328, 247130.25, 245867.406, 245530.062, 244078.844, 243299.422, 242365.312, 241417.625, 240753.609, 240240.656, 239071.969, 238509.531, 237951.25, 237034.641, 235884.656, 235926.25, 235053.562, 234597.234, 233661.609, 232938.125, 232786.906, 231751.516, 231039.781, 231963.062, 230986.344, 230490.234, 230449.797, 230425.484, 229114.453, 228315.328, 228288.531, 226126.766, 226566.234, 226985.453, 226579.328, 226127.062, 225473.891, 224967.969, 224667.516, 224240.969, 223488.328, 223672.281, 222827.234, 223103.359, 221520.703, 222741.906, 221465.891, 221009.219, 221483.266, 220688.797, 220534.109, 220153.953, 220276.453, 219240.016, 218264.312, 218398.984, 219133.422, 218446.922, 218183.516, 218899.453, 217226.922, 217108.0, 216823.062, 217234.203, 216926.906, 216459.906, 215967.875, 215419.125, 215372.828, 214587.734, 215299.922, 214870.219, 215347.562, 214060.656, 214828.359, 214091.031, 212991.609, 213129.656, 212720.141], 'val_loss': [3723.424, 3026.224, 2845.511, 2789.866, 2730.324, 2680.997, 2645.297, 2602.68, 2568.031, 2538.745, 2529.48, 2495.581, 2492.422, 2488.428, 2470.044, 2448.916, 2431.548, 2419.288, 2418.142, 2414.456, 2398.77, 2414.119, 2398.006, 2383.507, 2367.318, 2379.755, 2365.79, 2349.279, 2352.293, 2343.157, 2349.442, 2344.179, 2354.305, 2334.208, 2320.397, 2321.636, 2316.904, 2314.542, 2303.431, 2302.326, 2287.273, 2282.023, 2278.009, 2302.304, 2276.607, 2276.404, 2256.697, 2259.186, 2253.204, 2259.416, 2251.148, 2254.354, 2248.027, 2227.525, 2235.281, 2229.052, 2229.735, 2226.646, 2223.546, 2219.378, 2197.873, 2230.72, 2210.604, 2219.385, 2205.999, 2201.391, 2196.846, 2197.964, 2205.255, 2191.607, 2193.48, 2192.905, 2209.739, 2199.843, 2195.015, 2181.42, 2186.758, 2183.553, 2197.988, 2176.758, 2184.938, 2168.975, 2165.31, 2197.647, 2174.742, 2197.624, 2173.12, 2186.493, 2171.311, 2173.276, 2199.409, 2158.205, 2164.021, 2165.1, 2179.228, 2187.39, 2163.776, 2186.498, 2167.214, 2169.626]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:52 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	31687.02148		409128	11	-1	208.04739928245544	{'train_loss': [565203.75, 347475.75, 314320.938, 304171.375, 297870.25, 291567.688, 286527.75, 281056.25, 277629.125, 274720.0, 271775.625, 269449.219, 267798.875, 264762.062, 263308.406, 262186.531, 260388.75, 260945.219, 258160.625, 256201.406, 255525.188, 255163.078, 253606.031, 252899.0, 250915.766, 250357.328, 250013.25, 249080.578, 248112.797, 246926.812, 246646.047, 244778.578, 244579.922, 242880.0, 242376.547, 242556.266, 241795.625, 240274.766, 240802.219, 239410.922, 239493.734, 238200.016, 238587.125, 237328.203, 236627.266, 237008.109, 235734.719, 235276.234, 234748.922, 234536.094, 234786.469, 234155.422, 233650.703, 232671.047, 232158.562, 232961.719, 231445.609, 231223.125, 230931.734, 230476.641, 229678.031, 229458.391, 229769.422, 229223.891, 228852.766, 227438.297, 227094.125, 227123.094, 226949.562, 226514.828, 227321.359, 225686.141, 225043.781, 225088.969, 224660.578, 225568.406, 224549.312, 222994.875, 223840.469, 223160.781, 223387.281, 222656.328, 222359.5, 222325.359, 221680.828, 221711.172, 222128.281, 220464.719, 220541.484, 219745.156, 220663.484, 219630.281, 219702.578, 218109.25, 219922.062, 218092.047, 219346.484, 218227.906, 217941.797, 217417.344], 'val_loss': [3412.822, 2973.21, 2913.069, 2845.593, 2767.287, 2740.549, 2699.596, 2656.437, 2632.369, 2617.728, 2590.788, 2577.516, 2577.5, 2546.76, 2542.118, 2552.151, 2529.316, 2510.694, 2508.025, 2491.317, 2479.557, 2458.287, 2448.672, 2471.875, 2433.845, 2444.607, 2414.7, 2419.803, 2386.695, 2404.835, 2378.893, 2367.024, 2369.802, 2349.962, 2368.486, 2350.146, 2325.166, 2319.779, 2308.36, 2305.315, 2300.622, 2306.764, 2290.646, 2286.679, 2300.057, 2265.644, 2296.517, 2261.067, 2280.76, 2274.192, 2284.344, 2255.756, 2263.007, 2272.599, 2304.83, 2273.827, 2275.684, 2256.456, 2262.227, 2282.223, 2269.399, 2273.369, 2288.736, 2264.932, 2269.62, 2263.513, 2263.676, 2239.527, 2244.07, 2264.684, 2261.383, 2249.789, 2284.92, 2253.497, 2270.11, 2245.518, 2242.745, 2265.562, 2244.705, 2246.724, 2232.875, 2278.753, 2242.675, 2253.728, 2221.568, 2217.542, 2227.728, 2213.02, 2210.789, 2212.46, 2205.646, 2210.603, 2242.905, 2194.781, 2224.842, 2198.989, 2223.822, 2200.136, 2192.116, 2207.75]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	2000	True	31632.29688		559964	10	-1	200.03973865509033	{'train_loss': [630232.0, 353535.969, 315661.812, 301016.938, 295119.438, 290219.219, 285701.75, 282045.688, 277946.469, 274252.031, 270193.75, 267594.469, 265655.5, 262951.062, 261637.812, 259442.531, 257137.531, 256097.969, 254287.062, 253022.734, 250876.016, 250411.531, 249812.703, 248327.406, 247099.125, 246463.922, 245837.875, 244965.578, 243920.859, 243123.406, 242826.703, 241192.391, 241509.562, 240889.906, 240168.875, 238967.359, 238585.516, 237308.719, 237544.188, 236662.562, 237107.703, 235901.344, 235858.375, 235594.984, 234698.703, 235984.375, 234434.547, 233475.312, 232835.062, 232756.031, 231736.125, 232453.969, 232693.562, 231320.953, 231638.5, 231163.047, 231149.938, 230427.359, 230230.578, 230388.688, 229361.562, 228748.078, 228489.219, 228924.734, 228277.047, 228397.969, 227737.562, 227115.688, 226756.547, 227394.0, 226057.906, 227120.672, 226954.141, 226105.078, 226998.609, 226139.516, 225924.328, 225660.266, 225765.5, 225672.25, 225100.625, 225204.891, 223883.047, 224730.016, 223673.844, 224824.141, 224524.047, 223477.625, 223932.516, 222577.188, 223196.969, 222994.469, 222795.594, 222116.0, 222307.797, 221839.328, 222378.234, 221045.422, 222093.188, 221004.984], 'val_loss': [3451.79, 2974.495, 2825.728, 2788.366, 2752.74, 2714.881, 2679.929, 2643.362, 2601.856, 2573.793, 2540.185, 2512.396, 2495.689, 2472.459, 2493.37, 2445.846, 2437.434, 2390.53, 2415.171, 2406.031, 2405.841, 2389.353, 2413.982, 2394.673, 2369.003, 2376.314, 2379.164, 2363.084, 2335.398, 2340.957, 2329.801, 2350.742, 2321.242, 2306.476, 2316.553, 2313.919, 2313.707, 2318.707, 2306.961, 2306.161, 2300.871, 2302.091, 2273.67, 2280.251, 2284.519, 2290.428, 2298.398, 2276.03, 2262.331, 2265.724, 2276.875, 2258.448, 2247.532, 2278.377, 2291.79, 2250.833, 2255.081, 2221.314, 2236.686, 2232.509, 2239.223, 2240.736, 2225.817, 2250.25, 2244.711, 2250.23, 2236.802, 2224.472, 2245.186, 2248.967, 2202.569, 2219.75, 2230.547, 2236.409, 2209.331, 2217.104, 2213.117, 2206.826, 2197.314, 2198.416, 2219.033, 2208.218, 2213.147, 2212.53, 2197.739, 2194.409, 2224.479, 2198.024, 2206.096, 2202.598, 2225.76, 2204.193, 2214.816, 2204.658, 2201.667, 2188.118, 2194.949, 2198.68, 2178.924, 2193.52]}	0	100	True
