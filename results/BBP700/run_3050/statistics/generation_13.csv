id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	31855.32422		271701	12	-1	231.6790246963501	{'train_loss': [412203.969, 345475.406, 325680.25, 315007.281, 305747.125, 300158.781, 295960.844, 291693.0, 288000.562, 284980.875, 281376.531, 278916.906, 276154.031, 273382.562, 271110.156, 268622.656, 266701.188, 264197.719, 263225.938, 260879.297, 259977.047, 259188.25, 257628.312, 256779.047, 255164.859, 253816.656, 253658.125, 252014.844, 251254.281, 250903.484, 251073.875, 248736.344, 247642.375, 247652.047, 246761.688, 245207.734, 244922.234, 245100.922, 243496.578, 242818.594, 242676.359, 241989.297, 241171.156, 241526.734, 240766.688, 239710.5, 239135.719, 239213.359, 238676.047, 238544.078, 238139.438, 237008.812, 236082.859, 235497.406, 235722.219, 235805.656, 235662.109, 235705.266, 234793.688, 235079.359, 234205.828, 233582.422, 233397.422, 232549.125, 232893.234, 232302.016, 232370.047, 231925.828, 231610.312, 230812.156, 231556.672, 229961.938, 230769.0, 230438.547, 230595.281, 229356.641, 229049.969, 229923.172, 228838.312, 228789.109, 228761.891, 228555.0, 228680.891, 227593.641, 227653.359, 227898.953, 226954.0, 227143.469, 226577.891, 226451.75, 226394.141, 225578.578, 226212.781, 225384.0, 225793.219, 225197.328, 224754.016, 224744.344, 224479.672, 224601.281], 'val_loss': [3260.372, 3023.118, 2922.645, 2853.501, 2827.098, 2804.409, 2764.405, 2737.112, 2714.799, 2680.212, 2663.047, 2636.828, 2614.174, 2599.032, 2581.133, 2549.316, 2542.389, 2513.75, 2523.128, 2507.954, 2484.173, 2462.42, 2460.31, 2443.082, 2454.813, 2447.019, 2447.399, 2442.056, 2403.163, 2395.516, 2393.277, 2401.385, 2386.281, 2364.837, 2371.057, 2340.983, 2348.744, 2344.257, 2336.573, 2328.229, 2345.779, 2326.519, 2322.327, 2318.484, 2333.48, 2326.1, 2310.8, 2306.024, 2285.377, 2307.82, 2282.633, 2310.963, 2301.691, 2292.827, 2291.207, 2282.483, 2272.703, 2284.298, 2275.745, 2318.282, 2280.294, 2282.273, 2246.664, 2295.077, 2275.439, 2295.615, 2275.372, 2255.503, 2261.119, 2241.609, 2259.332, 2247.281, 2261.792, 2247.692, 2275.253, 2247.187, 2251.488, 2238.674, 2243.294, 2245.426, 2250.753, 2243.945, 2249.719, 2226.483, 2237.236, 2225.981, 2243.501, 2217.854, 2199.579, 2202.619, 2207.103, 2215.52, 2212.144, 2200.735, 2218.526, 2214.302, 2220.875, 2213.039, 2212.72, 2220.776]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:52 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	32230.40039		295753	13	-1	234.13059854507446	{'train_loss': [418950.812, 351740.188, 323375.0, 314065.562, 304927.625, 297940.281, 293458.375, 290251.812, 287504.344, 284978.875, 282194.969, 279715.469, 277160.562, 274735.969, 272632.344, 271360.5, 269195.156, 267496.375, 266500.0, 265165.562, 264220.625, 262900.406, 261813.0, 261495.266, 260542.609, 259092.609, 259182.25, 257892.781, 256917.453, 256665.094, 255739.203, 254536.469, 253024.312, 252510.516, 252346.516, 250713.344, 251119.234, 249769.984, 248848.297, 248241.312, 247128.734, 247534.281, 247303.844, 247033.641, 246474.203, 245656.891, 245177.594, 244572.875, 244090.422, 244397.062, 244196.953, 241993.891, 242466.953, 242237.109, 241147.938, 241542.312, 240861.469, 240824.734, 240442.297, 240001.953, 240105.312, 239391.109, 238852.906, 238387.156, 238907.953, 238172.625, 237043.922, 237130.109, 237025.016, 236480.484, 236974.438, 236077.391, 236645.578, 236496.281, 235560.406, 235697.828, 235898.891, 234487.891, 233847.781, 233228.703, 234275.672, 234452.828, 233711.828, 233339.891, 232925.375, 233038.922, 232223.922, 232547.844, 231352.375, 232004.109, 232260.016, 232346.391, 231781.906, 230663.453, 230290.531, 230692.828, 231141.453, 230150.359, 231236.469, 230188.656], 'val_loss': [3595.542, 3085.788, 3003.9, 2907.365, 2846.234, 2799.479, 2778.383, 2767.67, 2726.709, 2713.481, 2705.389, 2657.846, 2670.884, 2674.925, 2645.0, 2623.702, 2596.127, 2549.502, 2589.827, 2537.487, 2530.014, 2515.747, 2550.576, 2494.685, 2491.235, 2510.895, 2495.901, 2493.258, 2461.463, 2479.864, 2483.968, 2441.813, 2447.874, 2452.574, 2455.012, 2432.346, 2429.256, 2421.792, 2407.938, 2411.295, 2392.93, 2405.341, 2380.132, 2401.81, 2378.329, 2365.851, 2389.739, 2387.711, 2361.89, 2368.786, 2345.413, 2381.507, 2373.875, 2364.59, 2342.486, 2375.225, 2369.24, 2362.352, 2361.113, 2343.694, 2366.143, 2319.566, 2359.483, 2348.899, 2347.359, 2322.332, 2326.191, 2345.733, 2331.059, 2337.083, 2316.744, 2305.508, 2333.333, 2342.128, 2319.192, 2354.623, 2322.33, 2331.262, 2305.734, 2351.646, 2293.087, 2296.993, 2307.431, 2300.01, 2307.117, 2297.594, 2299.153, 2300.474, 2310.438, 2294.555, 2301.085, 2287.543, 2276.581, 2314.922, 2286.601, 2294.353, 2298.371, 2304.114, 2284.49, 2260.819]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:63 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:34 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:rmsprop lr:0.14123991052380824 alpha:0.9775602461633903 weight_decay:3.1617411037191506e-05 batch_size:32 epochs:100	100	1000	True	137272.85938		289199	13	-1	228.52472519874573	{'train_loss': [1261350.875, 1066921.0, 1066921.0, 1066921.0, 1750806.75, 1067016.875, 1066921.0, 1066921.0, 1066921.0, 1434021.5, 1066921.0, 1066921.0, 1066921.0, 1068709.0, 1066921.0, 1066921.0, 1590596.875, 1067301.5, 1066921.0, 1066921.0, 1066921.0, 1145170.5, 1066921.0, 1066921.0, 1066921.0, 1089313.5, 1066921.0, 1067357.375, 1781249.875, 1066921.0, 1271025.625, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1447401.125, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1164107.375, 1066921.0, 1078757.375, 2900016.25, 1065118.125, 1067700.625, 1068286.5, 1241366.25, 1066921.0, 1066921.0, 1092132.375, 1066921.0, 1066921.0, 1665875.625, 1068160.125, 1066920.875, 1765589.0, 1068814.375, 1067951.375, 1589305.25, 1066931.625, 1067334.875, 1293669.875, 1067263.625, 1067363.375, 1364690.375, 1067058.375, 1066921.0, 1123607.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1265225.0, 1071348.375, 1067056.125, 1066921.0, 1153868.125, 1066921.0, 1165198.125, 1066921.0, 1066921.0, 1590018.25, 1089627.25, 1082145.75, 1066921.0, 1149141.375, 1066921.0, 1697831.25, 1060357.625, 1059174.0, 1083315.75, 1066909.375, 1066921.0, 1066921.0, 1829979.25, 1067841.25, 1067362.75, 1080062.5], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10546.082, 10566.86, 10587.207, 10586.802, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.205, 10587.216, 10587.216, 10587.116, 10550.152, 10587.162, 10583.037, 10577.169, 10583.656, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10519.768, 10455.358, 10558.689, 10587.216, 10587.216, 10587.216, 10587.216, 11330.099, 10587.216, 10587.066, 10587.216]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:59 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	32462.83203		223009	10	-1	208.13757705688477	{'train_loss': [401763.469, 345118.625, 324333.719, 316068.312, 310965.469, 303735.125, 299955.625, 297379.438, 294100.812, 291748.438, 288410.906, 285403.625, 281398.406, 278640.938, 275822.5, 273166.594, 270875.031, 268145.25, 266174.844, 263191.812, 262588.094, 261438.266, 259161.281, 257042.203, 256037.719, 254975.578, 253855.969, 252969.609, 251349.734, 251732.094, 251139.078, 248791.875, 248992.734, 247710.484, 247160.219, 245597.797, 244555.75, 245263.938, 242931.625, 241983.656, 243393.891, 242121.953, 241778.453, 241680.344, 240832.438, 240576.469, 239779.734, 239351.0, 238976.562, 238263.312, 238157.406, 237641.141, 236637.297, 236313.797, 236049.75, 235855.812, 235628.75, 235904.562, 233772.906, 234277.766, 233720.141, 233926.938, 232954.797, 233028.906, 232042.734, 231986.562, 231450.859, 231187.875, 232428.25, 230715.609, 231479.203, 230078.016, 230727.938, 230252.406, 229930.922, 229261.75, 229534.219, 229366.531, 228422.188, 228428.703, 228689.562, 228412.578, 227592.828, 227993.125, 228524.312, 226644.875, 227030.25, 226577.234, 226729.562, 226509.844, 226265.562, 225854.406, 226434.219, 224950.203, 225690.297, 224930.75, 224232.625, 225275.484, 223606.484, 223572.125], 'val_loss': [3213.719, 3021.558, 2930.828, 2909.693, 2855.826, 2824.226, 2802.526, 2785.708, 2763.572, 2739.704, 2708.303, 2685.252, 2660.451, 2638.725, 2621.551, 2585.173, 2558.096, 2551.021, 2528.966, 2517.582, 2510.973, 2494.358, 2461.026, 2452.891, 2460.717, 2437.774, 2432.026, 2412.83, 2411.025, 2413.058, 2400.727, 2397.027, 2378.315, 2378.938, 2364.094, 2355.488, 2349.789, 2357.699, 2336.932, 2352.512, 2357.536, 2344.344, 2348.802, 2349.116, 2325.209, 2339.857, 2326.558, 2332.501, 2321.799, 2301.723, 2330.467, 2315.411, 2313.537, 2316.662, 2291.985, 2295.851, 2290.412, 2306.088, 2284.542, 2286.294, 2284.79, 2283.833, 2286.369, 2270.969, 2278.007, 2269.972, 2266.876, 2261.323, 2265.148, 2275.718, 2249.299, 2254.747, 2243.578, 2246.766, 2273.209, 2241.623, 2257.064, 2240.998, 2257.763, 2261.633, 2243.031, 2248.351, 2262.761, 2254.303, 2242.237, 2257.535, 2243.713, 2226.527, 2243.989, 2246.388, 2226.175, 2217.181, 2254.792, 2214.885, 2231.638, 2221.893, 2230.241, 2227.759, 2216.936, 2256.007]}	100	100	True
