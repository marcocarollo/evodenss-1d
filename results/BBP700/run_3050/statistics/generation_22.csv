id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	32129.63281		576588	11	-1	203.69034028053284	{'train_loss': [832514.875, 364693.188, 319447.375, 304459.594, 297819.375, 292517.875, 288149.25, 284263.188, 280237.469, 276808.594, 273255.906, 269838.281, 267113.625, 265363.75, 263586.031, 262109.125, 260688.234, 258973.844, 257569.062, 256607.047, 255290.359, 255618.766, 252316.297, 251776.125, 250605.125, 249282.156, 248648.406, 248215.266, 247211.844, 245766.969, 245609.234, 244302.031, 243616.922, 243313.906, 242565.344, 242585.672, 241542.719, 241403.0, 240506.312, 240483.891, 238884.953, 238879.125, 239332.844, 237949.422, 238410.375, 238456.812, 236250.344, 235287.391, 236119.734, 235220.594, 235152.609, 235596.484, 233806.453, 233849.438, 232922.578, 233067.922, 232695.547, 232752.672, 231141.5, 232043.328, 231160.922, 230559.531, 231069.312, 230994.594, 230305.703, 229606.0, 229442.438, 228887.156, 229103.188, 227742.5, 228519.891, 227645.438, 227869.453, 227625.531, 227161.406, 227650.156, 225711.375, 225789.094, 226327.203, 226100.234, 226235.609, 225849.312, 225298.344, 224565.609, 224537.797, 225774.719, 224928.047, 224390.297, 223249.688, 223704.141, 224214.297, 224073.578, 223488.312, 222765.953, 221948.516, 222604.734, 223460.375, 221264.75, 223416.312, 221364.172], 'val_loss': [3430.452, 2993.736, 2894.044, 2834.336, 2787.687, 2757.894, 2715.43, 2684.885, 2634.281, 2590.952, 2567.356, 2555.542, 2521.284, 2509.416, 2506.238, 2482.623, 2468.476, 2473.583, 2460.567, 2437.382, 2438.566, 2429.364, 2433.462, 2406.88, 2423.373, 2411.226, 2392.687, 2388.237, 2385.641, 2394.646, 2392.58, 2375.305, 2359.286, 2348.538, 2363.0, 2361.86, 2396.679, 2334.744, 2347.215, 2333.542, 2346.593, 2298.256, 2332.038, 2323.542, 2311.569, 2308.724, 2319.772, 2304.765, 2318.516, 2330.416, 2305.969, 2300.514, 2298.323, 2293.999, 2297.494, 2289.332, 2276.641, 2274.969, 2286.035, 2288.218, 2286.521, 2277.619, 2297.3, 2264.546, 2275.399, 2285.216, 2278.032, 2272.543, 2271.855, 2250.571, 2245.044, 2237.731, 2236.479, 2250.033, 2272.122, 2239.102, 2237.926, 2268.052, 2245.417, 2253.999, 2236.899, 2248.158, 2237.909, 2230.292, 2245.67, 2230.887, 2231.24, 2233.525, 2227.575, 2232.068, 2231.783, 2216.884, 2230.926, 2217.845, 2243.578, 2238.981, 2221.576, 2209.371, 2217.823, 2218.165]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:5 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:90 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:6 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:rmsprop lr:0.1277351612437152 alpha:0.9273619996225203 weight_decay:9.474037069062493e-05 batch_size:32 epochs:100	100	1000	True	137272.96875		1486765	13	-1	247.55927443504333	{'train_loss': [1308994.75, 1066921.0, 1428879.125, 3131680.5, 1066921.0, 2970795.25, 1139694.125, 1066921.0, 5195576.0, 2195033.0, 1068755.375, 8848706.0, 1066921.0, 1067918.5, 4496342.0, 1066921.0, 7066644.5, 1066921.0, 1066921.0, 2424495.25, 1066921.0, 4995800.5, 1066921.0, 1066921.0, 2916055.5, 1498648.625, 1108327.375, 5395750.5, 1077264.625, 1117495.0, 3384466.0, 1073101.125, 5566374.0, 1066921.0, 3611249.75, 1073901.875, 1066921.0, 1129325.875, 1678197.5, 1066945.25, 1448717.125, 4065292.5, 1066921.0, 2702956.0, 1945840.375, 1074049.375, 2706278.25, 1066921.0, 1066921.0, 1420388.375, 1066921.0, 1101685.125, 3824598.75, 1066921.0, 2093373.5, 4039221.25, 6107750.5, 1066921.0, 1066921.0, 4696619.5, 1066965.5, 3690548.75, 2323791.5, 1066921.0, 3329066.25, 1066921.0, 1066921.0, 1668958.875, 1095821.5, 1078826.375, 5474858.5, 1068481.875, 4975495.5, 1066921.0, 1066921.0, 2575667.25, 1066961.75, 1638504.5, 1253137.125, 1067982.625, 2781699.5, 1087251.5, 1068809.0, 2110085.0, 1181092.5, 1066982.625, 1717217.75, 1530502.0, 1417617.5, 1121205.25, 2472152.5, 1334110.5, 1629794.25, 1070873.75, 1810712.875, 1806808.375, 2338416.75, 1170430.875, 1376110.25, 1264528.125], 'val_loss': [10587.216, 10587.216, 33213.031, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10891.695, 11686.762, 10587.216, 10587.216, 10587.216, 229489.781, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.206, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 5895271.5, 10587.216, 10587.216, 308699.375, 10587.216, 10587.216, 326959.75, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 84661.938, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 14893.803, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:96 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	33307.89062		318630	11	-1	206.7771384716034	{'train_loss': [402620.438, 341248.375, 321100.906, 312313.531, 308029.188, 303669.531, 299128.719, 293592.781, 289088.75, 284421.75, 281109.969, 277796.594, 275295.25, 273827.094, 271651.719, 269247.969, 268274.688, 265971.281, 265580.219, 264436.312, 263172.125, 261831.391, 259915.703, 259250.469, 257656.078, 258705.062, 256372.406, 256780.953, 255680.875, 254285.766, 252956.844, 252635.562, 252317.734, 250981.109, 250468.219, 250573.25, 249318.531, 247982.938, 248840.969, 247347.578, 247099.891, 246316.156, 245907.828, 245114.766, 244834.844, 244342.219, 244010.484, 243358.328, 242436.516, 242734.75, 242202.703, 241836.484, 240117.406, 240712.406, 240158.969, 239975.125, 239183.75, 239239.016, 238216.281, 238570.375, 237021.531, 237637.344, 237826.406, 237085.109, 237094.984, 236496.391, 236561.562, 236206.0, 235506.234, 235671.078, 234962.422, 235070.656, 234554.531, 234562.078, 234535.953, 233732.672, 233891.625, 233301.5, 232793.984, 232886.359, 232905.188, 233117.391, 231975.594, 231966.234, 231445.844, 231441.938, 231424.062, 231242.062, 231150.859, 231058.75, 229986.672, 230651.219, 230500.875, 230726.625, 229861.016, 229762.281, 229264.547, 229531.297, 229051.203, 229398.406], 'val_loss': [3154.964, 2994.004, 2914.579, 2908.508, 2871.616, 2837.387, 2802.651, 2751.555, 2699.645, 2658.867, 2636.006, 2614.702, 2605.974, 2576.817, 2573.56, 2582.725, 2559.944, 2541.9, 2535.537, 2543.766, 2520.091, 2516.343, 2513.422, 2517.513, 2490.434, 2470.404, 2462.518, 2461.584, 2454.135, 2437.997, 2445.544, 2441.51, 2453.645, 2437.666, 2444.285, 2436.427, 2414.769, 2426.638, 2418.61, 2417.824, 2406.451, 2417.376, 2418.121, 2402.398, 2401.112, 2404.871, 2394.022, 2386.599, 2389.046, 2388.289, 2389.119, 2384.436, 2381.312, 2377.418, 2381.443, 2385.249, 2376.551, 2392.411, 2381.388, 2387.941, 2375.401, 2359.811, 2369.954, 2361.571, 2374.367, 2366.898, 2353.981, 2372.188, 2368.177, 2345.328, 2369.754, 2350.123, 2360.349, 2369.776, 2334.41, 2349.564, 2359.767, 2350.206, 2352.51, 2337.235, 2359.585, 2336.797, 2334.973, 2353.295, 2348.275, 2348.566, 2337.031, 2330.742, 2344.479, 2347.11, 2337.896, 2329.584, 2357.435, 2331.375, 2318.022, 2331.412, 2330.786, 2346.594, 2333.796, 2320.41]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:rmsprop lr:0.12769669663331293 alpha:0.9182001718345341 weight_decay:0.0004808477029278135 batch_size:32 epochs:100	100	1000	True	137268.09375		576588	11	-1	209.4110550880432	{'train_loss': [1145186.5, 1196613.625, 1992461.625, 1893825.375, 1066921.0, 1985067.75, 1066921.0, 2390899.25, 1066921.0, 1238330.75, 1268451.875, 1916245.875, 1066921.0, 1788247.875, 1066921.0, 1266655.625, 1066921.0, 1271272.75, 1784414.0, 1619757.875, 2798879.5, 1901929.25, 1074070.75, 1309294.75, 1723911.375, 2059035.875, 1066921.0, 2135959.5, 2288416.5, 1508756.5, 2885513.5, 2097782.5, 1351270.5, 1135285.875, 2070100.25, 2136333.25, 1366522.875, 1724019.0, 2108184.75, 1495166.25, 1807452.375, 1554788.125, 1088523.0, 1577214.75, 1074294.0, 1732366.875, 1074170.375, 1949688.375, 1978889.75, 1067704.75, 1725974.75, 1125438.5, 1279641.25, 1132523.5, 1557827.625, 1199024.625, 1364379.75, 1072882.375, 1102897.375, 1489697.625, 1309845.0, 1156024.875, 1179441.625, 1356595.0, 1254044.0, 1248333.125, 1176251.875, 1155329.0, 1622864.5, 1109584.5, 1143879.875, 1219853.125, 1447200.25, 1184997.5, 1211460.875, 1372742.125, 1176770.375, 1170352.25, 1317361.0, 1185581.75, 1173373.0, 1221839.875, 1311666.0, 1212569.875, 1210685.0, 1296254.25, 1186796.625, 1253517.5, 1294173.375, 1211340.625, 1213007.375, 1262978.5, 1246992.75, 1245205.625, 1167869.125, 1300449.125, 1301266.0, 1223398.125, 1216917.375, 1211122.375], 'val_loss': [10587.216, 381660.812, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10561.657, 10587.216, 10587.216, 10585.956, 466181.656, 10587.216, 10587.216, 10587.216, 12880.887, 10587.159, 250900.25, 10587.152, 10579.366, 10587.216, 10587.205, 10587.029, 11138.995, 230281.125, 10587.216, 10584.909, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10585.134, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10586.51, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10583.25, 10943.789, 10587.216, 10587.216, 10574.774, 17503.416, 19393.816, 10587.216, 10628.442, 10587.216, 11567.596, 10587.216, 10587.216, 10575.183, 10587.216, 10587.204, 10587.214, 12284.083, 10587.198, 14265.081, 11094.33, 10587.216, 40070.598, 10587.216, 10587.216, 13572.966, 10587.216, 38703.422, 10587.216, 10587.216, 11506.847, 10587.216, 10587.211, 12603.367, 10587.216, 10587.197, 13289.859, 13580.263, 10587.216, 11076.107, 10587.216]}	100	100	True
