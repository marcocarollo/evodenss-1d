id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	31285.9043		469896	10	-1	209.71765422821045	{'train_loss': [600348.812, 363708.781, 318658.125, 305114.25, 298438.969, 291739.906, 284906.406, 278811.844, 273984.656, 269110.469, 264291.75, 261743.219, 259035.266, 256361.859, 253557.516, 252113.266, 250615.781, 248901.469, 247550.312, 245468.625, 244943.594, 243578.766, 243510.406, 241498.141, 241239.875, 239317.562, 238723.609, 237690.547, 236983.703, 237812.5, 236180.375, 235531.719, 235098.047, 234896.406, 234174.562, 233538.281, 232954.5, 232436.984, 231915.859, 231913.422, 230739.391, 230984.078, 229927.188, 229427.234, 229446.391, 228332.438, 228295.812, 227892.984, 227468.938, 227191.562, 227773.766, 226686.75, 225944.125, 226267.688, 225850.719, 225322.875, 225463.734, 223544.219, 224233.406, 223447.016, 223216.109, 222992.25, 223706.219, 222875.312, 222557.781, 222392.703, 222345.75, 221615.594, 220952.016, 220751.203, 220204.359, 220161.219, 220249.5, 219295.875, 218599.844, 218767.016, 218143.141, 217939.25, 217362.719, 218007.0, 217220.422, 217488.375, 216806.281, 216805.734, 216989.531, 216494.656, 215361.938, 215967.047, 214474.297, 215517.391, 214893.828, 213954.016, 213751.875, 214297.531, 213968.266, 213855.609, 213342.594, 213047.469, 213281.906, 212561.719], 'val_loss': [3492.639, 3007.252, 2856.447, 2830.664, 2785.283, 2737.518, 2654.058, 2601.796, 2571.161, 2526.055, 2507.766, 2491.981, 2478.731, 2446.688, 2459.241, 2436.978, 2423.746, 2406.384, 2408.433, 2373.202, 2388.769, 2393.728, 2391.103, 2378.733, 2335.226, 2347.005, 2349.569, 2338.182, 2347.042, 2316.29, 2323.732, 2295.982, 2298.498, 2314.261, 2293.664, 2302.981, 2284.955, 2285.715, 2278.729, 2284.693, 2280.957, 2283.697, 2273.596, 2278.652, 2269.44, 2272.239, 2276.184, 2249.389, 2267.715, 2265.839, 2260.365, 2260.317, 2251.941, 2241.059, 2245.986, 2245.211, 2251.105, 2252.467, 2263.626, 2239.292, 2228.716, 2245.423, 2232.451, 2223.368, 2243.67, 2234.433, 2231.625, 2215.9, 2253.431, 2234.63, 2232.65, 2239.207, 2222.446, 2218.799, 2213.884, 2224.02, 2241.64, 2219.056, 2211.031, 2225.449, 2208.771, 2211.458, 2228.929, 2185.976, 2210.266, 2204.131, 2199.665, 2199.136, 2214.538, 2211.987, 2196.277, 2200.719, 2216.691, 2212.895, 2219.898, 2199.706, 2181.545, 2212.384, 2197.188, 2172.845]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:5 layer:conv1d out_channels:6 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:32 epochs:100	100	1000	True	33246.58203		1154695	9	-1	205.00016522407532	{'train_loss': [768039.5, 344562.0, 331151.625, 318235.469, 318875.406, 311931.688, 308533.0, 306569.562, 304511.531, 300971.875, 301929.344, 299485.344, 296552.906, 294608.844, 293651.5, 292621.594, 290079.938, 290402.094, 287668.312, 286887.438, 284654.125, 283110.469, 281862.656, 280406.844, 279226.125, 278101.0, 276607.125, 274733.906, 274461.219, 273286.5, 271774.031, 271684.938, 270202.125, 268619.125, 268599.75, 267365.281, 266275.062, 266211.0, 264878.219, 264237.625, 263478.031, 262249.219, 262381.344, 261317.531, 262135.312, 259557.75, 259189.078, 259451.656, 259324.734, 257372.656, 258108.766, 256956.0, 256321.391, 256089.422, 256089.891, 255639.734, 253471.094, 254301.469, 253438.094, 253027.406, 252033.25, 252700.391, 252474.625, 251909.062, 252189.562, 252086.969, 252326.781, 251386.359, 251788.047, 251134.547, 250217.344, 250055.609, 248924.484, 249253.531, 248634.844, 249579.281, 247924.781, 248253.375, 247631.688, 248988.266, 246546.625, 247184.281, 246347.281, 246595.703, 246666.828, 245814.953, 246333.484, 246172.188, 244851.453, 245303.641, 245360.609, 245620.141, 245473.719, 244264.141, 244217.172, 245894.609, 243415.531, 243695.016, 242888.797, 244502.906], 'val_loss': [3193.137, 2976.98, 2927.275, 2872.09, 2842.238, 2799.504, 2826.093, 2790.459, 2775.972, 2779.546, 2755.497, 2736.206, 2732.149, 2724.788, 2721.129, 2694.968, 2678.149, 2704.78, 2673.446, 2682.68, 2678.319, 2637.888, 2635.53, 2671.39, 2599.52, 2617.64, 2584.796, 2567.756, 2578.969, 2570.809, 2547.548, 2575.155, 2552.18, 2536.681, 2509.501, 2540.469, 2514.22, 2509.615, 2490.225, 2528.96, 2497.112, 2470.573, 2501.038, 2486.083, 2470.16, 2476.819, 2471.661, 2449.562, 2473.013, 2476.094, 2457.901, 2445.309, 2462.57, 2464.791, 2450.308, 2425.722, 2428.064, 2411.522, 2476.903, 2407.105, 2443.2, 2425.552, 2405.215, 2434.974, 2410.182, 2412.619, 2401.514, 2408.212, 2422.713, 2398.927, 2418.186, 2404.19, 2397.883, 2393.716, 2406.39, 2390.969, 2424.88, 2358.506, 2356.652, 2381.351, 2374.486, 2364.709, 2354.111, 2378.019, 2363.975, 2396.444, 2350.452, 2371.952, 2352.485, 2361.72, 2355.186, 2356.61, 2367.997, 2343.496, 2351.332, 2330.399, 2357.899, 2353.353, 2348.43, 2342.745]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:40 kernel_size:10 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	137274.625		3636600	10	-1	207.34851789474487	{'train_loss': [1119505.875, 1067153.625, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1068099.875, 1066921.0, 1068077.75, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.215, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	31433.00977		471496	10	-1	208.9182834625244	{'train_loss': [572639.25, 356519.656, 321979.312, 306626.844, 299894.156, 294721.594, 289121.031, 285286.188, 281223.312, 278185.188, 275046.219, 272978.438, 269995.406, 267866.188, 266223.969, 264044.469, 261622.109, 260435.078, 259975.859, 258049.031, 256689.344, 255957.297, 253757.25, 252850.578, 251564.781, 252152.516, 250304.594, 248790.531, 247179.016, 247238.812, 246119.875, 245894.062, 245016.188, 244310.578, 244130.328, 242827.062, 242851.891, 242019.844, 241158.547, 241474.953, 240504.344, 239748.562, 239823.359, 239282.219, 238645.312, 238319.609, 237022.891, 237918.891, 238083.906, 236436.016, 237061.234, 235834.031, 235750.344, 234593.672, 234797.188, 234587.953, 233319.812, 233777.297, 232359.891, 232259.156, 232072.625, 232372.266, 231356.156, 231365.953, 231314.531, 230396.703, 230454.375, 230612.219, 230859.531, 229815.281, 229122.703, 228622.188, 228913.781, 228629.5, 228452.344, 227435.781, 227583.703, 226975.297, 227423.156, 226752.859, 226031.656, 226812.25, 226750.734, 225795.297, 226087.234, 225483.109, 225018.297, 224721.547, 225071.031, 224518.984, 224207.688, 223970.797, 223216.297, 222646.109, 223117.828, 221921.062, 222747.766, 221344.516, 222071.156, 221108.234], 'val_loss': [3412.303, 3014.391, 2876.972, 2833.969, 2826.766, 2761.192, 2716.052, 2679.436, 2652.174, 2611.232, 2586.206, 2571.408, 2548.088, 2539.923, 2517.348, 2484.129, 2503.285, 2469.894, 2445.734, 2431.737, 2420.69, 2401.372, 2403.674, 2400.256, 2394.359, 2381.749, 2373.687, 2354.806, 2354.787, 2344.92, 2355.836, 2356.988, 2342.84, 2330.397, 2334.352, 2319.867, 2324.283, 2334.161, 2327.955, 2317.713, 2301.832, 2305.398, 2300.78, 2322.94, 2278.172, 2290.096, 2291.853, 2293.575, 2286.539, 2283.914, 2269.303, 2266.144, 2266.168, 2260.517, 2273.436, 2273.963, 2264.999, 2252.185, 2260.521, 2255.239, 2252.212, 2241.474, 2261.569, 2253.913, 2247.983, 2249.548, 2253.827, 2247.855, 2237.469, 2228.367, 2209.98, 2235.186, 2228.504, 2241.32, 2215.463, 2221.513, 2216.032, 2215.555, 2209.35, 2203.918, 2202.723, 2194.792, 2197.258, 2234.836, 2201.938, 2195.837, 2191.626, 2198.614, 2190.505, 2202.921, 2192.249, 2201.23, 2207.179, 2189.531, 2184.618, 2186.396, 2182.155, 2185.726, 2176.842, 2167.667]}	100	100	True
