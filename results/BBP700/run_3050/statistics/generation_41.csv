id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:5 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	31640.76172		399954	12	-1	210.07500791549683	{'train_loss': [577162.25, 348396.906, 320356.188, 312408.281, 307311.812, 301071.156, 296046.344, 291423.812, 285361.406, 281127.281, 277074.438, 273643.906, 270939.75, 268831.406, 266449.375, 265196.844, 262830.156, 262157.312, 260449.531, 258776.094, 258056.219, 257270.5, 256259.234, 254593.844, 253269.969, 253349.391, 252577.328, 250705.297, 250092.062, 249296.547, 248650.219, 247501.031, 246527.0, 245545.078, 245271.484, 245114.375, 243355.453, 243183.719, 243026.547, 242732.562, 241212.891, 240759.203, 241095.953, 240025.984, 239086.969, 238717.703, 237506.672, 238371.828, 237290.109, 236817.844, 236330.391, 235919.188, 236234.172, 236145.719, 236242.016, 235703.781, 234546.0, 235052.578, 234079.75, 233828.5, 232430.656, 233596.344, 231799.562, 231752.078, 232164.531, 231250.078, 232351.906, 231405.812, 230912.469, 230732.609, 230146.844, 229944.344, 229691.516, 230619.141, 229350.109, 228961.359, 228325.812, 228642.109, 227929.797, 228386.391, 227153.281, 228505.797, 227261.312, 227308.344, 227524.281, 226518.719, 226321.078, 226310.391, 225658.5, 226022.891, 226006.688, 225889.938, 225020.281, 225855.297, 225266.516, 225606.516, 224560.703, 224391.094, 224062.125, 223804.203], 'val_loss': [3298.151, 2995.082, 2924.143, 2897.136, 2850.205, 2800.384, 2758.984, 2709.897, 2673.134, 2629.343, 2595.378, 2581.747, 2550.408, 2543.172, 2550.427, 2514.278, 2541.168, 2509.056, 2487.876, 2492.365, 2468.574, 2466.529, 2444.656, 2436.391, 2411.367, 2423.459, 2412.63, 2421.283, 2393.24, 2400.325, 2381.9, 2387.772, 2361.732, 2367.623, 2366.843, 2362.42, 2378.51, 2362.648, 2350.281, 2336.862, 2328.455, 2329.67, 2332.188, 2320.432, 2306.849, 2321.583, 2324.75, 2310.069, 2297.684, 2289.038, 2301.984, 2319.484, 2294.235, 2304.101, 2286.026, 2276.161, 2294.905, 2268.192, 2269.383, 2282.001, 2290.645, 2266.631, 2268.985, 2268.79, 2276.795, 2251.883, 2245.848, 2258.016, 2249.513, 2245.05, 2244.892, 2236.621, 2242.183, 2235.529, 2240.021, 2237.112, 2251.062, 2235.059, 2251.49, 2241.865, 2250.888, 2257.497, 2226.607, 2232.649, 2220.64, 2245.275, 2243.7, 2222.134, 2230.479, 2237.355, 2213.76, 2217.226, 2221.653, 2208.644, 2227.937, 2222.609, 2218.629, 2230.696, 2225.605, 2221.728]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:5 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	2000	True	32069.7207		399954	12	-1	212.67910361289978	{'train_loss': [578331.938, 364752.625, 325997.75, 313710.156, 309746.531, 307035.375, 304540.531, 302590.094, 301233.0, 299699.219, 298384.781, 297120.312, 295214.219, 292668.062, 288186.469, 285750.844, 283245.562, 280571.344, 277723.625, 274632.719, 272565.781, 271002.875, 269866.438, 267527.438, 266331.875, 265325.375, 263686.125, 261760.625, 261601.438, 259575.594, 258805.531, 257468.062, 256872.781, 256079.797, 253912.141, 253686.031, 252242.156, 251766.047, 250177.906, 250560.297, 250380.859, 248591.703, 248071.219, 247237.594, 246703.594, 245194.812, 245723.984, 243643.859, 243725.062, 243914.406, 242606.938, 241750.984, 240752.047, 240982.594, 241088.047, 239515.094, 238876.578, 238595.016, 239060.453, 237270.875, 237921.328, 236958.906, 236128.109, 236437.094, 235545.031, 235312.859, 235307.484, 234349.781, 233626.609, 234065.109, 232738.859, 232531.359, 232693.312, 231373.516, 232148.844, 230571.344, 230136.734, 230174.719, 230163.031, 229307.781, 229457.625, 228784.109, 229195.547, 229471.719, 228324.906, 227833.156, 227169.891, 227741.953, 227006.641, 227850.75, 227470.625, 226233.703, 226816.547, 225758.312, 225155.219, 224932.641, 225551.25, 224806.969, 224494.891, 224875.766], 'val_loss': [3783.773, 3064.343, 2928.347, 2904.447, 2887.596, 2873.008, 2862.833, 2849.064, 2835.939, 2828.561, 2813.751, 2803.198, 2802.748, 2748.803, 2731.735, 2701.588, 2684.407, 2677.844, 2627.907, 2601.398, 2597.262, 2588.827, 2577.101, 2563.835, 2543.604, 2546.038, 2524.382, 2527.976, 2505.56, 2497.188, 2475.017, 2496.604, 2474.099, 2455.917, 2475.706, 2463.897, 2441.433, 2445.2, 2428.962, 2437.32, 2440.631, 2417.02, 2398.553, 2438.255, 2417.15, 2404.769, 2369.251, 2383.898, 2382.333, 2379.236, 2349.307, 2360.919, 2374.32, 2363.138, 2365.825, 2338.089, 2301.726, 2333.552, 2330.27, 2349.981, 2333.063, 2373.592, 2342.435, 2313.947, 2334.743, 2321.703, 2312.198, 2303.889, 2284.295, 2328.485, 2291.521, 2297.919, 2289.819, 2301.993, 2279.944, 2297.374, 2263.707, 2258.068, 2251.854, 2265.028, 2260.723, 2276.737, 2257.826, 2265.964, 2245.474, 2248.981, 2234.107, 2242.71, 2289.271, 2224.868, 2240.877, 2253.292, 2236.142, 2287.097, 2255.647, 2224.295, 2200.245, 2239.588, 2224.658, 2246.152]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:5 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	31238.22852		399954	12	-1	210.19743180274963	{'train_loss': [574157.938, 360431.344, 324417.438, 312489.156, 307564.094, 301371.938, 296508.719, 291440.5, 286282.438, 280199.781, 274819.062, 270942.719, 266267.0, 263942.844, 262063.656, 260595.688, 258749.172, 257104.797, 255452.422, 253209.734, 252853.188, 251534.047, 249979.766, 249937.141, 246423.406, 246836.781, 245504.969, 245009.875, 243547.094, 243129.188, 242048.219, 241859.109, 240558.578, 240011.094, 239008.328, 238697.594, 238494.734, 237458.891, 237243.594, 236634.781, 236227.781, 235255.984, 234582.031, 234407.719, 234633.312, 233853.938, 233630.844, 232468.25, 232053.391, 232700.203, 231962.234, 231183.312, 231137.438, 229544.141, 229945.094, 230119.453, 229779.391, 229300.859, 228075.031, 227775.156, 228785.906, 227578.562, 226947.672, 227632.969, 227038.547, 226023.406, 226404.641, 224964.344, 225944.422, 226151.844, 224692.75, 225818.812, 224522.828, 224346.266, 224072.578, 223598.672, 222994.234, 223511.703, 223702.156, 222694.359, 222214.75, 221859.438, 222849.266, 222136.375, 221124.734, 222473.562, 222151.047, 221619.094, 221330.0, 220979.578, 220759.797, 221346.797, 221037.234, 220360.75, 220100.797, 220360.156, 220416.172, 219165.906, 219731.0, 219127.188], 'val_loss': [3518.936, 3014.919, 2916.255, 2892.141, 2832.654, 2816.732, 2776.319, 2732.458, 2680.302, 2646.887, 2590.389, 2602.583, 2551.168, 2520.741, 2538.975, 2489.499, 2530.192, 2483.957, 2447.255, 2452.428, 2423.526, 2402.241, 2412.971, 2391.921, 2394.155, 2372.639, 2355.217, 2352.694, 2340.544, 2338.76, 2352.454, 2342.202, 2328.974, 2328.853, 2328.033, 2304.553, 2310.332, 2285.933, 2301.361, 2284.798, 2291.25, 2288.317, 2280.578, 2278.464, 2270.14, 2279.449, 2273.86, 2256.071, 2251.245, 2256.399, 2243.542, 2250.334, 2238.425, 2255.344, 2244.052, 2236.49, 2241.333, 2223.476, 2235.635, 2236.583, 2236.874, 2232.273, 2226.952, 2234.22, 2210.22, 2214.668, 2208.57, 2216.644, 2226.453, 2207.139, 2198.712, 2210.039, 2224.555, 2197.999, 2188.517, 2196.452, 2198.034, 2181.378, 2187.584, 2197.58, 2177.784, 2181.486, 2175.38, 2188.621, 2174.146, 2190.226, 2158.344, 2182.593, 2156.342, 2156.087, 2161.78, 2169.996, 2165.2, 2179.727, 2170.486, 2172.988, 2151.706, 2150.403, 2154.923, 2145.352]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:6 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	32111.10352		451551	11	-1	215.0417468547821	{'train_loss': [510709.125, 345018.656, 320955.531, 307150.75, 300620.0, 295123.812, 290340.781, 286733.531, 283311.344, 279881.438, 278489.719, 274266.281, 272432.719, 269965.031, 268820.188, 267224.188, 265624.844, 264375.719, 263354.469, 261866.016, 260479.844, 259784.094, 259554.297, 258239.812, 257740.609, 256899.406, 255691.328, 255178.109, 253866.234, 253827.812, 254167.906, 253065.406, 251297.797, 251347.844, 250650.625, 250108.5, 249486.797, 248449.25, 247777.531, 248112.156, 247343.625, 247073.406, 246785.719, 245454.188, 245763.578, 244670.031, 244945.344, 244767.984, 244386.219, 243964.453, 243080.125, 243828.75, 242162.188, 242321.188, 241812.812, 241145.422, 241448.672, 240869.859, 240812.859, 239582.922, 239764.984, 239551.234, 237826.406, 238532.797, 237706.875, 237582.0, 238151.0, 236172.594, 236416.875, 235987.172, 235565.656, 236840.312, 235360.703, 236243.203, 234736.906, 235201.766, 235222.016, 235247.578, 235108.109, 232991.172, 234334.547, 232474.125, 233640.922, 233907.688, 232301.734, 232461.641, 232296.359, 231447.203, 232322.312, 232462.453, 231196.312, 230614.406, 230811.156, 230410.219, 230773.812, 230038.641, 230287.953, 229110.188, 229611.844, 228845.719], 'val_loss': [3159.988, 3006.625, 2863.562, 2814.303, 2771.092, 2745.319, 2724.155, 2695.737, 2661.021, 2654.125, 2623.255, 2596.975, 2574.214, 2560.461, 2558.764, 2536.264, 2521.009, 2506.526, 2510.478, 2513.632, 2488.467, 2482.635, 2461.581, 2472.164, 2461.918, 2443.886, 2433.66, 2449.295, 2453.939, 2423.101, 2414.624, 2408.812, 2407.808, 2426.865, 2400.901, 2404.106, 2395.806, 2395.247, 2389.923, 2391.009, 2365.096, 2383.976, 2406.872, 2358.267, 2354.368, 2358.334, 2364.45, 2347.514, 2334.99, 2339.625, 2365.586, 2351.131, 2354.798, 2342.808, 2330.097, 2345.405, 2316.391, 2352.968, 2338.312, 2312.872, 2314.844, 2321.727, 2318.181, 2290.57, 2312.416, 2306.826, 2314.302, 2294.842, 2281.954, 2279.859, 2288.909, 2301.867, 2290.656, 2282.66, 2314.976, 2292.068, 2256.79, 2251.625, 2243.349, 2243.647, 2254.695, 2259.56, 2251.394, 2237.208, 2250.494, 2266.388, 2261.252, 2245.388, 2255.032, 2253.142, 2241.39, 2241.602, 2252.07, 2228.301, 2235.038, 2259.266, 2225.069, 2241.424, 2247.333, 2216.888]}	100	100	True
