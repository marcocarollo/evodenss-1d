id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:101 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	32301.34375		360190	11	-1	227.60664129257202	{'train_loss': [469700.344, 335773.375, 316823.75, 309725.188, 303157.344, 297208.531, 291143.0, 286118.875, 280462.906, 277453.875, 274735.688, 272219.844, 270258.969, 267659.906, 266843.188, 264552.469, 263485.656, 261926.703, 260092.703, 258829.312, 258056.766, 256134.531, 254992.109, 254817.547, 253173.203, 252617.703, 251304.125, 250892.734, 250214.641, 249183.781, 248639.656, 247165.016, 247163.391, 246070.641, 245648.578, 244988.797, 244668.531, 243616.75, 243020.219, 243171.141, 242342.562, 242038.25, 241127.703, 241380.891, 240450.906, 240171.188, 239701.859, 238822.922, 238540.109, 238483.609, 237771.688, 237199.938, 236915.906, 236291.156, 236988.469, 235693.281, 235323.062, 235357.797, 234399.797, 234457.062, 232805.969, 233940.797, 232188.609, 232361.0, 231390.969, 230883.719, 230891.047, 230749.312, 230959.453, 230148.062, 230552.609, 229574.109, 229634.438, 228415.875, 228269.359, 228379.859, 227771.516, 227597.906, 227191.984, 228180.875, 226310.219, 226352.172, 226378.625, 226031.625, 226006.438, 225828.453, 224940.562, 224310.484, 224567.125, 223827.125, 224377.625, 223613.938, 223719.297, 223369.797, 222162.094, 222134.469, 222115.594, 221018.703, 221498.578, 221532.922], 'val_loss': [3305.421, 2959.947, 2898.319, 2873.684, 2808.934, 2760.853, 2708.39, 2666.231, 2652.791, 2615.663, 2579.976, 2602.44, 2570.985, 2547.635, 2569.843, 2521.309, 2517.891, 2570.485, 2565.808, 2509.772, 2525.219, 2531.283, 2527.673, 2524.273, 2492.638, 2506.829, 2495.564, 2502.827, 2525.239, 2465.071, 2487.479, 2485.692, 2469.615, 2491.282, 2461.482, 2473.381, 2464.024, 2437.469, 2421.229, 2422.315, 2428.385, 2436.692, 2438.615, 2408.568, 2429.924, 2420.323, 2428.969, 2406.317, 2406.042, 2408.398, 2422.641, 2402.538, 2381.627, 2381.977, 2372.589, 2385.574, 2358.723, 2362.257, 2390.192, 2339.683, 2372.826, 2374.96, 2352.437, 2368.737, 2354.585, 2369.672, 2335.753, 2344.798, 2336.841, 2328.986, 2353.661, 2344.93, 2333.575, 2335.128, 2320.971, 2349.303, 2314.48, 2315.013, 2358.552, 2293.919, 2316.683, 2330.904, 2310.021, 2291.011, 2316.11, 2301.423, 2279.284, 2280.735, 2273.709, 2272.19, 2286.448, 2277.603, 2287.249, 2274.86, 2291.073, 2292.867, 2269.494, 2283.402, 2268.428, 2276.594]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	31453.04102		576588	11	-1	235.11753964424133	{'train_loss': [701284.25, 357795.938, 316211.5, 301759.688, 293455.0, 286524.094, 281065.406, 277543.25, 273390.031, 269837.188, 267928.719, 264807.594, 262423.0, 259506.938, 257930.609, 256427.5, 254939.891, 253173.766, 251837.719, 250741.734, 248983.172, 248273.578, 246900.094, 245800.953, 244689.062, 243909.469, 243832.828, 242926.078, 242065.391, 240291.547, 240763.266, 239756.516, 238578.109, 238621.953, 238207.938, 237414.172, 236442.953, 235619.094, 235190.578, 235416.688, 235153.031, 235052.234, 233854.281, 233988.172, 232568.016, 232741.094, 231513.297, 232421.031, 231154.547, 231049.5, 230797.422, 229959.969, 230603.062, 229713.062, 228907.156, 228570.453, 228649.109, 228628.703, 228124.312, 228030.062, 227992.156, 227897.078, 226298.172, 226493.906, 226863.188, 226805.891, 226200.203, 225395.344, 226060.156, 225431.891, 225474.859, 225087.484, 226095.688, 224077.344, 224046.625, 223331.094, 223785.938, 223345.594, 222876.125, 222830.672, 223197.844, 221966.5, 222869.828, 222274.297, 221555.844, 222403.719, 221854.125, 221899.578, 221337.734, 221066.203, 221033.375, 220459.781, 220890.406, 220547.0, 219438.891, 220372.25, 220784.953, 220745.828, 220422.766, 219150.016], 'val_loss': [3399.097, 2947.717, 2849.002, 2791.307, 2732.635, 2690.937, 2650.487, 2611.777, 2588.979, 2552.947, 2532.846, 2511.501, 2506.506, 2488.163, 2484.603, 2450.533, 2449.71, 2444.668, 2420.05, 2402.612, 2397.211, 2396.146, 2391.043, 2369.176, 2364.463, 2365.266, 2361.154, 2370.99, 2367.695, 2350.353, 2321.559, 2364.802, 2341.054, 2323.204, 2328.095, 2325.891, 2302.689, 2319.312, 2341.992, 2295.139, 2308.022, 2318.771, 2293.71, 2287.773, 2319.518, 2298.666, 2286.088, 2290.087, 2290.247, 2290.252, 2297.764, 2289.346, 2290.166, 2287.78, 2254.479, 2278.069, 2289.767, 2262.517, 2281.901, 2300.168, 2268.632, 2270.661, 2281.474, 2264.879, 2259.964, 2254.626, 2251.26, 2254.728, 2248.459, 2258.159, 2249.015, 2245.262, 2233.471, 2254.194, 2237.483, 2260.391, 2249.289, 2234.508, 2236.136, 2257.995, 2239.348, 2240.343, 2234.367, 2240.242, 2237.127, 2227.35, 2238.705, 2229.612, 2242.707, 2216.219, 2230.438, 2234.059, 2241.957, 2241.786, 2242.378, 2219.249, 2229.103, 2221.124, 2232.03, 2218.712]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:101 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	33072.82031		354919	10	-1	229.41054439544678	{'train_loss': [399960.75, 334030.406, 314977.438, 308138.844, 304171.938, 300095.938, 296608.375, 292514.844, 288408.688, 285583.531, 282019.188, 279783.438, 278210.312, 276831.656, 274825.438, 274227.781, 272655.0, 270887.562, 270188.562, 268863.625, 268973.625, 267993.594, 265574.062, 265467.938, 266055.719, 263884.906, 263998.625, 262325.781, 261803.016, 260804.672, 259966.234, 260527.734, 258238.422, 258610.047, 258482.578, 256747.688, 256508.547, 256843.641, 254730.016, 255031.188, 254668.984, 253999.5, 253026.656, 253251.766, 252433.672, 252134.5, 251695.141, 251567.016, 251058.141, 250459.172, 250321.125, 249410.688, 249159.016, 249505.953, 248474.562, 248462.984, 248644.5, 247294.922, 247054.672, 247309.562, 246361.578, 245556.109, 245591.672, 245721.375, 244966.875, 244785.156, 244943.422, 243540.688, 244017.578, 243479.422, 243059.672, 243127.844, 242331.375, 241676.359, 242220.531, 241842.656, 242335.094, 241571.828, 241463.766, 240985.828, 240268.406, 241222.375, 239506.969, 239763.328, 239038.703, 239429.844, 238756.641, 238788.031, 238211.266, 239003.172, 238107.516, 238352.234, 238018.5, 237861.75, 237553.266, 237505.906, 236349.578, 236000.938, 236012.828, 236153.141], 'val_loss': [3155.506, 2960.204, 2884.28, 2834.76, 2820.604, 2821.653, 2766.974, 2739.547, 2717.398, 2693.267, 2674.779, 2694.117, 2635.671, 2618.8, 2601.91, 2588.259, 2587.849, 2561.953, 2583.049, 2591.245, 2573.098, 2537.799, 2548.791, 2558.397, 2541.837, 2527.264, 2523.587, 2530.597, 2509.349, 2544.387, 2510.709, 2514.714, 2542.362, 2502.155, 2501.24, 2498.841, 2475.137, 2488.602, 2477.004, 2464.433, 2479.196, 2476.681, 2466.036, 2472.637, 2479.279, 2451.42, 2436.06, 2469.744, 2453.502, 2427.398, 2427.481, 2429.76, 2442.216, 2442.939, 2426.572, 2435.71, 2424.525, 2435.657, 2440.066, 2417.599, 2398.766, 2425.09, 2443.88, 2406.451, 2409.665, 2423.883, 2402.943, 2399.769, 2391.877, 2396.705, 2403.99, 2403.501, 2414.171, 2403.555, 2385.997, 2415.289, 2374.342, 2407.091, 2382.497, 2424.048, 2386.23, 2369.339, 2371.759, 2379.42, 2403.812, 2401.204, 2399.843, 2377.081, 2370.009, 2369.795, 2367.398, 2356.922, 2376.655, 2353.115, 2368.242, 2350.592, 2387.365, 2367.623, 2356.698, 2372.639]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:91 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:101 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:28 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	31655.44922		334347	10	-1	227.78482913970947	{'train_loss': [409564.781, 349581.875, 323434.812, 310684.062, 302632.062, 298719.594, 295174.469, 292115.125, 289537.719, 286915.594, 284529.25, 282848.969, 279720.375, 277718.281, 276239.438, 274242.969, 272438.375, 272123.844, 269992.0, 269537.0, 268365.531, 266642.094, 265274.812, 264380.812, 263032.344, 263325.312, 262456.188, 262379.969, 260972.484, 260622.5, 259271.594, 258762.188, 258408.609, 257447.016, 256549.859, 255452.172, 254793.234, 254268.906, 253212.422, 252235.125, 251645.719, 250621.984, 250070.328, 249465.078, 247919.906, 248314.234, 247065.062, 246743.797, 246725.422, 245515.047, 244356.75, 244540.641, 243509.516, 244279.422, 243000.594, 241980.078, 241535.375, 241727.875, 240522.266, 240423.766, 240213.828, 239683.109, 239735.328, 238800.375, 238810.578, 238584.203, 238793.984, 237771.422, 236756.0, 236474.047, 235831.078, 236252.828, 236319.828, 235903.594, 235502.062, 234892.562, 234181.625, 234786.203, 233928.062, 233839.375, 232889.875, 233747.938, 233251.391, 231987.453, 232013.125, 232149.219, 231295.375, 231536.531, 230755.781, 230942.156, 230269.609, 230606.578, 229842.297, 229643.891, 229705.906, 228849.0, 228299.922, 228774.938, 228317.031, 228188.531], 'val_loss': [3258.187, 3001.79, 2924.398, 2861.39, 2825.646, 2807.137, 2780.954, 2754.81, 2731.674, 2717.773, 2691.122, 2668.807, 2650.327, 2633.047, 2626.796, 2606.097, 2582.984, 2578.381, 2564.452, 2559.445, 2569.306, 2531.866, 2533.146, 2544.581, 2528.07, 2546.63, 2518.786, 2511.042, 2499.032, 2507.445, 2482.003, 2497.493, 2494.481, 2462.659, 2471.222, 2500.651, 2472.701, 2462.947, 2431.143, 2445.578, 2404.885, 2409.35, 2433.461, 2424.572, 2415.434, 2424.086, 2418.509, 2447.039, 2428.943, 2391.15, 2375.174, 2399.268, 2394.059, 2395.05, 2386.964, 2378.657, 2372.707, 2380.355, 2385.34, 2371.657, 2365.416, 2374.05, 2364.537, 2364.891, 2313.345, 2326.661, 2356.812, 2352.995, 2321.562, 2398.143, 2312.905, 2313.278, 2337.766, 2314.206, 2329.901, 2328.171, 2350.351, 2323.271, 2318.151, 2342.015, 2357.3, 2326.543, 2305.659, 2319.917, 2324.358, 2320.741, 2301.486, 2302.049, 2326.513, 2308.539, 2299.82, 2290.031, 2315.857, 2317.034, 2299.542, 2288.403, 2281.229, 2287.599, 2322.849, 2301.126]}	100	100	True
