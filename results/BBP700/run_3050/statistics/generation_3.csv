id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:53 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	32880.95312		490746	14	-1	269.3724546432495	{'train_loss': [492732.281, 368870.312, 327479.812, 314240.25, 306379.312, 301918.031, 299396.969, 296935.719, 295262.625, 293001.875, 292075.0, 291107.938, 290114.438, 288371.844, 285625.062, 279679.906, 275710.312, 272446.344, 269419.594, 266715.156, 264025.594, 262865.375, 260960.0, 259632.281, 258032.641, 257757.453, 256634.406, 254891.719, 254555.781, 253644.797, 253372.062, 251250.922, 250906.172, 251222.016, 250011.734, 249101.219, 248428.281, 248268.922, 247448.141, 246008.156, 245454.328, 245836.406, 244712.734, 244473.75, 244775.484, 243225.625, 242431.453, 242664.625, 242170.922, 242012.859, 241396.984, 241372.078, 240520.5, 240420.469, 239943.547, 238425.938, 239210.562, 238319.891, 237715.859, 237578.188, 237022.266, 236641.484, 236249.375, 235786.938, 236603.656, 235858.078, 235034.297, 234751.219, 234395.484, 234477.219, 234217.453, 233415.812, 233168.625, 233224.578, 232938.844, 232595.938, 232087.094, 232292.297, 231864.391, 231721.922, 231347.938, 231241.109, 229617.984, 230541.547, 228922.234, 230152.062, 229529.906, 229725.281, 229103.078, 228032.0, 228140.281, 227737.406, 227755.297, 228084.234, 227087.625, 226923.797, 226372.0, 226173.172, 226609.469, 225183.359], 'val_loss': [3692.011, 3121.558, 2939.373, 2879.37, 2845.003, 2840.573, 2822.978, 2810.327, 2802.728, 2786.184, 2800.093, 2772.738, 2748.234, 2752.865, 2694.419, 2641.923, 2656.314, 2586.329, 2551.019, 2536.349, 2513.498, 2494.392, 2502.909, 2474.072, 2483.305, 2448.279, 2444.887, 2448.114, 2432.319, 2441.234, 2421.302, 2414.994, 2402.0, 2400.831, 2397.518, 2395.826, 2389.867, 2396.999, 2402.432, 2365.743, 2377.335, 2393.591, 2384.909, 2389.076, 2360.642, 2356.523, 2378.275, 2397.26, 2376.144, 2355.854, 2360.673, 2364.473, 2350.131, 2370.454, 2335.187, 2352.545, 2370.631, 2341.761, 2354.095, 2340.117, 2338.892, 2326.489, 2340.519, 2333.773, 2334.743, 2349.448, 2324.185, 2340.609, 2357.761, 2320.396, 2337.924, 2310.126, 2321.975, 2337.175, 2349.835, 2317.373, 2320.135, 2361.589, 2314.888, 2307.375, 2322.523, 2302.119, 2311.525, 2320.588, 2312.619, 2312.202, 2325.648, 2303.34, 2273.063, 2298.582, 2293.441, 2296.593, 2294.824, 2272.462, 2281.727, 2275.255, 2274.337, 2277.145, 2294.02, 2272.14]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:7 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:53 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:deconv1d out_channels:85 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.11050629890196254 beta1:0.8254982615943328 beta2:0.9094345167984534 weight_decay:0.0004700315909099839 batch_size:32 epochs:100	100	1000	True	137341.10938		13808741	15	-1	334.02770471572876	{'train_loss': [1371619.5, 1670487.25, 5581722.5, 1164835.25, 3112695.25, 1070028.0, 3076559.0, 1112083.5, 4209659.5, 1271265.5, 2616111.0, 2813350.0, 1329860.25, 4456971.5, 1215044.5, 2341216.0, 1100203.25, 2846678.25, 1085072.375, 2274189.75, 2356109.5, 1736069.5, 1767725.0, 1686690.375, 2545983.75, 2287043.5, 3831086.5, 1767288.125, 1787821.25, 1855792.375, 1245672.5, 3279772.75, 1127994.25, 3205663.25, 1204997.5, 2230411.0, 2415631.0, 1125310.5, 3994486.5, 1162662.25, 2502841.25, 1214948.875, 1235350.5, 3231414.5, 1078123.5, 2800213.0, 1108297.875, 1988625.0, 1224386.375, 1100397.125, 2849086.5, 1137623.5, 2181275.5, 1200517.625, 1826192.375, 1405072.5, 1410264.625, 3766818.25, 1185531.0, 2202451.0, 1251193.5, 1659972.125, 1915701.875, 1232139.75, 2896485.75, 1208093.25, 2782923.0, 1766370.0, 1289803.0, 2612384.75, 1072041.125, 3045691.75, 1239057.0, 3326684.0, 2917098.5, 1433156.875, 2473790.0, 1745396.875, 2092371.875, 1318902.25, 2126865.25, 2218835.25, 2697127.0, 2774258.5, 1303566.5, 2325870.5, 1279980.375, 3006902.25, 1345154.375, 2262691.75, 1794896.5, 1714978.75, 2065901.625, 1244612.25, 2922200.5, 2549135.25, 3176403.75, 2015209.875, 1800297.625, 4046994.5], 'val_loss': [10587.216, 341899.531, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 69253.609, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 15613.549, 15603.933, 10586.979, 10587.216, 10587.184, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 138458.719, 10587.216, 10587.216, 10587.216, 10587.216, 15106.04, 10587.216, 10554.219, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 24622.949, 10587.216, 10587.216, 10594.611, 10587.216, 13532.484, 10587.216, 14469.547, 10587.216, 10587.216, 17362.969, 10587.216, 24064.695, 10587.216, 11612.118, 10587.216, 10587.216, 15334.454, 10576.422, 10593.955, 10587.216, 10587.216, 10587.216, 10587.216, 10587.209, 10587.216, 10587.212, 10587.216, 10587.216, 37663.02, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 21386.547, 10587.216, 25631.135, 10587.216, 10587.216, 10587.215, 10587.216, 10587.216, 12984.786, 10587.216, 27479.025, 10587.216, 121937.633, 10587.216, 48069.547, 10587.216, 10587.215, 13508.082, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:53 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.21231265851761494 alpha:0.9246576492456985 weight_decay:1.1635608541048283e-05 batch_size:32 epochs:100	100	1000	True	153086.875		630370	14	-1	255.30023431777954	{'train_loss': [1316412.125, 1088624.5, 1264442.875, 1066985.125, 1358148.625, 1301327.375, 1576989.875, 1134996.625, 1132126.5, 1377916.0, 1545891.375, 2118922.25, 1322528.5, 1066921.0, 1121779.875, 2045803.875, 1127737.0, 1145895.625, 1075712.0, 2289309.0, 1337178.125, 1899545.625, 1107085.5, 1143927.875, 1381473.125, 1346731.0, 1103199.875, 1129962.125, 1607190.625, 1251357.5, 1148726.125, 1138148.375, 1239778.875, 1247004.125, 1152112.125, 1280346.5, 1392082.875, 1167540.0, 1221366.625, 1323906.125, 1306457.25, 1203808.875, 1217777.75, 1267631.0, 1236072.375, 1297612.25, 1251115.375, 1279340.25, 1262311.625, 1318461.25, 1254680.625, 1236912.75, 1273147.125, 1312873.75, 1316258.125, 1256790.0, 1273823.75, 1146097.5, 1415358.125, 1360716.125, 1204993.625, 1165677.0, 1229874.375, 1557681.875, 1207889.0, 1193688.625, 1241471.875, 1260591.125, 1368462.625, 1217506.75, 1191168.5, 1153962.5, 1549548.25, 1327243.375, 1243054.75, 1270908.0, 1191151.25, 1331407.875, 1336273.375, 1344881.25, 1190470.375, 1297337.0, 1222527.75, 1445372.625, 1287241.625, 1368943.375, 1162479.0, 1252966.375, 1344164.125, 1247043.375, 1235374.75, 1235009.0, 1349544.375, 1322032.25, 1257325.375, 1227841.25, 1174152.75, 1352579.25, 1295745.625, 1209135.5], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 11388.641, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10643.426, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 13193.994, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 15142.49, 10587.216, 10587.216, 10587.216, 10587.216, 12518.087, 10587.216, 10587.216, 12129.837, 10745.956, 15962.325, 19574.898, 10587.216, 15232.717, 10587.216, 10587.148, 11132.877, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 12474.274, 10571.33, 10587.191, 10587.216, 10587.216, 14598.158, 10587.036, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10582.842, 10582.32, 10582.438, 32970.086, 10587.216, 10587.216, 10587.216, 14874.854, 10587.216, 10587.216, 10587.216, 11513.586, 10587.212, 10587.216, 10582.37, 10587.216, 10629.017, 10587.216, 15389.28, 10587.216, 13676.096, 10587.216, 10587.216, 12688.488, 11046.468, 11878.511]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	32382.80469		463085	13	-1	253.17281413078308	{'train_loss': [413026.344, 336135.438, 321594.875, 310651.688, 298186.594, 291948.375, 287221.719, 282887.469, 279588.438, 277104.188, 274070.375, 271545.219, 269299.0, 267549.5, 265392.406, 264421.188, 262391.0, 260240.703, 258965.609, 258513.469, 256733.875, 255546.141, 254095.875, 253241.484, 252690.703, 251719.719, 250645.484, 250115.5, 249139.594, 248879.125, 247581.094, 247306.422, 246441.25, 246320.438, 245812.328, 244968.859, 244307.938, 244579.703, 243701.594, 243474.516, 242785.578, 241944.391, 242437.359, 241503.5, 241115.859, 240506.344, 240202.734, 240309.562, 239347.484, 238812.547, 238626.047, 238618.766, 238368.312, 237917.641, 237443.531, 237010.625, 236453.984, 236993.766, 236454.469, 235649.453, 235150.172, 234965.891, 234690.297, 234922.984, 233649.516, 234148.766, 232909.812, 232739.25, 233913.031, 233723.391, 232695.406, 232840.062, 232421.719, 232142.438, 231087.641, 231808.766, 231415.766, 231203.906, 230112.734, 229729.25, 230539.391, 228944.984, 229982.469, 229755.234, 229078.531, 228958.078, 229138.906, 228738.875, 228370.031, 227962.594, 227622.609, 228122.344, 227639.359, 226862.484, 226698.547, 226881.203, 226847.953, 226041.922, 225602.766, 226147.906], 'val_loss': [3222.719, 3059.074, 2988.99, 2872.438, 2816.991, 2776.921, 2721.898, 2686.525, 2695.183, 2649.371, 2636.615, 2601.624, 2592.125, 2588.146, 2570.671, 2536.955, 2519.44, 2513.664, 2502.533, 2504.919, 2481.257, 2471.05, 2463.783, 2456.787, 2442.327, 2449.872, 2415.798, 2444.402, 2406.664, 2406.546, 2418.967, 2405.324, 2410.316, 2406.042, 2391.752, 2394.381, 2397.839, 2403.427, 2402.17, 2370.335, 2381.156, 2358.182, 2404.139, 2380.044, 2355.136, 2378.966, 2362.807, 2364.834, 2368.133, 2373.463, 2379.835, 2353.73, 2370.19, 2363.265, 2394.707, 2376.29, 2359.398, 2365.243, 2356.245, 2362.913, 2351.979, 2353.585, 2368.57, 2345.506, 2358.063, 2345.781, 2336.392, 2341.517, 2343.891, 2334.507, 2334.889, 2343.92, 2344.32, 2309.288, 2338.086, 2331.167, 2307.39, 2329.089, 2328.674, 2329.463, 2303.364, 2351.668, 2327.306, 2326.15, 2333.372, 2309.189, 2333.31, 2312.635, 2310.196, 2319.421, 2318.546, 2321.174, 2305.121, 2326.974, 2321.256, 2286.258, 2288.496, 2277.92, 2298.303, 2313.438]}	100	100	True
