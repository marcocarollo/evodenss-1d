id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	31912.63086		469896	10	-1	208.34446835517883	{'train_loss': [560505.688, 359375.094, 318304.25, 303057.656, 295444.625, 288594.812, 284472.844, 280179.906, 277097.25, 274471.344, 272092.438, 270093.875, 268252.969, 267041.0, 265118.375, 263347.75, 262601.875, 260493.109, 259690.406, 259303.031, 257665.266, 256337.875, 256132.078, 255429.375, 255016.844, 254016.312, 253740.422, 251191.391, 251843.984, 249795.312, 250177.062, 248175.812, 247639.422, 248396.031, 246990.531, 245578.344, 245957.328, 243860.484, 243995.547, 243372.438, 242353.547, 242132.828, 240988.672, 239800.766, 239591.922, 239786.297, 238325.078, 237574.031, 237737.625, 236979.125, 237059.875, 236464.109, 235513.672, 234652.469, 235637.922, 233819.969, 234498.297, 232880.438, 232943.125, 233123.359, 231614.469, 231718.859, 231597.875, 231135.953, 231335.234, 230129.375, 229898.359, 229561.953, 229922.234, 229688.984, 229259.906, 229113.0, 227372.141, 227910.656, 227805.344, 227555.953, 226444.578, 226543.578, 227557.781, 226115.484, 225928.672, 226351.828, 226306.75, 224510.875, 225354.422, 224750.828, 224531.062, 223798.484, 224338.016, 223256.672, 223607.797, 223541.156, 223577.344, 222840.156, 221880.125, 222515.359, 222367.312, 221791.078, 221639.719, 221638.984], 'val_loss': [3751.945, 2987.998, 2847.097, 2783.697, 2723.091, 2698.575, 2661.386, 2637.831, 2622.07, 2602.023, 2576.08, 2569.757, 2548.53, 2562.638, 2520.011, 2547.84, 2497.102, 2493.752, 2491.896, 2473.98, 2474.559, 2475.653, 2464.612, 2444.748, 2454.241, 2439.738, 2442.793, 2435.239, 2420.841, 2423.063, 2409.567, 2414.341, 2398.08, 2397.39, 2387.806, 2370.885, 2360.765, 2357.803, 2385.17, 2359.402, 2362.004, 2359.108, 2377.196, 2345.27, 2340.485, 2340.096, 2345.067, 2370.833, 2315.13, 2363.17, 2343.785, 2325.802, 2332.414, 2322.646, 2299.247, 2309.972, 2334.627, 2327.836, 2318.053, 2316.313, 2306.265, 2308.331, 2292.258, 2294.674, 2305.17, 2319.423, 2279.131, 2291.44, 2275.494, 2267.867, 2290.414, 2258.557, 2276.263, 2301.639, 2274.424, 2261.043, 2250.166, 2294.961, 2269.022, 2286.022, 2267.306, 2264.944, 2248.103, 2245.804, 2241.02, 2269.528, 2237.737, 2258.061, 2235.003, 2246.488, 2242.767, 2240.813, 2243.818, 2245.682, 2250.337, 2239.583, 2232.799, 2218.529, 2219.977, 2222.445]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:73 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:32 epochs:100	100	1000	True	137281.92188		6140311	9	-1	200.1127791404724	{'train_loss': [1140108.75, 1069084.75, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1074019.25, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:81 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:rmsprop lr:0.12273258174664141 alpha:0.8395306222077988 weight_decay:1.1693063471064435e-05 batch_size:32 epochs:100	100	1000	True	132278.64062		356316	9	-1	194.1691529750824	{'train_loss': [831585.875, 754676.438, 739915.188, 687078.375, 686920.688, 724480.562, 705254.688, 696817.562, 732279.5, 695184.875, 649494.312, 663110.375, 733398.812, 664006.5, 692911.75, 637881.938, 647691.812, 653118.875, 671542.562, 650760.312, 697460.375, 636216.75, 648241.0, 646158.875, 710274.125, 693627.812, 659833.5, 635367.625, 713083.875, 641960.062, 707000.625, 635143.375, 680769.75, 704150.0, 660173.938, 632062.25, 690889.188, 581723.938, 681882.25, 654491.062, 689326.812, 656716.438, 731457.0, 690800.5, 700566.938, 673390.625, 758013.188, 699473.688, 725110.375, 653396.875, 663800.562, 680231.938, 681842.438, 612364.0, 665008.125, 658859.0, 707428.812, 647207.25, 701495.438, 655860.562, 657420.625, 614684.375, 684963.688, 646245.375, 666615.938, 623228.812, 663083.562, 762672.875, 664132.062, 659510.562, 638525.562, 648199.5, 667531.812, 651449.125, 684960.562, 700647.875, 672389.75, 654290.938, 739968.062, 679845.375, 682683.312, 689663.688, 639833.0, 708884.875, 740210.812, 629673.875, 643911.75, 648006.0, 647994.625, 735341.688, 706002.812, 647638.312, 686688.188, 654801.688, 666533.875, 648591.375, 642922.688, 640012.125, 662589.375, 715356.312], 'val_loss': [7400.989, 11414.881, 3705.444, 5007.107, 3528.196, 3720.467, 4127.056, 4441.774, 6204.455, 10101.979, 9207.23, 3740.465, 4004.491, 4873.897, 3674.285, 3498.362, 3857.229, 3474.764, 3793.785, 3419.95, 3413.397, 3483.052, 3439.543, 3445.79, 3637.1, 5443.148, 5161.473, 3661.806, 5466.11, 6532.172, 4086.741, 3448.446, 9244.387, 6291.226, 3587.014, 4234.908, 3740.106, 3435.417, 3490.319, 3778.643, 3529.104, 3469.585, 3557.363, 4130.223, 3731.357, 3458.594, 3562.539, 7090.391, 4826.716, 4344.407, 3470.114, 3564.122, 3509.778, 6691.462, 3848.212, 9444.09, 4446.057, 10174.55, 9907.453, 3569.927, 3436.099, 10699.823, 6751.915, 3596.306, 3484.489, 12125.736, 3532.041, 4627.854, 3453.749, 12630.997, 3553.779, 3586.147, 12657.6, 3574.856, 13845.658, 3454.632, 8163.104, 12474.102, 3974.183, 3462.322, 3486.913, 28299.76, 4068.946, 9043.465, 3525.872, 3719.156, 3407.668, 3611.151, 6853.812, 3502.344, 7011.013, 4178.826, 4382.31, 3548.273, 3438.614, 4072.893, 9870.492, 11844.046, 3418.647, 10446.738]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:82 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:32 epochs:100	100	1000	True	32294.92188		248838	9	-1	186.3851945400238	{'train_loss': [399224.781, 339687.531, 321290.312, 312829.0, 307188.844, 302838.156, 299308.031, 294576.062, 290035.625, 286472.125, 282744.719, 279544.375, 276941.031, 274415.562, 271800.469, 270251.0, 268503.125, 267483.906, 264913.125, 264983.781, 262642.656, 262545.875, 261118.328, 259629.312, 258794.656, 258584.109, 257452.953, 256704.391, 256157.281, 255062.203, 254293.578, 253442.938, 253548.266, 252723.969, 251675.328, 251357.094, 251389.516, 251005.812, 250608.578, 249913.469, 249086.094, 249083.469, 248987.891, 247853.125, 247940.172, 247057.672, 246528.672, 246404.125, 246261.453, 245279.047, 245402.953, 244597.203, 245082.172, 244172.234, 244169.094, 242880.922, 243342.641, 243190.359, 243313.859, 241996.656, 241805.672, 241605.625, 240686.594, 241234.484, 240169.938, 241090.141, 240109.406, 239320.766, 238891.922, 239342.234, 239938.047, 238775.062, 238986.828, 238126.219, 237501.484, 238134.734, 237268.859, 236303.719, 237344.656, 236334.938, 236753.562, 236808.281, 236164.141, 236335.875, 235213.406, 235553.5, 234439.203, 233616.016, 234911.672, 234458.141, 234145.516, 233208.016, 233085.328, 232847.578, 232642.703, 233084.891, 232742.797, 231681.125, 232609.031, 231426.516], 'val_loss': [3162.923, 2989.336, 2910.569, 2862.559, 2831.052, 2805.411, 2778.705, 2746.023, 2720.779, 2685.639, 2670.274, 2627.256, 2604.079, 2598.856, 2581.276, 2567.0, 2563.49, 2558.01, 2538.101, 2507.384, 2510.653, 2493.164, 2492.08, 2473.508, 2470.323, 2475.382, 2458.327, 2459.093, 2452.263, 2438.541, 2425.599, 2445.654, 2433.905, 2442.412, 2424.633, 2419.081, 2413.097, 2420.82, 2414.859, 2406.613, 2399.63, 2408.176, 2419.695, 2401.769, 2408.124, 2403.854, 2417.548, 2407.932, 2383.397, 2394.68, 2371.835, 2390.527, 2370.413, 2365.097, 2378.8, 2363.978, 2367.942, 2367.017, 2348.462, 2352.336, 2355.872, 2371.206, 2346.941, 2339.498, 2346.56, 2335.328, 2322.456, 2345.396, 2332.342, 2339.315, 2329.878, 2364.649, 2332.819, 2331.335, 2338.087, 2326.684, 2324.445, 2315.029, 2329.07, 2309.296, 2312.464, 2319.853, 2311.579, 2343.899, 2318.352, 2309.088, 2303.846, 2314.999, 2289.044, 2320.931, 2280.824, 2287.418, 2299.387, 2281.029, 2287.612, 2292.816, 2287.353, 2276.873, 2293.568, 2278.285]}	100	100	True
