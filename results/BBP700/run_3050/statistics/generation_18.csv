id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	31781.37305		576588	11	-1	235.42025303840637	{'train_loss': [686882.125, 347462.625, 315322.625, 304836.094, 299520.031, 295400.781, 290699.875, 285162.562, 279811.406, 274338.156, 270461.25, 266024.969, 264240.062, 261177.281, 258559.547, 257039.344, 255638.031, 253635.453, 252259.75, 251007.266, 251504.578, 248210.688, 247235.828, 246554.438, 245087.359, 244978.219, 243742.547, 242229.875, 241803.094, 241439.422, 240847.188, 240674.609, 238161.172, 237967.312, 236956.766, 237361.109, 237101.438, 236325.031, 235104.969, 236812.219, 234859.375, 233788.0, 233700.469, 232888.875, 232276.031, 231525.375, 231524.844, 230701.062, 229880.5, 230646.891, 229785.203, 228835.906, 228751.5, 227811.953, 227921.969, 227009.172, 226596.75, 226509.047, 226415.984, 225121.25, 225663.703, 225009.031, 224832.438, 224268.859, 225095.75, 224406.625, 223109.344, 222452.719, 222363.25, 221651.609, 222077.562, 221380.469, 220721.094, 220800.812, 219859.438, 220790.078, 219373.016, 219073.062, 219020.047, 217866.078, 217822.281, 217647.688, 216841.953, 217198.5, 217049.406, 216731.906, 216404.922, 215353.844, 215521.406, 214891.297, 215023.188, 215010.844, 214536.75, 214446.062, 213941.656, 212884.359, 213612.641, 212594.359, 213709.328, 212865.656], 'val_loss': [3710.023, 2991.646, 2894.568, 2839.007, 2788.053, 2729.81, 2687.244, 2627.273, 2583.558, 2533.976, 2501.975, 2465.665, 2456.969, 2418.708, 2427.057, 2392.076, 2379.393, 2390.046, 2405.695, 2355.835, 2345.039, 2339.974, 2337.512, 2345.385, 2337.078, 2335.8, 2337.871, 2325.926, 2336.073, 2326.311, 2302.946, 2295.74, 2314.177, 2307.602, 2285.997, 2290.542, 2313.941, 2288.52, 2302.608, 2276.277, 2302.281, 2273.405, 2296.673, 2259.286, 2252.776, 2255.976, 2296.999, 2257.581, 2254.994, 2258.621, 2245.98, 2254.088, 2261.542, 2280.202, 2250.388, 2246.001, 2248.864, 2240.147, 2253.844, 2266.771, 2270.998, 2252.154, 2251.632, 2242.628, 2261.11, 2235.8, 2252.351, 2268.759, 2264.258, 2230.75, 2224.688, 2240.95, 2204.595, 2243.559, 2225.808, 2224.694, 2241.175, 2209.941, 2223.067, 2224.602, 2208.788, 2222.904, 2223.187, 2223.718, 2221.878, 2198.833, 2241.107, 2232.798, 2214.491, 2198.935, 2203.888, 2207.481, 2191.89, 2206.507, 2189.171, 2204.518, 2197.298, 2225.768, 2202.323, 2194.869]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:70 kernel_size:3 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:98 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:rmsprop lr:0.21346762790130575 alpha:0.9833857310175245 weight_decay:0.0006259381008076858 batch_size:32 epochs:100	100	1000	True	137267.10938		907715	11	-1	300.0649929046631	{'train_loss': [3736211.25, 1066921.0, 1066921.0, 1068945.625, 1067042.375, 1072179.375, 1774951.25, 1465611.875, 37630504.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 38452116.0, 1066921.0, 1066921.0, 1066921.0, 1066927.5, 1066921.0, 1066921.0, 1066921.0, 1098344.25, 9845052.0, 1110398.375, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 30687546.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 11373596.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1830107.875, 6441070.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 32543854.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066925.125, 1066921.0, 1066921.0, 1214345.125, 1066921.0, 1067418.625, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 58111516.0, 1066921.0, 1229718.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 2015411.25, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 7744882.0, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	31607.01172		576588	11	-1	236.26991963386536	{'train_loss': [777406.375, 364341.062, 315782.031, 300912.375, 292881.156, 284594.344, 278758.312, 274063.062, 269782.812, 267231.75, 264089.844, 262652.875, 260347.047, 258113.812, 256694.047, 255875.172, 254009.797, 252295.594, 251575.766, 250511.375, 249191.016, 248897.344, 247569.969, 245931.703, 245531.422, 244832.766, 243320.812, 242943.344, 242410.734, 240676.141, 241234.188, 240684.859, 238984.453, 238211.078, 238551.484, 238041.734, 237792.344, 236622.062, 236290.078, 236164.531, 236778.234, 235294.453, 234597.719, 233994.484, 233970.75, 233391.422, 233550.266, 232828.359, 232176.766, 231356.344, 231343.656, 231489.109, 230753.719, 231078.688, 229161.469, 230106.391, 230194.047, 229378.094, 228544.062, 229406.094, 228171.094, 227960.688, 228066.094, 228520.438, 227538.953, 227179.531, 227436.422, 227108.5, 226768.625, 226393.672, 226178.172, 226064.016, 225199.781, 224757.312, 226275.172, 224701.703, 225086.25, 223549.781, 223597.906, 224737.219, 223708.953, 224488.344, 224138.688, 223538.125, 222445.984, 223292.484, 223370.422, 222254.656, 222726.828, 222568.375, 222095.016, 221476.234, 221768.359, 221395.094, 221565.516, 221893.594, 220942.391, 220959.094, 221646.031, 220584.781], 'val_loss': [3374.809, 3054.37, 2869.361, 2782.051, 2701.866, 2649.532, 2613.355, 2578.594, 2542.101, 2543.769, 2529.407, 2500.668, 2473.13, 2464.623, 2463.613, 2453.372, 2433.906, 2427.807, 2437.475, 2394.065, 2395.915, 2411.625, 2397.015, 2383.782, 2366.666, 2382.104, 2353.625, 2355.703, 2332.454, 2363.921, 2343.006, 2346.367, 2334.777, 2318.281, 2336.954, 2310.152, 2305.429, 2308.03, 2302.778, 2293.7, 2320.83, 2314.5, 2305.677, 2282.914, 2275.122, 2276.072, 2277.182, 2280.748, 2279.441, 2283.893, 2287.563, 2266.073, 2279.601, 2275.974, 2312.548, 2264.129, 2263.803, 2261.248, 2292.43, 2267.964, 2255.506, 2253.527, 2248.094, 2254.394, 2247.194, 2253.213, 2252.017, 2237.228, 2245.917, 2228.323, 2231.621, 2234.038, 2262.697, 2263.019, 2232.367, 2234.923, 2240.338, 2238.785, 2253.622, 2245.169, 2228.587, 2250.858, 2242.49, 2229.836, 2226.266, 2223.849, 2227.134, 2223.052, 2226.827, 2221.619, 2256.833, 2233.027, 2234.959, 2214.828, 2209.717, 2214.483, 2217.422, 2203.508, 2207.962, 2216.455]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adadelta batch_size:32 epochs:100	100	1000	True	137305.07812		14082073	9	-1	239.55488467216492	{'train_loss': [1265105.5, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
