id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:108 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	32290.66016		363673	12	-1	242.23225021362305	{'train_loss': [419909.844, 374354.656, 331427.5, 317155.906, 309748.25, 305454.656, 301301.938, 295434.281, 289557.812, 284290.156, 279653.375, 275620.656, 272419.5, 270353.781, 267489.125, 265014.719, 262311.375, 261409.969, 259873.203, 258241.344, 256714.297, 255805.734, 254439.906, 252945.359, 252583.781, 251091.328, 250783.25, 249564.578, 248979.922, 248435.578, 247642.016, 246257.547, 245621.344, 244587.031, 244361.781, 242692.906, 242349.609, 242432.078, 241546.453, 241149.125, 240211.969, 240439.562, 239264.0, 238878.844, 238209.438, 236464.266, 236686.078, 237909.031, 236810.891, 236175.75, 234918.125, 235263.656, 235555.031, 234448.25, 234273.578, 233395.156, 232987.828, 233092.969, 232676.266, 233563.656, 232300.484, 233512.781, 233041.344, 231954.656, 231261.625, 231123.234, 230071.938, 230160.688, 229435.125, 230397.141, 229504.812, 229445.594, 229120.938, 229130.797, 228193.688, 228052.625, 228527.875, 227874.453, 227690.719, 227699.688, 227584.578, 226858.359, 226777.766, 227328.031, 226740.359, 226776.516, 226604.125, 227039.0, 225141.703, 226241.094, 227038.5, 225445.781, 225169.969, 225434.219, 224361.359, 224390.516, 224321.188, 224179.219, 224087.641, 223782.844], 'val_loss': [3586.738, 3148.084, 2981.758, 2914.032, 2897.946, 2865.47, 2822.574, 2763.459, 2715.154, 2666.432, 2660.319, 2621.81, 2627.142, 2620.532, 2565.011, 2523.466, 2511.326, 2509.293, 2510.123, 2459.837, 2462.748, 2461.676, 2432.24, 2458.478, 2427.417, 2429.183, 2427.655, 2395.27, 2394.033, 2387.954, 2392.285, 2382.006, 2387.226, 2388.036, 2377.618, 2381.834, 2349.127, 2366.485, 2337.786, 2358.725, 2337.191, 2331.302, 2349.705, 2328.774, 2314.893, 2301.974, 2336.084, 2336.461, 2325.12, 2311.273, 2323.195, 2291.597, 2283.903, 2329.508, 2321.92, 2305.859, 2341.681, 2301.634, 2327.73, 2299.716, 2294.336, 2314.255, 2292.225, 2295.06, 2276.459, 2286.596, 2303.416, 2283.596, 2290.026, 2276.32, 2273.724, 2261.315, 2257.159, 2280.214, 2277.056, 2282.112, 2273.667, 2289.251, 2261.957, 2259.286, 2272.167, 2262.028, 2290.148, 2305.485, 2294.455, 2277.441, 2258.772, 2277.831, 2287.501, 2253.576, 2259.051, 2257.373, 2269.51, 2289.995, 2283.033, 2287.964, 2257.769, 2277.958, 2271.947, 2270.196]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:108 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	32038.90234		363673	12	-1	241.98383212089539	{'train_loss': [415676.562, 357830.844, 326753.469, 316356.5, 309963.125, 301310.594, 294853.594, 289205.0, 284153.812, 279939.5, 275714.375, 272245.844, 269729.406, 266884.094, 265214.75, 262436.188, 261358.359, 259410.078, 257702.578, 257091.391, 255265.359, 254537.078, 254033.281, 252220.797, 251120.953, 249703.734, 249385.219, 249016.938, 248819.219, 246008.812, 245607.203, 244554.141, 244717.234, 243498.656, 242416.438, 241719.75, 241634.344, 240648.062, 239694.5, 240246.625, 239576.328, 237616.281, 238461.047, 237894.422, 237696.797, 237306.25, 236324.469, 236345.719, 235353.859, 234659.922, 234197.469, 234784.438, 233445.219, 233667.094, 232644.219, 232234.703, 231521.188, 232224.625, 231850.484, 231618.891, 230466.297, 230360.656, 230404.281, 230097.172, 230020.984, 229851.531, 229700.859, 228879.484, 228472.469, 228465.297, 228042.594, 227305.766, 227252.828, 227040.297, 227253.156, 227445.422, 226219.172, 226844.016, 225592.328, 225886.016, 226073.875, 225263.547, 224272.609, 225170.844, 224543.109, 224584.094, 224323.0, 223542.266, 223887.156, 223009.766, 223463.938, 223634.406, 223114.562, 222737.891, 221738.156, 222203.375, 222080.578, 222042.156, 222713.766, 221774.516], 'val_loss': [3485.834, 3053.068, 2927.094, 2905.964, 2838.528, 2792.611, 2754.166, 2713.947, 2677.199, 2644.241, 2610.292, 2591.805, 2574.15, 2569.302, 2514.502, 2482.575, 2495.572, 2470.806, 2464.655, 2459.704, 2477.466, 2460.406, 2430.469, 2421.841, 2459.612, 2427.333, 2393.479, 2438.057, 2422.928, 2391.846, 2393.609, 2388.335, 2390.828, 2392.863, 2377.909, 2365.412, 2388.35, 2350.966, 2339.367, 2343.825, 2335.799, 2354.186, 2329.946, 2314.445, 2333.198, 2307.892, 2297.266, 2301.241, 2302.12, 2298.426, 2289.074, 2304.478, 2297.478, 2299.279, 2303.421, 2275.496, 2293.613, 2287.797, 2295.684, 2279.212, 2265.615, 2288.24, 2294.222, 2281.524, 2273.256, 2287.424, 2294.367, 2288.423, 2275.143, 2282.117, 2275.663, 2259.419, 2266.062, 2245.798, 2276.367, 2257.051, 2272.355, 2278.527, 2257.181, 2255.689, 2245.948, 2253.62, 2247.884, 2260.586, 2271.418, 2250.833, 2236.246, 2241.24, 2220.192, 2237.919, 2260.53, 2260.219, 2236.962, 2250.258, 2242.931, 2229.493, 2239.782, 2217.182, 2242.393, 2245.574]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:108 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:52 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	137271.42188		2525248	12	-1	240.232985496521	{'train_loss': [1260970.0, 1067433.625, 1120321.5, 1068660.75, 1066893.375, 1083034.5, 1076421.5, 1066921.0, 1069122.375, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1068626.875, 1066917.625, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066917.125, 1068706.625, 1069890.875, 1067267.25, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.198, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 11410.243, 10812.58, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:85 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:108 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:deconv1d out_channels:78 kernel_size:4 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	35386.67969		3509897	12	-1	232.54321026802063	{'train_loss': [1164966.0, 1344273.125, 1085584.0, 1099138.5, 1083604.375, 1089492.125, 1082269.875, 1067737.875, 1067347.75, 1067482.625, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1067240.25, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1076974.0, 1067951.75, 1076452.125, 1070721.0, 1067952.875, 1078525.625, 1071222.75, 1069147.125, 1098936.5, 1081646.0, 1068421.375, 1077109.875, 1072587.875, 1090298.375, 733834.25, 341951.344, 322654.25, 318743.438, 315318.062, 315690.781, 311255.219, 310859.219, 306551.094, 305018.969, 301449.312, 297747.812, 295352.594, 293206.562, 289712.719, 289171.875, 286864.375, 283199.375, 281564.625, 280903.156, 281853.156, 277733.812, 279210.281, 275921.656, 276535.344, 273126.219, 272533.219, 271920.312, 270969.406, 270543.625, 269792.219, 269166.375, 269139.0, 266424.0, 267276.312, 265810.094, 266867.312, 264315.906, 264973.625, 263721.844, 264346.219, 263396.469, 262554.625, 261941.141, 260527.5, 262798.344, 260285.672, 259076.906, 258846.0, 259452.672, 259656.969, 258036.938, 258956.219, 256032.328, 257170.547, 256578.406, 259166.938], 'val_loss': [11097.439, 10587.215, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 11049.1, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10586.371, 10587.216, 10587.216, 10586.647, 10587.216, 10587.216, 10586.887, 10587.216, 10584.355, 10679.728, 10640.352, 3189.82, 2944.063, 2915.371, 2894.51, 2885.05, 2878.571, 2874.156, 2862.456, 2855.812, 2842.019, 2795.979, 2771.483, 2761.939, 2736.255, 2726.032, 2693.043, 2668.615, 2674.92, 2613.132, 2644.31, 2644.144, 2589.009, 2664.755, 2598.856, 2621.334, 2624.079, 2566.366, 2594.728, 2583.714, 2607.083, 2561.845, 2554.678, 2512.351, 2563.782, 2536.359, 2559.763, 2536.563, 2541.047, 2533.327, 2537.915, 2539.081, 2492.332, 2516.339, 2499.263, 2523.887, 2521.377, 2492.377, 2493.994, 2491.871, 2491.559, 2508.84, 2494.061, 2513.378, 2481.304, 2478.271, 2507.468, 2484.782]}	100	100	True
