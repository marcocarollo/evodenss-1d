id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	32991.62109		350017	12	-1	267.1732761859894	{'train_loss': [500323.594, 353088.188, 316213.531, 300891.844, 290805.406, 282508.281, 276535.094, 271906.344, 267744.656, 265001.906, 261782.453, 260410.75, 258036.766, 256848.781, 254943.312, 254290.047, 252556.875, 251678.25, 249866.562, 249046.016, 248256.438, 247201.328, 246539.234, 245800.031, 245242.406, 244701.297, 243517.344, 243118.391, 242396.359, 241442.141, 241034.625, 241041.172, 240321.984, 239817.078, 238330.219, 238763.969, 238196.531, 238247.234, 237620.203, 237356.797, 237482.0, 236404.016, 236134.547, 235281.094, 234922.25, 234647.062, 234900.078, 234183.781, 233644.562, 233342.891, 232981.844, 232876.5, 233032.406, 231995.016, 231767.609, 231066.25, 231138.594, 231553.578, 230432.297, 230397.328, 231230.875, 229316.375, 230087.953, 229170.188, 228464.266, 228678.156, 228428.297, 227824.203, 228303.875, 227406.375, 226894.672, 227355.922, 226493.406, 226968.766, 226194.609, 226053.734, 225776.375, 226127.781, 224638.656, 224510.781, 224806.0, 224724.031, 223657.328, 223898.688, 222451.328, 223861.359, 223360.719, 222885.781, 223371.0, 221865.078, 222278.672, 221992.281, 221110.094, 220500.422, 221185.688, 220885.969, 220844.062, 220024.797, 220312.516, 219767.844], 'val_loss': [4554.854, 3037.064, 2875.641, 2765.169, 2716.006, 2656.305, 2597.268, 2560.863, 2550.608, 2544.596, 2517.235, 2507.255, 2502.754, 2488.162, 2474.322, 2506.672, 2474.842, 2470.354, 2476.734, 2461.45, 2447.552, 2447.893, 2463.207, 2453.206, 2451.938, 2436.915, 2435.984, 2442.117, 2405.809, 2458.952, 2438.515, 2420.3, 2409.364, 2412.521, 2406.79, 2407.877, 2411.453, 2382.426, 2413.964, 2399.665, 2398.666, 2385.189, 2377.593, 2397.765, 2401.164, 2372.979, 2362.081, 2348.392, 2346.551, 2362.567, 2356.474, 2353.124, 2354.329, 2361.159, 2362.958, 2359.936, 2360.653, 2366.4, 2342.189, 2342.568, 2332.085, 2355.426, 2334.225, 2314.898, 2325.282, 2322.68, 2313.865, 2314.417, 2317.996, 2353.617, 2307.952, 2337.211, 2307.69, 2306.814, 2315.296, 2303.721, 2315.228, 2332.02, 2333.054, 2303.216, 2310.277, 2314.725, 2326.519, 2331.583, 2339.432, 2286.507, 2342.104, 2311.724, 2298.606, 2306.386, 2348.626, 2301.857, 2316.058, 2340.413, 2315.32, 2308.296, 2305.483, 2329.764, 2335.885, 2294.691]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	2000	True	31520.01758		350017	12	-1	272.1353003978729	{'train_loss': [471882.0, 345411.969, 319714.625, 307091.094, 299093.219, 292175.406, 285525.562, 281469.312, 278141.344, 274415.719, 270495.0, 267987.188, 265497.281, 263206.156, 261363.078, 260514.094, 259038.859, 257102.812, 255440.703, 254787.672, 252979.891, 251845.562, 250562.453, 249825.5, 248055.891, 248134.297, 246949.938, 246719.938, 245241.594, 244756.047, 244153.359, 242623.781, 242543.984, 241184.594, 241154.578, 240140.625, 239748.359, 239208.797, 237988.672, 238380.625, 236543.047, 236587.953, 235771.969, 235409.219, 235171.859, 236004.016, 234010.266, 233458.5, 233535.516, 232858.719, 233485.344, 231312.078, 231242.672, 230773.531, 230602.703, 230798.281, 230580.359, 230069.422, 229673.391, 229615.688, 228465.344, 228195.688, 227687.75, 226782.969, 227697.297, 226988.594, 226565.812, 226312.547, 226215.406, 225526.641, 225960.219, 225184.031, 224990.422, 225647.922, 224375.516, 223661.516, 224249.469, 223189.906, 223704.859, 223556.5, 223344.828, 222715.609, 222428.562, 222407.344, 221622.484, 221790.859, 222562.875, 221842.953, 221061.719, 221084.047, 221072.156, 221564.594, 221003.109, 220888.328, 220462.594, 220189.938, 220010.141, 219227.516, 220093.953, 219485.891], 'val_loss': [3493.094, 3043.12, 2878.783, 2819.626, 2765.704, 2723.964, 2681.104, 2649.989, 2631.988, 2611.281, 2595.766, 2547.813, 2531.877, 2535.706, 2527.254, 2502.116, 2490.558, 2489.97, 2469.637, 2466.808, 2450.06, 2435.611, 2426.044, 2416.868, 2434.423, 2421.057, 2420.981, 2407.235, 2400.328, 2396.392, 2409.429, 2385.497, 2380.974, 2353.98, 2362.896, 2358.746, 2346.841, 2332.594, 2370.026, 2348.012, 2344.292, 2344.98, 2315.063, 2319.411, 2353.814, 2309.335, 2316.125, 2329.571, 2316.406, 2346.964, 2309.293, 2333.786, 2311.222, 2329.875, 2315.602, 2329.157, 2307.23, 2280.114, 2293.562, 2310.203, 2294.922, 2309.494, 2309.93, 2277.681, 2289.197, 2300.843, 2285.969, 2287.016, 2293.844, 2256.165, 2270.628, 2296.705, 2285.872, 2263.847, 2284.642, 2296.049, 2274.645, 2276.146, 2280.191, 2252.707, 2255.914, 2260.291, 2277.472, 2249.068, 2271.875, 2261.701, 2264.356, 2248.125, 2232.571, 2260.436, 2247.492, 2237.481, 2240.802, 2237.355, 2224.574, 2223.701, 2266.231, 2236.048, 2230.596, 2238.796]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:deconv1d out_channels:123 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	137379.82812		39748714	12	-1	381.1719250679016	{'train_loss': [1161549.75, 1161008.125, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:45 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:30 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:30 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	2000	True	31907.07031		298108	13	-1	262.109876871109	{'train_loss': [402998.312, 336149.25, 309437.188, 297151.156, 288489.969, 280672.281, 272953.25, 267256.469, 263152.594, 260226.406, 257409.594, 255735.219, 253692.875, 253311.328, 251459.234, 249941.172, 248889.031, 246757.953, 246032.062, 244929.969, 243813.75, 243281.938, 241940.672, 240833.75, 239441.109, 239725.531, 237894.719, 237432.094, 237589.156, 236236.594, 236391.828, 235165.875, 235056.141, 233483.609, 233216.266, 233002.688, 232223.734, 231817.188, 231427.953, 231150.422, 229856.766, 229836.516, 230155.609, 229462.469, 228794.766, 228207.766, 229128.172, 227877.016, 227016.688, 227720.859, 226378.328, 227004.219, 227273.922, 225959.969, 226420.656, 225361.344, 224549.141, 224808.031, 224792.5, 224849.891, 223526.75, 222846.141, 223399.391, 223028.922, 222187.25, 222591.172, 222782.172, 221196.953, 222076.672, 220368.719, 221570.359, 220594.234, 220410.719, 220388.141, 220343.297, 219639.516, 220064.094, 219836.781, 219564.969, 218894.047, 218753.281, 217956.203, 217354.781, 217483.391, 218364.188, 218437.0, 217793.953, 216829.25, 217241.328, 217282.438, 215775.688, 216653.672, 216477.25, 216801.141, 215918.891, 215973.688, 215519.781, 215930.938, 215875.453, 215085.547], 'val_loss': [3195.473, 2989.347, 2910.504, 2859.656, 2811.667, 2731.902, 2634.625, 2584.842, 2523.557, 2500.654, 2470.971, 2441.932, 2444.062, 2435.606, 2429.339, 2434.603, 2426.859, 2422.6, 2409.144, 2408.286, 2383.305, 2390.865, 2378.447, 2380.042, 2369.185, 2363.707, 2355.588, 2344.027, 2351.753, 2339.059, 2355.524, 2345.232, 2343.198, 2324.031, 2349.787, 2356.933, 2341.104, 2313.448, 2311.433, 2353.489, 2297.393, 2287.739, 2279.071, 2331.347, 2327.789, 2309.987, 2315.145, 2297.17, 2288.396, 2287.911, 2295.82, 2302.883, 2295.382, 2291.815, 2273.15, 2258.877, 2268.786, 2265.126, 2276.116, 2275.611, 2259.677, 2257.655, 2287.87, 2253.623, 2277.162, 2267.266, 2257.879, 2280.706, 2247.884, 2283.1, 2285.567, 2270.339, 2254.939, 2256.693, 2249.075, 2260.097, 2272.165, 2232.276, 2236.828, 2272.526, 2240.873, 2259.433, 2249.822, 2245.027, 2249.119, 2250.844, 2234.241, 2240.895, 2248.954, 2222.114, 2231.172, 2238.209, 2229.561, 2242.207, 2215.747, 2215.349, 2208.231, 2229.671, 2215.572, 2229.653]}	0	100	True
