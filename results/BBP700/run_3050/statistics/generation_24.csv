id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	3000	True	31469.1543		576588	11	-1	206.06429171562195	{'train_loss': [644334.5, 353355.75, 317406.375, 304911.156, 295900.969, 290849.781, 285300.281, 280298.844, 275059.375, 270434.5, 266722.906, 264103.219, 261881.953, 260321.281, 258395.625, 256599.406, 254475.188, 253712.719, 251881.0, 251570.953, 249403.859, 248423.234, 248390.234, 247210.953, 246283.062, 245143.547, 244543.812, 245043.094, 244056.328, 242557.109, 241752.453, 242204.328, 240870.781, 240076.328, 239172.562, 239160.797, 238239.25, 238047.344, 237024.141, 236681.812, 236448.359, 235947.141, 236137.812, 234106.531, 235776.047, 234794.656, 234358.125, 234041.062, 233133.406, 233279.172, 232161.422, 231983.0, 232474.719, 231035.0, 230806.406, 230589.891, 230633.0, 229836.188, 229778.484, 228757.656, 229323.828, 228152.125, 228095.25, 227504.875, 227366.984, 227217.062, 226659.625, 226681.797, 226370.062, 226323.875, 226525.844, 225734.797, 224939.609, 225950.203, 225113.875, 226172.5, 223442.062, 223860.141, 225280.297, 223559.703, 223226.484, 224190.234, 222870.938, 222511.328, 223241.047, 222740.219, 221890.516, 221939.281, 221820.875, 221207.266, 221907.719, 221936.094, 220340.875, 220838.031, 220451.031, 220138.875, 219300.0, 220166.5, 220376.844, 220912.547], 'val_loss': [3414.334, 3044.086, 2852.92, 2820.529, 2768.322, 2715.934, 2652.544, 2614.888, 2567.056, 2548.357, 2529.699, 2523.55, 2490.214, 2503.3, 2449.774, 2464.168, 2445.066, 2442.56, 2414.086, 2405.545, 2416.79, 2394.52, 2407.529, 2388.714, 2386.896, 2388.994, 2389.342, 2364.817, 2384.442, 2344.587, 2344.271, 2331.079, 2316.309, 2334.021, 2339.19, 2319.374, 2331.003, 2330.41, 2297.405, 2327.706, 2308.737, 2326.894, 2297.21, 2310.9, 2306.902, 2293.292, 2307.108, 2290.737, 2264.302, 2292.638, 2281.728, 2262.631, 2274.858, 2275.134, 2265.23, 2273.46, 2258.776, 2267.091, 2247.254, 2237.423, 2256.433, 2266.133, 2246.888, 2259.726, 2231.53, 2267.804, 2238.457, 2239.645, 2257.695, 2297.461, 2239.849, 2232.317, 2229.567, 2236.006, 2243.59, 2234.965, 2221.879, 2229.503, 2245.869, 2232.73, 2234.409, 2222.572, 2228.01, 2220.025, 2208.896, 2216.185, 2231.106, 2207.956, 2223.01, 2220.371, 2205.914, 2227.794, 2218.763, 2210.952, 2206.88, 2209.66, 2220.645, 2211.229, 2216.959, 2219.007]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	3000	True	31101.73438		576588	11	-1	205.14579606056213	{'train_loss': [881185.688, 432397.094, 332875.75, 307353.719, 299010.062, 293969.656, 288706.5, 283090.625, 276463.375, 271218.719, 267841.406, 262936.969, 260920.234, 257313.859, 255835.719, 252362.812, 252059.797, 249742.828, 247930.922, 247343.844, 245101.359, 244260.719, 242856.312, 242411.969, 240959.016, 240111.109, 238524.578, 238020.672, 237368.688, 236637.234, 235640.734, 234573.234, 232902.531, 233165.688, 231885.094, 232170.766, 230633.859, 229660.609, 229314.875, 228948.719, 228489.172, 226367.812, 227491.234, 225147.922, 225297.312, 224407.453, 225370.688, 223788.438, 223126.578, 222435.641, 222528.625, 221549.609, 220307.156, 219886.734, 219978.891, 219275.562, 217969.578, 218169.328, 217776.656, 217031.984, 217074.969, 216328.266, 215979.406, 215432.453, 215565.438, 214843.156, 213721.016, 213283.703, 213393.172, 212776.094, 211489.281, 212545.875, 212172.422, 211872.438, 210699.062, 211017.109, 210088.594, 210318.625, 208881.469, 209517.281, 209363.766, 209712.938, 207433.438, 207863.297, 207587.906, 206885.547, 206880.344, 206897.562, 206970.531, 206171.516, 206258.422, 205577.219, 205463.719, 204132.453, 204202.797, 204270.094, 202825.656, 203808.094, 203937.625, 203146.531], 'val_loss': [5931.036, 3105.407, 2908.38, 2836.624, 2815.61, 2761.253, 2709.008, 2646.308, 2601.56, 2576.925, 2503.628, 2496.173, 2467.539, 2448.567, 2409.846, 2411.269, 2387.609, 2375.4, 2354.994, 2378.586, 2365.415, 2345.438, 2390.552, 2331.991, 2338.939, 2314.386, 2308.125, 2308.831, 2302.439, 2316.829, 2286.753, 2285.599, 2306.305, 2293.959, 2307.171, 2293.45, 2257.743, 2270.318, 2276.113, 2253.704, 2259.48, 2260.014, 2262.499, 2245.45, 2231.098, 2241.419, 2221.379, 2248.291, 2247.722, 2227.528, 2226.031, 2256.672, 2255.723, 2235.131, 2248.282, 2253.226, 2240.766, 2217.191, 2210.621, 2205.681, 2211.387, 2262.919, 2209.827, 2201.543, 2238.036, 2212.313, 2211.251, 2229.134, 2206.6, 2198.364, 2225.809, 2237.501, 2214.412, 2206.581, 2201.471, 2223.23, 2210.885, 2219.217, 2219.563, 2208.5, 2224.628, 2205.987, 2200.28, 2213.679, 2207.428, 2189.389, 2180.264, 2197.294, 2205.255, 2194.183, 2192.938, 2208.46, 2214.641, 2203.194, 2216.4, 2217.544, 2211.213, 2189.448, 2201.789, 2205.358]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	3000	True	31101.73438		576588	11	-1	205.13936138153076	{'train_loss': [881185.688, 432397.094, 332875.75, 307353.719, 299010.062, 293969.656, 288706.5, 283090.625, 276463.375, 271218.719, 267841.406, 262936.969, 260920.234, 257313.859, 255835.719, 252362.812, 252059.797, 249742.828, 247930.922, 247343.844, 245101.359, 244260.719, 242856.312, 242411.969, 240959.016, 240111.109, 238524.578, 238020.672, 237368.688, 236637.234, 235640.734, 234573.234, 232902.531, 233165.688, 231885.094, 232170.766, 230633.859, 229660.609, 229314.875, 228948.719, 228489.172, 226367.812, 227491.234, 225147.922, 225297.312, 224407.453, 225370.688, 223788.438, 223126.578, 222435.641, 222528.625, 221549.609, 220307.156, 219886.734, 219978.891, 219275.562, 217969.578, 218169.328, 217776.656, 217031.984, 217074.969, 216328.266, 215979.406, 215432.453, 215565.438, 214843.156, 213721.016, 213283.703, 213393.172, 212776.094, 211489.281, 212545.875, 212172.422, 211872.438, 210699.062, 211017.109, 210088.594, 210318.625, 208881.469, 209517.281, 209363.766, 209712.938, 207433.438, 207863.297, 207587.906, 206885.547, 206880.344, 206897.562, 206970.531, 206171.516, 206258.422, 205577.219, 205463.719, 204132.453, 204202.797, 204270.094, 202825.656, 203808.094, 203937.625, 203146.531], 'val_loss': [5931.036, 3105.407, 2908.38, 2836.624, 2815.61, 2761.253, 2709.008, 2646.308, 2601.56, 2576.925, 2503.628, 2496.173, 2467.539, 2448.567, 2409.846, 2411.269, 2387.609, 2375.4, 2354.994, 2378.586, 2365.415, 2345.438, 2390.552, 2331.991, 2338.939, 2314.386, 2308.125, 2308.831, 2302.439, 2316.829, 2286.753, 2285.599, 2306.305, 2293.959, 2307.171, 2293.45, 2257.743, 2270.318, 2276.113, 2253.704, 2259.48, 2260.014, 2262.499, 2245.45, 2231.098, 2241.419, 2221.379, 2248.291, 2247.722, 2227.528, 2226.031, 2256.672, 2255.723, 2235.131, 2248.282, 2253.226, 2240.766, 2217.191, 2210.621, 2205.681, 2211.387, 2262.919, 2209.827, 2201.543, 2238.036, 2212.313, 2211.251, 2229.134, 2206.6, 2198.364, 2225.809, 2237.501, 2214.412, 2206.581, 2201.471, 2223.23, 2210.885, 2219.217, 2219.563, 2208.5, 2224.628, 2205.987, 2200.28, 2213.679, 2207.428, 2189.389, 2180.264, 2197.294, 2205.255, 2194.183, 2192.938, 2208.46, 2214.641, 2203.194, 2216.4, 2217.544, 2211.213, 2189.448, 2201.789, 2205.358]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:37 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adam lr:0.24031418862111353 beta1:0.9038006274408903 beta2:0.9469931744690399 weight_decay:8.848177856776696e-05 batch_size:32 epochs:100	100	3000	True	137270.57812		365682	9	-1	205.6058337688446	{'train_loss': [1182940.75, 1066921.0, 1067292.375, 1204450.75, 1066921.0, 1066921.0, 1279244.875, 1221846.625, 1096438.5, 1267403.25, 1066921.0, 1115160.25, 1252516.625, 1089871.25, 1069967.5, 1074670.875, 1123851.75, 1113790.5, 1076865.125, 1069016.5, 1071831.5, 1083469.25, 1122524.375, 1092219.625, 1068193.5, 1070342.875, 1071805.75, 1103016.5, 1078067.125, 1075555.375, 1069311.25, 1071579.25, 1077521.375, 1119866.5, 1087936.375, 1079804.875, 1068658.5, 1080330.25, 1074742.0, 1099458.75, 1080051.25, 1077059.75, 1074772.375, 1078011.375, 1090202.625, 1088892.0, 1080893.0, 1083456.875, 1088663.875, 1080905.875, 1092638.375, 1092352.75, 1083992.75, 1077503.5, 1074692.75, 1077434.5, 1089512.75, 1091766.75, 1079290.875, 1093614.125, 1074668.25, 1076413.75, 1096202.0, 1102346.75, 1081596.75, 1093259.125, 1075949.875, 1076578.625, 1086377.125, 1084691.625, 1081290.25, 1088432.0, 1076717.0, 1082898.375, 1090303.25, 1098701.25, 1085500.0, 1091015.75, 1078888.125, 1082447.0, 1091081.375, 1095418.625, 1093805.375, 1087533.0, 1076899.25, 1082985.0, 1083727.375, 1090013.625, 1082898.5, 1090221.125, 1076671.25, 1080945.25, 1084722.0, 1094052.625, 1079731.75, 1090621.875, 1077993.625, 1081768.625, 1085552.875, 1090470.125], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10586.345, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 16340.206, 14292.107, 10587.216, 10587.216, 10587.216, 10587.216, 21536.268, 11062.779, 10587.216, 10587.216, 11392.252, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10586.82, 10587.17, 10587.216, 10587.216, 10587.216, 10587.216, 10874.456, 10583.157, 10629.486, 11127.725, 10587.216, 10587.216, 11103.087, 10587.216, 10562.88, 10587.216, 11856.894, 10587.216, 10587.216, 11060.043, 10587.216, 10587.216, 10566.884, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10873.907, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10586.433, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 11136.238, 10587.216, 10570.83, 11253.792, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10585.407, 10587.216, 10587.216, 10587.216, 10672.389, 10587.216, 10587.216, 10587.216]}	0	100	True
