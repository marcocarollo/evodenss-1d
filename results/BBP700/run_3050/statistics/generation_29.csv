id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	31433.24805		577492	11	-1	209.61612010002136	{'train_loss': [717024.562, 344697.781, 314423.0, 304575.156, 297767.438, 292801.844, 288262.844, 283639.156, 279180.375, 275369.75, 272918.438, 269480.688, 266602.531, 261853.797, 260969.531, 257920.016, 256877.219, 255514.234, 252057.344, 250557.0, 249509.969, 248375.453, 249919.312, 247699.797, 244935.25, 243268.297, 243796.141, 242278.203, 241733.641, 240540.219, 239635.234, 241534.562, 239071.391, 238089.0, 238145.359, 236484.359, 235833.547, 236283.609, 235805.281, 234822.719, 233768.547, 233208.438, 232934.453, 232790.453, 232581.516, 231430.094, 231542.859, 231392.266, 232441.844, 231290.719, 229781.156, 229521.469, 228885.781, 228690.391, 227575.703, 227065.594, 226516.438, 226616.703, 225767.031, 225429.641, 224250.438, 224081.531, 224370.188, 223525.844, 223678.547, 222656.25, 222191.031, 221632.281, 221946.969, 220890.516, 221394.609, 219337.453, 219692.875, 218406.953, 218750.844, 218162.641, 217626.312, 216769.859, 217619.0, 217513.016, 216926.047, 215405.703, 215963.391, 215736.781, 214415.391, 213613.391, 213473.844, 213182.984, 213046.297, 211237.516, 210664.938, 210438.047, 210463.406, 209983.453, 209706.016, 209958.031, 210429.734, 209533.641, 208359.75, 208449.578], 'val_loss': [3532.278, 2958.725, 2851.473, 2787.245, 2777.531, 2691.698, 2658.098, 2623.432, 2587.229, 2578.229, 2652.265, 2498.69, 2482.62, 2489.171, 2450.179, 2437.46, 2404.478, 2432.687, 2404.737, 2414.96, 2406.408, 2380.437, 2395.98, 2389.133, 2343.115, 2360.395, 2352.88, 2343.552, 2352.906, 2320.334, 2332.38, 2341.759, 2333.349, 2329.1, 2306.554, 2323.165, 2314.759, 2306.513, 2314.285, 2323.255, 2301.407, 2292.737, 2298.302, 2308.724, 2299.584, 2311.188, 2293.279, 2286.31, 2288.966, 2294.858, 2282.768, 2280.197, 2267.9, 2270.392, 2298.457, 2279.745, 2256.362, 2278.297, 2298.056, 2270.884, 2283.623, 2268.776, 2258.948, 2247.32, 2277.231, 2260.363, 2272.709, 2272.861, 2273.579, 2275.584, 2280.498, 2269.458, 2251.607, 2274.793, 2252.916, 2267.523, 2277.854, 2276.368, 2265.717, 2237.54, 2245.33, 2246.717, 2239.903, 2257.655, 2247.537, 2282.969, 2259.504, 2253.648, 2217.625, 2237.47, 2260.683, 2252.736, 2264.858, 2278.438, 2248.846, 2254.481, 2257.376, 2220.1, 2233.293, 2253.938]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:deconv1d out_channels:80 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:16 epochs:100	100	1000	True	137560.85938		51156692	12	-1	484.0756719112396	{'train_loss': [1648179.75, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1071680.0, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25], 'val_loss': [5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	30759.18945		577492	11	-1	213.25391483306885	{'train_loss': [698081.438, 368607.531, 319745.375, 304642.219, 295502.344, 287838.062, 280687.938, 275705.531, 271245.562, 266390.531, 263712.812, 260199.375, 257862.766, 255306.109, 252352.594, 250747.562, 249786.781, 248321.953, 247005.047, 245649.844, 243208.891, 243233.0, 242547.406, 240564.516, 240814.125, 239279.5, 238769.219, 238250.703, 236430.812, 236461.812, 235622.594, 235886.906, 234507.156, 233804.609, 232928.594, 232274.547, 232954.594, 231433.047, 231535.047, 230081.828, 231056.094, 229528.281, 229252.656, 229179.562, 228928.188, 228001.219, 228166.531, 227421.609, 226783.703, 226867.938, 226194.719, 227110.672, 224931.5, 225721.453, 224702.062, 223198.906, 224828.016, 224682.734, 223468.953, 223362.375, 224171.453, 224033.328, 222659.312, 221846.891, 222845.312, 221702.719, 222645.969, 220465.125, 221794.078, 220627.406, 220170.797, 220099.062, 221201.453, 219425.875, 219336.375, 219728.594, 218709.25, 219999.719, 220060.734, 218933.125, 218801.359, 218300.719, 219438.469, 217630.828, 217481.422, 217388.625, 217390.453, 217651.422, 216485.938, 216806.281, 216933.719, 216981.484, 216867.609, 216125.594, 216124.844, 215795.922, 215586.594, 215497.734, 215216.125, 215818.953], 'val_loss': [3805.891, 3038.921, 2874.958, 2814.939, 2765.245, 2674.7, 2638.453, 2617.009, 2566.994, 2534.813, 2507.949, 2500.718, 2449.891, 2426.447, 2408.34, 2406.552, 2408.471, 2406.977, 2373.92, 2353.452, 2372.188, 2336.786, 2339.97, 2321.241, 2338.142, 2323.187, 2343.609, 2309.438, 2320.309, 2309.027, 2325.402, 2312.842, 2311.534, 2292.361, 2282.245, 2264.931, 2267.424, 2256.288, 2276.021, 2279.168, 2272.582, 2245.365, 2249.516, 2248.727, 2254.234, 2260.712, 2245.172, 2249.511, 2255.885, 2253.701, 2248.421, 2259.195, 2255.223, 2236.576, 2250.759, 2246.729, 2227.354, 2227.287, 2216.924, 2220.663, 2289.635, 2212.396, 2237.849, 2240.902, 2225.442, 2214.537, 2210.906, 2232.005, 2215.276, 2217.866, 2216.613, 2210.707, 2212.7, 2227.628, 2207.469, 2200.496, 2196.494, 2263.511, 2187.571, 2184.255, 2197.266, 2191.878, 2205.11, 2194.941, 2213.9, 2212.41, 2190.654, 2188.713, 2179.605, 2193.503, 2190.112, 2269.442, 2178.944, 2184.633, 2182.139, 2168.581, 2197.787, 2168.617, 2183.644, 2164.603]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:87 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:87 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:82 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.10564136375030328 beta1:0.8625999972083672 beta2:0.8221377544826706 weight_decay:4.86870603794636e-05 batch_size:32 epochs:100	100	1000	True	137331.70312		13354011	11	-1	220.9169545173645	{'train_loss': [1905428.625, 3442353.5, 1701434.0, 1848971.0, 2098404.5, 1990013.375, 1759803.75, 2106302.0, 1934001.625, 2231533.5, 2194963.25, 2533094.0, 1963154.125, 1820414.0, 1748993.5, 1483247.875, 1396084.25, 1294210.125, 1501212.125, 1328546.5, 1223678.25, 1247199.875, 1129689.75, 1148534.625, 1290463.0, 1165911.0, 1245104.375, 1269833.375, 1163404.75, 1177234.0, 1095067.875, 1146158.375, 1093110.875, 1222620.625, 1159669.5, 1133012.0, 1140736.5, 1126549.25, 1166311.25, 1115174.75, 1117174.5, 1134799.0, 1114937.375, 1105312.0, 1141479.25, 1106069.75, 1159957.875, 1111541.375, 1149622.625, 1113868.125, 1135187.875, 1119581.25, 1107387.125, 1144901.75, 1109488.0, 1131948.375, 1113351.625, 1192914.25, 1124149.25, 1290669.625, 1102510.375, 1141397.375, 1139763.125, 1173945.25, 1112331.875, 1118034.625, 1087628.25, 1142677.125, 1101327.5, 1105365.25, 1111388.75, 1251551.875, 1122840.75, 1118586.5, 1144617.0, 1096590.375, 1150869.0, 1104035.875, 1178281.25, 1130704.875, 1111383.375, 1167161.25, 1107300.75, 1145493.5, 1083246.5, 1130443.75, 1103105.25, 1105581.125, 1107803.25, 1125594.625, 1103083.75, 1120963.125, 1129498.875, 1106608.75, 1160934.375, 1123189.875, 1127852.625, 1291400.0, 1163594.25, 1097230.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 40434.32, 10587.216, 10587.216, 19534.568, 14008.101, 569910.688, 11305.276, 10587.216, 39151.484, 20816.004, 10587.216, 13835.174, 12040.879, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10770.419, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
