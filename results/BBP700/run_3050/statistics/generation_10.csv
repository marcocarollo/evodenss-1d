id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	31588.66211		326197	11	-1	241.23636865615845	{'train_loss': [418246.594, 360753.969, 327924.812, 315905.375, 310136.594, 305915.594, 302841.812, 300229.406, 298007.594, 295080.406, 291101.156, 285477.562, 280410.812, 276415.469, 273702.688, 270810.125, 268910.656, 266281.031, 264553.062, 262278.562, 260742.234, 258384.516, 256963.844, 254760.5, 253855.875, 253481.375, 250167.391, 250441.391, 249633.953, 248822.875, 246956.75, 245709.203, 244902.609, 244644.078, 242968.625, 243072.797, 241655.172, 241580.984, 240123.391, 239816.828, 239601.125, 239903.703, 238721.297, 238462.594, 237508.234, 237276.875, 236766.672, 236070.062, 235506.609, 235081.922, 235177.969, 234813.562, 233412.594, 233790.219, 232632.625, 232242.828, 232456.078, 232157.438, 231900.797, 231774.141, 231565.75, 230234.984, 230299.844, 230428.609, 229165.859, 229197.797, 228820.906, 228446.75, 228167.781, 229302.531, 227801.5, 228385.031, 227330.969, 228042.031, 227112.641, 226408.375, 226585.172, 226896.844, 226063.609, 226124.047, 225830.891, 225793.438, 224821.047, 225174.203, 224521.109, 223831.594, 224533.969, 224291.828, 223485.594, 223695.516, 222886.812, 222824.203, 222503.516, 222614.5, 222038.25, 221803.031, 222516.234, 221696.422, 221873.5, 221929.906], 'val_loss': [3541.927, 3078.957, 2972.786, 2915.227, 2893.423, 2870.505, 2848.234, 2832.402, 2816.026, 2792.4, 2747.875, 2686.369, 2666.238, 2679.26, 2602.061, 2639.796, 2567.43, 2533.329, 2508.341, 2530.378, 2471.586, 2448.058, 2435.219, 2462.549, 2413.782, 2414.878, 2400.495, 2396.699, 2409.323, 2395.063, 2360.872, 2352.73, 2338.655, 2357.754, 2360.115, 2339.511, 2346.44, 2360.253, 2361.109, 2322.725, 2335.506, 2343.656, 2316.732, 2318.615, 2300.976, 2304.709, 2327.58, 2283.574, 2323.77, 2315.05, 2282.373, 2289.543, 2304.244, 2268.996, 2272.31, 2245.298, 2247.25, 2259.282, 2253.003, 2249.374, 2273.304, 2252.395, 2243.286, 2248.11, 2249.271, 2227.232, 2214.224, 2226.974, 2226.929, 2212.864, 2243.544, 2218.787, 2222.228, 2224.53, 2203.469, 2236.055, 2207.87, 2236.508, 2223.336, 2211.957, 2210.371, 2210.996, 2206.993, 2202.378, 2194.84, 2195.284, 2200.379, 2173.013, 2179.357, 2192.581, 2185.753, 2168.314, 2191.439, 2202.484, 2178.144, 2177.947, 2186.085, 2172.434, 2177.928, 2174.715]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:18 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	31520.01758		350017	12	-1	271.42905831336975	{'train_loss': [471882.0, 345411.969, 319714.625, 307091.094, 299093.219, 292175.406, 285525.562, 281469.312, 278141.344, 274415.719, 270495.0, 267987.188, 265497.281, 263206.156, 261363.078, 260514.094, 259038.859, 257102.812, 255440.703, 254787.672, 252979.891, 251845.562, 250562.453, 249825.5, 248055.891, 248134.297, 246949.938, 246719.938, 245241.594, 244756.047, 244153.359, 242623.781, 242543.984, 241184.594, 241154.578, 240140.625, 239748.359, 239208.797, 237988.672, 238380.625, 236543.047, 236587.953, 235771.969, 235409.219, 235171.859, 236004.016, 234010.266, 233458.5, 233535.516, 232858.719, 233485.344, 231312.078, 231242.672, 230773.531, 230602.703, 230798.281, 230580.359, 230069.422, 229673.391, 229615.688, 228465.344, 228195.688, 227687.75, 226782.969, 227697.297, 226988.594, 226565.812, 226312.547, 226215.406, 225526.641, 225960.219, 225184.031, 224990.422, 225647.922, 224375.516, 223661.516, 224249.469, 223189.906, 223704.859, 223556.5, 223344.828, 222715.609, 222428.562, 222407.344, 221622.484, 221790.859, 222562.875, 221842.953, 221061.719, 221084.047, 221072.156, 221564.594, 221003.109, 220888.328, 220462.594, 220189.938, 220010.141, 219227.516, 220093.953, 219485.891], 'val_loss': [3493.094, 3043.12, 2878.783, 2819.626, 2765.704, 2723.964, 2681.104, 2649.989, 2631.988, 2611.281, 2595.766, 2547.813, 2531.877, 2535.706, 2527.254, 2502.116, 2490.558, 2489.97, 2469.637, 2466.808, 2450.06, 2435.611, 2426.044, 2416.868, 2434.423, 2421.057, 2420.981, 2407.235, 2400.328, 2396.392, 2409.429, 2385.497, 2380.974, 2353.98, 2362.896, 2358.746, 2346.841, 2332.594, 2370.026, 2348.012, 2344.292, 2344.98, 2315.063, 2319.411, 2353.814, 2309.335, 2316.125, 2329.571, 2316.406, 2346.964, 2309.293, 2333.786, 2311.222, 2329.875, 2315.602, 2329.157, 2307.23, 2280.114, 2293.562, 2310.203, 2294.922, 2309.494, 2309.93, 2277.681, 2289.197, 2300.843, 2285.969, 2287.016, 2293.844, 2256.165, 2270.628, 2296.705, 2285.872, 2263.847, 2284.642, 2296.049, 2274.645, 2276.146, 2280.191, 2252.707, 2255.914, 2260.291, 2277.472, 2249.068, 2271.875, 2261.701, 2264.356, 2248.125, 2232.571, 2260.436, 2247.492, 2237.481, 2240.802, 2237.355, 2224.574, 2223.701, 2266.231, 2236.048, 2230.596, 2238.796]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:62 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:88 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adam lr:0.08438632253179976 beta1:0.8981162724433377 beta2:0.852598303508503 weight_decay:2.6310375882897816e-05 batch_size:32 epochs:100	100	1000	True	50200.76562		350075	12	-1	264.159898519516	{'train_loss': [419295.969, 375652.719, 370792.312, 370517.906, 371136.656, 368302.125, 367962.688, 370367.812, 365738.531, 366445.688, 367783.188, 363824.0, 365180.719, 366220.781, 367754.562, 370603.594, 367916.344, 366607.156, 372425.5, 374934.531, 366696.812, 365900.094, 365212.0, 365494.219, 366390.719, 368077.562, 366291.719, 368836.812, 367403.688, 371798.156, 368957.094, 366313.469, 367889.531, 370115.469, 368848.125, 365653.25, 365146.062, 368809.094, 366027.281, 369853.062, 368867.312, 366695.906, 371357.938, 369044.25, 368477.5, 369455.062, 370605.375, 368320.094, 367725.625, 370297.969, 367122.969, 365920.75, 368139.25, 366480.344, 365114.188, 372638.031, 368308.406, 371574.281, 368806.094, 365238.531, 369666.438, 368232.188, 370396.219, 366185.281, 369028.281, 365331.188, 367516.812, 374948.375, 366215.281, 368693.406, 367499.656, 367764.531, 365910.188, 368355.406, 364645.719, 367611.625, 367189.312, 365547.562, 365155.062, 366267.625, 365959.125, 370057.625, 364775.75, 367999.5, 368180.094, 367704.062, 367448.406, 366735.875, 363641.531, 365695.0, 367076.844, 365966.5, 370151.344, 365722.156, 367089.344, 365538.0, 366019.156, 373281.031, 370524.25, 368154.812], 'val_loss': [3675.83, 3770.47, 3504.425, 3413.433, 3580.382, 3447.05, 3433.687, 3476.975, 3530.854, 3603.268, 3508.735, 3418.967, 3391.937, 3412.707, 3481.862, 3623.376, 3419.949, 3489.078, 3532.247, 3424.969, 3478.624, 3606.975, 3498.79, 3409.419, 3426.524, 3415.246, 3691.886, 3626.993, 3596.091, 3549.431, 3451.997, 3416.716, 3432.688, 3646.774, 3479.988, 3590.089, 3500.373, 3421.811, 3599.531, 3513.91, 3549.842, 3476.29, 3654.686, 3423.101, 3407.201, 3520.249, 3416.812, 3415.226, 3426.248, 3388.378, 3423.334, 3502.945, 3701.814, 3532.325, 3432.009, 3555.761, 3566.45, 3443.867, 3803.546, 3438.68, 3490.096, 3478.568, 3539.662, 3518.455, 3673.465, 3467.917, 3515.777, 3638.09, 3652.247, 3412.963, 3402.389, 3405.479, 3540.436, 3406.876, 3407.374, 3447.865, 3505.909, 3490.622, 3515.103, 3421.343, 3441.758, 3443.831, 3698.893, 3495.763, 3625.535, 3430.09, 3428.415, 3385.856, 3434.046, 3513.626, 3409.182, 3427.37, 3405.33, 3399.488, 3427.112, 3388.197, 3506.11, 3490.145, 3418.948, 3570.208]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:60 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:16 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adam lr:0.17166097381369672 beta1:0.9540842338535763 beta2:0.8183640220433136 weight_decay:0.0004034078263915019 batch_size:32 epochs:100	100	1000	True	137809.21875		419474	12	-1	265.32768988609314	{'train_loss': [1141604.5, 1066921.0, 2749871.75, 2507042.5, 1095089.25, 3074691.25, 2889941.0, 1545083.75, 2905815.75, 1278859.75, 1128208.5, 1123746.125, 1153691.375, 1091515.0, 1096562.375, 1131872.125, 1117429.75, 1121502.125, 1113946.375, 1100724.75, 1144041.625, 1125349.625, 1179008.75, 1152502.75, 1101022.125, 1089276.0, 1155840.125, 1113403.75, 1108670.25, 1104541.875, 1096930.625, 1136480.375, 1114345.125, 1088183.5, 1118300.5, 1111113.625, 1221139.0, 1080275.25, 1214166.875, 1225027.125, 1087553.25, 1096645.625, 1109189.75, 1120976.125, 1169983.25, 1162312.625, 1218532.0, 1130838.5, 1220394.875, 2969988.75, 3527263.75, 1609617.625, 1175899.625, 1532752.625, 1448449.625, 1217473.75, 1235997.5, 1126445.375, 1129052.5, 1138790.75, 1197834.0, 1144371.0, 1152338.0, 1090409.75, 1090404.25, 1104194.625, 1076188.75, 1106889.75, 1095476.875, 1073785.875, 1087715.5, 1128796.375, 1090459.875, 1110799.625, 1082714.75, 1102068.125, 1103391.0, 1108513.0, 1138850.625, 1156194.75, 1096968.5, 1099970.625, 1085763.125, 1126354.625, 1119176.0, 1104585.875, 1073788.75, 1082235.125, 1081007.0, 1099485.25, 1085285.375, 1078651.625, 1071856.375, 1092299.5, 1096187.375, 1085294.125, 1126481.5, 1107787.0, 1079108.0, 1078492.625], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 48743.02, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 17710.012, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 13857.864, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
