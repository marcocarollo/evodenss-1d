id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	31324.29883		559871	11	-1	214.27354264259338	{'train_loss': [651330.125, 345318.656, 318655.0, 310614.875, 304515.625, 302556.844, 299087.906, 295221.625, 291272.188, 286328.75, 281372.031, 278936.312, 275386.781, 272324.688, 270238.406, 268319.75, 265778.375, 263886.094, 262195.719, 259900.562, 258707.125, 256366.031, 254863.922, 254061.656, 254107.172, 251228.344, 249705.188, 249312.344, 247385.969, 247006.156, 245240.594, 244162.609, 243363.328, 243030.422, 241940.266, 240339.953, 239868.219, 238908.953, 237610.094, 237027.188, 236128.453, 235376.734, 234497.828, 234120.344, 232916.547, 232582.203, 231898.984, 230614.656, 230438.281, 230270.297, 228959.125, 227913.828, 227345.188, 226764.078, 226986.969, 225948.734, 224845.625, 224744.312, 224224.344, 223189.469, 222213.641, 222516.844, 220874.234, 221497.641, 220692.312, 219407.812, 218770.781, 219109.469, 217795.406, 217765.297, 216409.094, 216778.594, 216251.766, 215637.266, 215215.578, 215027.891, 214969.156, 213898.391, 212569.266, 213055.141, 211581.266, 211539.0, 211928.859, 210927.109, 210514.297, 209867.547, 210371.422, 209504.359, 208417.859, 208274.375, 208197.719, 207139.453, 206732.75, 207430.672, 207506.562, 205640.188, 205786.984, 206153.547, 204963.469, 204286.641], 'val_loss': [3218.466, 3117.148, 2903.06, 2870.972, 2851.809, 2838.607, 2825.518, 2767.501, 2706.688, 2651.708, 2627.632, 2620.068, 2568.293, 2577.551, 2526.875, 2496.872, 2511.798, 2483.307, 2476.008, 2472.954, 2452.308, 2440.47, 2453.294, 2418.942, 2436.395, 2422.418, 2443.847, 2405.671, 2419.214, 2418.465, 2401.692, 2439.083, 2414.323, 2420.55, 2394.779, 2392.975, 2372.653, 2391.291, 2375.223, 2357.808, 2339.34, 2338.895, 2351.238, 2338.44, 2321.977, 2336.649, 2299.393, 2331.038, 2308.25, 2301.253, 2310.682, 2328.488, 2288.116, 2315.954, 2289.88, 2289.594, 2303.97, 2292.605, 2302.046, 2300.064, 2299.4, 2290.276, 2307.363, 2290.544, 2284.734, 2286.449, 2305.7, 2312.102, 2285.142, 2265.03, 2303.183, 2272.102, 2280.969, 2280.209, 2291.199, 2296.233, 2302.515, 2320.097, 2274.948, 2263.535, 2295.746, 2281.391, 2294.346, 2300.056, 2287.002, 2301.061, 2287.58, 2304.883, 2280.462, 2276.143, 2293.583, 2295.019, 2302.16, 2304.445, 2264.621, 2289.008, 2286.267, 2250.985, 2247.67, 2261.298]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	31264.35938		577492	11	-1	213.1995165348053	{'train_loss': [685472.312, 345642.062, 313149.594, 300921.375, 291316.188, 285285.25, 279064.062, 273126.094, 269416.281, 264362.438, 261542.766, 259421.875, 257616.812, 254736.469, 252862.312, 251861.391, 249317.422, 248208.125, 246640.375, 245460.0, 245485.734, 244054.438, 242441.828, 243132.203, 241429.266, 240811.844, 239288.703, 239190.672, 239159.219, 236632.719, 236806.031, 236451.391, 235651.391, 237407.297, 234697.141, 234054.188, 233697.859, 233201.688, 232416.984, 232284.484, 230937.234, 231544.125, 229830.328, 229364.094, 229559.969, 229442.438, 228449.875, 228501.328, 228534.469, 227793.109, 227074.859, 227046.078, 227478.094, 227117.734, 226131.703, 225795.844, 226390.703, 225523.375, 225356.359, 225835.656, 224113.703, 223687.938, 223910.938, 223026.859, 222774.281, 223165.828, 223291.375, 222815.906, 222799.219, 222943.984, 221886.922, 221582.406, 221797.203, 222171.859, 220880.906, 222024.641, 220478.062, 220025.422, 220267.125, 220226.375, 220028.484, 219881.656, 219440.078, 219600.438, 219438.906, 219209.859, 219210.844, 218442.078, 217646.297, 218572.969, 218714.438, 217733.078, 217672.453, 218054.438, 216555.172, 217301.703, 217102.953, 216538.266, 216819.797, 216745.281], 'val_loss': [3209.244, 2980.035, 2816.641, 2761.705, 2702.03, 2674.305, 2603.707, 2589.29, 2528.613, 2546.928, 2513.345, 2496.613, 2487.157, 2492.142, 2438.089, 2446.114, 2458.929, 2429.786, 2404.946, 2403.444, 2399.566, 2408.639, 2384.795, 2374.885, 2383.903, 2397.863, 2384.94, 2383.258, 2363.364, 2386.857, 2373.694, 2354.621, 2336.962, 2350.273, 2336.999, 2321.382, 2324.541, 2322.852, 2322.581, 2327.913, 2325.46, 2331.623, 2292.875, 2305.461, 2323.823, 2276.72, 2304.436, 2294.629, 2275.795, 2286.049, 2297.993, 2265.283, 2283.657, 2279.744, 2272.802, 2274.353, 2338.107, 2305.305, 2250.924, 2287.495, 2282.905, 2274.786, 2264.145, 2238.615, 2251.141, 2270.361, 2257.906, 2227.254, 2240.049, 2262.838, 2226.5, 2223.372, 2254.151, 2241.834, 2222.73, 2228.095, 2201.283, 2229.449, 2209.648, 2223.412, 2236.76, 2229.378, 2223.154, 2199.076, 2235.845, 2240.895, 2201.479, 2216.971, 2216.927, 2231.42, 2223.409, 2213.661, 2198.701, 2206.577, 2223.335, 2222.194, 2214.274, 2197.679, 2195.639, 2208.477]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adam lr:0.14127363485333666 beta1:0.8107197545997777 beta2:0.8782623107443435 weight_decay:9.223160382231347e-05 batch_size:32 epochs:100	100	1000	True	137344.82812		17995384	10	-1	233.57189059257507	{'train_loss': [6683347.5, 2852240.5, 8140117.5, 5380683.5, 2464103.25, 2381340.75, 1303159.125, 2742806.0, 3229581.25, 2927744.75, 5818001.5, 1320066.375, 3287108.0, 1178207.25, 2601512.5, 1388158.875, 1524687.375, 4533840.0, 1787148.25, 2810142.5, 2019758.25, 1283053.375, 1962798.5, 1274767.375, 1651679.375, 1586774.5, 1233909.375, 1839523.5, 1337803.875, 1512775.125, 1794952.875, 1404999.25, 1389909.125, 2268290.75, 1236465.5, 1389254.875, 1393706.625, 1999049.625, 1889757.0, 1285863.625, 1718037.375, 1369440.25, 1273959.375, 1360494.5, 1269105.75, 1261278.5, 1463891.875, 1284603.375, 1279186.75, 1602652.375, 1206373.125, 1330638.625, 1489198.5, 1424790.875, 1920601.625, 1204514.75, 1735907.5, 1650904.625, 1347789.25, 1554810.0, 1684655.125, 1548251.625, 1319615.375, 1245016.5, 1208496.125, 1220389.875, 1723605.0, 1314802.75, 1292266.5, 1339213.75, 1287841.625, 1557362.375, 1647551.875, 1171503.25, 1676420.0, 1320408.5, 1206205.25, 1383871.0, 1270050.75, 1242222.0, 1275018.875, 1316933.5, 1267869.625, 1331596.25, 1508308.75, 1344043.625, 1354036.5, 1405313.25, 1235602.875, 1230848.25, 1367219.125, 1351639.75, 1240986.625, 1254244.875, 1231008.75, 1663349.375, 1238485.5, 1277896.125, 1250596.875, 1205757.625], 'val_loss': [10587.216, 3768751.75, 10587.216, 121773.539, 10587.216, 16801.518, 10587.216, 10587.214, 43303.23, 3832102.0, 18457.477, 353595.031, 10587.216, 9675212.0, 811737.125, 10587.216, 27714.566, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 43906.199, 10587.216, 10587.216, 27141.658, 10587.216, 10587.216, 80558.758, 10587.216, 10587.216, 10587.216, 10587.216, 74404.641, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:87 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:8 epochs:100	100	1000	True	32437.04492		559871	11	-1	550.8878357410431	{'train_loss': [426083.156, 303816.562, 293530.5, 288262.0, 284120.75, 280625.875, 277449.688, 273759.469, 270476.688, 266716.688, 264522.094, 263292.906, 260190.016, 259084.453, 256375.922, 256256.719, 254440.812, 253689.297, 252672.906, 252425.734, 250649.953, 249739.531, 249195.344, 248996.188, 248034.406, 247164.953, 245671.109, 245197.047, 245963.219, 243472.125, 243002.516, 243118.297, 242505.75, 242657.453, 241867.812, 240733.547, 240549.625, 240356.844, 239103.344, 238637.297, 238574.578, 237652.453, 237634.25, 237288.797, 236931.734, 236420.344, 235988.266, 236016.359, 235208.297, 235579.141, 235052.719, 234086.297, 233655.359, 233192.703, 233615.078, 233043.219, 233012.953, 231131.891, 230960.703, 230918.5, 231149.156, 230594.469, 230002.828, 229338.453, 229568.172, 229694.344, 229519.922, 228903.703, 228450.562, 228420.984, 227668.641, 228095.859, 226601.578, 228130.203, 226309.062, 225007.375, 226175.266, 225345.969, 224785.688, 224661.422, 225173.531, 224202.094, 225494.938, 223722.641, 223908.469, 223239.297, 222857.688, 222902.156, 222137.594, 222716.516, 222833.828, 222237.531, 221484.016, 220932.859, 222002.375, 221423.109, 221432.203, 221022.172, 221254.391, 220517.0], 'val_loss': [732.276, 701.723, 691.909, 685.064, 679.876, 673.859, 668.817, 658.767, 649.826, 639.782, 637.526, 631.184, 629.019, 622.967, 622.363, 618.073, 616.257, 613.529, 603.504, 603.689, 605.137, 607.833, 598.41, 605.844, 595.322, 596.664, 594.601, 601.782, 594.213, 597.65, 604.551, 599.923, 601.481, 599.248, 590.817, 600.476, 597.015, 585.692, 602.643, 591.449, 593.818, 584.72, 588.569, 585.487, 580.335, 586.01, 584.049, 584.014, 586.201, 580.963, 580.933, 582.856, 585.444, 578.683, 581.384, 578.339, 582.955, 586.97, 573.082, 575.237, 577.036, 578.408, 576.883, 570.368, 576.173, 567.281, 569.822, 577.743, 569.529, 575.28, 570.954, 570.08, 571.458, 574.234, 565.566, 576.256, 567.94, 567.988, 572.666, 568.53, 569.211, 569.24, 565.442, 566.983, 570.578, 570.913, 574.61, 572.257, 566.213, 563.625, 563.355, 569.209, 561.779, 567.111, 564.211, 564.735, 564.868, 570.807, 566.544, 568.258]}	100	100	True
