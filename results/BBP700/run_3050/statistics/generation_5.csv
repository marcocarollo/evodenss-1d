id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	32125.32422		463085	13	-1	250.20356392860413	{'train_loss': [418869.562, 337412.438, 320791.688, 313166.938, 307045.031, 297371.844, 289277.156, 282493.188, 278172.0, 274899.75, 272581.344, 270082.094, 268504.844, 266493.656, 264547.812, 263593.188, 262256.125, 260288.0, 259712.25, 258802.625, 257585.156, 256658.062, 255813.594, 254813.188, 253640.906, 252160.297, 252501.562, 251462.938, 250815.281, 250480.172, 250264.594, 249917.422, 248692.094, 248617.547, 247831.141, 247196.391, 247132.219, 246127.438, 245331.547, 244910.938, 244568.016, 244476.719, 243937.859, 243141.062, 242908.656, 242757.328, 241528.484, 241104.594, 240362.453, 240154.859, 240175.281, 238966.484, 238830.062, 238678.703, 238368.781, 237885.875, 237407.594, 236843.203, 237215.609, 236285.141, 236143.609, 235352.547, 234925.891, 233893.438, 233959.312, 233748.594, 233582.719, 232939.953, 232459.078, 232313.125, 232022.891, 232173.781, 231315.891, 230791.938, 231372.797, 229850.609, 229805.5, 229561.172, 229476.797, 229250.188, 229455.625, 228772.172, 228751.641, 228355.125, 228018.891, 228357.328, 227783.391, 226607.922, 226699.938, 227111.188, 226506.875, 226616.047, 226048.734, 225603.875, 225217.0, 224324.172, 225099.0, 224984.922, 223405.859, 223530.969], 'val_loss': [3290.813, 3065.881, 2933.618, 2902.02, 2821.941, 2736.875, 2675.009, 2660.223, 2634.224, 2619.89, 2600.033, 2576.857, 2576.118, 2537.043, 2522.322, 2511.41, 2510.472, 2501.77, 2493.587, 2493.184, 2461.173, 2475.603, 2464.055, 2452.997, 2434.934, 2455.188, 2455.679, 2448.636, 2443.26, 2424.409, 2396.505, 2419.543, 2410.965, 2394.169, 2392.298, 2396.474, 2399.334, 2402.008, 2379.889, 2378.11, 2376.15, 2396.133, 2396.772, 2376.132, 2368.731, 2345.691, 2342.156, 2348.783, 2330.276, 2338.368, 2315.334, 2307.307, 2303.617, 2306.698, 2305.367, 2306.057, 2300.532, 2296.982, 2288.059, 2299.24, 2281.528, 2288.686, 2283.406, 2288.329, 2293.368, 2276.214, 2264.421, 2275.594, 2263.71, 2274.143, 2271.889, 2257.536, 2277.713, 2259.085, 2244.736, 2252.381, 2218.359, 2225.135, 2249.304, 2249.264, 2248.34, 2252.047, 2265.31, 2261.21, 2243.972, 2251.731, 2235.032, 2252.448, 2245.276, 2233.915, 2234.503, 2230.542, 2215.421, 2227.321, 2221.206, 2210.226, 2189.95, 2214.167, 2218.749, 2218.027]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:13 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:67 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:13 epochs:100	100	1000	True	32270.91406		408786	13	-1	424.4513282775879	{'train_loss': [420773.625, 323420.375, 309336.812, 304439.438, 301669.562, 299100.5, 296608.0, 294313.062, 292817.875, 291332.281, 290252.625, 286237.656, 279020.0, 274387.406, 270529.969, 266821.25, 264544.0, 261466.203, 260469.703, 258851.359, 256031.781, 255454.719, 254364.297, 252814.5, 252001.094, 250683.312, 250224.359, 249498.797, 249004.891, 247768.219, 246470.609, 246466.234, 245757.922, 243988.375, 243827.703, 242911.719, 242741.922, 243187.078, 241132.672, 240530.75, 239807.094, 239558.875, 238016.438, 238317.734, 236806.969, 236751.625, 236595.672, 235950.984, 235646.172, 236146.5, 235044.172, 234024.266, 234160.672, 232650.547, 233293.422, 232790.547, 231370.906, 232186.203, 231381.094, 230766.625, 230489.406, 230968.078, 229595.344, 229471.031, 230071.734, 229613.109, 228454.938, 229078.922, 228257.922, 227884.641, 228487.0, 227543.562, 226912.328, 226425.844, 226005.219, 226161.016, 225832.391, 225469.531, 226451.812, 225791.344, 225083.047, 224167.359, 224533.797, 223568.375, 223100.062, 223312.438, 222366.969, 223156.766, 223087.547, 221934.125, 222249.562, 222396.188, 221662.75, 221260.062, 221369.719, 220965.266, 221219.453, 220290.328, 220481.281, 220768.875], 'val_loss': [1250.719, 1165.846, 1153.305, 1144.437, 1137.584, 1130.522, 1123.718, 1124.238, 1114.599, 1113.075, 1115.214, 1075.771, 1065.729, 1069.455, 1050.331, 1037.5, 1036.172, 1019.332, 1022.352, 1009.32, 1015.982, 993.417, 988.45, 985.891, 988.565, 989.7, 977.865, 989.844, 970.84, 974.331, 969.289, 974.046, 973.861, 961.799, 964.765, 957.801, 972.057, 957.432, 968.273, 953.093, 973.916, 964.341, 947.604, 982.868, 968.468, 962.596, 943.025, 949.816, 927.664, 954.269, 941.221, 950.48, 931.535, 937.516, 928.625, 944.921, 926.153, 943.56, 930.799, 930.58, 936.527, 928.024, 925.353, 934.731, 930.086, 938.5, 926.935, 928.097, 926.039, 922.521, 932.058, 927.167, 933.56, 923.344, 923.942, 931.182, 939.341, 920.922, 930.031, 944.535, 933.867, 919.411, 916.378, 930.614, 918.531, 935.531, 918.538, 913.217, 910.789, 921.749, 942.631, 920.422, 921.783, 918.52, 920.7, 919.701, 927.113, 921.917, 925.766, 929.061]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:104 kernel_size:7 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta batch_size:32 epochs:100	100	1000	True	32081.875		463085	13	-1	248.77365732192993	{'train_loss': [408484.156, 330480.25, 315099.25, 304479.219, 296584.031, 290961.531, 286721.5, 283108.75, 279472.969, 276965.156, 274377.938, 272225.188, 270570.812, 268787.938, 267520.438, 265870.5, 264767.062, 263280.906, 262062.0, 261471.891, 260088.75, 259106.203, 257476.938, 256816.047, 256330.641, 255098.859, 253878.828, 253255.859, 252026.906, 251856.641, 250545.375, 249267.953, 248629.344, 248680.312, 248109.969, 248422.031, 246461.391, 245883.469, 245560.531, 245171.641, 243981.672, 244686.734, 243703.625, 243753.469, 243208.312, 243038.422, 242202.766, 241961.312, 241387.203, 240643.312, 240255.781, 239929.234, 238716.953, 238979.25, 238375.703, 238413.328, 237521.609, 238152.219, 237176.422, 237200.812, 236882.672, 235487.078, 236448.547, 235064.062, 235407.938, 234661.703, 234154.344, 234412.344, 234218.344, 233676.172, 233483.969, 232834.688, 232681.0, 232188.719, 231756.562, 231098.594, 231698.922, 231126.453, 229640.047, 230369.875, 229966.547, 229384.25, 229131.984, 229253.812, 228981.688, 228737.875, 228117.984, 228477.719, 227168.625, 226767.906, 227318.625, 226897.562, 227432.062, 226744.266, 226595.031, 226483.391, 225974.969, 225789.188, 226122.641, 225722.656], 'val_loss': [3216.087, 2942.851, 2852.283, 2862.86, 2815.051, 2797.835, 2774.12, 2807.432, 2699.542, 2654.043, 2647.704, 2620.734, 2607.783, 2587.561, 2568.797, 2590.605, 2559.168, 2562.127, 2531.048, 2524.794, 2524.537, 2516.916, 2503.049, 2496.033, 2496.645, 2495.127, 2479.257, 2452.84, 2451.904, 2451.569, 2455.841, 2455.293, 2441.42, 2446.055, 2453.259, 2420.782, 2446.981, 2422.891, 2426.705, 2399.815, 2414.79, 2413.169, 2404.893, 2382.402, 2396.546, 2410.532, 2410.947, 2360.102, 2379.807, 2335.94, 2340.41, 2354.35, 2357.413, 2344.836, 2337.595, 2338.398, 2348.044, 2329.278, 2343.874, 2332.46, 2344.637, 2334.635, 2320.185, 2309.682, 2305.459, 2315.218, 2313.591, 2307.012, 2301.67, 2313.642, 2298.394, 2282.193, 2295.209, 2309.45, 2296.427, 2276.837, 2276.028, 2285.39, 2287.939, 2289.628, 2289.622, 2279.336, 2284.376, 2285.252, 2287.725, 2258.052, 2286.649, 2267.304, 2252.44, 2263.829, 2248.489, 2253.969, 2261.844, 2242.367, 2262.742, 2232.485, 2248.205, 2250.712, 2240.388, 2240.458]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:67 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:13 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:41 kernel_size:7 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:85 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:11 epochs:100	100	1000	True	32141.94727		362888	14	-1	545.4051003456116	{'train_loss': [382508.344, 320798.438, 309720.594, 303851.031, 299995.719, 296639.844, 289721.938, 284277.312, 278381.625, 274175.594, 271820.406, 268777.25, 268673.375, 266507.719, 261942.781, 261012.781, 260143.938, 257185.297, 257711.234, 255823.766, 254179.219, 254097.312, 251477.344, 251452.031, 250509.875, 249233.344, 248613.141, 247475.156, 246477.625, 246229.281, 247973.281, 245617.375, 245026.281, 244895.078, 244951.469, 243009.969, 243117.562, 242584.344, 241951.359, 242015.516, 240891.906, 240971.547, 239401.875, 240070.875, 239189.375, 239257.188, 237574.641, 237122.203, 236966.203, 237166.344, 236708.188, 236854.172, 236761.406, 237094.891, 236069.141, 235570.812, 233956.688, 235428.359, 234955.531, 233399.516, 232789.828, 233070.891, 233340.625, 233019.734, 232796.062, 231906.266, 231658.781, 231257.469, 232281.359, 231275.734, 230840.219, 231294.719, 231667.859, 231410.359, 230090.625, 230610.828, 229954.328, 230005.953, 230391.406, 228547.875, 228044.797, 228865.828, 228273.312, 228139.594, 228443.844, 228183.094, 227587.422, 227720.438, 227249.047, 226713.5, 227274.938, 226637.656, 226705.719, 226041.703, 226630.125, 226779.047, 226741.812, 226563.375, 225189.062, 225279.562], 'val_loss': [1065.086, 1000.466, 981.431, 974.263, 969.4, 958.399, 937.96, 909.548, 908.3, 894.184, 880.932, 880.647, 887.107, 874.735, 863.386, 854.851, 856.355, 846.969, 852.206, 845.5, 836.007, 829.129, 820.965, 824.533, 822.584, 823.461, 815.053, 816.706, 820.321, 813.816, 804.057, 808.744, 798.899, 812.583, 799.633, 802.519, 804.471, 813.657, 799.951, 797.052, 804.462, 794.2, 801.368, 786.723, 785.861, 796.123, 805.98, 790.264, 781.521, 784.325, 785.017, 786.375, 780.596, 780.448, 784.11, 778.379, 781.447, 786.675, 786.769, 787.473, 781.668, 781.141, 781.478, 774.471, 779.538, 771.901, 766.687, 777.786, 766.062, 774.69, 776.651, 784.008, 777.451, 778.559, 769.572, 768.456, 782.136, 763.641, 772.597, 762.827, 767.98, 770.471, 772.391, 774.209, 772.452, 779.159, 775.214, 775.977, 767.81, 777.349, 767.799, 768.784, 775.836, 774.368, 770.056, 760.949, 776.268, 760.188, 766.939, 775.144]}	100	100	True
