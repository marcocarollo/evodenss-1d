id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:adadelta batch_size:32 epochs:100	100	1000	True	32315.5293		469896	10	-1	207.3518922328949	{'train_loss': [590200.0, 346687.625, 317298.219, 303318.469, 295585.062, 289538.062, 284489.344, 279526.375, 276454.438, 272927.125, 270097.406, 268755.875, 266300.594, 263844.594, 262950.688, 261302.531, 259080.094, 257985.953, 256858.875, 255493.875, 255195.188, 253762.828, 252659.203, 252097.688, 251071.203, 250406.469, 249735.938, 248903.141, 248731.234, 247854.625, 246561.516, 245870.203, 245924.469, 245132.484, 244663.312, 244342.547, 243547.844, 243176.625, 242808.625, 241964.469, 242393.25, 240912.781, 241414.594, 239883.219, 239532.0, 240634.609, 238933.234, 239289.141, 238783.172, 238043.484, 238004.766, 237108.797, 235747.297, 236946.875, 236812.953, 236462.031, 236210.5, 234431.156, 235093.75, 234435.344, 234538.469, 234380.219, 233202.703, 233605.766, 233067.312, 232771.156, 232516.953, 232763.25, 231896.234, 231567.344, 231283.703, 230728.203, 230839.797, 230225.078, 228784.547, 229736.797, 229613.625, 228767.562, 228181.406, 229009.719, 227916.641, 227718.031, 227095.0, 227261.422, 226644.719, 226686.938, 226613.109, 226617.375, 226418.328, 225101.188, 225698.688, 225083.734, 224501.281, 225217.609, 225531.938, 224154.266, 225249.391, 224215.562, 224055.516, 223634.25], 'val_loss': [3319.065, 3037.126, 2909.219, 2837.291, 2793.343, 2725.534, 2686.764, 2641.395, 2601.967, 2561.568, 2536.199, 2558.141, 2525.449, 2531.265, 2486.885, 2490.247, 2463.535, 2460.241, 2475.866, 2481.107, 2437.005, 2425.035, 2428.808, 2410.289, 2428.59, 2426.792, 2411.566, 2408.98, 2415.752, 2411.264, 2405.294, 2383.547, 2388.774, 2411.836, 2374.101, 2372.435, 2375.639, 2388.624, 2369.927, 2371.998, 2381.828, 2347.293, 2344.176, 2353.036, 2333.066, 2341.143, 2331.392, 2327.202, 2330.669, 2318.27, 2318.414, 2321.872, 2330.895, 2317.237, 2322.765, 2302.834, 2300.821, 2313.435, 2321.05, 2292.091, 2306.179, 2296.206, 2290.817, 2277.092, 2286.961, 2273.99, 2285.439, 2276.347, 2289.825, 2275.332, 2272.936, 2265.066, 2272.029, 2260.28, 2262.581, 2258.761, 2266.74, 2269.017, 2265.244, 2239.818, 2245.165, 2254.95, 2235.747, 2240.207, 2243.785, 2247.8, 2227.399, 2241.432, 2235.196, 2243.164, 2236.593, 2228.109, 2242.223, 2223.759, 2237.09, 2252.176, 2231.323, 2212.762, 2233.597, 2218.828]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:71 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:7 layer:fc act:selu out_features:200 bias:True input:8 learning:rmsprop lr:0.11373791513722888 alpha:0.9740452306504582 weight_decay:0.00022614920587110974 batch_size:32 epochs:100	100	1000	True	137268.92188		478408	10	-1	199.1861653327942	{'train_loss': [1173631.5, 1066921.0, 1119163.375, 1066921.0, 1410412.625, 1066921.0, 1070353.875, 1096992.375, 1066921.0, 1066948.0, 3164363.25, 1200689.875, 1068384.5, 1075887.75, 2599935.5, 1066921.0, 1066921.0, 1066921.0, 1657737.5, 1066921.0, 2434325.25, 1066921.0, 1066921.0, 1066921.0, 1641273.5, 1066921.0, 1078431.25, 1518549.5, 1066920.875, 1080883.875, 2421835.75, 1066921.0, 1082746.875, 1066921.0, 1616850.25, 1066921.0, 1086232.875, 3962495.0, 1177098.0, 1066921.0, 1066921.0, 2435464.25, 1074954.875, 1087683.75, 1068389.875, 2390342.75, 1067293.25, 1066921.0, 1066921.0, 1703617.75, 1066921.0, 1066921.0, 2185947.25, 1071080.0, 1073648.5, 1075328.0, 1066921.0, 1638747.25, 1066921.0, 1096721.25, 1075092.75, 1066921.0, 1130590.0, 1066921.0, 1629450.375, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1937230.375, 1066921.0, 1066921.0, 1066921.0, 2370372.75, 1066933.125, 1066957.625, 1068596.5, 1066927.375, 1562577.25, 1066921.0, 1251481.25, 1066921.0, 1066921.0, 1067109.5, 1066921.0, 1288054.25, 1068704.625, 1625200.125, 1066921.0, 1066932.375, 1066921.0, 2485285.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1093514.625, 1724445.75, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.214, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.209, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 15784.804, 10416.364, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.214, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.215, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.215, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:6 layer:fc act:selu out_features:200 bias:True input:7 learning:adam lr:0.11850562090607678 beta1:0.8336928442497346 beta2:0.961845153538911 weight_decay:8.144086508063139e-05 batch_size:32 epochs:100	100	1000	True	51432.91016		325838	9	-1	201.6632523536682	{'train_loss': [514135.406, 387088.094, 398204.812, 387472.062, 389289.844, 382717.031, 400864.281, 382011.844, 386946.781, 395019.219, 381593.938, 388368.5, 383817.906, 381279.406, 406653.469, 377504.125, 395580.625, 390309.5, 385034.312, 376794.156, 387715.375, 385388.75, 385590.031, 381718.781, 385246.438, 382351.469, 389918.938, 391828.812, 382929.844, 381548.656, 394271.438, 373922.0, 391697.719, 377351.75, 380982.531, 395253.0, 381324.562, 379538.062, 380674.469, 385672.625, 383311.656, 402301.281, 392055.312, 396300.406, 379936.469, 401019.656, 377849.406, 383573.781, 384850.062, 378787.188, 398646.375, 389479.438, 385351.844, 403221.438, 378102.406, 385688.906, 391078.25, 383024.5, 380848.625, 381406.188, 389219.812, 380997.219, 388618.812, 382370.375, 385246.0, 389794.312, 395469.094, 381096.125, 389090.25, 391565.812, 382570.031, 383030.5, 372544.344, 394552.094, 375607.812, 384473.094, 385175.531, 380566.719, 378705.656, 391121.5, 396697.25, 388279.344, 378624.125, 383876.375, 381189.312, 391410.719, 387289.906, 379703.844, 388960.531, 385602.969, 386788.344, 377489.125, 383610.031, 377431.938, 413657.562, 377603.688, 392998.031, 390071.75, 378398.156, 376357.375], 'val_loss': [3458.432, 3614.56, 3884.336, 3816.137, 5590.39, 3779.694, 3872.447, 3418.573, 3474.84, 3434.837, 3479.814, 3530.525, 3660.985, 3648.845, 3634.732, 4500.439, 3459.085, 5273.102, 3683.461, 3453.445, 4586.676, 3783.548, 3411.564, 3563.97, 3619.878, 3597.853, 3443.973, 4060.565, 3948.114, 3513.865, 3400.825, 3638.995, 3602.901, 3545.588, 3638.837, 3461.216, 3449.499, 3741.769, 3750.365, 3502.384, 3572.017, 3894.353, 5016.742, 3388.826, 3416.217, 3511.515, 3451.63, 4939.724, 3550.665, 3493.378, 3444.083, 3872.564, 4484.317, 3433.48, 3736.289, 3576.461, 7432.244, 3497.896, 3604.517, 3443.896, 3445.352, 4135.066, 3550.229, 3483.152, 3509.809, 4667.007, 3532.456, 3456.168, 3684.716, 3394.32, 3631.18, 3523.442, 3456.167, 3537.295, 3623.489, 3485.382, 3616.456, 3501.109, 3816.477, 3741.173, 3478.859, 3392.398, 3421.8, 3660.133, 3517.767, 3688.901, 3481.51, 4352.043, 3400.118, 3409.095, 3807.44, 3539.807, 3640.959, 3393.9, 3998.42, 3524.736, 4825.94, 3467.081, 3516.237, 3669.295]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:104 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:5 layer:deconv1d out_channels:72 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	32069.7207		399954	12	-1	212.03009462356567	{'train_loss': [578331.938, 364752.625, 325997.75, 313710.156, 309746.531, 307035.375, 304540.531, 302590.094, 301233.0, 299699.219, 298384.781, 297120.312, 295214.219, 292668.062, 288186.469, 285750.844, 283245.562, 280571.344, 277723.625, 274632.719, 272565.781, 271002.875, 269866.438, 267527.438, 266331.875, 265325.375, 263686.125, 261760.625, 261601.438, 259575.594, 258805.531, 257468.062, 256872.781, 256079.797, 253912.141, 253686.031, 252242.156, 251766.047, 250177.906, 250560.297, 250380.859, 248591.703, 248071.219, 247237.594, 246703.594, 245194.812, 245723.984, 243643.859, 243725.062, 243914.406, 242606.938, 241750.984, 240752.047, 240982.594, 241088.047, 239515.094, 238876.578, 238595.016, 239060.453, 237270.875, 237921.328, 236958.906, 236128.109, 236437.094, 235545.031, 235312.859, 235307.484, 234349.781, 233626.609, 234065.109, 232738.859, 232531.359, 232693.312, 231373.516, 232148.844, 230571.344, 230136.734, 230174.719, 230163.031, 229307.781, 229457.625, 228784.109, 229195.547, 229471.719, 228324.906, 227833.156, 227169.891, 227741.953, 227006.641, 227850.75, 227470.625, 226233.703, 226816.547, 225758.312, 225155.219, 224932.641, 225551.25, 224806.969, 224494.891, 224875.766], 'val_loss': [3783.773, 3064.343, 2928.347, 2904.447, 2887.596, 2873.008, 2862.833, 2849.064, 2835.939, 2828.561, 2813.751, 2803.198, 2802.748, 2748.803, 2731.735, 2701.588, 2684.407, 2677.844, 2627.907, 2601.398, 2597.262, 2588.827, 2577.101, 2563.835, 2543.604, 2546.038, 2524.382, 2527.976, 2505.56, 2497.188, 2475.017, 2496.604, 2474.099, 2455.917, 2475.706, 2463.897, 2441.433, 2445.2, 2428.962, 2437.32, 2440.631, 2417.02, 2398.553, 2438.255, 2417.15, 2404.769, 2369.251, 2383.898, 2382.333, 2379.236, 2349.307, 2360.919, 2374.32, 2363.138, 2365.825, 2338.089, 2301.726, 2333.552, 2330.27, 2349.981, 2333.063, 2373.592, 2342.435, 2313.947, 2334.743, 2321.703, 2312.198, 2303.889, 2284.295, 2328.485, 2291.521, 2297.919, 2289.819, 2301.993, 2279.944, 2297.374, 2263.707, 2258.068, 2251.854, 2265.028, 2260.723, 2276.737, 2257.826, 2265.964, 2245.474, 2248.981, 2234.107, 2242.71, 2289.271, 2224.868, 2240.877, 2253.292, 2236.142, 2287.097, 2255.647, 2224.295, 2200.245, 2239.588, 2224.658, 2246.152]}	100	100	True
