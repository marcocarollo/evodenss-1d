id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:108 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:32 epochs:100	100	1000	True	32060.33398		363673	12	-1	235.65053129196167	{'train_loss': [423007.844, 374764.281, 329997.5, 316882.562, 310117.188, 304770.625, 297238.75, 291106.75, 285767.594, 280334.312, 276549.594, 272581.156, 268870.25, 266615.25, 264211.469, 262121.172, 258625.484, 257707.344, 255763.828, 254279.922, 252782.719, 251352.469, 251036.656, 250085.828, 247667.531, 247832.422, 247173.828, 245671.719, 244925.719, 244540.938, 244224.828, 242548.656, 242518.859, 241037.281, 241119.688, 240841.188, 239485.891, 239638.172, 238409.344, 238881.219, 237461.094, 237310.859, 237009.219, 236322.094, 234827.328, 235348.406, 234215.469, 234655.422, 233970.844, 233549.812, 233417.672, 233318.141, 232323.484, 232121.453, 231322.469, 231131.953, 230665.391, 230341.172, 230432.203, 229532.578, 229442.391, 227787.516, 228606.375, 228570.516, 228730.703, 227605.062, 227845.766, 227084.469, 227855.531, 226694.562, 226527.016, 226102.094, 224771.641, 225672.141, 225549.578, 224897.031, 224925.672, 225250.453, 224871.234, 224582.078, 224066.219, 224551.984, 223872.594, 223997.047, 223940.812, 222769.219, 221100.312, 222415.797, 222570.109, 221917.203, 221630.391, 221966.984, 221344.984, 221243.203, 220716.234, 220667.781, 220741.906, 221289.062, 220243.547, 219776.453], 'val_loss': [4012.106, 3116.66, 2985.427, 2913.974, 2882.423, 2832.999, 2779.32, 2736.982, 2682.06, 2648.92, 2620.597, 2578.707, 2568.036, 2575.272, 2524.329, 2526.141, 2491.062, 2451.873, 2462.791, 2444.076, 2415.332, 2432.999, 2422.025, 2392.076, 2373.684, 2371.834, 2391.643, 2370.475, 2369.815, 2356.031, 2354.078, 2351.166, 2345.785, 2359.029, 2343.435, 2335.691, 2340.326, 2342.041, 2332.44, 2321.102, 2310.268, 2318.501, 2312.078, 2285.902, 2302.256, 2307.409, 2309.875, 2317.504, 2313.879, 2291.881, 2325.478, 2270.098, 2277.878, 2293.426, 2287.039, 2299.959, 2274.415, 2283.114, 2275.366, 2292.498, 2291.293, 2262.414, 2275.268, 2279.818, 2269.626, 2283.65, 2282.07, 2261.911, 2270.109, 2269.246, 2278.229, 2264.427, 2241.995, 2276.548, 2269.345, 2294.022, 2265.551, 2257.052, 2275.858, 2260.313, 2243.87, 2251.059, 2254.945, 2288.559, 2252.06, 2254.984, 2258.313, 2244.305, 2234.505, 2258.009, 2264.341, 2240.63, 2264.984, 2263.991, 2242.344, 2261.946, 2248.328, 2233.51, 2222.374, 2240.847]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:126 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:108 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta batch_size:4 epochs:100	89	1000	True	33523.76172		391366	12	-1	1004.6370418071747	{'train_loss': [360161.875, 315249.094, 308696.562, 302520.125, 296136.781, 290203.844, 285798.469, 282336.25, 279176.625, 276854.969, 274597.562, 272872.094, 271594.625, 269914.656, 268062.438, 267185.0, 266819.469, 265586.219, 264268.344, 263259.438, 262874.062, 261270.156, 261168.078, 259645.562, 258611.266, 257980.094, 257052.594, 256430.109, 255922.0, 254666.906, 253836.828, 253451.828, 252846.422, 252713.891, 251595.25, 251483.234, 250197.734, 249967.344, 250269.766, 249623.125, 248821.016, 247634.844, 247809.203, 247246.219, 246297.984, 245595.031, 245868.016, 245609.75, 245175.984, 244092.266, 243701.438, 243886.094, 243231.188, 244329.469, 243702.797, 243226.188, 242932.734, 241927.672, 241567.531, 241072.141, 240105.188, 241019.578, 239779.297, 239353.953, 239633.922, 239124.656, 238586.656, 238833.281, 237869.172, 237182.938, 238489.141, 237128.938, 237117.078, 236989.203, 237276.266, 237198.906, 236338.953, 236661.922, 236590.312, 235640.312, 235706.547, 235852.031, 235715.312, 234867.578, 234418.75, 235056.375, 234068.406, 234595.453, 233966.141], 'val_loss': [375.336, 365.175, 359.008, 351.239, 346.197, 341.077, 337.536, 333.111, 331.812, 328.832, 330.663, 328.606, 326.429, 322.728, 322.565, 321.635, 321.755, 321.473, 318.764, 318.855, 318.702, 317.652, 318.055, 317.351, 319.685, 315.207, 316.05, 315.242, 313.178, 314.956, 311.416, 310.274, 311.937, 313.382, 311.167, 312.11, 315.068, 311.896, 309.214, 308.237, 308.316, 311.154, 308.073, 307.986, 305.012, 304.052, 304.594, 305.873, 304.368, 305.431, 306.146, 307.599, 305.753, 304.052, 305.878, 304.669, 302.257, 308.131, 304.925, 302.02, 302.43, 300.886, 301.793, 301.888, 303.219, 300.565, 301.963, 298.483, 303.361, 297.903, 302.157, 305.453, 301.714, 303.508, 297.407, 299.861, 297.56, 297.288, 297.296, 299.844, 297.884, 298.74, 301.716, 297.162, 298.432, 297.466, 302.131, 294.701, 299.75]}	89	89	False
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:75 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:59 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	31606.85547		326197	11	-1	220.4483015537262	{'train_loss': [407984.656, 346221.0, 323312.188, 315066.625, 310884.938, 307618.094, 303108.844, 298603.906, 294862.875, 291020.625, 287030.812, 282764.344, 279490.062, 275738.281, 272748.438, 270929.281, 268446.781, 266558.562, 264672.375, 263138.844, 261480.422, 259845.344, 258334.953, 256548.406, 256573.984, 255988.578, 254450.812, 252924.406, 253033.078, 251697.812, 250092.672, 250370.516, 249546.391, 248736.812, 248598.438, 247329.078, 245707.891, 246853.422, 245454.844, 244746.641, 243544.797, 243766.672, 242576.5, 241855.422, 242151.328, 240816.391, 240308.141, 239643.641, 239703.219, 238402.562, 238148.906, 237610.344, 237346.188, 236557.641, 237578.859, 235847.406, 236266.734, 235090.375, 234054.453, 233914.625, 234145.859, 233178.078, 233510.844, 231882.609, 232604.562, 231602.953, 232647.531, 231202.828, 230539.734, 231016.922, 229609.828, 230955.891, 229792.891, 229044.312, 228994.719, 228926.656, 228884.578, 228439.438, 227929.922, 228345.25, 227223.859, 226563.469, 227017.719, 228002.109, 226850.0, 226079.516, 226639.344, 225846.938, 226130.641, 225684.469, 225928.391, 225263.672, 226132.172, 225467.297, 224713.594, 225218.656, 224777.891, 224744.516, 224345.766, 224093.828], 'val_loss': [3226.703, 2997.916, 2929.239, 2912.897, 2888.076, 2861.739, 2804.364, 2774.605, 2752.876, 2726.839, 2704.723, 2679.962, 2645.432, 2624.376, 2627.34, 2622.889, 2582.677, 2573.301, 2576.908, 2561.213, 2553.092, 2532.376, 2504.683, 2529.103, 2492.869, 2501.812, 2517.448, 2484.474, 2467.216, 2455.934, 2491.085, 2449.081, 2461.415, 2419.357, 2472.792, 2439.803, 2417.968, 2456.311, 2420.42, 2438.295, 2458.925, 2416.887, 2416.474, 2408.412, 2391.712, 2396.077, 2379.371, 2376.522, 2386.429, 2392.162, 2371.661, 2395.258, 2360.44, 2360.377, 2370.647, 2353.282, 2374.979, 2362.606, 2349.682, 2350.025, 2337.551, 2320.908, 2326.931, 2325.669, 2323.828, 2325.065, 2313.271, 2319.408, 2326.141, 2306.195, 2336.28, 2304.57, 2292.678, 2297.502, 2293.297, 2297.518, 2314.775, 2320.106, 2309.863, 2282.828, 2311.215, 2276.249, 2291.627, 2295.026, 2272.125, 2277.725, 2272.967, 2275.866, 2288.31, 2300.209, 2276.997, 2274.309, 2253.212, 2250.654, 2267.242, 2266.121, 2278.533, 2246.602, 2271.586, 2261.122]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:88 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:92 kernel_size:10 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:71 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:127 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:94 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.20317604014806984 alpha:0.9938276649112628 weight_decay:5.911723833121577e-05 batch_size:32 epochs:100	100	1000	True	137278.10938		485137	12	-1	223.1437373161316	{'train_loss': [1686489.125, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1067095.625, 1069920.0, 2203719.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066920.25, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 2002988.0, 1066921.0, 1066921.0, 4731348.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1069078.875, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 3685573.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 2033194.0, 1066921.0, 1066921.0, 1066934.875, 1067051.0, 1066921.0, 1066921.0, 1084526.5, 1066921.0, 1107498.125, 4631833.5, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0], 'val_loss': [10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 462869.125, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 27624.514, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216]}	100	100	True
