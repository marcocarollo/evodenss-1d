id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	31184.98438		577492	11	-1	210.1051857471466	{'train_loss': [752407.375, 359958.656, 317415.594, 304325.938, 297860.438, 290788.469, 283934.719, 278288.438, 274130.344, 270199.562, 267273.469, 264458.781, 262358.844, 259490.594, 257937.641, 256900.172, 254863.0, 253557.766, 258209.109, 251893.375, 247817.438, 249586.578, 247182.609, 245374.078, 244784.828, 244171.219, 242928.672, 242680.141, 240326.75, 241182.984, 239852.359, 238710.391, 238495.234, 240791.125, 237132.266, 237161.703, 236338.375, 235850.094, 235378.641, 234608.219, 233905.531, 234431.812, 233255.922, 233232.312, 231898.453, 231067.75, 232221.203, 231421.062, 230086.562, 229103.156, 229639.891, 230304.031, 227008.422, 227638.578, 227037.469, 228415.141, 226302.875, 227012.203, 226134.391, 226169.734, 226594.969, 225740.5, 226112.578, 225014.25, 224621.969, 224417.281, 223860.938, 223776.875, 223225.047, 222931.609, 223009.484, 223330.281, 222145.016, 222162.75, 222300.578, 222600.875, 222391.625, 221115.516, 220828.25, 221018.172, 220536.469, 220912.75, 220272.453, 220876.422, 219531.125, 219207.594, 218351.047, 219357.344, 219188.0, 217878.875, 217929.156, 218631.516, 217886.594, 218749.797, 217189.719, 218357.562, 217104.891, 216453.469, 216665.578, 216840.203], 'val_loss': [3373.039, 2974.353, 2870.927, 2842.156, 2757.673, 2689.004, 2648.349, 2617.904, 2654.698, 2542.382, 2531.183, 2535.565, 2474.972, 2465.467, 2459.151, 2432.407, 2461.981, 2444.064, 2440.01, 2429.886, 2421.949, 2442.517, 2409.291, 2400.863, 2386.929, 2390.705, 2380.82, 2366.616, 2324.488, 2338.643, 2315.909, 2327.969, 2285.481, 2296.664, 2306.12, 2292.864, 2293.041, 2298.809, 2300.659, 2289.712, 2281.927, 2273.054, 2267.765, 2267.475, 2268.435, 2270.375, 2251.766, 2266.403, 2249.68, 2271.431, 2321.152, 2254.386, 2247.676, 2264.402, 2238.479, 2253.281, 2250.023, 2251.349, 2248.917, 2238.591, 2366.259, 2252.63, 2234.875, 2241.352, 2221.246, 2230.68, 2233.001, 2229.101, 2227.059, 2220.119, 2238.342, 2243.621, 2237.379, 2229.506, 2214.553, 2236.159, 2234.76, 2247.33, 2250.097, 2241.997, 2252.781, 2241.48, 2225.198, 2220.891, 2193.105, 2221.452, 2220.851, 2229.99, 2223.11, 2211.956, 2224.762, 2230.062, 2222.907, 2204.881, 2240.285, 2213.486, 2201.258, 2199.744, 2210.665, 2187.199]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.24410634817019183 beta1:0.9174312251776535 beta2:0.8892468052054235 weight_decay:2.6505377472991928e-05 batch_size:32 epochs:100	100	1000	True	142686.46875		577492	11	-1	216.94792294502258	{'train_loss': [1105204.75, 1141592.75, 1116048.125, 1073284.75, 1088597.875, 1073236.625, 1075167.75, 1096437.75, 1069394.125, 1106625.0, 1119800.5, 1071384.625, 1094744.125, 1166264.375, 1072908.75, 1072628.125, 1156303.0, 1090644.125, 1077604.5, 1131922.5, 1106110.625, 1072325.875, 1102765.125, 1129855.0, 1072239.375, 1139364.875, 1188262.125, 1095568.25, 1138416.625, 1085892.75, 1090085.75, 1091837.875, 1106587.875, 1093662.0, 1099781.125, 1099773.75, 1095158.375, 1084146.125, 1094208.75, 1113532.125, 1090454.0, 1104063.75, 1110268.75, 1077945.625, 1102453.625, 1139760.875, 1100946.25, 1085800.125, 1087692.5, 1116841.625, 1083644.25, 1085631.0, 1085531.0, 1096771.5, 1082599.25, 1109161.5, 1143968.875, 1102845.375, 1092391.25, 1119907.875, 1085430.75, 1073460.0, 1094408.5, 1099365.625, 1113029.75, 1093439.625, 1089833.0, 1089375.25, 1076776.125, 1081693.5, 1126208.625, 1094427.875, 1077607.125, 1093143.25, 1105560.25, 1093585.125, 1090411.25, 1082253.25, 1100903.0, 1082281.875, 1101802.25, 1085075.125, 1088259.125, 1122323.0, 1111923.0, 1087790.125, 1079071.0, 1091955.375, 1089493.75, 1093949.75, 1075906.625, 1080353.25, 1080018.625, 1104126.5, 1087150.75, 1074499.25, 1122081.0, 1087813.5, 1077230.375, 1157544.0], 'val_loss': [10587.216, 37301.688, 10587.216, 10587.216, 10587.216, 10587.216, 10586.083, 10587.216, 10587.216, 11372.049, 10587.216, 11465.253, 11752.505, 11759.728, 10587.216, 11511.994, 14209.984, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 12736.781, 10587.216, 10587.216, 10693.906, 10587.216, 14626.041, 10587.216, 10587.216, 10587.216, 10673.382, 10948.366, 10587.216, 10563.002, 10587.216, 10587.216, 10587.216, 10584.303, 10587.216, 10587.216, 11309.21, 10582.951, 10587.216, 10586.435, 10689.326, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10828.116, 10954.647, 10587.216, 10587.216, 11111.956, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 10587.216, 11294.385, 10587.216, 10587.216, 10587.216, 13726.351, 10587.211, 10587.216, 11160.667, 10587.214, 10587.216, 10587.216, 11143.162, 10587.216, 10587.216, 10587.216, 10587.216, 11496.779, 11945.562, 11155.232, 10587.216, 10587.216, 11240.557, 11362.969, 10587.216, 10587.216, 10587.213, 10587.216, 11923.928, 10587.216, 10587.216, 11389.495, 10587.216, 10587.216, 11038.059]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:77 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	1000	True	32871.28125		593025	11	-1	205.30534291267395	{'train_loss': [737516.0, 366566.312, 322186.0, 309328.938, 301709.594, 296082.031, 289609.062, 285809.562, 279375.969, 275366.219, 272148.312, 269179.375, 266875.594, 265106.125, 264022.688, 262747.969, 261693.656, 260217.266, 258570.391, 257020.422, 256551.469, 254855.781, 255040.969, 254205.844, 252613.953, 252239.812, 251085.422, 251376.641, 249334.734, 249957.594, 249755.922, 247846.141, 248405.797, 247412.125, 246254.641, 245818.969, 244209.375, 244329.016, 243652.797, 243070.047, 242135.359, 242372.719, 241121.844, 241024.172, 240798.656, 239863.094, 239953.75, 240506.5, 239076.656, 238345.734, 237937.484, 236470.719, 236891.156, 236252.984, 235935.312, 235341.641, 235244.141, 234917.953, 234613.203, 234671.5, 234236.141, 234204.969, 232609.062, 232421.75, 232359.609, 232381.531, 231692.172, 231465.469, 230794.484, 229873.016, 230140.672, 230007.484, 229037.594, 228928.453, 228777.844, 228120.578, 228204.188, 227724.719, 227252.656, 226412.938, 226697.219, 225829.609, 226208.484, 226410.172, 224772.984, 225221.156, 224434.172, 224499.0, 224562.328, 223373.672, 223363.75, 222990.891, 223260.281, 222945.5, 222120.016, 221893.891, 222214.094, 221994.016, 221639.016, 221032.875], 'val_loss': [3423.066, 3056.163, 2897.646, 2836.751, 2807.8, 2762.324, 2698.988, 2653.17, 2615.413, 2602.891, 2579.089, 2574.106, 2548.231, 2541.701, 2556.632, 2523.486, 2502.156, 2497.751, 2511.722, 2492.241, 2478.72, 2466.112, 2473.857, 2447.416, 2449.59, 2428.992, 2433.827, 2441.508, 2444.536, 2425.698, 2397.414, 2399.972, 2400.562, 2396.701, 2377.818, 2391.5, 2386.283, 2371.578, 2360.771, 2364.379, 2396.19, 2366.676, 2376.567, 2384.637, 2362.765, 2363.546, 2381.362, 2363.017, 2345.497, 2361.479, 2367.39, 2345.55, 2349.032, 2330.659, 2324.141, 2317.669, 2328.465, 2332.066, 2327.35, 2339.48, 2306.845, 2312.099, 2329.494, 2295.684, 2298.196, 2293.664, 2279.202, 2295.299, 2286.383, 2311.063, 2282.774, 2276.871, 2272.498, 2287.306, 2280.02, 2259.953, 2274.797, 2253.591, 2284.457, 2261.511, 2258.361, 2280.466, 2284.963, 2273.663, 2245.979, 2256.899, 2245.267, 2264.824, 2215.906, 2236.457, 2244.317, 2219.641, 2245.003, 2246.067, 2250.279, 2238.329, 2239.497, 2264.696, 2256.005, 2239.386]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:71 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:34 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:94 kernel_size:5 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:28 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta batch_size:32 epochs:100	100	2000	True	31792.93945		577492	11	-1	209.44689011573792	{'train_loss': [744986.188, 362972.344, 315434.312, 301664.719, 292377.312, 285911.875, 279915.562, 275086.75, 270615.75, 267051.031, 265756.469, 261428.656, 258462.562, 256649.734, 255454.766, 254087.828, 251538.234, 250799.828, 249909.453, 248205.656, 248031.172, 245852.531, 245476.906, 245313.25, 243060.375, 242247.641, 242251.359, 242228.891, 239935.453, 239753.781, 239121.031, 238768.969, 236790.031, 236609.938, 235845.719, 235680.469, 235733.25, 235337.797, 234225.344, 232621.25, 232807.891, 232517.719, 232372.047, 232112.5, 230935.734, 231221.344, 230501.438, 230464.703, 229258.234, 229669.688, 229175.766, 229041.328, 228211.094, 228912.641, 227456.219, 227147.828, 226876.812, 226766.375, 226014.844, 226114.719, 226785.984, 225215.328, 226872.469, 224844.125, 225188.141, 225930.062, 224361.047, 224134.609, 225457.578, 223667.906, 224635.312, 223333.266, 223174.359, 222617.172, 222504.812, 221977.75, 221130.172, 221692.578, 222177.688, 221242.656, 221580.5, 220777.766, 220271.859, 220994.297, 219908.734, 220286.547, 219423.672, 219752.891, 219319.594, 219720.938, 218709.734, 218765.188, 219198.047, 217659.891, 217883.469, 217520.828, 218089.844, 216597.141, 217177.375, 216597.016], 'val_loss': [3489.862, 2954.914, 2867.456, 2802.352, 2756.98, 2709.446, 2664.022, 2644.301, 2624.709, 2617.249, 2543.034, 2504.877, 2545.46, 2486.247, 2465.963, 2420.603, 2415.892, 2391.348, 2359.788, 2368.128, 2358.652, 2330.831, 2337.698, 2324.727, 2321.858, 2320.875, 2314.845, 2318.251, 2302.01, 2304.295, 2288.586, 2299.548, 2286.539, 2282.497, 2288.112, 2278.908, 2276.892, 2263.483, 2262.764, 2271.515, 2265.838, 2270.856, 2250.904, 2249.448, 2251.302, 2254.227, 2265.138, 2278.689, 2245.043, 2236.514, 2235.446, 2237.227, 2229.341, 2255.012, 2257.234, 2238.139, 2233.193, 2228.442, 2227.2, 2221.95, 2237.736, 2234.857, 2236.465, 2251.774, 2249.104, 2215.545, 2233.543, 2236.511, 2227.367, 2214.839, 2206.809, 2214.86, 2212.521, 2224.06, 2197.632, 2203.001, 2210.985, 2200.802, 2217.063, 2225.454, 2197.207, 2218.829, 2225.645, 2222.417, 2215.24, 2190.316, 2202.745, 2210.212, 2214.11, 2202.181, 2198.825, 2186.619, 2208.11, 2190.196, 2198.927, 2189.251, 2198.967, 2194.655, 2202.789, 2183.013]}	0	100	True
