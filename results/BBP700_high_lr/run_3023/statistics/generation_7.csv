id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.21492880952165166 batch_size:27 epochs:100	100	1000	True	31051.40234		617205	14	-1	281.65278244018555	{'train_loss': [456436.656, 387603.781, 347806.625, 332375.375, 324081.531, 319275.094, 315308.312, 313141.094, 310212.125, 306916.781, 303852.281, 299170.25, 294114.719, 288862.531, 283912.688, 279081.688, 275095.344, 272525.594, 268387.656, 264679.812, 262584.781, 259627.594, 257994.344, 255133.188, 252141.266, 250496.734, 248543.297, 246978.172, 244679.375, 243041.141, 242245.109, 240350.062, 239725.547, 238857.281, 237477.219, 237182.609, 235491.516, 234654.859, 234210.188, 233523.359, 232250.016, 232307.344, 231135.641, 230905.75, 229336.031, 228612.656, 228220.219, 227863.562, 227303.094, 227101.188, 226227.562, 225884.406, 225511.141, 224471.828, 223757.375, 223787.562, 224376.078, 222921.0, 222145.328, 222076.938, 221743.859, 221686.094, 219976.188, 219881.891, 219402.938, 218517.016, 218139.984, 218882.703, 217521.25, 217749.484, 217519.656, 216857.406, 216608.531, 215879.578, 215757.875, 214663.078, 215021.859, 214361.406, 214184.969, 214856.156, 213045.703, 213325.031, 212551.891, 213075.094, 212603.594, 212712.438, 211391.156, 211099.125, 211704.391, 210293.719, 210876.859, 210598.641, 209787.141, 209107.188, 209344.188, 209411.281, 208811.625, 208966.047, 208293.391, 207460.641], 'val_loss': [3072.868, 2595.086, 2477.377, 2412.137, 2378.402, 2372.846, 2357.888, 2321.434, 2300.406, 2301.678, 2264.49, 2244.127, 2212.049, 2173.076, 2141.188, 2116.627, 2093.587, 2056.134, 2024.755, 2001.988, 1986.503, 1960.021, 1976.9, 1923.439, 1921.974, 1909.567, 1897.884, 1863.229, 1883.11, 1850.495, 1847.632, 1844.876, 1843.328, 1831.919, 1829.297, 1824.812, 1814.988, 1842.724, 1805.536, 1807.823, 1802.545, 1802.023, 1786.679, 1792.778, 1797.669, 1793.635, 1797.12, 1799.982, 1804.304, 1779.012, 1808.805, 1779.004, 1784.281, 1784.372, 1797.497, 1793.71, 1766.955, 1782.686, 1781.419, 1786.947, 1774.967, 1775.627, 1777.677, 1787.597, 1781.622, 1779.909, 1767.629, 1764.698, 1767.145, 1772.323, 1779.557, 1779.576, 1760.638, 1776.085, 1774.632, 1758.767, 1756.575, 1753.857, 1770.466, 1757.0, 1748.557, 1763.129, 1745.274, 1762.18, 1740.838, 1741.373, 1772.955, 1762.083, 1751.594, 1744.087, 1738.077, 1753.24, 1737.415, 1737.18, 1729.926, 1743.22, 1724.663, 1747.247, 1720.377, 1734.432]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:92 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.15158472503345372 batch_size:27 epochs:100	100	1000	True	31707.29883		689191	14	-1	282.51130294799805	{'train_loss': [413800.531, 340816.562, 321397.719, 310762.594, 303514.094, 297148.438, 291882.844, 287560.062, 284154.156, 280870.25, 277639.312, 274857.406, 272458.969, 270676.969, 268675.844, 265277.844, 264162.5, 262736.562, 260717.812, 259429.719, 257748.438, 256339.609, 255638.703, 254166.984, 252612.578, 252097.766, 251199.109, 249766.625, 249676.344, 247808.812, 246728.531, 246315.797, 246081.266, 245245.406, 244714.984, 243560.578, 243475.188, 242222.109, 241502.547, 240141.969, 240530.875, 240568.422, 238852.969, 239066.766, 238493.219, 237517.969, 236979.25, 236415.406, 235926.438, 236140.141, 234252.672, 234764.016, 233732.984, 233154.953, 232563.906, 232722.484, 232513.016, 231345.5, 230712.703, 231256.578, 230207.203, 230127.453, 229795.5, 229404.438, 229189.547, 228875.406, 227416.453, 226952.188, 227409.328, 227474.547, 227190.859, 226292.0, 226579.453, 225899.953, 225361.344, 225671.438, 225540.547, 224349.078, 224193.109, 224825.422, 223386.25, 223245.719, 224092.125, 222749.438, 222067.125, 222570.047, 221844.562, 221421.406, 221423.047, 220489.359, 220824.188, 219966.594, 219878.203, 219767.469, 219195.812, 219229.625, 218824.078, 218180.844, 219060.547, 217817.219], 'val_loss': [2731.735, 2457.917, 2359.927, 2343.71, 2333.81, 2266.824, 2222.001, 2209.365, 2195.705, 2180.284, 2136.827, 2131.727, 2114.261, 2094.489, 2086.092, 2060.249, 2051.162, 2041.243, 2034.909, 2020.382, 1992.914, 1997.967, 1990.068, 1974.039, 1979.584, 1963.957, 1960.191, 1965.994, 1941.071, 1940.604, 1946.83, 1937.882, 1926.773, 1922.371, 1915.219, 1927.96, 1916.223, 1903.847, 1896.761, 1894.831, 1909.864, 1886.572, 1899.86, 1888.274, 1879.686, 1882.154, 1884.591, 1895.151, 1862.143, 1885.982, 1862.553, 1863.039, 1871.005, 1869.277, 1849.878, 1855.347, 1851.46, 1858.453, 1861.38, 1850.964, 1871.15, 1840.267, 1856.308, 1854.228, 1850.744, 1832.356, 1857.735, 1838.865, 1846.518, 1845.201, 1854.818, 1847.524, 1847.642, 1839.335, 1840.265, 1833.893, 1850.099, 1850.371, 1843.792, 1835.937, 1830.161, 1838.546, 1849.034, 1834.944, 1836.521, 1822.138, 1824.196, 1842.57, 1818.377, 1828.885, 1835.135, 1835.665, 1837.073, 1826.981, 1847.159, 1843.957, 1833.644, 1828.936, 1838.313, 1837.005]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:deconv1d out_channels:113 kernel_size:7 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:73 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:32 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.21492880952165166 beta1:0.9459481330070071 beta2:0.9759177372913367 weight_decay:2.480462216706894e-05 batch_size:27 epochs:100	100	1000	True	138868.60938		84462512	15	-1	466.0011565685272	{'train_loss': [1403692.625, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 12762134.0, 21269970.0, 1066921.0, 1066921.0, 1066921.0, 1066921.0, 1790046.875, 3993835.25, 11976156.0, 6220025.5, 9618589.0, 1067036.875, 1066921.0, 1066921.0, 1721898.75, 3659706.5, 3893612.75, 9147267.0, 10391024.0, 21241748.0, 1352147.625, 1066921.0, 1066926.375, 1066918.25, 5654438.5, 2129161.0, 13572072.0, 11036241.0, 5611051.0, 6323204.5, 1082788.75, 1120979.125, 1066947.375, 4627870.0, 2237580.25, 7401477.5, 5146981.5, 12329668.0, 1436657.875, 10972610.0, 1071044.375, 2526407.75, 1540213.75, 3790781.5, 5475325.5, 8505698.0, 6406862.0, 3777073.0, 4003640.25, 1138008.875, 10387602.0, 1066934.5, 4577803.5, 4951887.5, 4162365.0, 5332774.0, 6472215.0, 7984604.0, 5023589.0, 1066993.25, 1106843.75, 2042202.0, 3831134.25, 3740982.25, 7181710.0, 5507820.5, 2834583.0, 7673525.0, 4349226.0, 3856165.0, 1192305.875, 2264968.5, 1846441.125, 5044345.0, 5931346.5, 5883129.5, 2662347.5, 6204775.5, 4452601.5, 4514393.0, 2558535.75, 1435864.625, 1067051.125, 1654380.625, 4074631.0, 3653811.5, 3422914.0, 5517935.5, 3394977.5, 3983183.75, 3870110.75, 3372895.0, 3818814.75, 2207019.25], 'val_loss': [8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 712982.25, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8465.835, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 98247.117, 8469.773, 85790.602, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 16976.137, 645226.688, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.21492880952165166 batch_size:27 epochs:100	100	2000	True	31531.74805		617205	14	-1	295.7454454898834	{'train_loss': [447691.406, 355226.875, 331683.312, 317218.5, 306469.906, 297991.656, 290971.312, 284338.094, 279267.031, 275908.344, 271870.969, 270042.844, 267570.75, 265224.281, 262785.75, 260780.562, 259382.344, 257582.734, 255770.516, 253739.234, 253067.719, 251169.547, 250916.406, 249363.391, 248403.453, 247437.312, 246014.328, 245865.594, 244863.703, 243544.531, 242884.594, 242362.969, 241260.938, 240513.281, 239310.891, 239334.203, 238041.266, 237936.453, 237114.953, 236267.469, 235511.922, 235011.359, 233638.938, 233605.406, 232904.953, 232487.203, 231269.484, 231122.875, 230951.453, 229921.688, 229219.484, 228731.625, 229250.734, 228326.688, 228098.891, 226957.562, 226377.75, 226829.688, 225575.609, 224984.859, 225391.375, 225087.688, 224107.672, 223780.719, 223721.281, 222371.0, 222814.438, 222464.891, 221290.25, 221924.844, 220939.078, 220424.797, 220285.984, 219329.391, 220281.219, 218480.828, 218611.938, 217918.0, 218353.688, 217634.812, 217017.109, 217182.281, 216853.266, 215925.641, 215445.328, 215479.703, 214393.266, 214855.016, 214560.766, 214362.609, 213736.344, 213881.422, 214085.281, 212815.625, 213321.234, 212835.766, 211788.188, 212870.375, 212161.75, 211620.141], 'val_loss': [2746.406, 2561.773, 2416.77, 2306.402, 2248.08, 2196.81, 2140.584, 2115.612, 2091.436, 2090.132, 2104.403, 2052.813, 2032.961, 2021.216, 2007.062, 2001.768, 1997.647, 1960.292, 1949.562, 1943.988, 1949.668, 1925.201, 1900.412, 1937.212, 1910.135, 1903.661, 1896.338, 1894.49, 1887.86, 1899.758, 1893.788, 1882.587, 1875.728, 1877.688, 1866.669, 1876.437, 1862.606, 1856.442, 1872.519, 1854.524, 1873.43, 1861.443, 1855.652, 1856.341, 1846.633, 1838.977, 1841.358, 1845.462, 1832.376, 1841.714, 1829.986, 1833.405, 1824.688, 1808.251, 1818.19, 1829.245, 1826.679, 1836.715, 1832.131, 1808.271, 1793.354, 1813.488, 1795.841, 1811.519, 1824.685, 1809.381, 1810.093, 1810.44, 1809.891, 1823.057, 1808.446, 1809.931, 1802.005, 1808.411, 1798.906, 1801.167, 1811.333, 1796.316, 1807.545, 1823.91, 1798.136, 1797.784, 1811.841, 1802.515, 1805.342, 1794.953, 1776.095, 1787.839, 1785.981, 1794.511, 1788.226, 1789.141, 1790.189, 1797.104, 1795.274, 1797.868, 1780.398, 1780.901, 1804.259, 1773.123]}	0	100	True
