id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:79 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.1864603712075097 batch_size:20 epochs:100	100	1000	True	33423.27344		486464	15	-1	355.24278235435486	{'train_loss': [380664.875, 334317.156, 320697.406, 310282.625, 303128.688, 298392.5, 295505.625, 292492.312, 290052.125, 287449.094, 286058.688, 284353.938, 282008.406, 280539.719, 278711.75, 277474.125, 276036.875, 274746.031, 274013.688, 273355.344, 271516.906, 270471.469, 269268.031, 268919.594, 268396.625, 267221.344, 266045.0, 266276.469, 264808.844, 264051.688, 262694.125, 262543.938, 262047.938, 261193.203, 260403.141, 260465.578, 259836.516, 258260.344, 258389.0, 257248.547, 257499.703, 256260.641, 256448.219, 255561.453, 255447.078, 254753.125, 253706.078, 253185.047, 252989.281, 251927.5, 252693.812, 251946.156, 251497.688, 250973.0, 250473.047, 249912.938, 249230.891, 249271.797, 248929.359, 248952.812, 247795.547, 248131.453, 247994.25, 247338.734, 246172.141, 246516.219, 246398.906, 245655.0, 245361.328, 245352.094, 245789.859, 243894.969, 243844.484, 243711.344, 244242.75, 243916.016, 243186.844, 242162.328, 242332.312, 241606.625, 241683.609, 242140.812, 241319.906, 241587.844, 240844.188, 240360.781, 240343.203, 240049.234, 239840.016, 239450.172, 238843.844, 238935.953, 239658.922, 238391.047, 238013.266, 237930.219, 237052.078, 237595.547, 236706.609, 237273.906], 'val_loss': [2067.275, 1915.225, 1846.395, 1798.471, 1786.922, 1772.087, 1757.971, 1744.005, 1746.207, 1741.391, 1721.105, 1721.387, 1709.852, 1699.764, 1688.969, 1675.818, 1679.375, 1662.584, 1661.51, 1653.254, 1637.625, 1641.004, 1642.779, 1625.792, 1613.928, 1614.498, 1609.01, 1596.358, 1595.097, 1592.569, 1585.861, 1586.536, 1581.607, 1572.469, 1572.102, 1566.154, 1570.479, 1567.068, 1548.332, 1557.471, 1555.041, 1543.604, 1549.964, 1564.211, 1544.972, 1547.401, 1534.222, 1541.965, 1538.304, 1535.68, 1547.806, 1536.984, 1545.295, 1537.867, 1540.043, 1528.67, 1533.581, 1528.568, 1530.484, 1523.896, 1521.263, 1512.824, 1514.613, 1509.726, 1522.395, 1520.36, 1513.591, 1530.495, 1521.211, 1506.186, 1504.41, 1497.22, 1507.444, 1510.685, 1527.503, 1503.385, 1521.598, 1523.299, 1501.309, 1505.912, 1493.625, 1511.307, 1510.523, 1514.426, 1500.329, 1510.488, 1503.113, 1504.251, 1482.306, 1491.987, 1515.844, 1491.67, 1497.881, 1486.463, 1486.804, 1506.243, 1495.754, 1497.8, 1479.456, 1482.839]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:79 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1864603712075097 batch_size:5 epochs:100	100	1000	True	32947.86719		523368	14	-1	1011.1138780117035	{'train_loss': [349462.188, 316554.562, 306702.625, 299870.562, 293956.0, 289261.594, 285601.625, 281729.969, 278017.469, 275164.0, 272971.375, 270773.719, 269968.938, 267635.0, 266470.969, 264857.844, 264107.094, 262933.875, 262234.094, 260402.359, 260113.969, 259165.484, 257980.375, 257764.281, 256839.875, 256133.938, 254401.812, 254004.047, 253669.406, 252839.094, 252601.047, 250912.875, 250825.125, 249807.562, 249873.0, 248376.688, 246760.016, 247226.109, 246659.297, 246368.469, 245968.75, 244694.078, 244801.906, 243545.016, 243026.688, 243362.5, 242174.062, 242705.281, 241438.062, 241114.875, 240046.656, 239571.047, 240432.234, 238689.297, 239683.562, 238543.828, 238083.109, 237876.938, 237509.844, 237539.859, 237802.281, 236816.609, 236580.891, 236271.812, 235708.0, 235475.984, 234680.547, 234690.359, 233938.812, 233758.859, 233981.781, 233319.344, 232604.609, 232200.797, 232857.844, 231496.781, 231443.922, 230920.484, 231863.266, 231252.969, 230420.562, 230629.797, 229406.25, 229522.172, 229008.312, 229009.078, 229125.422, 228047.328, 228396.125, 227942.875, 228035.484, 227556.844, 227575.328, 226970.297, 227851.062, 227255.172, 225941.484, 227200.828, 225750.703, 225779.828], 'val_loss': [465.178, 451.188, 441.426, 436.194, 429.291, 424.216, 419.846, 416.681, 410.291, 408.61, 407.557, 409.176, 401.577, 401.009, 398.932, 403.02, 398.552, 397.075, 395.503, 393.902, 392.536, 392.372, 393.064, 393.888, 392.719, 389.531, 391.957, 390.508, 388.918, 386.644, 386.827, 388.106, 387.952, 384.641, 384.448, 390.779, 386.147, 385.61, 387.724, 380.448, 382.522, 387.122, 382.887, 387.386, 383.274, 380.769, 380.755, 379.03, 384.34, 381.623, 379.668, 375.049, 374.695, 380.099, 375.859, 377.813, 380.205, 375.22, 376.939, 376.083, 372.853, 374.058, 374.029, 375.607, 379.766, 373.02, 373.816, 374.991, 376.799, 375.827, 377.78, 372.276, 374.222, 375.461, 372.736, 377.352, 373.953, 376.213, 372.747, 371.939, 371.012, 370.964, 368.303, 369.611, 370.885, 371.102, 369.01, 370.006, 369.866, 368.85, 367.463, 365.81, 369.234, 364.598, 366.273, 366.933, 365.571, 362.95, 367.695, 367.804]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:111 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:79 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.1864603712075097 beta1:0.9698561105555524 beta2:0.9219234655863195 weight_decay:1.2154434870170628e-05 batch_size:20 epochs:100	100	1000	True	141738.92188		5674354	14	-1	378.1823425292969	{'train_loss': [1099714.625, 1067569.625, 1573763.625, 1068888.0, 1669335.75, 5102437.0, 1637185.75, 1436905.25, 2968258.25, 2541186.75, 1681200.0, 1629672.0, 1742394.0, 2123604.5, 1807744.0, 1735393.25, 1637823.5, 1957894.875, 1600297.125, 1438706.875, 1560448.5, 1325689.0, 1760665.75, 1665196.625, 2178696.5, 1272347.625, 1214750.375, 1181242.125, 1169899.625, 1358069.375, 1704961.625, 1817926.75, 1487616.625, 1548399.75, 1155652.375, 1247208.25, 1289156.875, 1263987.0, 1255131.125, 1271840.5, 1167659.5, 1205789.375, 1187302.25, 1540460.75, 1631228.125, 1357055.75, 1154371.125, 1187827.125, 1184335.125, 1222417.625, 1210437.125, 1471921.75, 1673783.875, 1175268.0, 1169944.0, 1486502.375, 1542487.75, 1165837.625, 1127373.375, 1382714.5, 1695943.375, 1253975.0, 1184194.125, 1235069.75, 1350933.375, 1287047.875, 1194682.25, 1227271.375, 1245195.875, 1160443.625, 1311973.0, 1481392.75, 1368430.75, 1281194.125, 1498363.25, 1520442.25, 1372376.75, 1517581.5, 1269144.625, 1864426.25, 1399783.25, 1276443.0, 1362456.375, 1477173.5, 1494439.125, 1317733.375, 1435576.875, 1516519.875, 1880710.625, 1435879.875, 1333057.125, 1400252.125, 1574714.5, 1340100.25, 1450078.875, 1657861.625, 1758312.875, 1413565.875, 1323347.5, 1372354.5], 'val_loss': [6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 45002.199, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664, 6686.664]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.1864603712075097 batch_size:19 epochs:100	100	1000	True	32611.91406		429236	16	-1	388.0361182689667	{'train_loss': [373025.406, 328997.688, 319093.875, 314199.688, 312094.375, 309851.5, 308033.594, 306478.812, 304920.625, 302980.219, 301543.281, 299811.219, 298117.344, 296083.906, 293888.438, 292087.938, 289382.562, 287638.312, 285738.344, 283910.781, 282763.594, 281452.625, 279726.375, 277579.594, 276381.125, 275022.562, 273382.75, 272369.531, 271375.594, 270272.5, 268940.25, 268069.156, 265395.844, 264305.969, 264473.344, 262362.562, 261699.281, 260925.562, 259684.125, 257602.172, 257817.828, 257272.234, 256890.203, 256201.016, 255983.297, 255428.234, 254312.891, 253788.578, 253727.5, 253650.5, 252359.875, 252132.594, 251837.578, 251226.281, 250667.297, 250196.0, 249307.625, 249403.688, 248438.969, 248415.547, 247562.078, 247958.734, 247444.734, 247589.203, 245869.875, 246470.141, 246091.891, 245736.984, 244961.125, 243756.359, 244495.547, 243608.172, 243266.266, 242558.938, 242421.422, 242194.828, 242292.047, 240771.688, 241930.672, 241108.578, 241063.344, 239861.141, 240085.109, 239739.547, 240587.906, 239379.328, 239685.547, 239644.188, 238132.469, 238540.641, 237758.547, 238906.078, 238525.594, 237579.781, 237446.578, 236403.297, 236785.953, 236367.484, 235597.188, 235178.953], 'val_loss': [1842.236, 1787.141, 1754.277, 1739.251, 1733.201, 1728.848, 1724.736, 1720.599, 1713.935, 1709.215, 1699.76, 1693.282, 1689.46, 1677.117, 1662.929, 1660.094, 1634.566, 1628.429, 1624.473, 1616.984, 1610.219, 1606.276, 1594.207, 1598.446, 1585.828, 1577.076, 1569.035, 1557.341, 1553.713, 1539.109, 1532.261, 1519.367, 1516.584, 1510.62, 1507.706, 1502.943, 1499.683, 1496.15, 1489.669, 1487.548, 1482.882, 1473.931, 1475.713, 1464.27, 1461.226, 1461.991, 1462.528, 1447.703, 1455.638, 1447.054, 1455.684, 1440.802, 1438.093, 1424.161, 1443.916, 1424.236, 1430.538, 1424.772, 1427.651, 1427.687, 1403.459, 1425.323, 1396.543, 1400.578, 1395.922, 1398.52, 1394.334, 1389.947, 1389.428, 1393.871, 1392.7, 1393.354, 1372.687, 1372.271, 1381.413, 1401.998, 1362.145, 1365.516, 1373.318, 1366.08, 1369.914, 1366.079, 1367.359, 1370.68, 1371.96, 1354.218, 1362.78, 1359.957, 1355.97, 1361.653, 1352.148, 1360.591, 1356.005, 1352.019, 1349.106, 1355.443, 1342.97, 1365.366, 1350.385, 1344.736]}	100	100	True
