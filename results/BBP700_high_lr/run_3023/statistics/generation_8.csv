id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.21492880952165166 batch_size:27 epochs:100	100	2000	True	31620.88477		617205	14	-1	270.48582100868225	{'train_loss': [449245.562, 341756.781, 318813.531, 306685.5, 298310.125, 292672.188, 288242.188, 284639.438, 281473.875, 278175.906, 276066.656, 273265.75, 271610.688, 268833.656, 266421.719, 265034.062, 263129.625, 261368.734, 259441.562, 257512.781, 255693.938, 254587.688, 252376.172, 251589.672, 250468.078, 249428.844, 248439.781, 246941.219, 246175.609, 245026.578, 244610.094, 243966.969, 242937.844, 242014.375, 241448.031, 240093.219, 240105.906, 238405.359, 238025.969, 237776.594, 237990.781, 236783.391, 235961.312, 235243.219, 235199.922, 233979.531, 234588.391, 233857.953, 232509.281, 233180.25, 232283.188, 231625.312, 231194.812, 230812.094, 229644.047, 229787.531, 229992.297, 229472.781, 228518.297, 228741.016, 228488.797, 229071.484, 226871.844, 227423.109, 226911.172, 225643.172, 226141.422, 225801.453, 226336.344, 225007.969, 224211.203, 225015.594, 224718.828, 224289.562, 222899.547, 222873.75, 223525.906, 223253.234, 222546.203, 221392.188, 221670.312, 221747.141, 221191.781, 220990.938, 220607.469, 220781.531, 220047.781, 220377.234, 219928.219, 219434.297, 219437.953, 219187.656, 218575.641, 218049.328, 218622.859, 218277.297, 217523.875, 217274.812, 216990.031, 216385.375], 'val_loss': [2803.362, 2415.55, 2321.668, 2278.125, 2241.047, 2239.69, 2214.706, 2171.471, 2157.603, 2136.844, 2127.234, 2107.099, 2106.005, 2074.132, 2078.006, 2059.492, 2044.245, 2033.95, 2034.844, 2023.277, 2020.585, 1995.193, 1992.606, 1992.439, 1983.2, 1969.572, 1983.564, 1967.998, 1948.051, 1943.644, 1943.449, 1943.984, 1922.919, 1929.255, 1938.061, 1911.362, 1912.667, 1907.576, 1912.432, 1913.168, 1885.508, 1903.837, 1913.925, 1888.523, 1890.879, 1876.118, 1885.258, 1865.616, 1867.257, 1862.286, 1857.67, 1866.677, 1845.719, 1861.556, 1848.217, 1859.48, 1858.356, 1852.278, 1841.841, 1844.392, 1838.885, 1845.471, 1835.073, 1843.236, 1847.535, 1833.192, 1824.181, 1837.307, 1840.38, 1828.43, 1811.495, 1847.764, 1822.735, 1830.969, 1820.734, 1823.334, 1821.05, 1819.804, 1799.298, 1820.784, 1804.433, 1820.924, 1817.243, 1803.109, 1819.047, 1814.775, 1822.453, 1810.863, 1806.431, 1809.811, 1797.752, 1826.807, 1790.064, 1790.266, 1802.282, 1821.263, 1790.244, 1799.813, 1807.564, 1800.215]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.21492880952165166 batch_size:27 epochs:100	100	2000	True	31051.40234		617205	14	-1	282.4062616825104	{'train_loss': [456436.656, 387603.781, 347806.625, 332375.375, 324081.531, 319275.094, 315308.312, 313141.094, 310212.125, 306916.781, 303852.281, 299170.25, 294114.719, 288862.531, 283912.688, 279081.688, 275095.344, 272525.594, 268387.656, 264679.812, 262584.781, 259627.594, 257994.344, 255133.188, 252141.266, 250496.734, 248543.297, 246978.172, 244679.375, 243041.141, 242245.109, 240350.062, 239725.547, 238857.281, 237477.219, 237182.609, 235491.516, 234654.859, 234210.188, 233523.359, 232250.016, 232307.344, 231135.641, 230905.75, 229336.031, 228612.656, 228220.219, 227863.562, 227303.094, 227101.188, 226227.562, 225884.406, 225511.141, 224471.828, 223757.375, 223787.562, 224376.078, 222921.0, 222145.328, 222076.938, 221743.859, 221686.094, 219976.188, 219881.891, 219402.938, 218517.016, 218139.984, 218882.703, 217521.25, 217749.484, 217519.656, 216857.406, 216608.531, 215879.578, 215757.875, 214663.078, 215021.859, 214361.406, 214184.969, 214856.156, 213045.703, 213325.031, 212551.891, 213075.094, 212603.594, 212712.438, 211391.156, 211099.125, 211704.391, 210293.719, 210876.859, 210598.641, 209787.141, 209107.188, 209344.188, 209411.281, 208811.625, 208966.047, 208293.391, 207460.641], 'val_loss': [3072.868, 2595.086, 2477.377, 2412.137, 2378.402, 2372.846, 2357.888, 2321.434, 2300.406, 2301.678, 2264.49, 2244.127, 2212.049, 2173.076, 2141.188, 2116.627, 2093.587, 2056.134, 2024.755, 2001.988, 1986.503, 1960.021, 1976.9, 1923.439, 1921.974, 1909.567, 1897.884, 1863.229, 1883.11, 1850.495, 1847.632, 1844.876, 1843.328, 1831.919, 1829.297, 1824.812, 1814.988, 1842.724, 1805.536, 1807.823, 1802.545, 1802.023, 1786.679, 1792.778, 1797.669, 1793.635, 1797.12, 1799.982, 1804.304, 1779.012, 1808.805, 1779.004, 1784.281, 1784.372, 1797.497, 1793.71, 1766.955, 1782.686, 1781.419, 1786.947, 1774.967, 1775.627, 1777.677, 1787.597, 1781.622, 1779.909, 1767.629, 1764.698, 1767.145, 1772.323, 1779.557, 1779.576, 1760.638, 1776.085, 1774.632, 1758.767, 1756.575, 1753.857, 1770.466, 1757.0, 1748.557, 1763.129, 1745.274, 1762.18, 1740.838, 1741.373, 1772.955, 1762.083, 1751.594, 1744.087, 1738.077, 1753.24, 1737.415, 1737.18, 1729.926, 1743.22, 1724.663, 1747.247, 1720.377, 1734.432]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:93 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:97 kernel_size:10 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:123 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:79 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.13994511696345116 batch_size:27 epochs:100	100	1000	True	34044.83984		488022	15	-1	288.0801212787628	{'train_loss': [381103.844, 342205.938, 320936.5, 313956.312, 307787.062, 303003.156, 298624.0, 295416.125, 292439.0, 289989.188, 287904.844, 285841.875, 283948.562, 282633.844, 281254.656, 279360.031, 278028.844, 277077.938, 275604.812, 274185.281, 273492.094, 271494.5, 270398.281, 270018.188, 268891.969, 268164.781, 267666.812, 266320.781, 265511.406, 265626.312, 264167.719, 263162.5, 262944.094, 262029.797, 260839.25, 260363.531, 259902.234, 258932.875, 258370.234, 258031.125, 257207.188, 257349.781, 256990.672, 255792.0, 255677.016, 255414.969, 254841.281, 254284.984, 253666.219, 253144.516, 252022.688, 252716.703, 251560.219, 252222.516, 251609.312, 250690.281, 250649.203, 250084.406, 249084.625, 249171.078, 249754.734, 248532.734, 248424.531, 248276.531, 248193.125, 247992.781, 247407.781, 247341.797, 246745.969, 246305.984, 246073.266, 246588.203, 245810.625, 245044.562, 245165.906, 244944.5, 243782.688, 243875.359, 244578.031, 244033.203, 243934.469, 242896.016, 244005.0, 242981.359, 243308.469, 243278.109, 243144.312, 241524.422, 241678.047, 241145.047, 241693.141, 241410.156, 241177.656, 241030.984, 241254.656, 241136.312, 240888.234, 240812.812, 239908.516, 240474.359], 'val_loss': [2750.502, 2437.111, 2371.02, 2314.94, 2277.844, 2249.047, 2221.915, 2217.246, 2207.914, 2181.368, 2191.77, 2171.552, 2168.526, 2156.142, 2147.557, 2137.88, 2134.947, 2122.968, 2117.114, 2109.624, 2112.596, 2092.58, 2088.729, 2079.778, 2086.611, 2077.715, 2044.276, 2062.662, 2048.841, 2035.475, 2046.244, 2026.179, 2025.395, 2046.819, 2025.6, 2012.398, 2008.42, 2013.891, 1989.058, 2012.016, 2002.638, 1999.464, 2009.25, 1967.233, 2000.786, 1995.856, 2001.87, 1984.964, 1988.535, 2013.504, 1978.829, 1961.64, 1975.366, 2026.433, 1964.808, 1976.801, 1954.947, 1955.412, 2013.615, 1956.566, 1935.718, 1926.637, 1957.534, 1949.536, 1979.349, 1936.837, 1987.671, 1946.523, 1957.49, 1914.878, 1936.245, 1939.394, 1955.407, 1935.335, 1952.522, 1994.898, 1922.657, 1952.664, 1935.878, 1924.818, 1931.906, 1923.932, 1911.595, 1929.302, 1940.15, 1942.139, 1910.114, 1894.49, 1890.499, 1932.518, 1926.537, 1966.126, 1900.26, 1913.203, 1932.533, 1919.45, 1891.97, 1931.871, 1930.199, 1929.942]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.21492880952165166 batch_size:27 epochs:100	100	1000	True	31636.70703		603109	13	-1	257.1574008464813	{'train_loss': [439546.531, 354071.469, 326821.906, 314966.25, 306481.594, 299602.156, 293989.594, 290109.812, 285142.969, 281224.625, 277651.5, 274382.688, 271655.156, 269767.281, 268605.25, 265918.125, 264324.719, 262900.062, 260620.266, 258213.078, 256689.422, 256190.328, 253601.266, 252589.391, 252514.547, 251301.141, 249243.797, 248304.719, 247615.422, 245664.547, 244709.812, 244535.484, 241918.219, 241752.281, 240916.484, 240072.562, 239821.125, 238563.547, 238141.641, 236947.938, 235626.766, 234826.016, 234315.156, 234006.656, 233849.812, 232585.109, 231470.047, 230935.016, 230407.641, 230247.172, 229319.031, 227618.344, 227187.469, 227212.438, 227200.797, 227057.0, 225681.172, 225286.578, 224208.875, 223621.453, 223489.906, 223385.984, 222038.812, 221879.094, 221448.781, 221380.188, 221144.047, 220286.531, 219386.719, 218679.422, 218615.109, 217595.156, 218282.641, 217575.422, 216772.828, 216690.891, 215367.859, 216302.516, 215820.969, 215293.953, 214431.172, 214507.281, 213626.719, 213191.75, 213423.281, 212921.703, 212993.969, 213038.406, 210578.141, 211001.203, 211622.016, 210714.109, 210813.203, 209730.641, 208948.375, 209372.734, 209127.828, 208737.156, 207923.562, 208126.891], 'val_loss': [2824.689, 2514.437, 2410.247, 2306.282, 2252.677, 2262.226, 2206.022, 2153.972, 2131.547, 2098.975, 2080.854, 2073.055, 2040.823, 2040.807, 2050.277, 2008.987, 2026.418, 2020.469, 1978.441, 1986.135, 1968.144, 1953.934, 1958.407, 1958.553, 1958.973, 1927.628, 1937.811, 1934.032, 1938.321, 1927.213, 1909.235, 1908.928, 1907.619, 1896.2, 1889.408, 1880.151, 1882.91, 1877.611, 1875.148, 1872.419, 1861.429, 1869.727, 1861.629, 1866.123, 1864.855, 1866.61, 1850.21, 1846.351, 1841.558, 1840.461, 1850.528, 1838.809, 1837.384, 1845.073, 1844.443, 1865.731, 1858.395, 1851.558, 1829.412, 1818.122, 1819.29, 1811.917, 1822.161, 1835.82, 1819.779, 1810.859, 1823.336, 1807.621, 1822.05, 1818.185, 1814.238, 1802.516, 1802.551, 1800.639, 1779.687, 1810.38, 1813.732, 1797.881, 1803.605, 1798.508, 1809.681, 1802.152, 1788.052, 1795.778, 1787.754, 1790.993, 1810.886, 1780.274, 1781.952, 1795.891, 1797.598, 1800.739, 1784.115, 1784.103, 1799.22, 1781.072, 1788.948, 1799.093, 1799.146, 1802.932]}	100	100	True
