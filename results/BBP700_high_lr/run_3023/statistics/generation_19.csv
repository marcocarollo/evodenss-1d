id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	1000	True	31302.00781		675818	15	-1	314.0594141483307	{'train_loss': [435287.344, 345648.125, 322506.188, 309577.312, 300576.344, 295294.469, 290375.906, 286677.156, 282878.25, 279751.094, 277731.875, 274819.844, 271807.719, 269596.594, 266155.344, 264347.594, 261859.047, 260813.891, 259367.75, 256447.875, 254469.031, 252988.438, 251220.609, 250288.844, 248810.422, 246935.281, 245472.328, 245548.031, 243378.75, 242959.812, 241265.391, 240751.953, 240007.031, 239083.266, 238308.109, 237237.766, 235878.016, 235486.672, 233941.844, 233978.234, 233201.719, 231992.5, 232059.828, 232068.391, 231192.719, 230934.172, 229711.953, 229616.391, 228653.0, 228579.109, 227798.984, 227264.078, 226213.25, 225722.281, 226526.156, 226691.781, 226064.406, 224447.562, 224812.391, 224369.594, 223774.266, 223542.656, 224090.688, 222758.922, 222748.844, 221865.578, 220917.469, 222169.766, 221889.125, 220557.172, 220519.172, 219825.609, 219522.359, 219356.016, 219544.047, 217929.266, 218369.484, 217924.375, 217771.812, 217754.125, 217743.516, 217124.219, 217906.828, 216904.875, 216558.781, 216324.297, 216023.703, 215485.891, 215681.766, 215173.453, 214616.438, 213743.609, 214125.891, 215141.188, 213774.422, 213561.594, 213694.031, 213648.891, 212280.891, 212282.969], 'val_loss': [2548.747, 2254.42, 2157.047, 2113.635, 2049.948, 2041.471, 2028.624, 1984.435, 1986.334, 1961.252, 1951.505, 1925.888, 1911.291, 1900.31, 1902.863, 1901.782, 1889.988, 1865.615, 1849.967, 1854.963, 1829.587, 1837.042, 1840.725, 1844.536, 1824.783, 1815.01, 1803.402, 1827.145, 1776.954, 1793.348, 1774.365, 1786.162, 1751.633, 1750.17, 1752.018, 1779.769, 1731.821, 1742.327, 1735.36, 1731.454, 1710.19, 1707.869, 1703.896, 1702.242, 1695.65, 1702.531, 1679.456, 1716.051, 1694.79, 1699.562, 1682.88, 1703.693, 1686.076, 1676.354, 1681.061, 1683.581, 1668.938, 1668.2, 1674.431, 1665.207, 1659.421, 1668.329, 1653.876, 1661.707, 1660.947, 1661.939, 1660.967, 1662.056, 1654.293, 1655.84, 1648.275, 1657.312, 1658.702, 1645.424, 1661.362, 1642.355, 1639.678, 1631.028, 1628.974, 1650.666, 1632.459, 1648.967, 1631.337, 1638.372, 1645.527, 1643.27, 1638.792, 1625.425, 1633.222, 1643.344, 1636.698, 1636.337, 1639.586, 1628.282, 1636.405, 1633.687, 1631.434, 1623.708, 1641.573, 1637.568]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:70 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:70 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:61 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:57 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:61 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	53	1000	True	138469.45312		310530622	15	-1	1038.6999411582947	{'train_loss': [1241480.5, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1067178.0, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1072309.25, 1070307.5, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125], 'val_loss': [7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411]}	53	53	False
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:83 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.20691114802933186 batch_size:24 epochs:100	100	1000	True	31934.28906		686519	16	-1	317.3783223628998	{'train_loss': [456835.281, 361463.906, 332148.75, 317158.219, 306515.844, 299521.844, 295003.781, 290231.594, 286125.625, 283334.25, 280040.0, 277295.281, 274941.406, 272340.5, 270849.375, 268005.719, 267071.25, 264806.5, 263043.594, 261964.688, 260392.688, 258933.828, 256678.781, 256201.625, 254534.719, 253710.859, 253405.844, 251963.234, 250852.906, 250197.5, 248800.328, 249066.219, 247708.031, 247235.984, 246039.062, 245354.359, 244318.5, 244146.438, 243334.031, 243052.719, 242879.031, 241761.859, 241159.781, 241678.641, 240981.75, 240169.078, 239780.828, 238997.031, 238137.219, 238673.172, 236974.391, 237714.219, 237013.703, 236875.703, 236169.219, 235724.641, 235232.609, 234594.844, 234261.188, 233772.859, 233623.828, 232743.172, 232554.703, 232096.391, 232077.797, 232640.453, 230943.547, 231302.594, 230213.0, 229785.938, 229337.828, 230629.828, 230429.047, 229192.344, 229050.484, 229172.672, 228797.625, 228478.203, 228511.328, 228490.703, 227281.078, 227102.875, 226422.766, 226219.906, 226198.203, 226156.531, 225620.938, 225300.734, 225844.828, 224848.453, 224865.703, 224253.125, 224628.328, 223932.219, 223994.859, 223920.516, 223907.609, 223817.734, 222369.078, 222870.984], 'val_loss': [2993.425, 2397.445, 2205.394, 2102.731, 2077.081, 2034.469, 2024.675, 2024.323, 1984.199, 1975.216, 1965.512, 1961.473, 1923.778, 1925.961, 1926.429, 1896.863, 1909.696, 1888.041, 1865.597, 1862.679, 1844.777, 1846.246, 1832.897, 1830.035, 1817.795, 1810.831, 1799.922, 1795.803, 1780.844, 1796.469, 1799.009, 1772.24, 1781.866, 1764.548, 1774.471, 1771.204, 1749.701, 1755.721, 1750.685, 1755.823, 1741.107, 1735.961, 1746.155, 1742.505, 1737.154, 1741.435, 1749.664, 1729.339, 1726.302, 1722.305, 1725.223, 1729.601, 1725.689, 1728.052, 1714.623, 1717.802, 1716.419, 1716.574, 1706.067, 1710.349, 1715.508, 1714.498, 1713.04, 1701.609, 1713.259, 1698.647, 1697.467, 1713.745, 1707.271, 1698.267, 1693.455, 1701.228, 1697.365, 1696.918, 1697.842, 1695.565, 1697.745, 1688.761, 1688.415, 1692.626, 1693.385, 1687.082, 1688.151, 1685.555, 1688.253, 1702.314, 1700.004, 1685.848, 1680.771, 1682.892, 1680.697, 1680.88, 1684.73, 1685.629, 1680.734, 1689.084, 1678.901, 1676.269, 1690.482, 1688.777]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:6 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.11944721315603904 batch_size:24 epochs:100	100	1000	True	32071.61133		609898	14	-1	284.14393043518066	{'train_loss': [421357.938, 356034.312, 332765.0, 320230.938, 312573.469, 305172.406, 299363.656, 294099.938, 290704.469, 286723.906, 284724.062, 281761.531, 278937.719, 277759.125, 275287.906, 273915.688, 272792.75, 271173.281, 270279.344, 268967.719, 267750.312, 266992.562, 265080.906, 264457.75, 263020.031, 261761.297, 260816.188, 260125.188, 258763.688, 258318.594, 257309.672, 256737.953, 255460.094, 254528.359, 253810.062, 253764.25, 253253.734, 252230.031, 251737.344, 251056.141, 250005.188, 249497.422, 249745.344, 248965.672, 249058.531, 247129.391, 246977.969, 246356.078, 246203.203, 245561.203, 244376.156, 245176.656, 243955.172, 243887.219, 242960.344, 242633.547, 242582.953, 241329.359, 241958.828, 241257.031, 240271.844, 240799.578, 240043.344, 240009.734, 238495.344, 238332.016, 237991.828, 237738.125, 237960.094, 236210.391, 236553.922, 235708.109, 235618.422, 236047.125, 235076.688, 234741.188, 234363.422, 233585.547, 234165.625, 235158.766, 233770.938, 233239.578, 233222.078, 232451.906, 232675.109, 232606.297, 231715.625, 231307.344, 231656.406, 231189.297, 230625.766, 230505.312, 230370.609, 229861.516, 229148.844, 229307.297, 229754.328, 228920.125, 229030.594, 228219.109], 'val_loss': [2664.017, 2387.565, 2227.563, 2188.869, 2152.402, 2123.236, 2091.663, 2052.339, 2047.555, 2016.399, 1995.705, 1981.934, 1979.972, 1971.281, 1966.356, 1953.588, 1962.328, 1935.136, 1924.09, 1917.004, 1907.46, 1913.071, 1897.399, 1891.49, 1893.302, 1912.818, 1873.165, 1853.796, 1858.845, 1874.215, 1858.317, 1849.067, 1839.8, 1866.309, 1840.691, 1840.52, 1832.575, 1829.7, 1825.668, 1826.016, 1823.702, 1824.239, 1820.965, 1806.599, 1803.635, 1812.74, 1799.482, 1799.467, 1800.859, 1804.601, 1801.859, 1798.993, 1812.327, 1803.49, 1784.041, 1794.056, 1778.67, 1774.675, 1768.959, 1785.789, 1781.32, 1780.552, 1764.798, 1768.198, 1780.241, 1773.601, 1761.711, 1759.015, 1756.932, 1757.926, 1756.025, 1755.221, 1759.129, 1765.182, 1752.948, 1742.704, 1737.59, 1738.997, 1749.167, 1746.012, 1737.18, 1723.248, 1746.126, 1722.981, 1734.792, 1733.919, 1730.244, 1744.683, 1740.082, 1733.444, 1741.64, 1724.602, 1722.487, 1721.75, 1731.516, 1731.224, 1736.259, 1718.842, 1721.976, 1711.097]}	100	100	True
