id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:18 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.20382428609940764 batch_size:29 epochs:100	100	1000	True	32474.88672		2883865	15	-1	305.7319269180298	{'train_loss': [531939.125, 497482.188, 470880.375, 422598.031, 365087.875, 328198.094, 306012.344, 295376.312, 287481.375, 282275.062, 276844.875, 274006.75, 271661.844, 268611.875, 266007.188, 263742.875, 261679.562, 259865.641, 258062.641, 256115.469, 254984.031, 253261.5, 251571.922, 249829.75, 249063.422, 248288.062, 247106.0, 245279.875, 244484.781, 242649.203, 242304.531, 241775.422, 240442.469, 239771.625, 239213.938, 237644.953, 236883.375, 236439.484, 235368.703, 234789.047, 234692.297, 233294.656, 232435.156, 232950.031, 231433.781, 231184.141, 230362.656, 229237.891, 229756.641, 228668.906, 227641.953, 227286.203, 226674.672, 225610.656, 224780.906, 224925.656, 224638.234, 223934.562, 222895.484, 222203.219, 222571.609, 221949.516, 221499.656, 220189.156, 219159.609, 219878.375, 218712.047, 218149.703, 218051.266, 216796.156, 217436.969, 216880.219, 215644.984, 215413.391, 215136.859, 214402.844, 214487.797, 213248.125, 213107.766, 212504.156, 212404.25, 212697.547, 211380.781, 211268.156, 210601.078, 210141.953, 210240.75, 210772.281, 209322.203, 209559.234, 208053.219, 208817.625, 208002.156, 207348.141, 207231.328, 206101.453, 206140.141, 206551.25, 204753.281, 205432.688], 'val_loss': [3476.304, 3467.605, 3325.776, 3057.523, 2725.634, 2480.746, 2382.355, 2347.071, 2309.904, 2250.913, 2244.947, 2216.196, 2198.096, 2194.778, 2176.005, 2154.396, 2200.314, 2154.788, 2168.465, 2130.953, 2127.674, 2118.562, 2107.78, 2092.418, 2103.002, 2077.82, 2095.397, 2096.854, 2076.96, 2071.122, 2054.65, 2067.021, 2049.326, 2042.031, 2020.923, 2024.286, 2017.82, 2025.768, 2023.003, 2017.432, 2022.933, 2015.198, 2033.351, 2011.565, 1990.779, 2004.604, 2009.188, 1978.243, 1963.603, 1982.031, 1984.353, 1969.441, 1998.636, 1952.757, 1978.316, 1953.346, 1933.191, 1945.717, 1960.06, 1937.331, 1934.86, 1956.318, 1934.971, 1938.225, 1930.733, 1942.286, 1949.81, 1906.911, 1928.548, 1935.283, 1931.021, 1918.008, 1936.05, 1953.222, 1922.398, 1917.609, 1929.926, 1924.187, 1932.153, 1942.232, 1950.69, 1945.685, 1948.042, 1929.484, 1921.12, 1932.266, 1956.478, 1932.259, 1927.972, 1931.344, 1924.016, 1941.695, 1939.97, 1946.012, 1927.031, 1922.242, 1957.265, 1927.628, 1934.12, 1924.856]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:18 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.20382428609940764 batch_size:13 epochs:100	100	1000	True	31465.72461		2854171	15	-1	520.838427066803	{'train_loss': [511901.844, 450887.406, 343444.188, 300994.219, 289380.062, 281106.719, 274076.688, 268857.531, 264391.938, 260611.828, 256670.625, 253859.75, 251212.297, 248627.375, 246716.297, 244347.031, 243098.812, 241592.625, 240209.359, 237972.953, 237142.562, 235969.203, 233849.438, 232386.016, 232423.234, 230501.938, 229253.844, 228468.109, 226436.672, 226502.938, 224809.562, 224118.047, 222940.672, 222018.625, 221434.094, 221171.578, 220450.891, 218582.984, 217506.109, 216995.484, 216419.781, 215557.672, 215108.859, 214514.328, 213818.844, 213592.016, 212566.453, 211017.984, 210562.578, 210257.766, 208501.062, 208784.562, 208518.484, 207615.781, 206333.375, 205430.453, 206315.766, 204416.906, 205432.812, 204057.406, 203634.766, 203535.422, 201781.969, 202781.406, 201447.484, 201342.047, 201239.531, 200494.828, 199812.359, 199252.609, 198224.141, 198229.938, 198260.344, 197903.547, 196461.719, 196837.875, 195684.859, 195319.797, 194763.484, 195062.062, 195198.609, 195123.078, 193573.328, 193706.078, 192906.906, 192325.344, 192565.156, 192140.25, 190816.031, 191345.047, 189931.828, 189407.375, 189155.75, 188579.688, 188275.172, 188672.391, 187726.734, 187083.578, 188028.812, 187265.688], 'val_loss': [1566.914, 1456.875, 1199.992, 1126.066, 1077.53, 1043.513, 1026.102, 1004.383, 995.324, 979.285, 969.449, 955.333, 959.942, 948.159, 947.391, 944.731, 938.456, 924.579, 931.101, 929.27, 923.511, 919.848, 919.939, 922.716, 916.675, 917.783, 920.474, 911.821, 913.855, 920.828, 901.951, 912.608, 912.865, 906.209, 909.387, 904.705, 902.857, 899.224, 902.4, 902.179, 888.94, 895.708, 885.458, 899.111, 897.973, 909.528, 897.624, 889.296, 892.928, 909.374, 899.119, 898.796, 893.392, 888.967, 893.017, 897.245, 899.169, 899.822, 890.267, 889.01, 882.915, 902.597, 905.312, 900.84, 899.737, 895.451, 890.489, 894.867, 906.497, 896.393, 891.739, 893.577, 894.926, 898.059, 898.295, 894.61, 894.5, 896.281, 887.395, 894.625, 889.768, 887.698, 886.996, 901.454, 896.158, 895.801, 900.326, 897.878, 889.133, 904.824, 888.544, 901.132, 890.506, 895.711, 901.779, 905.439, 907.85, 902.706, 901.727, 898.312]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:29 epochs:100	100	1000	True	31297.7832		625826	15	-1	274.11515045166016	{'train_loss': [416014.938, 350584.219, 329426.812, 317019.094, 308742.5, 301809.156, 295898.719, 290059.188, 286908.875, 283640.469, 280866.312, 278712.844, 275615.812, 273322.281, 271850.812, 270356.188, 267329.125, 265489.219, 263524.5, 262388.5, 261206.828, 259124.281, 258684.641, 257452.062, 255166.047, 254569.094, 253390.672, 252300.906, 251836.516, 249553.688, 249429.812, 248361.625, 247369.281, 247114.188, 244323.016, 243963.859, 243568.531, 242575.297, 241422.906, 241325.484, 239809.797, 239474.219, 238486.375, 238987.031, 236821.609, 235866.266, 235816.781, 234961.625, 233978.875, 233055.891, 232673.734, 231750.781, 231055.391, 231493.438, 230371.359, 230243.953, 229905.156, 228689.75, 228672.641, 228275.906, 227017.672, 226463.453, 226667.203, 225533.0, 225298.578, 224893.406, 223763.438, 223349.094, 223855.844, 223485.094, 222721.891, 222013.031, 221696.953, 221662.156, 220612.828, 220981.859, 219792.531, 220581.516, 218435.734, 218743.453, 219024.703, 219421.188, 217939.391, 217787.844, 217179.719, 217581.656, 217216.406, 216611.094, 215999.797, 215981.859, 214463.062, 214734.062, 215067.594, 214392.531, 214401.406, 213465.828, 213454.547, 211907.203, 213092.078, 212529.531], 'val_loss': [2829.41, 2650.005, 2546.567, 2529.369, 2488.239, 2424.951, 2366.39, 2326.054, 2312.33, 2295.619, 2268.722, 2266.762, 2222.597, 2243.026, 2243.748, 2183.588, 2184.383, 2154.558, 2146.794, 2162.668, 2114.639, 2112.567, 2094.737, 2105.967, 2080.886, 2069.087, 2110.695, 2081.552, 2074.541, 2034.858, 2058.184, 2045.671, 2036.82, 2029.417, 2031.524, 2004.69, 1998.565, 2019.706, 1988.243, 1977.325, 2002.228, 1989.296, 1983.725, 2008.942, 1989.939, 1950.933, 1956.551, 1969.295, 1944.363, 1948.179, 1926.519, 1915.539, 1956.708, 1912.414, 1938.775, 1897.037, 1922.265, 1918.018, 1922.601, 1897.804, 1908.789, 1902.411, 1905.51, 1909.972, 1895.534, 1903.573, 1885.442, 1899.519, 1890.901, 1886.585, 1884.943, 1885.593, 1910.069, 1881.774, 1907.423, 1891.631, 1865.636, 1895.105, 1869.855, 1891.009, 1882.084, 1894.562, 1889.981, 1880.557, 1876.556, 1868.722, 1883.178, 1875.126, 1864.221, 1886.985, 1877.883, 1860.227, 1869.51, 1881.354, 1853.16, 1869.265, 1845.467, 1869.88, 1859.072, 1859.05]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.20382428609940764 batch_size:28 epochs:100	100	1000	True	32120.32031		1572219	14	-1	288.59721970558167	{'train_loss': [481812.594, 427285.75, 381065.531, 348348.0, 327144.75, 314217.219, 303732.812, 296179.375, 289875.375, 283063.5, 276933.312, 270848.469, 267513.062, 262782.719, 261428.469, 257732.516, 255330.922, 252602.891, 250544.938, 249836.984, 246998.422, 245697.406, 244971.172, 242293.516, 241663.984, 240045.328, 238080.75, 237732.891, 235864.172, 235202.438, 234612.578, 234517.203, 232315.172, 230975.609, 230954.875, 230680.688, 229410.188, 228714.062, 228413.516, 227440.688, 226930.109, 225820.156, 225071.156, 225033.516, 223790.109, 224103.828, 224068.266, 222786.922, 222590.078, 222318.422, 221944.641, 220447.328, 218941.094, 218884.891, 219244.938, 218867.984, 218275.531, 216907.781, 216927.344, 217257.484, 215837.156, 215348.703, 215071.406, 214097.516, 214906.406, 214397.5, 214401.859, 213343.5, 211922.875, 212014.391, 211996.734, 211443.125, 211719.781, 210396.906, 210596.859, 210321.75, 209813.875, 210095.781, 208760.578, 209601.938, 208235.078, 208582.828, 208944.266, 208126.766, 207884.734, 207185.109, 207169.406, 206266.359, 206459.594, 205891.594, 205859.344, 205170.156, 206137.938, 203732.578, 205401.469, 204628.125, 203590.359, 204209.828, 204296.188, 203470.922], 'val_loss': [3183.97, 3209.247, 2900.186, 2691.343, 2488.114, 2485.594, 2420.654, 2339.822, 2291.737, 2290.63, 2225.775, 2194.303, 2171.285, 2156.846, 2140.303, 2157.594, 2120.106, 2099.764, 2096.209, 2061.954, 2089.903, 2110.913, 2093.131, 2066.342, 2044.758, 2051.848, 2038.266, 2049.435, 2033.186, 2017.43, 2008.795, 2010.255, 2009.581, 2028.686, 1989.565, 1991.369, 2016.932, 2017.833, 1975.225, 1981.49, 1974.889, 1990.139, 1955.723, 1957.234, 1968.824, 1973.127, 1942.713, 1943.906, 1957.334, 1938.627, 1978.901, 1933.86, 1941.918, 1929.492, 1931.349, 1923.715, 1911.386, 1923.059, 1918.415, 1928.508, 1926.22, 1910.925, 1906.333, 1909.393, 1912.638, 1913.497, 1920.316, 1915.101, 1904.948, 1894.319, 1894.612, 1899.074, 1914.068, 1897.499, 1885.398, 1887.757, 1919.431, 1909.903, 1906.336, 1909.715, 1911.281, 1912.657, 1917.626, 1917.42, 1920.652, 1898.488, 1921.237, 1908.708, 1901.228, 1921.257, 1891.507, 1912.824, 1887.042, 1914.045, 1894.399, 1904.154, 1889.881, 1890.787, 1892.207, 1906.992]}	100	100	True
