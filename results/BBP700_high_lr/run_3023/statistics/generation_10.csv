id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.21492880952165166 batch_size:27 epochs:100	100	2000	True	31850.59961		617205	14	-1	272.0138828754425	{'train_loss': [466317.156, 408231.5, 354294.406, 333881.406, 324729.156, 318035.562, 313323.062, 307315.781, 299972.781, 293820.812, 287827.594, 282307.906, 277742.438, 274399.531, 271771.031, 269661.438, 266460.562, 264262.25, 262365.906, 261420.656, 259286.812, 258278.078, 256526.484, 254430.922, 253894.516, 252709.109, 251845.734, 250885.516, 249269.906, 248983.422, 248585.156, 246654.969, 246832.891, 245817.203, 244282.516, 243377.688, 242622.562, 242924.516, 242350.438, 240742.203, 241354.344, 238669.312, 239874.578, 238044.562, 237687.734, 236693.953, 235969.953, 235584.906, 235818.141, 234928.922, 234562.969, 233937.234, 233118.562, 232512.234, 232581.047, 231047.5, 231611.703, 231825.047, 230293.828, 230476.734, 229476.281, 229477.172, 229282.578, 228801.672, 227675.844, 228275.453, 227524.781, 226888.766, 227217.594, 226560.094, 225973.844, 224692.281, 223883.031, 225015.391, 225206.516, 223862.922, 223611.953, 223726.375, 222570.672, 222320.062, 222271.062, 222520.328, 221384.828, 221087.766, 221334.891, 220501.531, 220402.531, 220568.859, 220611.406, 218982.688, 218747.656, 219097.188, 219163.719, 218752.391, 218553.219, 219091.656, 218241.422, 217334.922, 218065.406, 217311.125], 'val_loss': [3334.794, 2619.449, 2466.73, 2405.53, 2361.448, 2351.616, 2310.178, 2349.369, 2243.347, 2219.714, 2182.244, 2154.874, 2145.559, 2104.993, 2086.667, 2078.525, 2063.071, 2051.807, 2045.426, 2014.931, 2020.009, 2020.157, 2028.82, 1976.062, 1984.794, 1962.898, 1952.882, 1941.581, 1935.204, 1936.511, 1926.883, 1928.376, 1911.984, 1905.32, 1902.634, 1882.335, 1880.083, 1885.071, 1877.407, 1888.591, 1860.769, 1867.256, 1872.139, 1853.816, 1858.686, 1855.843, 1853.934, 1849.571, 1847.838, 1844.591, 1838.688, 1841.662, 1853.557, 1831.659, 1837.658, 1820.129, 1836.11, 1831.147, 1844.163, 1835.319, 1838.549, 1821.406, 1830.921, 1816.062, 1816.036, 1801.845, 1811.169, 1824.584, 1798.45, 1810.821, 1813.471, 1805.668, 1808.606, 1795.368, 1802.633, 1807.133, 1790.461, 1804.453, 1810.794, 1797.389, 1796.054, 1799.907, 1792.083, 1781.328, 1788.285, 1789.156, 1772.808, 1773.724, 1804.93, 1791.494, 1757.609, 1767.247, 1781.477, 1762.869, 1780.611, 1777.309, 1774.873, 1779.003, 1769.621, 1768.837]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.21492880952165166 batch_size:27 epochs:100	100	3000	True	32056.78711		617205	14	-1	274.2182469367981	{'train_loss': [416761.125, 338842.812, 319951.375, 310041.312, 302118.562, 296668.906, 292103.469, 287819.312, 285409.625, 281470.469, 278486.0, 275522.094, 271590.906, 269588.625, 267276.906, 265160.688, 263292.281, 261586.688, 259361.125, 258388.484, 256874.75, 255190.266, 253806.141, 252942.172, 251638.469, 249552.344, 249136.281, 247388.219, 246884.078, 245022.125, 244606.344, 243684.125, 242757.516, 241838.094, 241052.266, 239903.719, 238543.328, 237952.797, 238075.531, 237236.391, 236714.109, 236147.578, 235136.406, 234339.406, 233963.203, 232923.0, 232747.0, 231262.828, 231774.531, 230507.875, 230734.656, 229937.797, 229168.562, 229220.875, 228990.281, 227897.75, 227069.344, 226686.203, 226234.984, 225405.453, 225762.203, 224906.328, 224845.422, 223249.734, 223932.094, 223708.906, 222352.5, 222403.625, 221901.109, 222109.672, 221844.438, 220814.922, 220781.859, 220333.172, 219888.328, 219265.359, 219110.734, 219086.203, 218998.703, 217953.562, 217784.109, 217924.188, 217717.641, 216492.688, 216506.609, 215785.859, 215964.609, 215343.891, 215161.797, 214824.172, 214841.609, 215091.016, 214201.359, 212815.219, 213163.016, 212983.25, 213252.844, 212059.453, 212955.5, 211978.531], 'val_loss': [2609.846, 2406.727, 2353.682, 2278.934, 2267.193, 2237.059, 2236.264, 2187.418, 2186.588, 2175.247, 2127.332, 2097.87, 2097.818, 2062.582, 2052.712, 2046.311, 2039.824, 2017.985, 2018.137, 2017.012, 2003.484, 1986.913, 1970.355, 1973.896, 1961.29, 1955.732, 1945.784, 1944.593, 1933.977, 1918.295, 1925.53, 1906.502, 1900.438, 1893.487, 1895.497, 1888.245, 1878.437, 1882.754, 1884.223, 1874.223, 1866.309, 1887.522, 1884.511, 1877.473, 1871.673, 1864.076, 1853.793, 1860.606, 1850.981, 1851.695, 1854.628, 1878.13, 1857.78, 1858.553, 1846.09, 1841.156, 1830.73, 1837.047, 1854.721, 1867.422, 1835.393, 1863.046, 1837.102, 1813.111, 1814.544, 1821.881, 1795.671, 1827.984, 1823.179, 1823.746, 1809.744, 1830.806, 1814.856, 1837.45, 1808.265, 1804.844, 1799.311, 1801.811, 1795.832, 1798.601, 1801.253, 1803.553, 1788.716, 1791.46, 1794.725, 1814.463, 1787.503, 1813.912, 1808.378, 1775.184, 1803.94, 1790.226, 1796.084, 1796.132, 1790.395, 1788.727, 1807.666, 1817.327, 1789.453, 1805.771]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:122 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:10 kernel_size:4 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.21492880952165166 batch_size:32 epochs:100	100	1000	True	31632.26758		607717	14	-1	242.48820853233337	{'train_loss': [428921.0, 351868.0, 332318.969, 321799.438, 314167.156, 307568.812, 300939.031, 295865.344, 291139.656, 286849.062, 283220.25, 279343.562, 275437.531, 272465.531, 269509.594, 265840.562, 264369.438, 262045.375, 259341.984, 258632.672, 257134.938, 254921.953, 253619.828, 252664.312, 251455.781, 250493.922, 249414.375, 247198.25, 246164.047, 246213.625, 245188.859, 244586.172, 243283.609, 242369.391, 241151.078, 240208.312, 240355.047, 238475.672, 238468.641, 237803.25, 237085.344, 236699.812, 236075.297, 235401.547, 234681.375, 233840.297, 234120.875, 232661.031, 232263.312, 232316.016, 231755.594, 230271.328, 230229.484, 230318.547, 228352.484, 228469.703, 227882.797, 228835.391, 227587.547, 227304.562, 227202.375, 225976.5, 225719.938, 225481.188, 224311.047, 225183.344, 223784.125, 224312.172, 223810.016, 223705.0, 223363.578, 222345.125, 222254.75, 221660.906, 222581.828, 221876.266, 221364.219, 220226.391, 220406.016, 219913.156, 219269.578, 219801.391, 218715.172, 219019.703, 218165.422, 218746.047, 218361.953, 219043.547, 217263.219, 216548.125, 217267.562, 217755.391, 215690.734, 216285.406, 216123.203, 216435.141, 215383.938, 214748.672, 215696.641, 214714.297], 'val_loss': [3381.11, 3151.898, 3015.207, 2949.97, 2872.049, 2845.239, 2767.195, 2749.365, 2699.127, 2676.479, 2675.827, 2649.688, 2606.87, 2570.824, 2558.481, 2533.828, 2499.777, 2504.978, 2488.149, 2465.329, 2465.875, 2456.715, 2446.379, 2425.881, 2417.697, 2415.623, 2418.875, 2400.845, 2410.031, 2396.084, 2375.56, 2390.282, 2377.022, 2377.824, 2380.571, 2367.023, 2367.854, 2353.918, 2371.272, 2345.129, 2339.7, 2335.085, 2324.796, 2326.767, 2323.659, 2327.705, 2326.301, 2312.891, 2340.223, 2327.838, 2325.044, 2307.33, 2318.879, 2300.145, 2308.142, 2317.098, 2308.092, 2304.696, 2307.266, 2307.361, 2291.897, 2286.81, 2284.444, 2289.185, 2269.858, 2277.435, 2284.763, 2294.07, 2271.731, 2296.725, 2245.393, 2268.789, 2270.022, 2257.89, 2285.444, 2260.672, 2263.208, 2278.423, 2258.684, 2246.19, 2267.46, 2248.324, 2249.542, 2235.969, 2253.275, 2246.911, 2234.55, 2227.279, 2231.542, 2254.322, 2243.408, 2234.902, 2249.293, 2229.29, 2243.87, 2227.038, 2236.442, 2247.072, 2249.093, 2221.618]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:18 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:94 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.20382428609940764 batch_size:27 epochs:100	100	1000	True	31445.21289		975651	15	-1	321.21281909942627	{'train_loss': [494570.844, 411049.938, 362034.781, 330414.344, 315083.844, 304230.188, 295148.562, 286961.094, 281614.062, 276806.344, 273170.531, 267602.156, 264181.688, 261215.609, 258665.125, 256863.766, 252965.609, 251761.891, 249461.375, 247290.266, 246396.594, 244766.766, 241635.859, 242059.672, 239908.906, 238528.203, 236682.172, 235690.094, 234902.266, 234965.281, 232470.422, 232207.844, 230269.609, 229869.25, 228868.547, 228274.406, 227120.688, 226020.094, 225803.969, 225028.531, 223283.141, 223441.047, 222691.891, 221331.094, 220888.438, 219799.875, 219308.266, 218777.375, 218221.297, 217074.188, 217272.344, 216388.359, 216157.641, 214723.406, 214570.109, 213182.953, 213322.781, 212680.156, 212565.531, 211314.172, 210608.5, 211022.516, 209994.328, 208735.203, 208751.062, 208503.031, 207340.031, 206924.078, 206608.453, 205524.125, 205624.609, 205336.984, 204801.109, 204751.578, 205239.078, 204512.5, 202956.531, 202820.844, 201540.047, 201473.781, 200819.875, 201709.5, 200715.141, 200083.391, 199758.188, 199694.953, 198972.875, 198638.688, 197196.656, 197918.375, 197454.75, 196448.234, 195971.203, 196091.828, 196374.516, 196409.797, 195711.828, 195714.938, 195306.531, 194502.266], 'val_loss': [3435.655, 2878.967, 2571.805, 2439.865, 2331.107, 2302.27, 2189.187, 2164.184, 2100.612, 2078.384, 2061.147, 2010.426, 2018.875, 2003.368, 1975.327, 1963.917, 1936.106, 1947.058, 1930.624, 1913.089, 1916.08, 1925.259, 1892.983, 1875.441, 1881.336, 1868.357, 1867.732, 1872.763, 1864.321, 1827.66, 1854.16, 1849.431, 1838.199, 1836.946, 1827.85, 1839.077, 1823.149, 1820.501, 1809.581, 1806.218, 1838.499, 1813.068, 1826.476, 1796.077, 1793.089, 1812.553, 1793.49, 1791.576, 1787.45, 1791.289, 1809.177, 1794.394, 1791.894, 1772.577, 1806.235, 1794.912, 1814.869, 1817.82, 1802.651, 1780.52, 1786.398, 1785.097, 1772.531, 1802.806, 1787.343, 1776.893, 1795.78, 1799.236, 1795.104, 1810.497, 1792.75, 1787.732, 1803.449, 1779.611, 1784.972, 1784.021, 1801.037, 1799.337, 1802.984, 1791.083, 1811.476, 1813.054, 1802.69, 1785.239, 1832.034, 1823.969, 1783.753, 1786.658, 1775.12, 1790.587, 1789.715, 1784.549, 1802.827, 1790.646, 1804.908, 1803.225, 1815.373, 1772.317, 1784.044, 1768.908]}	100	100	True
