id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.21492880952165166 batch_size:27 epochs:100	100	2000	True	32056.78711		617205	14	-1	273.4734845161438	{'train_loss': [416761.125, 338842.812, 319951.375, 310041.312, 302118.562, 296668.906, 292103.469, 287819.312, 285409.625, 281470.469, 278486.0, 275522.094, 271590.906, 269588.625, 267276.906, 265160.688, 263292.281, 261586.688, 259361.125, 258388.484, 256874.75, 255190.266, 253806.141, 252942.172, 251638.469, 249552.344, 249136.281, 247388.219, 246884.078, 245022.125, 244606.344, 243684.125, 242757.516, 241838.094, 241052.266, 239903.719, 238543.328, 237952.797, 238075.531, 237236.391, 236714.109, 236147.578, 235136.406, 234339.406, 233963.203, 232923.0, 232747.0, 231262.828, 231774.531, 230507.875, 230734.656, 229937.797, 229168.562, 229220.875, 228990.281, 227897.75, 227069.344, 226686.203, 226234.984, 225405.453, 225762.203, 224906.328, 224845.422, 223249.734, 223932.094, 223708.906, 222352.5, 222403.625, 221901.109, 222109.672, 221844.438, 220814.922, 220781.859, 220333.172, 219888.328, 219265.359, 219110.734, 219086.203, 218998.703, 217953.562, 217784.109, 217924.188, 217717.641, 216492.688, 216506.609, 215785.859, 215964.609, 215343.891, 215161.797, 214824.172, 214841.609, 215091.016, 214201.359, 212815.219, 213163.016, 212983.25, 213252.844, 212059.453, 212955.5, 211978.531], 'val_loss': [2609.846, 2406.727, 2353.682, 2278.934, 2267.193, 2237.059, 2236.264, 2187.418, 2186.588, 2175.247, 2127.332, 2097.87, 2097.818, 2062.582, 2052.712, 2046.311, 2039.824, 2017.985, 2018.137, 2017.012, 2003.484, 1986.913, 1970.355, 1973.896, 1961.29, 1955.732, 1945.784, 1944.593, 1933.977, 1918.295, 1925.53, 1906.502, 1900.438, 1893.487, 1895.497, 1888.245, 1878.437, 1882.754, 1884.223, 1874.223, 1866.309, 1887.522, 1884.511, 1877.473, 1871.673, 1864.076, 1853.793, 1860.606, 1850.981, 1851.695, 1854.628, 1878.13, 1857.78, 1858.553, 1846.09, 1841.156, 1830.73, 1837.047, 1854.721, 1867.422, 1835.393, 1863.046, 1837.102, 1813.111, 1814.544, 1821.881, 1795.671, 1827.984, 1823.179, 1823.746, 1809.744, 1830.806, 1814.856, 1837.45, 1808.265, 1804.844, 1799.311, 1801.811, 1795.832, 1798.601, 1801.253, 1803.553, 1788.716, 1791.46, 1794.725, 1814.463, 1787.503, 1813.912, 1808.378, 1775.184, 1803.94, 1790.226, 1796.084, 1796.132, 1790.395, 1788.727, 1807.666, 1817.327, 1789.453, 1805.771]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:37 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:117 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.21492880952165166 batch_size:27 epochs:100	100	2000	True	32564.93164		734485	14	-1	269.32823371887207	{'train_loss': [437737.094, 342197.875, 320986.125, 304713.812, 298270.75, 293879.031, 289890.656, 287738.031, 285032.75, 282845.969, 280046.75, 278207.594, 276002.688, 274252.75, 272280.125, 270127.5, 267772.688, 265650.625, 265120.031, 263020.125, 261827.094, 260408.141, 258815.328, 257898.594, 256879.125, 255169.094, 254010.188, 253112.172, 252798.469, 251637.922, 250499.062, 249981.297, 248836.5, 248964.656, 247556.438, 246739.312, 246469.484, 245983.0, 245494.984, 245185.031, 245006.312, 243721.859, 243226.516, 241854.234, 242048.984, 241469.875, 241631.234, 240427.625, 240244.266, 239376.359, 239360.359, 238626.75, 239401.922, 237939.953, 237464.203, 236745.906, 236751.156, 236090.047, 234781.938, 235634.484, 234889.547, 234221.203, 233540.219, 233270.406, 233528.516, 233142.609, 232527.719, 231667.703, 231446.172, 232031.109, 230532.625, 230370.547, 229993.047, 229396.484, 229490.688, 229344.859, 228719.156, 228304.812, 227785.703, 227456.844, 227050.188, 226277.094, 226391.625, 225880.828, 225822.75, 225949.969, 225622.109, 225032.016, 225285.453, 224816.141, 224125.594, 224522.781, 223048.125, 223363.953, 222758.547, 222431.719, 223041.359, 222185.297, 222331.641, 221612.703], 'val_loss': [2763.448, 2425.265, 2306.21, 2244.825, 2252.698, 2206.386, 2201.377, 2179.656, 2155.093, 2141.932, 2133.578, 2109.452, 2091.178, 2090.096, 2064.243, 2073.085, 2060.592, 2035.719, 2032.635, 2046.669, 2031.18, 2042.927, 2024.806, 2006.561, 2009.15, 1992.292, 1998.67, 1978.603, 1997.659, 1990.481, 1978.906, 1970.006, 1967.396, 1963.632, 1965.667, 1974.048, 1951.265, 1945.732, 1945.815, 1945.847, 1939.828, 1943.022, 1945.283, 1939.24, 1953.67, 1939.783, 1946.489, 1937.453, 1928.013, 1928.289, 1932.758, 1931.679, 1918.715, 1906.485, 1901.583, 1909.277, 1909.844, 1903.493, 1909.38, 1904.979, 1896.041, 1894.71, 1902.081, 1889.011, 1883.053, 1890.469, 1887.486, 1878.278, 1881.099, 1875.937, 1871.188, 1862.735, 1881.037, 1856.446, 1868.425, 1864.919, 1854.751, 1863.666, 1861.844, 1859.213, 1872.47, 1858.608, 1876.967, 1860.758, 1861.718, 1857.948, 1854.023, 1863.362, 1850.278, 1847.261, 1841.911, 1850.46, 1836.707, 1845.42, 1857.918, 1828.121, 1836.523, 1831.894, 1842.204, 1840.271]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:80 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:78 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.21492880952165166 alpha:0.9197087874022595 weight_decay:6.381945433882175e-05 batch_size:27 epochs:100	100	1000	True	258112.3125		102167004	14	-1	552.9841401576996	{'train_loss': [2183883.5, 1068809.75, 202145840.0, 203092544.0, 1066921.0, 1820255.0, 1068877888.0, 1066921.0, 6986252.5, 218898336.0, 1068013.125, 59189500.0, 1066921.0, 17230874.0, 27811270.0, 1066921.0, 853878400.0, 4589272.0, 5371878.0, 598292160.0, 1066921.0, 21647762.0, 218705504.0, 1068692.875, 15353486.0, 179599296.0, 9234611.0, 46041940.0, 102249912.0, 131582008.0, 22868230.0, 73691168.0, 150139520.0, 202713440.0, 51410908.0, 37130660.0, 61997332.0, 211041536.0, 26157406.0, 49415496.0, 137793056.0, 66063684.0, 34114632.0, 85852240.0, 314993440.0, 6883268.5, 142136416.0, 71914952.0, 130006432.0, 55616116.0, 57639132.0, 49503420.0, 92174240.0, 1066954.375, 142155792.0, 13318670.0, 23222590.0, 87648648.0, 91528968.0, 70754616.0, 48174056.0, 33614632.0, 72758272.0, 94169736.0, 32138738.0, 119529808.0, 53360500.0, 188589680.0, 19370154.0, 19587334.0, 83454456.0, 114222648.0, 9312127.0, 79135304.0, 73554840.0, 56229172.0, 45781740.0, 56931188.0, 117567920.0, 46796472.0, 17757906.0, 97353296.0, 1090446.0, 135970912.0, 41099344.0, 89009464.0, 139790560.0, 1068175.375, 36284264.0, 1301516.5, 63766620.0, 23111672.0, 33904988.0, 58471560.0, 88028416.0, 32381920.0, 53260452.0, 76051552.0, 91296880.0, 114908296.0], 'val_loss': [8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 23332.861, 8469.773, 8469.773, 8469.773, 8469.773, 42273.996, 70429.359, 8469.773, 418622.406, 8469.773, 17135.248, 19591.896, 8469.773, 14366.935, 352513.781, 2423424.5, 412244.5, 458819.344, 362210.156, 23595884.0, 6903785.5, 3021258.25, 313890.656, 387113.281, 8469.773, 28998.133, 483482.469, 337765.656, 15693343.0, 2999280.5, 4530361.0, 4207293.0, 6084232.5, 69241.172, 36438056.0, 63786.938, 8469.773, 8469.773, 8469.773, 8469.773, 6541110.5, 8978422.0, 1890716.0, 29412.811, 173274.75, 3290145.0, 120840.633, 3234078.0, 8469.773, 46409.875, 1061358.25, 10558201.0, 14051434.0, 9464.032, 46784.875, 12184.605, 8469.773, 8469.773, 8469.773, 8469.773, 10889.962, 40400.504, 41189.125, 31591.129, 8469.773, 8469.773, 2228397.0, 307503.844, 463704.938, 563578.5, 638063.5, 2971377.25, 530677.688, 16409.971, 8469.773, 8469.773, 8469.773, 8469.773, 1968706.25, 8469.773, 496844.75, 13529.185, 20893.312, 8469.773, 96640.977, 16320.367]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:53 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:conv1d out_channels:97 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.21492880952165166 beta1:0.8397829168246922 beta2:0.9351345839515198 weight_decay:0.0002785869725856322 batch_size:27 epochs:100	100	1000	True	137369.95312		16220277	15	-1	261.07043623924255	{'train_loss': [1552221.0, 1066981.875, 12805669.0, 1734493.75, 6016707.0, 2332086.75, 1836908.625, 4565366.5, 1311395.625, 5692585.5, 2931155.5, 1372555.875, 9343382.0, 1968557.75, 3444043.5, 4019715.5, 1620303.0, 7462907.5, 2386762.25, 1529275.625, 4602574.5, 3288037.5, 3474982.0, 3220444.5, 1785189.0, 4446844.5, 4682392.5, 1497398.0, 11658906.0, 1066921.0, 2121570.5, 3461973.0, 1066921.0, 2072539.5, 3922616.75, 1068283.625, 3333327.75, 1809316.0, 1103438.0, 3834822.5, 1615799.75, 1140792.5, 3597877.25, 1126876.5, 1369548.875, 2789209.75, 1259031.375, 1544444.0, 3856432.75, 1372647.125, 3267460.0, 5349697.0, 1573710.5, 3607720.0, 7785654.0, 3178036.5, 4061232.5, 1297968.0, 6396940.5, 2159168.0, 1132337.125, 2948738.5, 1821599.5, 1196768.125, 7134443.0, 3650432.75, 1374429.0, 3768042.5, 3081187.0, 1470541.5, 4205213.0, 1292850.625, 2743959.0, 6332260.5, 1088802.125, 7311493.0, 1609589.0, 1115365.25, 6543490.5, 1252044.625, 6272677.0, 3963660.5, 1201961.625, 2302872.0, 1479857.75, 1592478.25, 2528848.0, 1405205.75, 1586165.5, 4625724.5, 1242052.875, 4108722.5, 1709314.625, 1231375.625, 6043182.5, 2846076.75, 1175827.375, 7225028.0, 1524463.375, 1479066.625], 'val_loss': [8469.773, 8469.773, 8469.773, 108587.445, 517765.438, 8469.773, 8469.773, 8469.773, 8469.773, 54785.762, 8469.773, 8469.773, 170747.578, 13284.291, 8469.773, 8469.773, 8469.773, 8469.771, 8469.773, 26851.209, 23444.938, 8469.773, 8642.979, 8469.773, 8737.009, 8469.773, 8469.773, 8469.756, 20413.504, 8469.773, 8469.773, 8469.773, 8469.773, 27354.725, 140737.969, 8469.773, 8469.773, 8469.773, 8469.773, 307415.031, 8469.773, 8469.773, 33098.461, 378518.469, 8469.773, 216254.797, 8469.773, 8551.345, 8469.773, 8469.773, 24367.27, 43815.16, 8469.773, 122724.797, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 67498.789, 2136873.25, 8469.773, 8469.773, 34095.902, 12900.5, 169677.188, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 10850.049, 8469.773, 17670.867, 36763.145, 8469.773, 8469.773, 8469.773, 8469.773, 2658775.0, 8469.773, 8469.773, 368552.719, 308642.25, 14467.5, 463230.0, 8469.773, 22387.66, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 9185.197, 8469.773, 8469.773]}	100	100	True
