id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	1000	True	30752.54492		675818	15	-1	310.64976835250854	{'train_loss': [444811.031, 379513.812, 348563.281, 336423.219, 328266.219, 322002.469, 315482.594, 308431.469, 302008.188, 295821.906, 290108.969, 285313.0, 279690.312, 274315.656, 269843.438, 266514.406, 262327.656, 259311.281, 257091.141, 255072.484, 251716.344, 250987.5, 249298.828, 246620.25, 245292.406, 244015.062, 241634.734, 240223.188, 240501.766, 239668.406, 238059.688, 236730.75, 235337.25, 234955.438, 235049.547, 233329.047, 232801.562, 232282.188, 231814.078, 231739.75, 230565.375, 229067.797, 228328.031, 227915.172, 227742.672, 226297.875, 226197.812, 226277.641, 225316.062, 225243.078, 223374.609, 224403.609, 223573.062, 223492.938, 222140.375, 221876.25, 221921.203, 220498.219, 221278.734, 220205.375, 220459.469, 220018.891, 219917.422, 218065.828, 218682.453, 218555.594, 217828.719, 217535.703, 217520.281, 216604.578, 216003.094, 216162.312, 215734.438, 215008.188, 214913.031, 215592.891, 214792.062, 214351.078, 213195.281, 212884.203, 213983.047, 213995.906, 212555.078, 213532.703, 213514.781, 212417.703, 212677.344, 212304.609, 211869.859, 211098.125, 210368.812, 210460.047, 209944.328, 210664.797, 210353.188, 210230.25, 209976.719, 208614.953, 209941.922, 209505.828], 'val_loss': [2739.417, 2446.74, 2311.036, 2270.304, 2224.321, 2201.968, 2156.29, 2115.37, 2082.578, 2060.363, 2038.711, 2019.512, 1982.053, 1946.392, 1884.561, 1874.25, 1848.715, 1851.331, 1831.927, 1821.162, 1805.645, 1779.924, 1777.559, 1785.843, 1751.995, 1767.62, 1747.718, 1764.431, 1732.521, 1760.509, 1740.587, 1738.932, 1727.85, 1726.014, 1727.341, 1721.337, 1717.309, 1720.42, 1718.531, 1727.367, 1710.813, 1705.181, 1709.781, 1700.902, 1693.361, 1674.836, 1687.235, 1718.453, 1696.453, 1702.95, 1700.651, 1700.017, 1696.774, 1679.011, 1702.209, 1686.922, 1708.683, 1696.546, 1684.807, 1697.014, 1700.092, 1706.776, 1690.176, 1702.984, 1691.399, 1677.85, 1684.667, 1683.335, 1686.596, 1689.786, 1673.135, 1693.32, 1710.154, 1706.828, 1656.893, 1673.03, 1670.625, 1668.291, 1655.024, 1685.455, 1676.124, 1668.635, 1680.725, 1679.448, 1690.861, 1695.306, 1678.742, 1665.56, 1654.471, 1676.615, 1656.755, 1655.105, 1656.207, 1661.298, 1694.416, 1679.654, 1669.717, 1646.269, 1657.624, 1653.407]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:42 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:6 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:43 kernel_size:10 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:10 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	1000	True	30832.74414		1566717	16	-1	352.33987522125244	{'train_loss': [506845.156, 484825.875, 452302.281, 378548.562, 334048.906, 309073.219, 294627.844, 284068.062, 277675.75, 271183.219, 266552.969, 263693.281, 259977.078, 257257.281, 254532.891, 252551.719, 250394.172, 248037.594, 246773.625, 244521.078, 243067.312, 241422.344, 240203.109, 238423.344, 237388.453, 235791.969, 235429.969, 233565.141, 232600.625, 231184.844, 230766.562, 229924.062, 228473.531, 226948.641, 227203.781, 225854.656, 224570.844, 224546.109, 222629.234, 221969.719, 221810.547, 220929.984, 220514.359, 219928.047, 219286.312, 217576.75, 217847.312, 217216.453, 216450.656, 215227.25, 215764.172, 215492.703, 214941.328, 213648.547, 213780.234, 212384.766, 212323.906, 211338.469, 211444.375, 210344.484, 210474.234, 210228.359, 209213.781, 209220.766, 207798.359, 207894.188, 207821.172, 207846.094, 207038.969, 205926.391, 205242.766, 204910.5, 204442.953, 203969.344, 204210.953, 203884.641, 203504.188, 201881.219, 202705.297, 201488.312, 202176.531, 201344.766, 201063.0, 200526.359, 200712.562, 200242.234, 199260.672, 199582.531, 198211.281, 197937.531, 198516.344, 199337.938, 196388.438, 196310.406, 196348.828, 197778.875, 195629.359, 195888.281, 195261.328, 196578.359], 'val_loss': [2650.285, 2608.001, 2440.622, 2275.495, 2191.53, 2094.122, 2058.546, 2036.405, 1987.205, 1959.845, 1928.102, 1889.927, 1893.366, 1865.742, 1851.454, 1828.359, 1817.253, 1812.588, 1804.907, 1782.369, 1786.599, 1781.879, 1784.093, 1779.554, 1780.354, 1763.329, 1774.182, 1755.306, 1750.176, 1747.358, 1747.638, 1759.888, 1732.749, 1739.516, 1714.565, 1749.619, 1741.474, 1719.716, 1745.754, 1744.653, 1728.626, 1728.007, 1725.143, 1724.797, 1708.638, 1701.439, 1729.415, 1704.315, 1715.261, 1715.932, 1719.78, 1717.533, 1686.484, 1700.586, 1695.211, 1701.264, 1689.89, 1687.349, 1676.586, 1688.542, 1684.738, 1677.953, 1687.725, 1685.29, 1686.481, 1680.314, 1697.479, 1682.853, 1676.811, 1690.431, 1699.972, 1683.537, 1686.565, 1672.373, 1680.204, 1690.845, 1674.605, 1659.692, 1674.961, 1662.069, 1667.843, 1681.197, 1673.241, 1651.197, 1675.31, 1674.115, 1672.111, 1660.456, 1666.64, 1676.975, 1672.306, 1656.354, 1658.733, 1662.853, 1649.224, 1668.051, 1657.59, 1675.671, 1656.844, 1651.356]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	1000	True	31862.79688		406007	14	-1	271.51274847984314	{'train_loss': [374196.938, 332364.25, 323223.844, 318431.75, 315702.938, 313458.438, 311080.219, 308691.656, 306021.594, 303630.062, 301894.875, 299980.906, 298526.312, 296933.688, 295291.875, 293097.969, 290836.094, 288241.062, 283932.469, 281243.125, 278659.781, 275731.656, 273718.812, 271731.594, 269161.25, 267286.656, 267264.594, 264663.406, 263055.094, 262403.688, 261052.016, 259506.359, 259061.078, 257724.438, 256726.406, 255764.062, 254079.219, 252960.125, 251455.594, 250379.734, 250722.75, 249016.344, 247981.828, 247728.312, 246540.797, 245772.031, 245693.875, 244404.391, 243668.594, 242904.281, 242389.234, 241690.547, 240757.125, 240245.453, 239985.031, 238833.484, 238601.953, 237868.953, 237215.781, 237302.719, 236643.594, 235317.781, 235708.375, 235620.578, 235632.109, 234083.141, 234773.859, 234164.984, 232838.281, 233699.359, 233326.984, 232298.688, 231462.766, 231499.062, 231217.125, 230128.625, 229823.297, 228464.484, 229072.156, 229081.641, 229090.047, 227957.938, 229266.375, 228468.094, 228141.891, 228336.188, 227997.641, 227475.953, 226759.0, 226390.109, 225927.0, 225983.062, 226355.547, 225669.578, 225334.531, 224137.047, 223843.844, 224567.312, 223740.484, 224498.672], 'val_loss': [2329.782, 2270.767, 2236.747, 2205.581, 2196.236, 2182.995, 2169.894, 2156.08, 2147.442, 2140.249, 2126.593, 2125.111, 2109.9, 2107.633, 2099.249, 2084.581, 2067.848, 2037.154, 2006.071, 1985.096, 1960.877, 1935.715, 1925.621, 1917.108, 1897.949, 1878.296, 1879.708, 1885.232, 1855.086, 1877.32, 1841.692, 1832.195, 1821.145, 1829.101, 1812.157, 1836.895, 1795.146, 1796.243, 1810.607, 1794.431, 1798.507, 1784.326, 1795.637, 1763.33, 1782.186, 1772.203, 1779.612, 1759.117, 1741.318, 1755.949, 1755.347, 1736.102, 1728.73, 1735.737, 1709.18, 1734.922, 1760.091, 1710.583, 1731.38, 1738.873, 1730.103, 1721.725, 1745.889, 1716.883, 1719.552, 1703.834, 1726.323, 1697.573, 1686.906, 1715.509, 1683.989, 1722.287, 1694.004, 1693.042, 1704.499, 1687.316, 1731.657, 1695.408, 1721.751, 1710.327, 1706.522, 1682.252, 1699.891, 1718.669, 1688.761, 1686.89, 1670.91, 1689.715, 1688.615, 1683.776, 1704.74, 1674.41, 1717.052, 1683.072, 1676.855, 1674.659, 1660.572, 1662.678, 1659.038, 1660.925]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:100 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:100 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.2103871844621022 beta1:0.8704595569378917 beta2:0.9670488935119623 weight_decay:1.1142319844147528e-05 batch_size:24 epochs:100	100	1000	True	137634.03125		16956888	14	-1	318.43516659736633	{'train_loss': [1192346.375, 1066921.125, 1066921.125, 1066921.125, 2010933.5, 9523629.0, 1066921.125, 1066921.125, 1066921.125, 1107057.75, 6289505.5, 1067017.25, 1066921.125, 1133539.875, 1066921.125, 1737615.75, 2381061.25, 1438464.875, 1066921.125, 1066921.125, 1287225.625, 1426271.875, 3108900.0, 3317719.5, 3637430.0, 1066942.0, 1076036.25, 1176652.875, 1863922.5, 2346049.25, 1642029.375, 1858034.875, 1067604.75, 1091197.875, 1245312.125, 1480755.75, 1841584.5, 1903909.75, 2065894.125, 1416948.875, 1102812.875, 1410984.125, 1089866.125, 1883138.75, 2973953.25, 2092716.5, 1977502.0, 1221993.375, 1196930.0, 1415185.375, 3285197.5, 1345913.125, 1606588.875, 1280218.375, 1066924.25, 1452632.375, 1573475.125, 1752165.625, 1204775.0, 1244754.125, 1439544.0, 1264879.5, 1250635.125, 1546431.25, 1297381.0, 1726828.5, 1083532.375, 1083769.75, 1185097.0, 1638026.0, 1439349.5, 1599602.25, 1397037.625, 1288524.0, 2177449.25, 1734707.125, 1085654.125, 1749086.625, 1558693.875, 1491435.375, 1207270.75, 1330730.625, 1263565.125, 1110643.375, 1128475.5, 1286476.875, 2407947.5, 1697835.875, 1111070.125, 1643784.375, 1417234.75, 1185235.0, 1276208.0, 1251313.25, 1330711.5, 1576090.625, 1300892.25, 1214722.75, 1579043.375, 1172820.5], 'val_loss': [7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 90783.219, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 164331.688, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 31015.934, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411]}	100	100	True
