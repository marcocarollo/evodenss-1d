id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	1000	True	31688.7168		675818	15	-1	313.266663312912	{'train_loss': [446656.125, 354592.188, 327016.906, 312135.469, 302967.156, 296698.781, 290958.0, 284904.812, 281446.031, 277994.344, 274935.406, 272390.906, 270276.406, 269219.125, 267394.719, 265099.406, 262889.875, 262866.406, 260135.922, 259275.562, 258611.953, 257097.141, 256763.25, 254182.922, 253708.531, 253492.422, 251407.812, 250414.5, 249961.859, 248624.547, 248990.812, 247231.062, 247327.766, 245056.953, 245681.469, 244880.219, 243687.516, 243237.672, 243123.781, 242637.703, 241645.859, 241277.875, 239978.297, 239866.891, 239635.047, 238622.828, 237985.453, 237078.406, 237727.031, 236544.016, 236757.297, 236198.812, 235133.531, 235421.5, 234284.406, 235379.453, 234006.625, 234257.094, 232105.438, 232769.328, 232426.438, 231776.031, 231439.469, 232119.156, 230060.891, 230192.562, 230631.438, 229732.406, 229782.484, 229378.406, 228924.234, 228567.781, 228204.094, 228346.875, 227836.469, 227676.172, 227329.438, 227975.391, 227348.281, 227235.844, 225833.172, 225438.422, 225499.25, 225793.672, 224932.656, 225286.25, 224609.766, 223854.141, 224270.594, 224330.688, 223816.406, 223441.047, 222299.438, 223111.484, 222864.656, 222700.094, 221261.578, 222363.969, 221352.484, 220681.297], 'val_loss': [2675.145, 2333.948, 2207.769, 2147.54, 2115.757, 2081.979, 2033.308, 2006.389, 1968.505, 1961.143, 1962.8, 1962.881, 1933.583, 1926.362, 1916.63, 1919.327, 1909.026, 1888.232, 1875.881, 1884.259, 1866.507, 1871.547, 1862.349, 1860.342, 1855.24, 1848.138, 1850.227, 1854.745, 1846.747, 1835.184, 1848.851, 1818.539, 1815.581, 1823.349, 1821.171, 1832.606, 1818.217, 1807.939, 1809.263, 1811.787, 1797.818, 1789.135, 1786.137, 1790.829, 1792.486, 1781.694, 1784.261, 1775.453, 1781.121, 1771.622, 1787.263, 1798.218, 1783.364, 1788.986, 1777.315, 1794.127, 1769.838, 1776.355, 1767.857, 1759.925, 1771.583, 1778.743, 1752.375, 1767.586, 1763.016, 1752.086, 1758.527, 1754.078, 1756.236, 1757.987, 1747.826, 1747.386, 1742.427, 1748.09, 1746.021, 1729.699, 1734.432, 1728.698, 1737.347, 1727.229, 1728.823, 1738.833, 1732.435, 1739.757, 1738.829, 1734.865, 1731.696, 1730.113, 1719.935, 1746.39, 1720.477, 1739.649, 1728.967, 1723.342, 1735.014, 1737.784, 1726.65, 1719.784, 1750.784, 1725.375]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:19 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:19 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.2103871844621022 beta1:0.9830475692422084 beta2:0.8080272467225751 weight_decay:6.71945431049287e-05 batch_size:24 epochs:100	100	1000	True	104160704.0		6570102	15	-1	334.3330738544464	{'train_loss': [5045257.0, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 19189880.0, 91506552.0, 79555760.0, 79217184.0, 65471560.0, 10843623.0, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 2415071.0, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125], 'val_loss': [7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:63 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:88 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	1000	True	32753.64453		673313	14	-1	288.2634987831116	{'train_loss': [441524.062, 351175.438, 325568.812, 311996.344, 303923.688, 298578.062, 293149.438, 289641.156, 285634.625, 282406.375, 280387.438, 277664.062, 276393.094, 273025.438, 272452.406, 269929.0, 268776.469, 267889.031, 265967.344, 265244.969, 263802.062, 261954.969, 261936.547, 261836.875, 260149.438, 259479.406, 258316.891, 256232.969, 256971.188, 256239.016, 255149.328, 254719.547, 254009.812, 253753.953, 252354.75, 252399.578, 251846.188, 250502.016, 250381.766, 250255.984, 250800.531, 250138.422, 249227.0, 248660.828, 248538.172, 247543.625, 247194.781, 247008.688, 246926.609, 245983.469, 245760.609, 244380.125, 245086.766, 246058.141, 244967.891, 244716.234, 243751.828, 243212.297, 242904.016, 242258.312, 243045.031, 242382.359, 241906.766, 241591.406, 241379.516, 241193.828, 240316.891, 240173.641, 239583.734, 238900.75, 239631.781, 239111.578, 238443.438, 238419.891, 238640.125, 238181.297, 237521.578, 237681.25, 236246.203, 236992.109, 236798.875, 236553.641, 237113.438, 235796.359, 234860.172, 235208.094, 235640.047, 234941.25, 233835.062, 233746.5, 233374.812, 234524.656, 233441.906, 232198.969, 233490.062, 232290.531, 232549.688, 232703.438, 232665.797, 231822.766], 'val_loss': [2690.831, 2311.053, 2202.334, 2158.633, 2125.463, 2078.146, 2059.694, 2030.348, 2021.414, 2008.176, 1987.475, 1963.207, 1950.251, 1938.967, 1929.447, 1928.763, 1927.372, 1913.495, 1897.877, 1895.027, 1875.608, 1885.988, 1871.709, 1858.125, 1849.531, 1881.01, 1840.525, 1843.841, 1845.841, 1836.969, 1836.747, 1828.285, 1820.17, 1842.793, 1839.808, 1824.035, 1828.135, 1813.694, 1817.541, 1809.648, 1811.927, 1819.942, 1791.124, 1799.155, 1796.552, 1789.789, 1793.121, 1777.836, 1783.218, 1790.229, 1782.875, 1784.201, 1791.822, 1782.251, 1772.338, 1764.261, 1772.94, 1777.211, 1764.092, 1764.193, 1762.252, 1783.353, 1769.619, 1770.655, 1754.691, 1759.006, 1758.827, 1756.646, 1745.945, 1752.206, 1758.95, 1753.227, 1746.11, 1738.885, 1743.659, 1740.051, 1738.039, 1734.594, 1741.544, 1742.259, 1738.597, 1731.476, 1731.208, 1725.379, 1735.253, 1719.913, 1730.965, 1715.382, 1719.326, 1715.325, 1714.601, 1735.691, 1739.372, 1720.805, 1711.375, 1720.534, 1719.335, 1718.23, 1722.644, 1725.529]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:70 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:70 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:conv1d out_channels:91 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	1000	True	33014.05859		339148	14	-1	273.20984721183777	{'train_loss': [383876.0, 333609.281, 320777.562, 313614.219, 307634.406, 303255.219, 299889.781, 297983.844, 295888.031, 293878.25, 292019.438, 289996.688, 288456.594, 287226.875, 285561.312, 283602.031, 282394.594, 281874.719, 280809.781, 279347.344, 278971.188, 277530.531, 276503.906, 275194.5, 274263.656, 273551.156, 273243.469, 271322.938, 270893.062, 270141.719, 268425.562, 269268.0, 267618.031, 266988.281, 266544.562, 265588.719, 264698.719, 263642.469, 264712.875, 263798.062, 263100.469, 262851.688, 261674.438, 260419.734, 260978.875, 259759.516, 259310.031, 258453.312, 258802.531, 257839.406, 257110.547, 256147.375, 256197.203, 255562.75, 255799.406, 254595.969, 255095.812, 253819.672, 254077.219, 253609.016, 253343.094, 252799.656, 251996.734, 251497.609, 251989.578, 251447.422, 251079.469, 250990.922, 250308.938, 250160.578, 249644.672, 249757.562, 248644.156, 248383.109, 248350.984, 248818.891, 248324.797, 248460.078, 247471.125, 246942.688, 247877.234, 246927.641, 246554.375, 246633.375, 246905.047, 246442.25, 246478.812, 246414.672, 244168.719, 244957.312, 244816.875, 245622.281, 243849.141, 244129.203, 244259.625, 244431.766, 242774.391, 243398.766, 243141.812, 243712.484], 'val_loss': [2342.338, 2238.334, 2195.559, 2145.492, 2120.776, 2098.756, 2090.408, 2080.857, 2074.018, 2069.166, 2059.221, 2056.699, 2046.417, 2038.677, 2038.36, 2025.887, 2014.999, 2009.69, 2007.905, 1994.714, 1985.666, 1980.401, 1972.395, 1961.869, 1961.677, 1951.67, 1946.412, 1944.945, 1935.358, 1937.154, 1929.002, 1913.406, 1919.097, 1905.438, 1899.595, 1904.002, 1890.443, 1893.503, 1882.972, 1872.86, 1878.883, 1870.286, 1864.285, 1858.187, 1856.134, 1852.889, 1841.022, 1838.726, 1838.746, 1830.058, 1821.49, 1825.504, 1807.788, 1814.646, 1818.346, 1821.095, 1798.011, 1790.816, 1776.231, 1792.598, 1785.901, 1785.213, 1770.77, 1776.958, 1774.507, 1770.824, 1764.674, 1762.442, 1760.284, 1775.054, 1754.859, 1746.569, 1738.591, 1748.365, 1744.328, 1746.892, 1728.672, 1733.263, 1728.959, 1721.132, 1735.309, 1727.022, 1731.717, 1731.817, 1720.687, 1727.655, 1727.151, 1720.921, 1706.526, 1714.642, 1712.344, 1705.338, 1697.693, 1705.42, 1705.605, 1712.498, 1695.661, 1704.766, 1697.853, 1701.03]}	100	100	True
