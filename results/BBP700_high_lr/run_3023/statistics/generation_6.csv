id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.1864603712075097 batch_size:27 epochs:100	100	1000	True	31624.13867		642608	15	-1	320.8049566745758	{'train_loss': [443454.281, 354505.406, 329751.531, 316688.062, 310262.75, 304208.312, 299886.594, 294167.375, 290705.875, 286886.531, 283590.656, 280699.469, 277624.094, 275535.625, 273337.281, 271435.281, 269818.812, 268380.969, 266510.312, 264639.625, 262457.0, 261162.422, 260612.25, 257291.984, 257143.906, 256191.141, 255243.469, 253872.156, 253459.375, 251351.031, 250740.141, 250328.828, 249131.109, 248332.328, 247812.859, 246596.766, 246210.078, 244654.828, 244116.656, 243744.422, 243205.297, 241634.219, 241498.625, 240509.109, 239620.25, 239472.375, 238999.672, 238435.328, 237325.719, 236399.078, 235854.688, 235803.031, 235076.609, 234170.938, 233860.609, 233769.109, 233570.906, 232281.5, 232451.953, 231679.938, 230805.25, 230294.781, 229813.375, 229391.172, 228466.672, 229117.906, 227042.516, 227633.125, 226761.281, 227241.656, 227331.578, 226234.391, 226835.672, 225609.375, 225504.25, 225465.812, 223887.344, 224214.344, 223920.625, 223526.547, 223251.156, 223239.578, 223396.047, 222587.719, 222387.047, 221259.125, 221309.312, 221192.547, 221081.328, 220547.828, 219968.672, 219332.875, 219755.281, 219494.672, 218903.203, 218763.422, 219553.969, 217978.016, 218345.297, 217930.641], 'val_loss': [2871.437, 2488.473, 2403.725, 2395.942, 2300.312, 2265.498, 2242.099, 2207.771, 2154.882, 2126.923, 2132.872, 2120.48, 2104.311, 2077.384, 2070.729, 2089.289, 2069.66, 2067.874, 2044.776, 2038.038, 2021.656, 2041.289, 2003.562, 2005.951, 2004.784, 1991.341, 1988.584, 2010.483, 1996.476, 1977.212, 1988.362, 1979.683, 1985.352, 1973.231, 1955.768, 1967.15, 1951.126, 1993.689, 1946.646, 1942.949, 1939.494, 1946.42, 1919.216, 1939.456, 1918.473, 1911.306, 1917.877, 1939.017, 1904.098, 1908.782, 1884.383, 1875.522, 1894.825, 1865.236, 1873.762, 1861.876, 1859.367, 1867.248, 1853.034, 1851.686, 1866.479, 1850.871, 1875.276, 1853.609, 1854.45, 1838.113, 1851.903, 1838.101, 1846.547, 1826.939, 1827.383, 1825.639, 1835.855, 1836.355, 1833.921, 1839.975, 1825.379, 1843.819, 1830.366, 1834.679, 1819.782, 1820.916, 1869.641, 1828.668, 1835.475, 1829.777, 1837.951, 1823.025, 1825.484, 1848.89, 1815.669, 1814.513, 1821.479, 1806.03, 1814.158, 1807.171, 1814.218, 1802.545, 1793.874, 1798.724]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.21492880952165166 batch_size:27 epochs:100	100	1000	True	31531.74805		617205	14	-1	294.98754358291626	{'train_loss': [447691.406, 355226.875, 331683.312, 317218.5, 306469.906, 297991.656, 290971.312, 284338.094, 279267.031, 275908.344, 271870.969, 270042.844, 267570.75, 265224.281, 262785.75, 260780.562, 259382.344, 257582.734, 255770.516, 253739.234, 253067.719, 251169.547, 250916.406, 249363.391, 248403.453, 247437.312, 246014.328, 245865.594, 244863.703, 243544.531, 242884.594, 242362.969, 241260.938, 240513.281, 239310.891, 239334.203, 238041.266, 237936.453, 237114.953, 236267.469, 235511.922, 235011.359, 233638.938, 233605.406, 232904.953, 232487.203, 231269.484, 231122.875, 230951.453, 229921.688, 229219.484, 228731.625, 229250.734, 228326.688, 228098.891, 226957.562, 226377.75, 226829.688, 225575.609, 224984.859, 225391.375, 225087.688, 224107.672, 223780.719, 223721.281, 222371.0, 222814.438, 222464.891, 221290.25, 221924.844, 220939.078, 220424.797, 220285.984, 219329.391, 220281.219, 218480.828, 218611.938, 217918.0, 218353.688, 217634.812, 217017.109, 217182.281, 216853.266, 215925.641, 215445.328, 215479.703, 214393.266, 214855.016, 214560.766, 214362.609, 213736.344, 213881.422, 214085.281, 212815.625, 213321.234, 212835.766, 211788.188, 212870.375, 212161.75, 211620.141], 'val_loss': [2746.406, 2561.773, 2416.77, 2306.402, 2248.08, 2196.81, 2140.584, 2115.612, 2091.436, 2090.132, 2104.403, 2052.813, 2032.961, 2021.216, 2007.062, 2001.768, 1997.647, 1960.292, 1949.562, 1943.988, 1949.668, 1925.201, 1900.412, 1937.212, 1910.135, 1903.661, 1896.338, 1894.49, 1887.86, 1899.758, 1893.788, 1882.587, 1875.728, 1877.688, 1866.669, 1876.437, 1862.606, 1856.442, 1872.519, 1854.524, 1873.43, 1861.443, 1855.652, 1856.341, 1846.633, 1838.977, 1841.358, 1845.462, 1832.376, 1841.714, 1829.986, 1833.405, 1824.688, 1808.251, 1818.19, 1829.245, 1826.679, 1836.715, 1832.131, 1808.271, 1793.354, 1813.488, 1795.841, 1811.519, 1824.685, 1809.381, 1810.093, 1810.44, 1809.891, 1823.057, 1808.446, 1809.931, 1802.005, 1808.411, 1798.906, 1801.167, 1811.333, 1796.316, 1807.545, 1823.91, 1798.136, 1797.784, 1811.841, 1802.515, 1805.342, 1794.953, 1776.095, 1787.839, 1785.981, 1794.511, 1788.226, 1789.141, 1790.189, 1797.104, 1795.274, 1797.868, 1780.398, 1780.901, 1804.259, 1773.123]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:119 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:101 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:90 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1864603712075097 batch_size:17 epochs:100	100	1000	True	35576.69141		29172998	14	-1	483.145991563797	{'train_loss': [1038900.438, 986877.812, 1021152.562, 1035468.812, 1040289.75, 1041694.5, 1040987.75, 1036718.938, 1028480.188, 1014759.562, 978818.312, 864912.75, 467354.438, 303191.781, 292265.812, 285675.844, 279765.344, 276201.438, 272392.406, 270445.844, 267680.906, 264797.656, 262320.469, 259790.766, 256485.078, 254472.781, 252077.484, 249514.594, 247759.422, 244667.5, 243002.953, 241623.609, 240123.078, 237554.672, 235657.516, 233721.562, 231748.234, 230303.438, 228118.953, 225814.344, 225043.812, 223592.422, 220948.438, 219604.969, 218560.484, 217168.969, 214468.141, 213807.609, 212199.609, 210220.328, 209374.562, 208212.297, 206105.609, 205014.984, 204692.203, 203543.031, 201635.0, 201451.859, 200002.188, 198770.156, 197148.234, 196122.734, 196175.031, 195112.062, 193518.422, 193134.219, 191777.453, 191033.641, 189765.672, 189109.203, 189106.094, 188318.938, 187994.453, 185740.062, 185632.859, 184948.375, 183996.469, 183452.203, 182432.297, 182212.672, 182341.391, 181325.625, 181054.891, 180560.609, 178862.641, 178228.391, 178422.984, 178772.859, 177725.453, 177858.812, 176786.766, 176265.641, 175898.891, 174040.312, 174427.922, 173710.672, 172935.953, 172801.234, 173117.297, 172729.781], 'val_loss': [5206.01, 5568.227, 5630.306, 5717.65, 5829.24, 5419.713, 5548.475, 5470.934, 5410.998, 5031.501, 4423.146, 3096.946, 1508.538, 1454.28, 1427.158, 1406.302, 1386.851, 1378.969, 1350.148, 1366.201, 1345.984, 1334.102, 1321.865, 1321.71, 1303.163, 1313.419, 1295.315, 1295.52, 1278.45, 1286.945, 1296.844, 1277.079, 1286.985, 1282.393, 1288.487, 1295.036, 1287.882, 1299.595, 1292.722, 1319.963, 1289.744, 1301.945, 1296.046, 1293.78, 1302.202, 1304.684, 1317.488, 1314.945, 1309.489, 1300.992, 1328.203, 1317.933, 1344.843, 1347.514, 1323.429, 1341.016, 1313.44, 1343.474, 1336.932, 1341.718, 1353.652, 1350.693, 1349.766, 1364.583, 1347.641, 1347.38, 1337.478, 1344.795, 1332.922, 1347.831, 1338.386, 1356.072, 1351.424, 1352.138, 1354.217, 1362.912, 1351.989, 1348.309, 1359.313, 1387.339, 1347.719, 1333.466, 1361.403, 1360.046, 1363.784, 1386.504, 1329.508, 1374.711, 1370.824, 1346.553, 1358.44, 1372.485, 1354.219, 1360.574, 1352.063, 1348.353, 1369.507, 1354.052, 1336.932, 1352.317]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:73 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.1836256046164309 batch_size:27 epochs:100	100	1000	True	32452.71484		986731	16	-1	347.4578101634979	{'train_loss': [488051.562, 421408.125, 345421.344, 323164.188, 312002.75, 304940.062, 299899.656, 293794.812, 289930.844, 285250.531, 281279.156, 278597.969, 275578.875, 273224.594, 271269.844, 269532.656, 266720.281, 264641.188, 261956.625, 259896.422, 259169.266, 256161.562, 254796.703, 254181.609, 253612.922, 251231.141, 249291.359, 249605.094, 247983.312, 246826.391, 245457.906, 244100.844, 243339.031, 242192.234, 241368.172, 241430.531, 240096.891, 239441.188, 237942.312, 237297.984, 236882.531, 236095.141, 235853.031, 234194.906, 234513.016, 232771.531, 232278.312, 231962.688, 232424.156, 232170.766, 231087.719, 230485.328, 230014.953, 229173.781, 228838.219, 228857.891, 227772.031, 228304.922, 227630.016, 226273.016, 226412.141, 226547.016, 225521.859, 225462.656, 224733.688, 225306.484, 224332.062, 223009.688, 222946.391, 222291.266, 221895.859, 222205.828, 221575.641, 220353.938, 220464.109, 219384.891, 219823.25, 218826.312, 219443.688, 219243.781, 217906.891, 218291.656, 217165.578, 217459.922, 217310.188, 216728.594, 216850.609, 216115.25, 215373.203, 215539.672, 215217.188, 214569.203, 214502.25, 213893.453, 213908.828, 213599.609, 213594.891, 213351.266, 213127.375, 211816.641], 'val_loss': [3584.906, 2824.112, 2469.679, 2408.717, 2300.628, 2272.248, 2232.833, 2208.814, 2204.76, 2161.951, 2122.97, 2099.192, 2084.144, 2072.178, 2055.115, 2035.946, 2021.991, 2011.817, 1997.377, 2020.003, 2004.221, 2008.511, 1964.23, 1967.032, 1978.22, 1967.908, 1949.637, 1944.122, 1918.453, 1933.221, 1920.184, 1904.378, 1893.454, 1907.598, 1897.191, 1893.237, 1899.242, 1872.198, 1878.067, 1883.965, 1866.932, 1882.376, 1862.265, 1865.65, 1852.034, 1855.551, 1855.811, 1860.939, 1851.276, 1842.621, 1835.838, 1841.655, 1837.577, 1845.781, 1838.788, 1841.513, 1838.035, 1833.75, 1826.245, 1838.806, 1827.239, 1815.245, 1812.616, 1819.049, 1810.29, 1821.712, 1805.123, 1796.636, 1800.356, 1819.474, 1801.896, 1814.653, 1801.801, 1791.412, 1807.39, 1801.041, 1797.926, 1800.194, 1793.681, 1806.894, 1795.458, 1798.92, 1808.253, 1817.358, 1789.059, 1807.886, 1795.972, 1793.569, 1802.481, 1800.799, 1810.776, 1788.288, 1798.026, 1779.047, 1792.114, 1795.817, 1784.478, 1786.317, 1779.341, 1794.046]}	100	100	True
