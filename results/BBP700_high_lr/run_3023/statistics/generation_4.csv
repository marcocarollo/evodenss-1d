id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.1864603712075097 batch_size:19 epochs:100	100	2000	True	32044.27148		429236	16	-1	396.01600527763367	{'train_loss': [370804.156, 323618.094, 312535.812, 306698.094, 302168.938, 298185.594, 295073.562, 292798.688, 289760.062, 286809.844, 285917.531, 284081.844, 282942.438, 281108.938, 279825.094, 278616.312, 276976.438, 275766.062, 274978.156, 273866.938, 272284.0, 271498.031, 271391.625, 270219.781, 269642.656, 268000.812, 267060.344, 267060.062, 266294.156, 265695.625, 264476.438, 264439.656, 263146.0, 263377.75, 263076.781, 261616.0, 260724.062, 260463.547, 259693.953, 258161.578, 258397.234, 257344.625, 258166.469, 256947.469, 256329.781, 256337.125, 255333.125, 254666.109, 253947.297, 253034.172, 252447.359, 252440.766, 251631.922, 251384.203, 250689.234, 250474.156, 249537.188, 248784.0, 248024.172, 248177.516, 247042.875, 247103.766, 246649.625, 246620.359, 245887.406, 244952.812, 245470.094, 244647.312, 245543.516, 243950.047, 243297.844, 242391.172, 242726.266, 242513.266, 241420.766, 242005.766, 242051.922, 240113.547, 240049.922, 241124.125, 240597.266, 239249.266, 240292.125, 239613.016, 239381.891, 238846.312, 238689.953, 238564.438, 237299.953, 237744.172, 238104.062, 237357.984, 237105.234, 237037.5, 235867.516, 237270.375, 236850.781, 236546.891, 235692.375, 234787.766], 'val_loss': [1808.947, 1756.785, 1716.795, 1696.361, 1685.052, 1676.897, 1654.319, 1661.358, 1642.258, 1639.758, 1639.481, 1618.447, 1608.341, 1614.198, 1598.167, 1583.676, 1580.448, 1575.799, 1568.349, 1589.077, 1560.731, 1561.515, 1561.1, 1539.807, 1560.072, 1544.492, 1536.643, 1535.161, 1529.353, 1542.61, 1521.5, 1529.075, 1518.642, 1513.429, 1514.862, 1502.634, 1522.608, 1497.208, 1499.972, 1507.022, 1497.706, 1498.965, 1499.439, 1486.041, 1493.582, 1479.0, 1477.175, 1478.055, 1472.019, 1465.085, 1469.478, 1468.806, 1472.709, 1482.363, 1454.99, 1445.954, 1447.418, 1451.778, 1459.823, 1439.012, 1446.133, 1437.787, 1434.084, 1432.687, 1434.866, 1437.709, 1446.339, 1435.577, 1416.328, 1432.599, 1418.613, 1406.676, 1401.48, 1429.368, 1420.627, 1431.735, 1387.961, 1401.731, 1428.298, 1416.427, 1414.768, 1417.667, 1410.548, 1405.804, 1418.369, 1411.071, 1394.648, 1389.784, 1391.336, 1387.87, 1400.105, 1392.04, 1380.858, 1392.587, 1392.53, 1388.389, 1383.564, 1410.621, 1406.521, 1376.949]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:22 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.1864603712075097 batch_size:13 epochs:100	100	1000	True	33059.58594		2946907	16	-1	526.7722578048706	{'train_loss': [598514.312, 565705.062, 523800.75, 397751.062, 340249.812, 323373.188, 313531.406, 304614.281, 298493.562, 293936.5, 290560.188, 287258.156, 283854.562, 281917.719, 279811.594, 277241.219, 276231.5, 272922.344, 270934.375, 269636.875, 267891.875, 267233.062, 264699.875, 263925.219, 263220.062, 262580.344, 260595.328, 259416.125, 257129.875, 257153.391, 256198.328, 255861.516, 255023.984, 254247.266, 253262.125, 254303.203, 251508.656, 252319.656, 251347.281, 249896.969, 248723.469, 248535.969, 246703.297, 247253.906, 247201.125, 245341.531, 245915.141, 245591.484, 245238.25, 243719.688, 243871.312, 242162.469, 241342.172, 242260.156, 240595.844, 240671.297, 241052.109, 240110.859, 239162.406, 238721.172, 238136.422, 238378.859, 238255.828, 237077.891, 236094.078, 236374.906, 234612.797, 235103.547, 234958.469, 234780.922, 234639.906, 233894.641, 233594.875, 234109.75, 233075.266, 231787.484, 232410.828, 231091.031, 231262.703, 231095.344, 230488.844, 230814.406, 231792.875, 230684.953, 229338.234, 229195.453, 230026.938, 228806.406, 229003.75, 228467.516, 227648.062, 227381.625, 225721.641, 227316.125, 226054.766, 225420.375, 225489.734, 226208.281, 225037.594, 225120.062], 'val_loss': [2359.052, 1768.64, 1629.912, 1292.528, 1194.786, 1158.24, 1141.218, 1133.898, 1105.554, 1096.999, 1090.816, 1071.834, 1067.441, 1072.343, 1046.436, 1051.168, 1043.958, 1018.176, 1023.656, 1004.867, 1010.117, 1002.068, 999.719, 988.645, 994.231, 993.249, 981.202, 997.028, 982.589, 983.153, 975.8, 975.423, 971.888, 962.569, 959.731, 967.105, 957.507, 961.733, 961.602, 959.18, 959.2, 955.827, 948.955, 951.852, 948.878, 951.261, 953.141, 950.286, 940.735, 947.68, 945.926, 930.724, 937.504, 940.939, 932.294, 938.714, 935.102, 929.169, 927.633, 944.076, 932.344, 931.077, 943.839, 932.9, 921.857, 929.162, 923.193, 930.795, 926.755, 922.355, 933.424, 934.48, 927.408, 921.503, 920.688, 930.131, 925.084, 923.919, 925.936, 932.434, 927.317, 926.452, 926.38, 928.483, 924.103, 923.262, 925.523, 927.008, 930.026, 925.756, 923.222, 934.513, 939.668, 924.107, 930.423, 933.262, 920.466, 923.398, 923.531, 926.753]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:81 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:22 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:conv1d out_channels:54 kernel_size:9 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:15 layer:fc act:selu out_features:200 bias:True input:16 learning:adam lr:0.1864603712075097 beta1:0.8703943892660946 beta2:0.8702323676225117 weight_decay:3.295428519073327e-05 batch_size:19 epochs:100	100	1000	True	137298.0625		2463616	18	-1	453.26392579078674	{'train_loss': [1435614.25, 1336240.0, 1820477.625, 1473816.5, 1186092.125, 1160392.75, 1141668.25, 1125617.125, 1144902.0, 1137354.875, 1115071.625, 1154402.875, 1153768.75, 1114793.875, 1111981.125, 1119679.75, 1127232.125, 1106927.625, 1103957.125, 1104006.25, 1098608.25, 1109098.625, 1120621.75, 1111888.75, 1130435.5, 1114262.0, 1104618.625, 1109913.75, 1124145.375, 1100736.25, 1112461.625, 1109182.5, 1106167.75, 1116067.875, 1108446.375, 1117243.625, 1117355.5, 1093204.375, 1103943.75, 1102945.125, 1097528.875, 1118519.0, 1148180.875, 1105366.5, 1104996.625, 1143403.875, 1134507.5, 1113654.5, 1103708.5, 1113670.25, 1114514.0, 1093686.0, 1112500.125, 1099456.875, 1118121.75, 1100129.125, 1113223.875, 1096323.125, 1128735.75, 1117744.875, 1103701.375, 1117117.0, 1127644.625, 1119163.0, 1108839.875, 1118465.875, 1112562.75, 1103007.5, 1126791.5, 1095224.125, 1112326.75, 1101726.375, 1116008.875, 1126170.25, 1120189.25, 1115020.875, 1115682.5, 1113859.375, 1116222.0, 1102347.125, 1107868.75, 1095441.5, 1112424.0, 1109854.5, 1099056.375, 1100099.5, 1103124.0, 1106867.625, 1114728.875, 1108536.0, 1099213.25, 1109258.125, 1110003.0, 1102433.0, 1102409.75, 1122034.625, 1098851.75, 1103322.125, 1097452.625, 1106105.875], 'val_loss': [109715.078, 21930.416, 51721.398, 7016.83, 6352.33, 6352.33, 2665525.0, 6352.33, 6352.33, 62176.113, 1760250.125, 6352.33, 6352.33, 19708.561, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 10498.129, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 9343.77, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 26697.613, 6352.33, 6352.33, 6352.33, 6352.33, 7053.266, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.088, 6352.33, 8604.293, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 11839.436, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6873.407, 6352.33, 6352.33, 6352.33, 6352.33, 8587.043, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.1864603712075097 batch_size:27 epochs:100	100	1000	True	30827.23047		642608	15	-1	312.87673234939575	{'train_loss': [438980.688, 386220.781, 345513.375, 331111.0, 322778.312, 317365.312, 310882.844, 304508.688, 297980.031, 292976.625, 288242.0, 284618.875, 280783.156, 276793.844, 274250.594, 272949.781, 269212.156, 267345.812, 266162.219, 263667.281, 261697.703, 261015.656, 260038.422, 258326.984, 257154.656, 255996.125, 253709.5, 252840.938, 252775.141, 251158.844, 250880.938, 249699.672, 248439.953, 248246.594, 247889.578, 245238.812, 245406.0, 245178.594, 244578.047, 242974.719, 243616.734, 242683.828, 240510.156, 240304.219, 240454.922, 240318.906, 238437.25, 238210.406, 238126.312, 237045.297, 236904.594, 237223.906, 235368.25, 235822.875, 234129.438, 233814.594, 233446.875, 233427.531, 232639.984, 232124.641, 232078.406, 231103.484, 231295.844, 230215.125, 230505.453, 229133.812, 229056.906, 228785.109, 227754.656, 227613.891, 226864.625, 226612.594, 226274.859, 226015.719, 225605.094, 225892.594, 225751.688, 225230.172, 223326.5, 223354.781, 223684.797, 223523.156, 222760.406, 221535.234, 222139.094, 220834.438, 221360.0, 220433.875, 220885.703, 220139.359, 220026.391, 219618.156, 219699.516, 219540.578, 217795.047, 218433.938, 218049.594, 217271.094, 217462.875, 216976.609], 'val_loss': [2978.359, 2613.456, 2465.056, 2392.884, 2362.821, 2322.397, 2268.496, 2251.997, 2245.191, 2212.423, 2181.763, 2164.56, 2121.283, 2116.011, 2126.681, 2070.281, 2091.319, 2060.22, 2036.829, 2010.306, 1996.748, 1997.198, 1978.292, 1980.585, 1973.507, 1944.252, 1937.139, 1938.722, 1926.487, 1934.52, 1920.787, 1898.239, 1891.941, 1910.849, 1897.756, 1914.636, 1881.086, 1890.556, 1872.048, 1876.092, 1864.016, 1852.945, 1854.66, 1862.388, 1855.091, 1856.264, 1851.035, 1834.174, 1812.806, 1830.886, 1825.035, 1822.954, 1829.457, 1814.085, 1811.602, 1802.724, 1799.365, 1794.816, 1805.603, 1813.325, 1814.66, 1799.074, 1801.204, 1791.627, 1826.361, 1799.896, 1798.507, 1797.664, 1798.792, 1801.368, 1782.709, 1795.549, 1782.969, 1785.127, 1772.433, 1785.885, 1783.937, 1777.979, 1801.999, 1784.371, 1773.744, 1770.834, 1760.218, 1749.288, 1769.625, 1764.935, 1775.518, 1762.719, 1752.392, 1762.616, 1766.547, 1745.983, 1759.362, 1733.066, 1757.776, 1758.647, 1756.677, 1746.437, 1744.218, 1755.255]}	100	100	True
