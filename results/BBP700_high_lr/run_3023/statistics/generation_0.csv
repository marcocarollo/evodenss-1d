id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1864603712075097 batch_size:20 epochs:100	100	1000	True	32087.29297		452251	14	-1	323.5099878311157	{'train_loss': [378782.594, 337246.156, 321275.031, 314999.562, 311374.5, 308335.062, 304459.0, 299349.5, 295230.125, 291573.094, 288593.812, 285240.281, 282806.094, 279758.844, 277080.594, 275448.688, 273533.75, 270974.281, 268947.125, 267891.781, 265819.188, 265461.5, 263960.0, 262218.344, 260512.172, 259452.641, 259595.703, 258727.312, 256277.781, 256355.719, 255019.641, 254650.562, 253390.516, 252435.281, 251487.922, 250884.516, 250524.172, 250139.062, 248640.609, 248222.109, 247946.766, 247541.094, 247053.578, 246297.016, 245786.75, 245808.578, 244720.062, 245025.469, 243697.516, 243558.875, 243414.656, 242476.656, 242285.359, 241708.531, 241782.859, 241454.438, 240153.641, 239700.438, 240975.75, 238572.781, 238813.375, 239343.359, 238462.641, 238000.219, 237682.734, 237718.766, 237640.203, 237160.516, 236663.953, 236538.156, 236135.094, 236287.328, 235762.219, 235851.391, 235501.812, 235337.422, 235255.922, 234681.016, 234525.141, 234320.188, 233681.656, 234070.656, 234712.859, 233707.484, 232280.734, 232767.141, 232465.141, 231660.172, 232973.984, 231648.828, 232339.938, 231769.781, 231908.5, 231421.141, 231488.953, 231817.562, 231055.078, 231290.875, 230685.172, 230495.859], 'val_loss': [2107.771, 1900.555, 1864.92, 1841.518, 1820.988, 1804.544, 1770.455, 1750.701, 1739.406, 1726.789, 1709.621, 1701.273, 1687.465, 1664.525, 1642.816, 1631.791, 1617.096, 1614.994, 1610.868, 1604.104, 1617.405, 1614.374, 1580.404, 1577.475, 1564.037, 1556.178, 1571.016, 1564.131, 1563.077, 1552.357, 1554.648, 1540.468, 1539.401, 1518.751, 1519.186, 1549.772, 1543.828, 1530.866, 1509.335, 1524.639, 1515.555, 1524.419, 1513.43, 1513.724, 1510.782, 1498.467, 1497.151, 1497.938, 1501.974, 1502.251, 1492.6, 1493.502, 1501.255, 1504.819, 1500.699, 1474.19, 1491.863, 1485.648, 1515.183, 1495.893, 1489.823, 1491.977, 1479.464, 1496.585, 1508.662, 1483.01, 1482.26, 1508.415, 1483.195, 1487.675, 1487.482, 1488.402, 1495.647, 1483.689, 1497.805, 1490.94, 1486.511, 1489.861, 1489.681, 1498.587, 1477.177, 1472.826, 1474.071, 1479.192, 1490.701, 1467.518, 1484.407, 1477.146, 1487.334, 1473.644, 1476.182, 1479.506, 1499.322, 1480.791, 1470.501, 1477.98, 1487.665, 1466.835, 1469.323, 1477.996]}	100	100	True
