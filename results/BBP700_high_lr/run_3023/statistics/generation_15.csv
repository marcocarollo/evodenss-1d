id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	1000	True	31257.02539		675818	15	-1	309.8998975753784	{'train_loss': [459434.125, 367664.625, 334444.75, 318647.938, 305740.594, 296195.844, 287866.531, 280551.438, 276440.156, 272250.219, 269367.5, 266577.594, 264760.688, 261896.609, 260861.344, 258885.719, 257082.781, 255046.625, 254497.453, 252598.766, 251555.891, 250227.25, 248670.594, 247417.703, 247274.562, 246228.016, 245771.906, 244715.938, 243261.359, 243345.422, 242004.906, 240483.391, 240311.125, 240033.281, 240076.844, 239028.688, 237686.859, 237969.219, 236698.578, 236390.438, 236081.188, 235971.547, 234720.0, 234393.953, 233516.438, 232673.594, 232183.266, 232012.391, 231616.047, 232023.625, 231081.25, 230593.266, 229317.406, 228978.984, 229713.203, 228338.766, 228305.672, 227021.359, 227385.484, 226675.766, 226659.875, 225530.672, 225552.875, 225104.344, 224739.297, 224785.531, 224291.797, 223982.75, 223245.5, 223553.094, 222980.469, 221839.016, 222253.062, 222270.297, 221195.516, 221910.125, 220718.375, 221217.719, 219782.281, 220883.891, 220302.391, 220343.0, 218650.219, 219224.062, 218347.047, 217601.547, 217256.531, 218251.703, 217101.844, 217137.453, 216328.531, 216892.453, 216890.359, 215988.5, 216043.359, 215680.5, 216109.312, 215281.484, 215208.125, 214840.109], 'val_loss': [2907.752, 2374.828, 2247.481, 2140.025, 2092.298, 2023.451, 1996.318, 1980.137, 1958.872, 1914.854, 1928.929, 1893.86, 1903.485, 1859.093, 1851.433, 1848.89, 1836.629, 1836.773, 1840.472, 1824.516, 1815.618, 1819.482, 1813.231, 1808.854, 1800.878, 1795.375, 1793.224, 1795.161, 1785.055, 1790.111, 1790.631, 1790.838, 1778.683, 1779.372, 1781.82, 1767.068, 1758.846, 1763.04, 1757.458, 1760.373, 1756.792, 1749.72, 1746.28, 1754.43, 1737.145, 1744.923, 1735.257, 1739.838, 1714.972, 1719.282, 1713.744, 1715.322, 1715.331, 1738.503, 1707.314, 1704.021, 1722.202, 1709.901, 1701.694, 1709.804, 1710.025, 1706.514, 1702.533, 1691.634, 1702.932, 1699.159, 1688.611, 1691.796, 1695.181, 1694.812, 1698.961, 1693.469, 1684.375, 1701.402, 1686.246, 1696.785, 1685.957, 1673.737, 1689.025, 1675.545, 1673.697, 1687.922, 1670.963, 1677.507, 1674.296, 1698.435, 1678.427, 1671.417, 1661.714, 1683.556, 1665.221, 1670.619, 1672.635, 1662.679, 1679.216, 1671.098, 1684.483, 1663.881, 1665.614, 1643.528]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:85 kernel_size:5 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:deconv1d out_channels:77 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.21732632996108475 batch_size:24 epochs:100	100	1000	True	137649.65625		99409006	15	-1	597.8384850025177	{'train_loss': [1143710.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125], 'val_loss': [7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:9 layer:conv1d out_channels:81 kernel_size:2 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:4 epochs:100	82	1000	True	32884.30078		522729	15	-1	1006.0719525814056	{'train_loss': [382910.125, 332621.594, 314152.781, 301872.75, 294843.875, 290549.375, 285986.938, 282952.406, 279864.625, 276020.969, 273743.531, 271098.75, 267837.75, 266169.469, 264330.938, 263077.188, 260955.062, 259516.516, 258439.234, 256184.516, 254382.453, 252646.781, 252127.531, 250026.312, 250640.047, 248777.375, 248248.062, 246723.344, 246006.609, 245754.016, 243811.75, 243318.25, 242810.234, 241471.938, 242014.656, 240369.984, 240568.766, 239562.188, 239187.5, 238803.234, 238487.828, 237872.812, 236688.219, 235905.875, 236381.719, 234827.359, 234474.281, 234289.641, 233550.281, 232769.234, 233029.516, 232258.906, 231801.25, 231220.922, 231690.969, 230244.328, 230431.094, 229712.031, 228928.938, 228108.906, 229142.891, 227978.656, 227128.156, 226623.172, 226334.906, 226053.5, 225798.406, 225854.719, 224872.469, 224102.109, 224323.875, 224080.438, 222695.594, 222602.234, 222594.234, 222007.281, 221596.297, 221709.547, 221542.812, 220206.188, 220457.328, 219609.234], 'val_loss': [405.331, 377.522, 360.502, 351.812, 347.853, 343.774, 341.194, 335.49, 334.915, 333.025, 330.292, 327.736, 325.102, 323.549, 325.654, 319.766, 317.833, 319.358, 317.492, 315.276, 310.287, 313.081, 307.545, 312.028, 307.581, 305.874, 309.314, 306.974, 309.467, 302.26, 303.438, 307.882, 306.183, 301.736, 303.181, 306.7, 302.697, 300.255, 301.098, 302.785, 304.516, 300.765, 298.35, 298.623, 302.099, 298.433, 302.319, 297.266, 297.167, 296.557, 299.588, 294.548, 294.532, 296.271, 292.42, 295.211, 293.068, 295.001, 293.04, 292.389, 292.561, 290.055, 292.022, 289.394, 289.781, 290.991, 289.261, 294.263, 292.577, 289.616, 291.719, 289.085, 291.487, 296.439, 287.686, 290.185, 289.217, 289.52, 292.219, 290.656, 290.949, 290.901]}	82	82	False
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:118 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.2103871844621022 beta1:0.9146899735103897 beta2:0.8241135270569776 weight_decay:2.127089956664928e-05 batch_size:24 epochs:100	100	1000	True	181823.15625		1131311	16	-1	338.9166691303253	{'train_loss': [1182371.25, 1911278.75, 4408439.0, 2614215.75, 3602205.0, 4526684.5, 1162796.5, 1249380.125, 1262983.0, 1183669.0, 1185153.0, 1172923.625, 1113592.5, 1089855.625, 1095459.75, 1155546.625, 1110621.875, 1095012.125, 1113946.625, 1110913.5, 1086752.625, 1101565.75, 1083957.25, 1103149.5, 1092083.375, 1089218.875, 1104370.625, 1095598.125, 1118937.5, 1115285.75, 1112073.5, 1132810.75, 1092692.125, 1158724.625, 1096703.5, 1157333.125, 1121384.375, 1106961.375, 1103400.875, 1148204.875, 1169784.375, 1106972.625, 1100963.25, 1088472.0, 1092303.375, 1087379.875, 1109139.375, 1109303.125, 1120656.875, 1093640.125, 1114329.25, 1100026.125, 1116856.875, 1116815.875, 1134788.875, 1126306.75, 1104879.625, 1098091.0, 1097556.25, 1086249.625, 1092965.25, 1099637.875, 1125389.875, 1124626.5, 1121725.875, 1092419.5, 1168276.125, 1122118.625, 1105960.125, 1122091.5, 1098103.375, 1109071.5, 1089413.25, 1098963.625, 1097288.5, 1096275.625, 1103020.375, 1110051.375, 1168020.75, 1132892.875, 1121184.875, 1128402.125, 1113551.375, 1094078.625, 1110293.75, 1096711.0, 1086663.0, 1103860.625, 1093578.625, 1086897.25, 1107017.5, 1127268.625, 1092282.625, 1136777.5, 1098254.75, 1158841.75, 1103488.875, 1085819.25, 1109361.75, 1125247.75], 'val_loss': [7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 12773.522, 26340.051, 7940.411, 11943.934, 7940.411, 65119.02, 7940.411, 7940.411, 8608.057, 15036.513, 7940.411, 7940.411, 7940.411, 7940.411, 7940.312, 7940.411, 7940.411, 7940.411, 7940.411, 8878.645, 7939.188, 7940.411, 7940.411, 7940.411, 7940.411, 8845.526, 7940.411, 7940.411, 7940.411, 8133.943, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 9511.502, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 8040.715, 7940.411, 8183.828, 8971.054, 7940.411, 7940.411, 7940.411, 7940.407, 9175.977, 15864.86, 7940.411, 7940.411, 7940.411, 7940.411, 7923.274, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 11508.921, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 8494.555, 8326.17, 10666.419]}	100	100	True
