id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	1000	True	31508.00586		675818	15	-1	312.29247093200684	{'train_loss': [455318.344, 380210.875, 341030.375, 326062.531, 318493.156, 313360.844, 307034.094, 300833.688, 296544.812, 292803.719, 289288.406, 286135.969, 282745.406, 278969.188, 276639.781, 273358.062, 271503.938, 268586.031, 265672.875, 264208.938, 263115.5, 260718.141, 258918.484, 257965.438, 256403.328, 255064.453, 254237.75, 253545.266, 251564.266, 250361.375, 249618.297, 248764.359, 248211.297, 246899.812, 246094.828, 245753.062, 245198.109, 244175.547, 243090.469, 243276.359, 241959.891, 242061.344, 240643.625, 240792.859, 240094.75, 239066.156, 238906.0, 237960.359, 237058.859, 237789.672, 236004.328, 236198.172, 235363.875, 235054.156, 235408.938, 234419.109, 234709.547, 233165.625, 232542.781, 232160.109, 231769.703, 232352.234, 231521.234, 231915.719, 230320.953, 230325.531, 229489.656, 228812.109, 229577.125, 229914.656, 228777.938, 229203.219, 227918.766, 228040.25, 227416.75, 227628.859, 228015.438, 226924.859, 226549.859, 226452.922, 225422.344, 225766.078, 224958.719, 225388.125, 224338.922, 224636.422, 224053.203, 223319.109, 224213.781, 223922.344, 222647.938, 222785.953, 222512.062, 222077.906, 222116.359, 222190.812, 221345.891, 221608.938, 220258.891, 220160.734], 'val_loss': [2794.752, 2405.896, 2268.703, 2220.916, 2182.822, 2145.87, 2114.819, 2083.439, 2066.552, 2049.724, 2026.163, 2019.127, 1985.789, 1976.158, 1953.009, 1962.583, 1932.257, 1904.432, 1884.398, 1879.96, 1876.458, 1875.809, 1846.482, 1837.557, 1845.132, 1820.569, 1834.924, 1822.297, 1814.081, 1810.913, 1796.3, 1796.107, 1802.502, 1782.149, 1778.837, 1793.584, 1776.366, 1749.708, 1773.762, 1749.804, 1765.6, 1757.794, 1744.891, 1742.615, 1745.025, 1702.529, 1745.256, 1721.393, 1725.381, 1742.489, 1714.441, 1724.682, 1706.054, 1711.872, 1711.363, 1719.337, 1708.436, 1707.463, 1707.561, 1701.923, 1700.377, 1703.717, 1707.92, 1712.814, 1716.654, 1700.974, 1710.329, 1698.889, 1683.34, 1674.273, 1712.168, 1694.138, 1697.749, 1699.51, 1682.724, 1681.084, 1675.677, 1686.444, 1668.231, 1673.283, 1653.06, 1667.954, 1665.58, 1669.922, 1665.839, 1663.375, 1677.033, 1679.138, 1664.925, 1657.196, 1669.972, 1657.657, 1652.503, 1651.077, 1642.082, 1656.971, 1656.77, 1654.896, 1647.673, 1626.952]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:conv1d out_channels:93 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.24144358233646407 batch_size:24 epochs:100	100	1000	True	32589.97461		416978	15	-1	297.3187584877014	{'train_loss': [392103.688, 336952.469, 318830.469, 309055.406, 301198.75, 294817.625, 288680.406, 283974.188, 278878.312, 275102.812, 271637.094, 268054.469, 265558.625, 263698.312, 261904.172, 260055.406, 259209.328, 258601.875, 256943.578, 256260.203, 254627.609, 253900.609, 253137.891, 253204.188, 251387.547, 251336.641, 251043.469, 249834.422, 250261.891, 248864.328, 248307.594, 248203.469, 247280.109, 247143.656, 246303.188, 246427.953, 245624.141, 245680.938, 244831.578, 244596.5, 243345.75, 243775.344, 243168.734, 243510.203, 242353.266, 242146.703, 241570.453, 241833.672, 241549.297, 239953.922, 240758.125, 239146.547, 240625.0, 239349.219, 239038.297, 238290.031, 239083.453, 238471.266, 239089.797, 237857.453, 237524.094, 237592.562, 236780.516, 236107.438, 236040.766, 235171.406, 235619.422, 235349.5, 235250.141, 235299.031, 234362.984, 235032.5, 234227.062, 233974.078, 233491.812, 232556.828, 232862.469, 232350.578, 232402.859, 232787.984, 232490.672, 231127.078, 232090.359, 231310.656, 230569.047, 231231.844, 231056.562, 229937.609, 229902.641, 229803.406, 228959.391, 229549.344, 228688.328, 228760.797, 229228.094, 229769.703, 228600.391, 227795.422, 228077.172, 227921.625], 'val_loss': [2344.363, 2274.706, 2197.869, 2145.89, 2106.891, 2080.767, 2047.294, 2024.497, 1996.475, 1962.475, 1947.104, 1919.851, 1889.137, 1877.642, 1866.021, 1853.836, 1857.43, 1850.767, 1847.968, 1842.739, 1836.754, 1836.711, 1825.284, 1818.527, 1813.854, 1809.325, 1807.304, 1814.167, 1812.949, 1787.561, 1801.489, 1779.676, 1780.506, 1779.552, 1770.239, 1764.984, 1782.077, 1760.72, 1757.915, 1762.609, 1758.27, 1754.883, 1750.695, 1767.201, 1750.1, 1743.621, 1744.301, 1739.201, 1733.969, 1740.159, 1743.484, 1727.409, 1731.67, 1737.813, 1722.693, 1732.027, 1721.247, 1717.077, 1741.492, 1713.049, 1723.795, 1722.519, 1718.826, 1728.159, 1717.189, 1716.28, 1716.299, 1714.085, 1710.857, 1702.839, 1706.582, 1704.233, 1700.687, 1700.159, 1703.134, 1710.682, 1699.532, 1694.219, 1721.251, 1709.751, 1695.927, 1713.084, 1701.948, 1707.055, 1717.018, 1696.961, 1699.655, 1708.022, 1710.373, 1697.975, 1692.869, 1699.861, 1699.582, 1693.47, 1692.182, 1699.766, 1689.007, 1705.829, 1699.976, 1691.244]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:124 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:124 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:78 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:55 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:deconv1d out_channels:55 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adam lr:0.2103871844621022 beta1:0.9884324817526081 beta2:0.9572157089294746 weight_decay:2.6828797836291094e-06 batch_size:24 epochs:100	100	1000	True	3087408.25		2706047	16	-1	309.5504803657532	{'train_loss': [1097334.25, 1067726.25, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1283397.375, 167229936.0, 20249800.0, 1270234.5, 3130943.0, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 4195725.0, 2481025.75, 1312629.5, 1066921.125, 1084010.5, 1066921.125, 1248892.25, 1095436.625, 1123468.0, 1066921.125, 1365098.125, 2551536.25, 2523140.5, 1711387.625, 1066921.125, 1908388.125, 1167614.375, 1157365.5, 2170902.5, 2304237.75, 1446699.125, 2056696.875, 1734875.25, 1459133.0, 1247880.125, 1471715.0, 1487104.75, 2194654.25, 1772985.25, 1270252.0, 1323801.0, 1076427.375, 1231045.125, 1115717.125, 1166189.75, 1066921.125, 1209187.125, 1364143.875, 1176410.375, 1192396.0, 2179615.75, 1152149.375, 1066921.125, 1066921.125, 1338566.5, 2002753.375, 1082050.75, 1149447.25, 1303279.5, 1768106.375, 2501945.5, 4207925.0, 1909475.625, 2107336.75, 1393969.125, 1205825.375, 1753921.625, 3665365.25, 2397427.75, 3069360.0, 3641674.0, 2539233.25, 1809987.625], 'val_loss': [7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:100 kernel_size:7 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:conv1d out_channels:58 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.2103871844621022 batch_size:25 epochs:100	100	1000	True	33121.79688		382898	14	-1	279.03242206573486	{'train_loss': [378630.469, 332627.375, 320907.031, 316478.531, 314325.438, 312199.906, 310493.125, 308814.469, 307694.719, 305928.688, 304701.5, 303404.719, 301730.375, 299345.438, 296812.875, 294150.188, 291122.031, 288020.188, 285462.969, 282618.562, 279648.875, 277651.469, 275672.0, 274047.875, 272477.969, 270583.562, 269141.75, 266990.062, 266268.0, 263960.812, 262907.719, 262062.906, 259672.172, 259197.219, 258308.672, 256922.734, 256626.484, 255801.609, 255091.453, 254454.969, 253881.031, 252337.938, 252071.609, 251234.906, 250484.953, 250588.547, 249528.484, 248280.484, 247754.531, 248512.453, 247077.078, 246988.438, 246370.922, 245413.406, 244596.969, 244565.891, 243729.578, 244258.266, 242628.984, 242166.094, 241798.219, 241575.0, 241467.484, 240780.031, 241681.844, 240556.484, 240491.969, 239228.281, 239326.75, 238576.594, 239169.422, 238647.25, 238149.172, 237817.922, 238134.938, 236774.594, 237189.141, 236277.938, 236534.203, 236020.672, 234968.672, 234571.141, 235025.75, 235106.219, 235129.328, 234607.328, 233709.844, 233146.656, 232283.062, 232729.734, 232751.141, 232328.875, 232285.75, 231518.062, 231941.859, 231522.547, 231219.766, 230691.25, 230463.906, 230119.031], 'val_loss': [2362.063, 2275.817, 2232.663, 2202.718, 2189.273, 2173.37, 2166.164, 2157.26, 2154.197, 2141.326, 2135.986, 2128.43, 2115.686, 2098.056, 2078.146, 2070.323, 2045.891, 2036.979, 2013.826, 2015.457, 1984.928, 1968.293, 1961.548, 1961.944, 1925.463, 1923.538, 1911.229, 1901.628, 1871.567, 1881.691, 1857.662, 1862.775, 1851.497, 1861.198, 1839.412, 1838.245, 1823.579, 1842.873, 1822.607, 1815.401, 1802.905, 1789.038, 1798.476, 1797.577, 1778.135, 1777.557, 1777.416, 1785.258, 1761.526, 1761.868, 1755.363, 1750.979, 1744.779, 1752.204, 1750.699, 1752.349, 1736.247, 1734.431, 1747.609, 1722.146, 1723.7, 1716.61, 1716.896, 1728.882, 1724.626, 1731.296, 1704.551, 1692.831, 1704.516, 1716.996, 1717.973, 1708.822, 1721.507, 1700.403, 1697.941, 1702.625, 1697.831, 1683.863, 1684.872, 1698.472, 1694.958, 1695.03, 1692.226, 1678.501, 1689.838, 1720.519, 1678.896, 1694.936, 1696.049, 1674.617, 1694.855, 1679.606, 1692.804, 1695.421, 1651.39, 1669.517, 1682.613, 1658.899, 1657.493, 1693.378]}	100	100	True
