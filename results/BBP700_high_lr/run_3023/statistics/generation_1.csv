id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1864603712075097 batch_size:20 epochs:100	100	1000	True	32995.17969		452251	14	-1	331.46137142181396	{'train_loss': [376101.219, 330797.094, 313034.844, 305090.031, 300682.375, 296933.906, 293172.25, 291083.906, 288546.719, 286120.75, 284774.344, 283429.0, 281694.312, 279761.281, 277450.688, 276754.344, 275265.062, 273499.906, 272668.75, 271673.0, 269780.125, 269233.062, 267742.719, 266401.156, 265325.906, 264785.75, 262979.094, 262050.875, 261976.125, 260771.75, 259974.266, 258292.266, 258179.703, 256634.031, 256388.359, 255894.5, 254989.531, 254571.328, 253848.656, 253270.812, 252666.344, 252039.094, 251892.703, 250860.984, 250681.328, 250490.266, 249889.094, 249401.453, 248483.781, 247494.0, 248080.531, 247371.984, 246339.875, 245998.5, 246414.297, 245529.766, 244864.828, 244607.484, 243953.922, 243783.125, 242684.688, 243218.984, 242813.062, 240951.516, 242460.312, 241814.156, 240607.234, 240283.078, 239787.578, 239687.312, 239548.453, 239192.047, 238799.156, 238561.578, 238192.219, 237775.578, 237425.531, 237608.953, 236471.609, 235595.672, 235409.422, 235481.688, 234894.281, 234718.5, 234823.219, 234961.203, 234601.594, 234414.344, 233926.984, 233840.031, 233555.328, 232930.578, 233089.375, 233179.641, 233136.906, 232798.156, 232156.938, 231730.188, 231863.359, 231308.688], 'val_loss': [2088.158, 1867.618, 1806.84, 1793.235, 1774.654, 1758.911, 1745.835, 1732.648, 1721.288, 1715.152, 1711.42, 1700.715, 1683.51, 1677.124, 1681.46, 1661.077, 1654.152, 1651.574, 1640.856, 1637.899, 1630.161, 1617.607, 1618.869, 1615.662, 1605.816, 1616.422, 1602.158, 1577.916, 1581.521, 1581.891, 1567.083, 1569.098, 1560.069, 1561.857, 1552.343, 1558.784, 1544.833, 1547.766, 1531.136, 1538.274, 1533.56, 1535.31, 1526.536, 1521.994, 1524.269, 1526.14, 1540.797, 1515.622, 1519.88, 1506.043, 1503.288, 1508.148, 1515.156, 1510.458, 1502.081, 1514.277, 1506.319, 1497.909, 1490.547, 1488.285, 1494.47, 1489.776, 1496.724, 1480.8, 1490.461, 1485.693, 1487.829, 1484.107, 1470.484, 1468.521, 1475.612, 1464.775, 1466.088, 1464.04, 1469.464, 1458.241, 1475.399, 1447.093, 1457.142, 1460.848, 1464.424, 1471.486, 1445.181, 1451.575, 1449.537, 1439.759, 1447.068, 1434.89, 1435.638, 1443.354, 1435.097, 1438.846, 1434.5, 1419.677, 1431.836, 1431.894, 1433.845, 1421.027, 1421.876, 1428.206]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:19 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:112 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1864603712075097 batch_size:4 epochs:100	73	1000	True	34333.58203		1680001	14	-1	1005.5560057163239	{'train_loss': [546032.062, 474789.25, 362021.875, 308087.281, 296414.844, 289077.75, 285193.062, 280689.938, 276910.969, 272886.281, 269373.875, 267714.969, 266381.875, 263512.938, 261564.344, 259958.094, 258969.594, 256689.609, 257223.766, 255850.625, 253830.047, 253057.156, 252729.656, 251013.922, 250832.188, 251010.156, 249119.109, 248177.922, 247555.906, 246888.438, 245953.406, 244931.641, 244927.016, 243964.625, 242960.438, 241965.891, 241850.25, 241353.094, 239858.469, 239860.672, 239640.891, 238023.938, 237585.969, 237781.312, 237308.969, 235618.531, 235250.859, 234560.406, 233851.891, 234332.266, 233401.141, 232832.234, 232927.344, 232687.844, 232311.328, 231744.516, 230862.5, 230442.672, 229983.797, 229494.625, 229857.703, 229849.281, 229005.297, 228682.609, 228076.031, 227146.547, 227301.047, 227554.203, 227229.312, 226369.859, 226281.734, 224790.094, 225629.578], 'val_loss': [620.73, 526.146, 369.37, 347.857, 342.238, 335.388, 333.701, 332.357, 327.112, 322.169, 326.094, 321.229, 323.366, 319.052, 316.607, 315.392, 313.391, 314.048, 313.91, 312.906, 311.416, 310.74, 313.177, 311.192, 311.741, 310.76, 314.348, 307.882, 310.778, 310.561, 310.396, 308.431, 309.089, 308.656, 309.82, 309.939, 308.976, 312.83, 308.356, 310.456, 308.786, 309.366, 307.359, 308.86, 305.859, 310.451, 308.557, 307.715, 308.453, 311.025, 310.923, 313.547, 312.443, 311.544, 311.959, 313.813, 311.117, 310.961, 315.479, 316.424, 309.937, 310.279, 311.427, 311.993, 316.108, 312.924, 311.993, 315.12, 315.909, 317.884, 313.464, 316.322, 315.238]}	73	73	False
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:79 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.1864603712075097 batch_size:20 epochs:100	100	1000	True	32606.21484		486464	15	-1	364.9918940067291	{'train_loss': [375330.938, 327320.938, 311594.406, 304820.719, 300693.531, 297636.375, 295415.625, 293426.0, 291133.125, 288765.344, 285657.875, 283197.781, 279701.969, 278522.594, 275742.656, 274341.812, 273196.562, 270736.312, 270239.438, 268305.688, 267104.25, 265288.375, 263761.844, 263125.594, 262094.5, 260674.078, 259763.797, 258778.359, 258042.875, 256960.219, 256704.438, 255443.875, 255301.047, 254289.594, 253258.516, 252035.375, 251910.531, 251372.766, 250866.953, 250868.656, 249145.641, 248805.609, 248578.125, 248127.234, 246815.688, 246972.562, 246433.578, 245870.703, 245404.812, 245016.938, 244002.203, 244517.719, 243739.062, 243653.25, 242986.031, 242335.828, 241966.094, 241095.469, 240680.891, 240764.812, 240324.875, 239941.625, 240236.0, 239385.906, 238488.906, 239169.578, 238372.578, 237895.203, 237486.969, 237183.844, 236048.969, 236213.469, 237003.172, 235822.141, 235668.109, 235716.172, 234994.547, 234736.469, 233913.219, 233985.156, 233422.609, 233632.828, 233533.156, 232774.688, 232353.875, 232114.656, 232973.0, 231859.031, 231499.453, 231149.344, 232416.141, 230683.359, 230956.547, 230914.156, 230447.094, 230228.531, 229473.75, 229717.469, 229610.156, 228900.531], 'val_loss': [1980.493, 1848.763, 1821.623, 1788.241, 1771.999, 1750.958, 1756.861, 1732.374, 1724.313, 1716.601, 1701.249, 1696.911, 1678.592, 1681.798, 1655.787, 1654.976, 1649.601, 1639.697, 1624.541, 1615.576, 1612.304, 1596.458, 1590.896, 1596.012, 1579.927, 1572.253, 1571.394, 1564.485, 1577.356, 1556.218, 1561.101, 1543.551, 1534.163, 1544.847, 1538.038, 1519.398, 1527.973, 1512.284, 1509.35, 1512.498, 1501.015, 1507.948, 1504.099, 1505.806, 1517.08, 1492.837, 1493.999, 1496.197, 1488.377, 1492.226, 1480.986, 1482.007, 1491.959, 1478.091, 1476.545, 1479.256, 1467.981, 1476.039, 1467.289, 1473.114, 1470.509, 1481.015, 1469.009, 1463.369, 1461.437, 1455.591, 1458.88, 1449.332, 1446.942, 1446.581, 1444.514, 1444.656, 1445.371, 1440.781, 1441.529, 1445.046, 1433.031, 1431.12, 1441.088, 1455.406, 1429.326, 1434.002, 1419.892, 1438.111, 1421.14, 1427.814, 1422.84, 1428.856, 1426.683, 1421.797, 1425.693, 1422.651, 1419.379, 1416.042, 1422.411, 1421.201, 1420.796, 1421.501, 1415.029, 1419.284]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:76 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:108 kernel_size:6 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.1864603712075097 beta1:0.8922277412357087 beta2:0.9552009986703434 weight_decay:0.0001800862108817838 batch_size:20 epochs:100	100	1000	True	57158.05078		504395	15	-1	374.80638551712036	{'train_loss': [461517.625, 428057.25, 418252.875, 412130.875, 412060.031, 416333.812, 436888.031, 401972.344, 421909.188, 405388.094, 423606.594, 414522.0, 404936.531, 415961.844, 423395.438, 810005.5, 1068546.5, 1108836.875, 1067674.875, 946686.75, 466102.438, 401785.344, 438266.781, 404798.375, 408420.938, 401466.375, 405931.469, 412926.219, 422459.406, 408460.844, 440148.344, 408197.312, 403404.156, 412885.344, 403823.75, 425701.281, 430666.344, 416733.719, 411084.75, 407298.344, 411015.531, 415541.312, 404271.938, 425287.719, 402776.094, 408916.625, 397454.625, 403597.875, 415309.156, 404720.875, 436482.25, 423053.281, 435977.469, 402750.75, 403483.25, 411180.5, 412313.562, 454169.531, 405412.375, 413261.406, 404352.75, 403342.562, 424286.281, 425874.031, 406328.781, 407772.031, 421919.344, 395256.969, 412880.438, 412795.719, 425872.219, 425476.0, 412831.938, 402947.031, 408477.969, 417964.0, 429390.281, 411288.719, 416712.344, 430483.031, 418185.438, 416149.75, 403853.094, 414420.156, 417817.719, 421574.281, 409936.875, 424700.312, 404125.0, 418104.219, 413739.531, 414089.375, 429109.906, 405446.625, 411622.906, 444030.594, 407218.781, 410944.281, 439849.688, 442671.531], 'val_loss': [2877.269, 2826.992, 3510.885, 2610.574, 2353.459, 3006.017, 2398.164, 2472.668, 2643.434, 2429.723, 3274.154, 3982.999, 2869.152, 2438.777, 2456.971, 6686.639, 6686.664, 6686.664, 6686.664, 3085.284, 2199.029, 3496.63, 2680.374, 2395.345, 2222.722, 2666.646, 2823.704, 2990.814, 4022.542, 2935.902, 2228.041, 3762.823, 2339.823, 2270.915, 2501.642, 7550.157, 2609.923, 2787.728, 2319.91, 4622.972, 2851.595, 4177.284, 2376.739, 2312.347, 2596.042, 2268.383, 2775.543, 2833.507, 3147.634, 2512.965, 2309.849, 2276.528, 3343.283, 2911.267, 2404.749, 3560.763, 4530.432, 2496.424, 2396.678, 3300.097, 2338.048, 2353.886, 2511.383, 2563.911, 2286.295, 3820.684, 2246.574, 2489.818, 2289.668, 3089.813, 2234.711, 5135.922, 2812.255, 2430.188, 3888.503, 2685.356, 2226.51, 2506.588, 2590.663, 2431.98, 2198.153, 2205.007, 2816.375, 2545.33, 3988.175, 2384.607, 3521.05, 2407.73, 2381.938, 2287.238, 3991.798, 2200.595, 2285.411, 2990.135, 3033.256, 2640.523, 2792.11, 4188.697, 3477.396, 2579.786]}	100	100	True
