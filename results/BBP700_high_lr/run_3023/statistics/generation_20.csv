id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	2000	True	31650.93555		675818	15	-1	315.81337451934814	{'train_loss': [469402.156, 376498.125, 343672.312, 329135.938, 320466.281, 314546.812, 310098.781, 306154.188, 301225.562, 294964.688, 289874.25, 284407.625, 280450.75, 276558.938, 273108.938, 270145.156, 267211.969, 264707.25, 262881.344, 261787.328, 259910.969, 257277.922, 256802.781, 255409.234, 253923.438, 252445.688, 251135.781, 248135.203, 246420.5, 246868.688, 244852.141, 243141.703, 242720.438, 241333.438, 240284.469, 239805.25, 237759.625, 237419.078, 237304.156, 235638.922, 235429.797, 233144.625, 233740.484, 233324.141, 232253.406, 232038.734, 231154.234, 229625.828, 229486.719, 228529.328, 228520.203, 227231.062, 227944.156, 227089.078, 225903.188, 225176.562, 224945.172, 225077.219, 225051.969, 223411.312, 222985.0, 222940.844, 221843.281, 222033.859, 222010.125, 220628.719, 220860.531, 220741.812, 219618.219, 219510.438, 219187.359, 217943.172, 218005.656, 218108.562, 218132.25, 216972.203, 216359.547, 216724.562, 215953.047, 216093.859, 216315.094, 215661.625, 215307.141, 214731.156, 214389.547, 214512.406, 213771.984, 214087.219, 212998.594, 213429.266, 212912.094, 213630.094, 212093.859, 212277.453, 211351.031, 211690.672, 210808.188, 211076.047, 211644.312, 210632.953], 'val_loss': [2811.104, 2413.62, 2297.642, 2237.389, 2192.192, 2187.726, 2137.564, 2131.128, 2090.89, 2065.44, 2034.482, 2006.7, 1973.836, 1956.241, 1930.813, 1935.979, 1920.225, 1895.374, 1881.704, 1882.232, 1858.502, 1849.419, 1849.774, 1848.094, 1826.673, 1813.526, 1809.243, 1779.839, 1769.823, 1769.646, 1754.161, 1762.643, 1748.836, 1725.471, 1717.086, 1717.765, 1714.785, 1726.597, 1709.675, 1711.467, 1714.823, 1692.085, 1685.375, 1699.371, 1683.05, 1695.105, 1685.357, 1699.116, 1681.662, 1687.337, 1681.821, 1680.79, 1690.248, 1675.503, 1657.933, 1666.19, 1670.352, 1670.292, 1667.548, 1674.83, 1653.287, 1659.946, 1653.198, 1646.514, 1666.964, 1648.302, 1657.653, 1652.284, 1659.977, 1636.358, 1643.607, 1652.234, 1655.905, 1641.242, 1655.564, 1664.193, 1653.399, 1653.06, 1638.249, 1657.719, 1628.45, 1644.016, 1646.673, 1630.944, 1646.843, 1632.983, 1647.981, 1646.912, 1640.801, 1646.685, 1654.108, 1621.879, 1643.021, 1655.822, 1634.533, 1637.606, 1637.621, 1625.776, 1644.475, 1636.697]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	2000	True	31302.00781		675818	15	-1	314.8687598705292	{'train_loss': [435287.344, 345648.125, 322506.188, 309577.312, 300576.344, 295294.469, 290375.906, 286677.156, 282878.25, 279751.094, 277731.875, 274819.844, 271807.719, 269596.594, 266155.344, 264347.594, 261859.047, 260813.891, 259367.75, 256447.875, 254469.031, 252988.438, 251220.609, 250288.844, 248810.422, 246935.281, 245472.328, 245548.031, 243378.75, 242959.812, 241265.391, 240751.953, 240007.031, 239083.266, 238308.109, 237237.766, 235878.016, 235486.672, 233941.844, 233978.234, 233201.719, 231992.5, 232059.828, 232068.391, 231192.719, 230934.172, 229711.953, 229616.391, 228653.0, 228579.109, 227798.984, 227264.078, 226213.25, 225722.281, 226526.156, 226691.781, 226064.406, 224447.562, 224812.391, 224369.594, 223774.266, 223542.656, 224090.688, 222758.922, 222748.844, 221865.578, 220917.469, 222169.766, 221889.125, 220557.172, 220519.172, 219825.609, 219522.359, 219356.016, 219544.047, 217929.266, 218369.484, 217924.375, 217771.812, 217754.125, 217743.516, 217124.219, 217906.828, 216904.875, 216558.781, 216324.297, 216023.703, 215485.891, 215681.766, 215173.453, 214616.438, 213743.609, 214125.891, 215141.188, 213774.422, 213561.594, 213694.031, 213648.891, 212280.891, 212282.969], 'val_loss': [2548.747, 2254.42, 2157.047, 2113.635, 2049.948, 2041.471, 2028.624, 1984.435, 1986.334, 1961.252, 1951.505, 1925.888, 1911.291, 1900.31, 1902.863, 1901.782, 1889.988, 1865.615, 1849.967, 1854.963, 1829.587, 1837.042, 1840.725, 1844.536, 1824.783, 1815.01, 1803.402, 1827.145, 1776.954, 1793.348, 1774.365, 1786.162, 1751.633, 1750.17, 1752.018, 1779.769, 1731.821, 1742.327, 1735.36, 1731.454, 1710.19, 1707.869, 1703.896, 1702.242, 1695.65, 1702.531, 1679.456, 1716.051, 1694.79, 1699.562, 1682.88, 1703.693, 1686.076, 1676.354, 1681.061, 1683.581, 1668.938, 1668.2, 1674.431, 1665.207, 1659.421, 1668.329, 1653.876, 1661.707, 1660.947, 1661.939, 1660.967, 1662.056, 1654.293, 1655.84, 1648.275, 1657.312, 1658.702, 1645.424, 1661.362, 1642.355, 1639.678, 1631.028, 1628.974, 1650.666, 1632.459, 1648.967, 1631.337, 1638.372, 1645.527, 1643.27, 1638.792, 1625.425, 1633.222, 1643.344, 1636.698, 1636.337, 1639.586, 1628.282, 1636.405, 1633.687, 1631.434, 1623.708, 1641.573, 1637.568]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:conv1d out_channels:109 kernel_size:6 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	1000	True	137333.75		18051079	15	-1	317.23682045936584	{'train_loss': [1123637.5, 1076339.25, 1070591.125, 1075369.125, 1068835.0, 1071176.25, 1073580.625, 1070685.125, 1067662.75, 1070141.25, 1069317.75, 1067056.875, 1068174.75, 1067612.25, 1067554.0, 1067440.25, 1068107.25, 1067272.0, 1066944.75, 1066921.0, 1069061.0, 1067234.625, 1066924.875, 1072909.125, 1068333.375, 1068001.625, 1067151.625, 1067443.375, 1067178.875, 1067045.625, 1067787.0, 1066930.5, 1066921.125, 1071362.75, 1067584.875, 1067600.625, 1067396.625, 1067406.375, 1066922.125, 1067726.25, 1066959.25, 1067008.25, 1066922.5, 1066921.125, 1066921.125, 1066921.125, 1067310.75, 1066921.125, 1070835.375, 1067470.0, 1067517.5, 1066921.125, 1068923.0, 1066921.125, 1066921.375, 1068023.375, 1067828.25, 1067274.125, 1066966.75, 1066921.125, 1067455.375, 1067367.0, 1067038.375, 1067432.75, 1067619.125, 1067012.0, 1066921.125, 1069469.625, 1067022.75, 1066961.375, 1066920.625, 1067851.0, 1067294.375, 1066921.75, 1066925.5, 1067336.625, 1067309.75, 1067188.25, 1067454.875, 1067199.25, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066921.125, 1066920.5, 1067220.125, 1068440.25, 1067528.375, 1067123.0, 1067882.625, 1067020.5, 1067181.0, 1066952.5, 1066972.25, 1066921.125, 1066939.25, 1066945.375, 1066954.125], 'val_loss': [7940.411, 7922.009, 7940.411, 7940.411, 7940.411, 7916.13, 7939.612, 7940.411, 7940.298, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7943.831, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 8281.83, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.396, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411, 7940.411]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:conv1d out_channels:105 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.2103871844621022 batch_size:29 epochs:100	100	1000	True	31709.79297		634430	16	-1	295.7833044528961	{'train_loss': [466829.875, 371318.594, 332144.562, 318073.562, 307959.281, 299578.188, 293132.031, 288835.156, 285176.562, 282329.875, 278586.875, 275851.875, 273315.344, 270698.656, 268312.438, 266665.938, 264559.344, 262555.375, 260549.125, 258444.75, 256601.219, 255442.141, 253323.609, 252956.234, 251501.156, 249417.344, 248694.688, 247562.344, 246850.891, 245155.688, 244297.906, 243230.25, 242724.891, 241368.969, 240279.703, 240043.344, 239429.016, 239203.172, 237677.906, 237839.438, 238090.547, 236688.656, 235750.703, 235253.781, 234873.031, 234012.391, 234149.219, 233813.359, 233078.734, 232877.688, 232513.297, 232226.203, 231660.25, 231550.094, 230402.453, 229827.469, 229610.984, 229200.5, 229213.109, 229217.828, 228844.141, 227038.688, 228390.125, 227056.953, 226873.922, 225793.188, 227203.172, 226064.969, 225031.578, 225846.375, 224992.969, 223426.547, 224363.422, 224287.688, 223994.984, 223197.516, 223298.812, 222787.656, 222879.719, 222219.953, 222585.719, 221927.641, 221361.406, 221406.391, 221634.094, 218941.5, 220533.734, 219808.875, 218679.531, 219370.516, 219285.312, 219187.438, 218258.875, 219153.547, 218087.141, 218137.594, 218938.688, 217729.359, 217651.891, 217758.438], 'val_loss': [3160.227, 2665.435, 2512.68, 2458.194, 2375.093, 2346.769, 2337.759, 2304.242, 2261.676, 2254.719, 2232.458, 2216.739, 2201.047, 2188.392, 2172.339, 2174.115, 2150.034, 2130.948, 2123.233, 2122.708, 2110.223, 2083.603, 2077.947, 2084.573, 2095.26, 2069.414, 2065.452, 2036.207, 2016.105, 2040.973, 2019.777, 2014.856, 2010.182, 1999.27, 1992.002, 2001.184, 1998.952, 1988.898, 1965.367, 1966.621, 1961.192, 1979.833, 1967.006, 1970.544, 1973.842, 1939.339, 1945.65, 1947.456, 1934.906, 1932.581, 1947.664, 1931.472, 1949.845, 1939.254, 1947.944, 1923.296, 1944.584, 1938.565, 1938.297, 1912.21, 1944.249, 1923.894, 1927.773, 1921.856, 1945.627, 1932.773, 1917.359, 1909.992, 1917.896, 1922.277, 1913.752, 1900.637, 1907.845, 1901.872, 1918.814, 1886.795, 1922.911, 1891.349, 1890.631, 1907.324, 1901.366, 1903.721, 1895.621, 1891.173, 1879.82, 1879.407, 1887.625, 1883.326, 1886.233, 1883.4, 1882.686, 1883.906, 1880.164, 1887.743, 1877.988, 1869.282, 1883.283, 1886.845, 1886.046, 1870.445]}	100	100	True
