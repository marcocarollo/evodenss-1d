id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:18 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:94 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.20382428609940764 batch_size:27 epochs:100	100	1000	True	32706.55273		975651	15	-1	336.66336250305176	{'train_loss': [479487.0, 400129.625, 341371.062, 318166.844, 304313.281, 296275.125, 290652.062, 284682.156, 279516.906, 276336.562, 272212.031, 269631.688, 266048.062, 263457.25, 260935.25, 258617.0, 256472.5, 254076.281, 252531.484, 250085.938, 249309.422, 246981.453, 246291.938, 245211.766, 243765.812, 242504.109, 241607.859, 240276.797, 239342.078, 238118.594, 237107.453, 235728.703, 234105.625, 234157.641, 233479.875, 232755.969, 231566.297, 231516.734, 230189.906, 229894.891, 229007.516, 228338.984, 227725.734, 227087.531, 225831.438, 224875.344, 225210.219, 224813.266, 223766.844, 222812.438, 223211.438, 221652.328, 221722.703, 220464.5, 221091.297, 219639.75, 219837.031, 218549.156, 218287.094, 218194.438, 216829.516, 216969.031, 215556.266, 216033.047, 215990.344, 215745.062, 214813.422, 213480.531, 213732.328, 213781.609, 213210.078, 212325.672, 212322.078, 211740.75, 212000.0, 211705.578, 210652.875, 210069.25, 210225.719, 209522.906, 209082.281, 208200.719, 208899.328, 207874.438, 208565.375, 207879.125, 206589.484, 206546.0, 206876.688, 206288.469, 206159.516, 205992.312, 204734.234, 205287.312, 205321.594, 203759.344, 203515.281, 202507.75, 202425.453, 202540.953], 'val_loss': [4049.579, 2660.67, 2388.611, 2323.136, 2330.918, 2228.748, 2187.724, 2146.047, 2152.561, 2098.937, 2104.177, 2070.489, 2038.009, 2009.925, 2004.203, 1988.922, 2012.578, 1970.423, 1961.491, 1972.528, 1969.717, 1953.632, 1945.472, 1922.124, 1932.298, 1926.435, 1924.736, 1900.115, 1913.56, 1891.156, 1909.5, 1898.383, 1881.286, 1879.253, 1879.62, 1870.893, 1862.831, 1861.167, 1862.802, 1853.439, 1871.467, 1849.51, 1844.45, 1847.906, 1853.836, 1852.0, 1850.368, 1847.162, 1853.808, 1845.544, 1860.719, 1824.264, 1860.825, 1826.049, 1828.426, 1844.21, 1830.054, 1832.208, 1817.527, 1837.058, 1833.367, 1829.574, 1808.734, 1836.079, 1839.545, 1820.857, 1831.5, 1825.26, 1822.035, 1844.371, 1826.272, 1842.547, 1849.563, 1835.451, 1861.877, 1822.907, 1854.902, 1846.366, 1829.956, 1819.61, 1829.303, 1855.474, 1844.857, 1849.755, 1818.489, 1812.374, 1830.834, 1865.98, 1822.836, 1842.842, 1823.429, 1807.195, 1851.68, 1812.436, 1816.356, 1844.027, 1821.19, 1832.273, 1844.304, 1823.973]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:111 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:117 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:117 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:106 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:18 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:48 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.20382428609940764 alpha:0.9240006335912854 weight_decay:0.00015089993317453268 batch_size:27 epochs:100	100	1000	True	137269.09375		673996	15	-1	260.82076501846313	{'train_loss': [2050473.0, 1078981.0, 2195560.5, 1593836.375, 1106142.375, 1501890.375, 1691616.5, 1168136.5, 1448218.25, 1456685.5, 1508492.125, 1244834.25, 1500807.5, 1365759.125, 1268272.5, 1461817.375, 1265599.25, 1139252.5, 1442007.5, 1216121.0, 1333006.625, 1236900.75, 1482933.25, 1229606.25, 1212332.25, 1275652.625, 1730959.5, 1282819.125, 1340058.0, 1420156.375, 1323350.25, 1146701.75, 1400619.75, 1258062.125, 1501452.5, 1121584.875, 1347316.0, 1086661.875, 1118760.125, 1285709.125, 1162820.375, 1140469.25, 1172585.875, 1250489.375, 1175382.875, 1245984.75, 1126085.25, 1137836.75, 1221262.0, 1156863.375, 1224130.875, 1203613.375, 1195513.25, 1145403.5, 1171564.625, 1213492.375, 1209831.625, 1105687.875, 1175790.625, 1232503.75, 1107027.25, 1231386.75, 1170353.5, 1172650.25, 1164676.375, 1156117.125, 1135163.25, 1237227.125, 1154727.875, 1133025.625, 1214287.5, 1167154.375, 1136185.875, 1235102.75, 1147037.625, 1158032.0, 1183886.0, 1172578.25, 1148692.375, 1224649.125, 1137053.375, 1147680.375, 1200901.125, 1211553.375, 1135919.625, 1153239.125, 1227027.625, 1150961.5, 1131246.0, 1210670.125, 1192788.875, 1121726.375, 1233563.0, 1192227.125, 1113068.375, 1208898.125, 1195730.125, 1130396.75, 1164923.375, 1208110.875], 'val_loss': [62018.172, 8469.77, 8469.773, 8457.308, 8469.773, 8469.773, 8469.771, 8469.773, 8469.761, 8469.769, 8469.771, 8464.016, 8469.733, 8469.744, 8469.773, 8469.764, 8469.682, 8469.773, 8469.563, 8469.773, 9480.429, 8469.623, 8469.657, 8469.76, 8864.228, 8469.505, 8382.482, 11195.694, 8469.703, 8469.675, 8469.756, 8469.214, 8469.769, 8469.773, 8469.771, 8469.773, 8469.773, 8469.773, 12465.564, 8437.739, 8469.773, 16237.244, 8540.299, 8469.773, 8659.034, 8469.773, 8469.773, 8468.845, 8777.418, 8469.773, 8469.613, 8469.771, 8469.773, 8469.773, 10340.241, 8442.968, 8469.773, 9618.076, 8831.525, 8469.773, 9125.565, 9342.941, 8469.747, 8469.312, 8469.773, 9259.125, 8469.771, 8469.771, 8469.771, 8469.501, 8469.773, 8469.771, 9612.917, 8461.871, 15320.527, 8469.773, 10150.859, 8871.461, 9000.717, 8469.773, 8466.995, 9038.455, 8469.773, 8469.773, 8469.741, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8828.443, 8469.744, 8469.656, 8469.773, 8469.773, 8453.567, 8469.76, 9615.907, 8469.769, 8469.773]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:18 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:52 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:94 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.20382428609940764 beta1:0.9196508081663796 beta2:0.9631500832299702 weight_decay:0.0009581669408107036 batch_size:27 epochs:100	100	1000	True	137269.17188		649799	14	-1	303.6581017971039	{'train_loss': [1081043.25, 1066921.0, 1184900.5, 1075172.75, 1097033.125, 1555327.5, 1066921.125, 1080394.75, 1242046.875, 1069287.0, 1104804.75, 1143426.5, 1095327.5, 1071051.125, 1079196.25, 1117629.625, 1117101.625, 1071557.0, 1070938.0, 1077498.125, 1086963.25, 1095632.75, 1069036.625, 1078171.25, 1089901.75, 1102572.375, 1110928.125, 1094133.125, 1069070.125, 1094301.375, 1094703.625, 1131062.375, 1149359.5, 1087856.5, 1082118.875, 1100566.375, 1160493.625, 1124740.75, 1115099.0, 1073251.625, 1074091.375, 1080052.25, 1095262.375, 1116742.625, 1133482.875, 1092048.75, 1075409.375, 1092730.875, 1105541.625, 1128977.875, 1109592.625, 1103420.75, 1100668.875, 1079257.375, 1098743.0, 1101494.0, 1114988.375, 1115228.875, 1099853.0, 1084020.875, 1089845.75, 1095230.75, 1092851.875, 1098679.125, 1077640.0, 1075076.75, 1074617.75, 1093348.5, 1100836.625, 1096517.125, 1084484.0, 1080758.875, 1083233.125, 1079955.875, 1081903.625, 1106544.125, 1084217.0, 1082217.875, 1084637.625, 1080153.25, 1099239.5, 1128946.875, 1109243.625, 1138663.125, 1122166.25, 1113668.375, 1111419.625, 1137514.25, 1092797.625, 1098391.625, 1085315.375, 1082834.875, 1082320.375, 1079196.0, 1100689.125, 1086055.75, 1100606.625, 1082526.875, 1090982.125, 1084373.375], 'val_loss': [8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.771, 71847.344, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8771.382, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 9375.76, 8469.773, 8469.773, 8469.773, 8469.773, 9227.251, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8925.222, 8469.773, 8468.36, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 9618.376, 8469.773, 8469.773, 8469.773, 9164.73, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 9645.113, 8469.773, 8469.773, 8469.773, 8469.773, 8469.772, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8744.277, 8741.078, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 9603.474, 8469.773, 8469.773, 8469.773]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:18 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.20382428609940764 batch_size:29 epochs:100	100	1000	True	30476.75391		2883865	15	-1	304.70558619499207	{'train_loss': [533908.875, 497200.938, 468853.469, 431744.688, 384657.375, 341107.375, 311614.312, 296270.781, 289373.875, 284576.5, 279522.062, 274286.406, 269843.844, 266749.062, 263159.125, 260715.094, 258179.562, 256366.562, 254378.688, 252610.312, 251178.562, 250747.047, 249234.719, 247916.484, 246417.391, 245772.219, 244687.484, 243676.297, 243315.953, 242183.5, 240991.422, 240241.219, 239276.406, 238919.219, 238593.5, 237304.875, 236651.703, 235734.797, 234355.516, 234966.141, 233252.297, 233329.891, 231826.828, 231450.906, 230998.672, 229877.172, 228862.281, 228708.531, 228667.641, 227343.297, 227183.484, 226713.562, 226334.891, 225090.938, 225227.188, 224380.266, 223816.312, 223991.125, 222801.406, 221039.422, 222379.406, 220366.156, 220827.688, 219705.047, 218845.531, 220001.844, 219080.703, 217853.938, 217975.031, 216002.0, 215896.0, 216089.172, 215121.781, 215389.734, 214316.922, 213869.922, 213669.906, 213845.562, 212914.484, 212475.859, 211886.016, 211729.156, 210829.516, 210045.922, 210419.516, 210609.156, 209279.625, 208577.391, 208236.562, 207327.484, 208087.531, 207618.719, 207001.609, 206673.859, 205695.906, 205337.266, 205328.109, 204871.562, 204737.328, 204646.766], 'val_loss': [3267.378, 3464.091, 3058.668, 2838.798, 2800.513, 2552.599, 2531.477, 2439.647, 2437.921, 2374.055, 2388.431, 2343.754, 2257.244, 2212.531, 2237.315, 2141.142, 2199.23, 2175.461, 2121.396, 2117.465, 2123.029, 2102.063, 2115.781, 2104.094, 2072.204, 2102.944, 2088.135, 2071.064, 2086.545, 2043.824, 2078.726, 2022.213, 2057.378, 2024.423, 2013.108, 2015.735, 2016.476, 2016.572, 2011.315, 2003.945, 2005.19, 2020.536, 2008.519, 1984.206, 1991.514, 1974.822, 1962.344, 1983.287, 1973.87, 1977.63, 1973.197, 1954.032, 1958.318, 1952.002, 1958.389, 1938.869, 1941.899, 1946.148, 1941.677, 1926.714, 1921.151, 1929.29, 1915.222, 1900.453, 1906.187, 1935.995, 1898.43, 1920.292, 1895.163, 1903.872, 1905.85, 1914.796, 1916.551, 1885.644, 1887.334, 1885.218, 1872.876, 1898.381, 1879.794, 1870.275, 1870.46, 1887.703, 1883.582, 1863.565, 1854.792, 1862.266, 1869.146, 1876.792, 1857.532, 1864.284, 1882.453, 1887.008, 1887.526, 1855.525, 1871.08, 1884.638, 1850.743, 1862.871, 1863.345, 1863.3]}	100	100	True
