id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	3000	True	32874.0625		675818	15	-1	316.85255217552185	{'train_loss': [418016.156, 347158.062, 324691.031, 311018.594, 301917.094, 294855.75, 289792.844, 285188.969, 282379.969, 279704.25, 275547.125, 272732.75, 270653.469, 268942.219, 266387.875, 264381.469, 262720.219, 261213.188, 260616.438, 259251.766, 257937.719, 256677.391, 255314.422, 254398.25, 253168.094, 252157.219, 250348.344, 250882.25, 249899.234, 249706.625, 248785.109, 248313.375, 247312.906, 244783.828, 246369.891, 244338.578, 244544.266, 243945.203, 243349.25, 243204.5, 242753.828, 241358.109, 241172.734, 240297.766, 240640.203, 239587.594, 239080.906, 239006.188, 238495.125, 238326.859, 237634.047, 238273.188, 236697.203, 236231.766, 235951.453, 236559.625, 235520.062, 233987.125, 234291.953, 234360.531, 233347.0, 234138.891, 233429.594, 232751.609, 231814.562, 231342.5, 232648.062, 232074.062, 231231.531, 230601.125, 230095.281, 230881.516, 230007.875, 228207.281, 229176.312, 229215.969, 229183.281, 228094.953, 227923.438, 227736.359, 228641.781, 226906.531, 226725.25, 226800.109, 227137.469, 226585.281, 226323.406, 225894.703, 225607.344, 226330.125, 225274.328, 225201.406, 224337.25, 223780.078, 224519.719, 223776.297, 223886.812, 223828.391, 222891.0, 223184.922], 'val_loss': [2578.587, 2362.415, 2199.033, 2173.613, 2089.165, 2067.209, 2007.708, 2024.686, 1985.377, 1976.05, 1950.445, 1915.303, 1913.613, 1898.735, 1895.52, 1887.344, 1846.709, 1861.613, 1847.5, 1833.76, 1846.758, 1812.528, 1796.843, 1813.37, 1793.828, 1812.112, 1796.843, 1788.632, 1786.228, 1789.181, 1790.997, 1782.62, 1777.512, 1775.735, 1767.543, 1763.947, 1776.419, 1768.351, 1769.607, 1785.544, 1767.797, 1768.312, 1765.147, 1753.729, 1762.142, 1756.839, 1759.116, 1735.49, 1762.309, 1763.591, 1747.465, 1754.566, 1745.879, 1751.007, 1754.467, 1727.269, 1749.517, 1749.097, 1755.213, 1770.001, 1739.877, 1749.206, 1741.64, 1739.311, 1722.183, 1742.66, 1723.441, 1739.184, 1742.652, 1730.957, 1750.119, 1716.814, 1732.383, 1725.2, 1725.265, 1716.7, 1732.345, 1711.0, 1732.581, 1720.128, 1713.319, 1710.544, 1721.838, 1708.647, 1704.178, 1718.659, 1719.251, 1736.881, 1708.264, 1714.413, 1710.208, 1710.668, 1703.75, 1714.907, 1719.523, 1696.286, 1705.916, 1716.197, 1711.477, 1708.72]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:67 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:67 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:97 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:53 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.2103871844621022 batch_size:4 epochs:100	100	3000	True	34238.63281		410005	16	-1	1269.969070672989	{'train_loss': [356682.469, 321154.75, 314032.438, 310239.344, 307016.375, 303393.625, 301017.625, 298129.344, 295591.281, 293410.469, 290126.75, 288416.656, 286417.781, 284655.719, 282288.25, 280192.031, 279466.781, 278561.656, 276391.531, 276463.531, 274338.375, 273645.625, 273017.625, 272431.844, 270717.906, 271508.531, 268819.938, 268880.625, 268243.562, 268111.188, 266958.719, 265963.406, 264820.938, 265057.5, 264611.188, 262794.594, 263008.312, 262761.031, 262071.969, 262521.719, 260662.688, 260860.672, 260449.219, 260202.641, 259377.281, 260237.609, 260123.859, 257935.141, 258405.688, 256899.391, 257095.0, 256919.891, 256679.719, 257433.859, 257010.875, 256193.5, 255813.984, 254947.875, 254713.719, 255057.703, 255791.031, 253656.344, 255043.969, 253908.609, 254362.781, 253627.453, 252303.375, 252818.766, 251791.312, 253024.859, 251812.922, 251737.5, 251911.172, 251277.766, 250689.109, 250639.922, 250426.812, 250258.641, 250155.016, 250436.5, 250147.422, 249921.703, 249600.172, 248818.625, 249690.688, 248847.031, 249312.672, 248063.703, 247555.766, 248294.516, 248588.109, 247781.438, 247143.938, 246341.219, 246749.609, 246833.219, 247195.828, 247022.266, 246505.172, 246754.031], 'val_loss': [378.883, 366.407, 362.711, 357.012, 353.735, 350.037, 347.48, 344.27, 341.213, 339.313, 339.495, 338.168, 337.376, 332.846, 332.559, 331.613, 332.083, 328.741, 328.181, 327.696, 326.852, 329.57, 324.939, 324.541, 325.572, 326.952, 327.14, 322.768, 323.247, 325.177, 326.524, 320.88, 320.981, 317.914, 318.021, 320.407, 319.369, 324.8, 315.026, 316.836, 321.561, 316.473, 314.333, 313.991, 320.543, 317.593, 322.03, 315.931, 315.567, 318.063, 320.576, 310.038, 314.045, 313.956, 314.247, 321.436, 311.818, 314.184, 314.827, 315.365, 311.436, 309.338, 312.54, 317.716, 314.22, 315.947, 313.092, 316.836, 314.68, 315.34, 310.607, 311.444, 316.003, 317.202, 314.59, 317.538, 313.966, 307.305, 316.889, 309.357, 307.145, 309.27, 316.554, 312.481, 312.329, 306.697, 309.945, 308.406, 304.95, 311.661, 303.678, 310.133, 311.138, 306.302, 308.509, 310.317, 309.483, 309.799, 306.292, 310.593]}	20	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	3000	True	31302.00586		675818	15	-1	315.8712282180786	{'train_loss': [435287.344, 345648.125, 322506.188, 309577.312, 300576.344, 295294.469, 290375.906, 286677.156, 282878.25, 279751.094, 277731.875, 274819.844, 271807.719, 269596.594, 266155.344, 264347.594, 261859.047, 260813.891, 259367.75, 256447.875, 254469.031, 252988.438, 251220.609, 250288.844, 248810.422, 246935.281, 245472.328, 245548.031, 243378.75, 242959.812, 241265.391, 240751.953, 240007.031, 239083.266, 238308.109, 237237.766, 235878.016, 235486.672, 233941.844, 233978.234, 233201.719, 231992.5, 232059.828, 232068.391, 231192.719, 230934.172, 229711.953, 229616.391, 228653.0, 228579.109, 227798.984, 227264.078, 226213.25, 225722.281, 226526.156, 226691.781, 226064.406, 224447.562, 224812.391, 224369.594, 223774.266, 223542.656, 224090.688, 222758.922, 222748.844, 221865.578, 220917.469, 222169.766, 221889.125, 220557.172, 220519.172, 219825.609, 219522.359, 219356.016, 219544.047, 217929.266, 218369.484, 217924.375, 217771.812, 217754.125, 217743.516, 217124.219, 217906.828, 216904.875, 216558.781, 216324.297, 216023.703, 215485.891, 215681.766, 215173.453, 214616.438, 213743.609, 214125.891, 215141.188, 213774.422, 213561.594, 213694.031, 213648.891, 212280.891, 212282.969], 'val_loss': [2548.747, 2254.42, 2157.047, 2113.635, 2049.948, 2041.471, 2028.624, 1984.435, 1986.334, 1961.252, 1951.505, 1925.888, 1911.291, 1900.31, 1902.863, 1901.782, 1889.988, 1865.615, 1849.967, 1854.963, 1829.587, 1837.042, 1840.725, 1844.536, 1824.783, 1815.01, 1803.402, 1827.145, 1776.954, 1793.348, 1774.365, 1786.162, 1751.633, 1750.17, 1752.018, 1779.769, 1731.821, 1742.327, 1735.36, 1731.454, 1710.19, 1707.869, 1703.896, 1702.242, 1695.65, 1702.531, 1679.456, 1716.051, 1694.79, 1699.562, 1682.88, 1703.693, 1686.076, 1676.354, 1681.061, 1683.581, 1668.938, 1668.2, 1674.431, 1665.207, 1659.421, 1668.329, 1653.876, 1661.707, 1660.947, 1661.939, 1660.967, 1662.056, 1654.293, 1655.84, 1648.275, 1657.312, 1658.702, 1645.424, 1661.362, 1642.355, 1639.678, 1631.028, 1628.974, 1650.666, 1632.459, 1648.967, 1631.337, 1638.372, 1645.527, 1643.27, 1638.792, 1625.425, 1633.222, 1643.344, 1636.698, 1636.337, 1639.586, 1628.282, 1636.405, 1633.687, 1631.434, 1623.708, 1641.573, 1637.568]}	0	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	3000	True	31302.00781		675818	15	-1	315.6665267944336	{'train_loss': [435287.344, 345648.125, 322506.188, 309577.312, 300576.344, 295294.469, 290375.906, 286677.156, 282878.25, 279751.094, 277731.875, 274819.844, 271807.719, 269596.594, 266155.344, 264347.594, 261859.047, 260813.891, 259367.75, 256447.875, 254469.031, 252988.438, 251220.609, 250288.844, 248810.422, 246935.281, 245472.328, 245548.031, 243378.75, 242959.812, 241265.391, 240751.953, 240007.031, 239083.266, 238308.109, 237237.766, 235878.016, 235486.672, 233941.844, 233978.234, 233201.719, 231992.5, 232059.828, 232068.391, 231192.719, 230934.172, 229711.953, 229616.391, 228653.0, 228579.109, 227798.984, 227264.078, 226213.25, 225722.281, 226526.156, 226691.781, 226064.406, 224447.562, 224812.391, 224369.594, 223774.266, 223542.656, 224090.688, 222758.922, 222748.844, 221865.578, 220917.469, 222169.766, 221889.125, 220557.172, 220519.172, 219825.609, 219522.359, 219356.016, 219544.047, 217929.266, 218369.484, 217924.375, 217771.812, 217754.125, 217743.516, 217124.219, 217906.828, 216904.875, 216558.781, 216324.297, 216023.703, 215485.891, 215681.766, 215173.453, 214616.438, 213743.609, 214125.891, 215141.188, 213774.422, 213561.594, 213694.031, 213648.891, 212280.891, 212282.969], 'val_loss': [2548.747, 2254.42, 2157.047, 2113.635, 2049.948, 2041.471, 2028.624, 1984.435, 1986.334, 1961.252, 1951.505, 1925.888, 1911.291, 1900.31, 1902.863, 1901.782, 1889.988, 1865.615, 1849.967, 1854.963, 1829.587, 1837.042, 1840.725, 1844.536, 1824.783, 1815.01, 1803.402, 1827.145, 1776.954, 1793.348, 1774.365, 1786.162, 1751.633, 1750.17, 1752.018, 1779.769, 1731.821, 1742.327, 1735.36, 1731.454, 1710.19, 1707.869, 1703.896, 1702.242, 1695.65, 1702.531, 1679.456, 1716.051, 1694.79, 1699.562, 1682.88, 1703.693, 1686.076, 1676.354, 1681.061, 1683.581, 1668.938, 1668.2, 1674.431, 1665.207, 1659.421, 1668.329, 1653.876, 1661.707, 1660.947, 1661.939, 1660.967, 1662.056, 1654.293, 1655.84, 1648.275, 1657.312, 1658.702, 1645.424, 1661.362, 1642.355, 1639.678, 1631.028, 1628.974, 1650.666, 1632.459, 1648.967, 1631.337, 1638.372, 1645.527, 1643.27, 1638.792, 1625.425, 1633.222, 1643.344, 1636.698, 1636.337, 1639.586, 1628.282, 1636.405, 1633.687, 1631.434, 1623.708, 1641.573, 1637.568]}	0	100	True
