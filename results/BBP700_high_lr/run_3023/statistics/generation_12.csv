id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:18 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.20382428609940764 batch_size:29 epochs:100	100	1000	True	31717.15625		2883865	15	-1	302.47032046318054	{'train_loss': [529863.188, 490068.188, 468063.375, 433667.344, 376796.719, 332126.5, 307813.219, 294202.594, 286148.062, 281705.938, 276576.781, 272462.875, 269389.219, 265552.219, 263186.938, 260898.859, 258200.188, 256950.609, 253862.688, 252923.156, 251859.266, 249496.906, 248240.125, 246160.609, 246721.859, 243752.75, 243348.781, 242236.5, 241163.125, 240759.969, 238583.625, 237895.656, 237106.031, 235155.797, 234746.125, 233739.016, 233468.234, 232557.25, 231825.156, 229871.844, 231512.906, 229236.328, 227862.922, 228940.141, 226907.625, 226168.562, 226023.078, 226072.5, 224772.359, 223566.656, 223254.109, 222841.266, 221840.719, 221118.188, 221058.438, 219597.328, 219509.594, 219626.766, 218874.844, 217987.969, 218152.5, 216905.234, 217327.562, 217052.266, 214893.438, 216136.984, 215456.875, 213465.844, 214127.453, 213563.062, 212912.234, 211567.688, 212222.359, 211362.688, 212138.578, 210013.812, 210464.109, 209337.734, 209760.797, 208415.5, 209136.266, 209233.062, 208082.188, 208065.062, 206824.109, 206996.219, 205934.484, 206563.984, 205511.016, 204974.75, 205273.672, 204735.953, 203899.203, 204488.141, 202658.375, 203085.094, 202192.719, 202317.344, 201885.516, 202157.266], 'val_loss': [3758.695, 3580.7, 3158.246, 3089.113, 2772.307, 2497.123, 2397.352, 2340.084, 2274.559, 2273.815, 2246.773, 2220.338, 2206.809, 2167.245, 2182.803, 2165.907, 2120.839, 2108.328, 2126.58, 2154.26, 2092.896, 2119.297, 2082.528, 2083.626, 2075.13, 2044.802, 2044.111, 2038.358, 2020.169, 2029.513, 2024.121, 2028.745, 2017.118, 2001.299, 1997.322, 2006.152, 1995.289, 2012.126, 1986.935, 1971.357, 1982.793, 1984.448, 2001.816, 1964.534, 1959.884, 1947.246, 1952.718, 1932.401, 1932.158, 1947.668, 1948.938, 1929.733, 1937.847, 1936.346, 1933.544, 1948.696, 1916.445, 1923.568, 1911.45, 1904.612, 1930.448, 1920.68, 1915.816, 1908.53, 1910.521, 1937.991, 1900.006, 1919.4, 1923.891, 1910.402, 1897.073, 1885.922, 1934.673, 1921.812, 1909.807, 1915.583, 1913.108, 1908.766, 1883.601, 1906.596, 1895.641, 1914.91, 1924.321, 1897.265, 1913.663, 1943.773, 1914.192, 1926.667, 1911.641, 1906.592, 1889.526, 1908.538, 1921.83, 1914.492, 1909.569, 1904.806, 1919.942, 1916.668, 1894.244, 1915.804]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:18 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.16998396055344062 batch_size:29 epochs:100	100	1000	True	32255.88477		1636781	15	-1	285.6167907714844	{'train_loss': [487279.688, 441168.094, 401330.625, 359092.406, 333441.188, 317542.5, 308561.562, 299569.844, 294376.656, 288541.344, 283671.219, 279051.875, 276066.719, 273487.531, 269480.156, 267327.062, 265529.719, 264006.438, 261243.141, 259540.062, 258911.688, 257150.125, 255443.078, 254239.047, 253382.266, 251810.047, 250936.766, 249780.656, 248400.641, 247584.547, 247416.375, 245997.562, 246568.734, 245208.719, 244486.828, 242648.266, 242598.562, 242258.75, 240237.781, 240700.453, 240166.375, 239098.422, 238966.484, 238033.75, 237226.297, 236574.406, 236708.75, 235867.625, 236557.797, 235068.625, 234795.25, 234321.047, 234738.359, 233247.25, 232069.047, 232513.656, 231751.125, 232503.656, 231483.5, 231061.297, 230345.328, 229466.625, 229717.328, 229774.344, 228476.656, 228419.625, 228304.703, 228351.922, 228350.109, 227104.219, 227247.141, 225973.75, 226055.375, 226272.125, 226064.812, 224818.156, 224702.25, 223498.875, 223262.594, 223708.844, 223115.0, 222933.812, 222770.844, 221999.375, 222349.812, 222295.297, 221510.391, 221810.078, 221141.688, 220845.344, 220783.219, 220103.266, 220993.234, 219825.609, 219288.734, 219269.266, 218912.641, 219511.844, 217977.281, 217483.031], 'val_loss': [3058.812, 3268.848, 2915.723, 2744.036, 2552.212, 2426.426, 2444.603, 2353.431, 2297.873, 2263.19, 2249.344, 2283.257, 2246.759, 2205.363, 2208.541, 2176.452, 2168.539, 2124.499, 2145.626, 2121.248, 2106.686, 2117.222, 2092.602, 2082.161, 2075.439, 2057.973, 2069.692, 2058.803, 2049.39, 2061.16, 2036.805, 2052.743, 2049.303, 2017.374, 2021.804, 2027.732, 2020.222, 2008.386, 2012.111, 2011.547, 1998.97, 2002.775, 1990.726, 1992.096, 1976.658, 1984.15, 2010.825, 1981.957, 1978.364, 1977.209, 2003.418, 2003.569, 1971.904, 1957.863, 1971.247, 1964.127, 1979.021, 1945.429, 1976.397, 1973.324, 1946.94, 1989.298, 1962.308, 1979.11, 1947.48, 1945.689, 1942.728, 1940.006, 1959.197, 1958.227, 1954.727, 1951.122, 1956.497, 1948.424, 1961.596, 1940.265, 1935.595, 1939.565, 1946.349, 1942.635, 1963.401, 1935.293, 1945.983, 1949.833, 1958.04, 1929.474, 1941.705, 1936.996, 1945.499, 1960.265, 1954.796, 1945.775, 1951.595, 1962.546, 1974.877, 1955.735, 1943.787, 1927.67, 1947.35, 1968.172]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:18 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:14 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.20382428609940764 alpha:0.9185239478457888 weight_decay:7.179909604772032e-05 batch_size:29 epochs:100	100	1000	True	137278.73438		1581477	14	-1	258.54017782211304	{'train_loss': [1772021.875, 1067104.875, 3238869.75, 1066921.0, 7795562.0, 1066921.0, 1066921.0, 5047873.5, 1066921.0, 1277544.625, 10039563.0, 7811938.5, 1066921.0, 1066921.0, 5826612.5, 1066921.0, 2378142.75, 10398769.0, 1066921.0, 7768442.0, 1066921.0, 11078767.0, 1066921.0, 1066921.0, 7888638.5, 1066921.0, 1206685.0, 1066921.0, 8642053.0, 1066921.0, 1066921.0, 5141679.0, 1066921.0, 7417271.0, 1066921.0, 1066921.0, 15542088.0, 1066921.0, 11976770.0, 1066921.0, 5328441.5, 1066921.0, 1066921.0, 13293710.0, 1066921.0, 2301302.25, 6914159.0, 6417577.5, 1066921.0, 11815672.0, 1066921.0, 8142294.5, 1066921.0, 1066921.0, 13062137.0, 1066946.875, 2130295.25, 1066921.0, 1887260.625, 1067274.875, 1066921.0, 3470924.75, 1066921.0, 3128270.75, 3081129.0, 1073584.75, 14701766.0, 1066921.0, 6692886.0, 1079225.125, 2211530.5, 3110032.25, 4970117.0, 2202694.5, 2462310.5, 7480790.0, 1277594.25, 1074201.25, 5181933.0, 7150432.5, 1171382.875, 6484893.5, 3314130.0, 1752208.25, 1797149.0, 2851625.0, 2982753.25, 4112255.25, 4957984.5, 1066924.625, 6701294.5, 1066921.0, 8774184.0, 1066921.0, 1066921.0, 12621497.0, 1066921.0, 10205630.0, 1066921.0, 12534379.0], 'val_loss': [9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.757, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.757, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 145077.844, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 33369.621, 13145.596, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:8 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:8 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:18 kernel_size:8 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:14 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.20382428609940764 batch_size:29 epochs:100	100	1000	True	32610.76953		890417	15	-1	260.6194541454315	{'train_loss': [452288.688, 374812.188, 348178.656, 332209.219, 320230.625, 310733.344, 302816.25, 295216.5, 290552.781, 284749.281, 281648.562, 279063.094, 276301.562, 273781.0, 272453.375, 270260.625, 268989.344, 267889.719, 265924.406, 265040.156, 263109.688, 262471.625, 261075.906, 260448.062, 259221.297, 258258.906, 257217.641, 256414.547, 254874.828, 255193.812, 254005.562, 253335.875, 252579.484, 251487.062, 251606.062, 251185.766, 250314.75, 248995.609, 248588.656, 248509.875, 247491.922, 247595.234, 247361.672, 245899.969, 245549.266, 245030.094, 245521.031, 245059.5, 243634.422, 243213.406, 241645.938, 241871.172, 242217.062, 241215.766, 240597.141, 240596.688, 240158.047, 239747.391, 240359.266, 239585.219, 238917.359, 238559.281, 238412.594, 238532.0, 238223.25, 237036.484, 236523.75, 236395.203, 236015.969, 236273.703, 235295.328, 235335.062, 235408.5, 234307.453, 235304.672, 234080.172, 233969.328, 233623.406, 234237.906, 233191.516, 233428.391, 233565.375, 232414.797, 232460.562, 231579.812, 232082.547, 231503.25, 231527.875, 230956.875, 230597.391, 230632.812, 230143.922, 230347.688, 229318.859, 228029.094, 230116.906, 229821.484, 229102.875, 227824.109, 227867.109], 'val_loss': [3263.056, 2815.145, 2653.927, 2580.138, 2486.514, 2430.177, 2372.587, 2332.077, 2263.67, 2261.609, 2221.925, 2215.412, 2230.921, 2214.333, 2209.662, 2179.666, 2144.139, 2167.293, 2139.083, 2126.755, 2106.532, 2124.782, 2095.388, 2117.499, 2103.153, 2078.4, 2084.432, 2050.695, 2053.475, 2056.657, 2070.5, 2045.475, 2038.224, 2038.869, 2041.712, 2019.26, 2020.783, 2015.721, 2013.358, 2005.85, 1997.456, 2001.303, 2004.082, 2011.467, 1989.049, 1984.079, 1978.379, 1978.714, 1978.383, 1988.483, 1990.419, 1974.424, 1964.193, 1973.429, 1976.433, 1970.816, 1961.746, 1953.512, 1953.672, 1956.435, 1954.441, 1947.502, 1958.678, 1946.534, 1939.494, 1933.406, 1944.129, 1934.872, 1939.676, 1930.354, 1929.745, 1924.188, 1914.741, 1910.949, 1932.942, 1920.983, 1917.677, 1909.002, 1924.765, 1920.927, 1930.202, 1907.839, 1912.273, 1913.469, 1927.121, 1914.018, 1910.837, 1920.58, 1912.319, 1909.242, 1919.209, 1898.799, 1919.083, 1900.364, 1901.364, 1902.98, 1912.091, 1903.519, 1887.463, 1911.136]}	100	100	True
