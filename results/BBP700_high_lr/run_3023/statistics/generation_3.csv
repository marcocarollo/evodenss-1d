id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.1864603712075097 batch_size:19 epochs:100	100	1000	True	33467.38672		429236	16	-1	388.4937996864319	{'train_loss': [373443.938, 328961.406, 317352.406, 309709.188, 304392.375, 300130.25, 296602.562, 292936.562, 290742.0, 287695.812, 285035.219, 282111.469, 280043.156, 278587.344, 276938.562, 275240.062, 273687.031, 272878.312, 271615.688, 270980.844, 269522.125, 268844.031, 267687.969, 267546.062, 266400.125, 265073.406, 264433.375, 264125.969, 263254.938, 262665.281, 261850.984, 261405.484, 261419.312, 260528.891, 260052.25, 259520.781, 259597.297, 258988.422, 257742.609, 257778.359, 257374.203, 256891.516, 256510.25, 255800.25, 255967.438, 255184.328, 255067.875, 254505.625, 254199.766, 254332.688, 253741.531, 253397.984, 252624.344, 252159.078, 252862.078, 251964.141, 252234.438, 251813.812, 250797.578, 250907.281, 251352.781, 250213.75, 249881.984, 250219.484, 249554.219, 249204.391, 248971.75, 248720.609, 248650.609, 248419.375, 247938.656, 247641.016, 247027.938, 247313.766, 247300.438, 246849.016, 246480.609, 246309.016, 245704.359, 246619.812, 246579.594, 244729.297, 244931.938, 244364.094, 245628.906, 244732.547, 244601.031, 244144.312, 243627.734, 244204.703, 243556.625, 243800.609, 243072.75, 242229.969, 242673.266, 243480.234, 243151.047, 242693.578, 242383.906, 242109.812], 'val_loss': [1863.819, 1791.311, 1751.687, 1726.671, 1696.438, 1692.422, 1676.701, 1664.893, 1654.496, 1638.243, 1621.041, 1605.947, 1600.098, 1587.502, 1575.275, 1574.846, 1561.861, 1552.749, 1557.073, 1543.16, 1547.828, 1540.828, 1540.79, 1536.149, 1529.956, 1532.9, 1537.099, 1538.512, 1529.13, 1527.144, 1516.0, 1516.137, 1535.471, 1511.542, 1516.935, 1513.349, 1514.994, 1509.397, 1511.199, 1503.417, 1504.176, 1510.403, 1518.683, 1510.326, 1499.479, 1491.733, 1505.053, 1497.406, 1486.727, 1495.469, 1491.033, 1483.804, 1481.954, 1488.291, 1481.215, 1488.095, 1480.027, 1480.557, 1488.76, 1482.558, 1465.546, 1468.407, 1466.066, 1477.435, 1484.268, 1461.999, 1474.445, 1469.131, 1462.578, 1455.985, 1478.143, 1456.838, 1464.031, 1461.059, 1471.089, 1455.3, 1457.953, 1465.173, 1456.575, 1464.819, 1449.678, 1451.101, 1453.597, 1448.451, 1451.282, 1449.792, 1448.775, 1455.936, 1451.791, 1448.404, 1443.836, 1445.185, 1440.002, 1450.989, 1456.074, 1440.179, 1436.134, 1434.125, 1445.411, 1442.208]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.1864603712075097 batch_size:19 epochs:100	100	2000	True	32611.91406		429236	16	-1	388.9002616405487	{'train_loss': [373025.406, 328997.688, 319093.875, 314199.688, 312094.375, 309851.5, 308033.594, 306478.812, 304920.625, 302980.219, 301543.281, 299811.219, 298117.344, 296083.906, 293888.438, 292087.938, 289382.562, 287638.312, 285738.344, 283910.781, 282763.594, 281452.625, 279726.375, 277579.594, 276381.125, 275022.562, 273382.75, 272369.531, 271375.594, 270272.5, 268940.25, 268069.156, 265395.844, 264305.969, 264473.344, 262362.562, 261699.281, 260925.562, 259684.125, 257602.172, 257817.828, 257272.234, 256890.203, 256201.016, 255983.297, 255428.234, 254312.891, 253788.578, 253727.5, 253650.5, 252359.875, 252132.594, 251837.578, 251226.281, 250667.297, 250196.0, 249307.625, 249403.688, 248438.969, 248415.547, 247562.078, 247958.734, 247444.734, 247589.203, 245869.875, 246470.141, 246091.891, 245736.984, 244961.125, 243756.359, 244495.547, 243608.172, 243266.266, 242558.938, 242421.422, 242194.828, 242292.047, 240771.688, 241930.672, 241108.578, 241063.344, 239861.141, 240085.109, 239739.547, 240587.906, 239379.328, 239685.547, 239644.188, 238132.469, 238540.641, 237758.547, 238906.078, 238525.594, 237579.781, 237446.578, 236403.297, 236785.953, 236367.484, 235597.188, 235178.953], 'val_loss': [1842.236, 1787.141, 1754.277, 1739.251, 1733.201, 1728.848, 1724.736, 1720.599, 1713.935, 1709.215, 1699.76, 1693.282, 1689.46, 1677.117, 1662.929, 1660.094, 1634.566, 1628.429, 1624.473, 1616.984, 1610.219, 1606.276, 1594.207, 1598.446, 1585.828, 1577.076, 1569.035, 1557.341, 1553.713, 1539.109, 1532.261, 1519.367, 1516.584, 1510.62, 1507.706, 1502.943, 1499.683, 1496.15, 1489.669, 1487.548, 1482.882, 1473.931, 1475.713, 1464.27, 1461.226, 1461.991, 1462.528, 1447.703, 1455.638, 1447.054, 1455.684, 1440.802, 1438.093, 1424.161, 1443.916, 1424.236, 1430.538, 1424.772, 1427.651, 1427.687, 1403.459, 1425.323, 1396.543, 1400.578, 1395.922, 1398.52, 1394.334, 1389.947, 1389.428, 1393.871, 1392.7, 1393.354, 1372.687, 1372.271, 1381.413, 1401.998, 1362.145, 1365.516, 1373.318, 1366.08, 1369.914, 1366.079, 1367.359, 1370.68, 1371.96, 1354.218, 1362.78, 1359.957, 1355.97, 1361.653, 1352.148, 1360.591, 1356.005, 1352.019, 1349.106, 1355.443, 1342.97, 1365.366, 1350.385, 1344.736]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:53 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:108 kernel_size:10 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:11 layer:deconv1d out_channels:111 kernel_size:6 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adam lr:0.1864603712075097 beta1:0.9637911167504969 beta2:0.9690354804058786 weight_decay:0.0006596485803228374 batch_size:19 epochs:100	100	1000	True	137362.73438		9614894	15	-1	399.7050139904022	{'train_loss': [1110183.375, 1067132.75, 2061459.125, 1067794.375, 1653215.75, 1278225.5, 1228026.125, 1320295.625, 1406533.0, 1132370.875, 1214003.625, 1167398.75, 1134348.125, 1131118.5, 1204046.625, 1204788.125, 1447352.375, 1114660.875, 1322955.25, 1166549.75, 1105223.75, 1135860.25, 1172171.625, 1096777.125, 1181807.75, 1092013.875, 1118504.5, 1118095.25, 1127079.25, 1170281.875, 1116958.25, 1118941.5, 1082524.0, 1149355.0, 1087561.0, 1126022.75, 1092254.5, 1113440.375, 1111080.625, 1114868.75, 1164026.0, 1187262.75, 1080209.125, 1148671.875, 1214748.625, 1074629.625, 1108686.375, 1131777.0, 1081045.625, 1152658.0, 1076118.5, 1079769.25, 1238699.5, 1071182.375, 1088586.625, 1129039.5, 1108999.375, 1094405.375, 1128086.625, 1081799.875, 1118204.0, 1087301.625, 1070402.5, 1077999.75, 1168822.0, 1103032.0, 1095533.25, 1095850.875, 1075267.5, 1085483.25, 1212709.0, 1121011.625, 1084388.125, 1122046.25, 1094367.75, 1157746.0, 1106116.5, 1112517.0, 1095659.75, 1081165.75, 1145861.125, 1076191.0, 1075946.875, 1168167.125, 1128739.375, 1150273.875, 1090587.0, 1121125.625, 1096812.125, 1078855.5, 1093144.25, 1085674.625, 1074975.5, 1208796.5, 1093409.75, 1245329.875, 1087183.75, 1132812.25, 1091691.5, 1070976.125], 'val_loss': [6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 9440.646, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 46361.004, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33, 6352.33]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:29 kernel_size:4 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:22 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:13 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:14 layer:fc act:selu out_features:200 bias:True input:15 learning:adadelta lr:0.1864603712075097 batch_size:19 epochs:100	100	2000	True	32984.91406		426058	17	-1	405.61833453178406	{'train_loss': [383439.062, 350654.75, 324243.344, 317431.062, 312975.688, 310312.25, 308057.312, 305997.75, 303621.562, 301792.594, 300355.844, 298896.844, 297603.344, 296052.0, 293865.594, 291053.719, 288851.281, 285558.406, 283517.438, 281286.219, 280009.531, 277883.094, 276762.062, 275714.219, 273417.344, 272390.0, 271677.156, 270340.906, 268321.062, 268763.375, 266190.125, 265941.594, 264629.562, 264130.469, 263058.562, 262689.594, 261037.109, 260170.75, 259835.719, 259425.938, 258412.359, 257649.094, 256141.562, 255874.484, 255496.422, 254880.266, 253944.406, 251848.75, 252619.344, 251662.875, 251997.812, 250427.219, 251034.25, 251158.531, 248961.141, 248520.703, 248665.828, 247803.609, 247516.516, 246858.828, 247786.281, 246432.078, 246951.484, 245543.891, 246970.062, 244714.922, 245383.359, 245162.578, 243622.0, 243556.0, 244262.125, 244475.734, 243792.844, 244012.0, 243336.156, 243435.953, 242101.641, 244045.156, 241139.844, 241696.578, 241165.891, 241598.562, 241995.766, 240870.906, 241283.438, 240100.047, 239031.766, 240347.516, 240301.594, 239066.422, 239866.531, 238723.703, 239747.469, 239202.844, 239635.438, 239076.828, 238037.797, 238603.172, 237224.812, 237972.016], 'val_loss': [2051.454, 1819.447, 1779.462, 1753.739, 1734.972, 1721.424, 1714.802, 1708.993, 1707.649, 1703.269, 1694.033, 1686.156, 1677.206, 1672.247, 1656.68, 1640.474, 1625.73, 1617.823, 1604.947, 1587.756, 1578.176, 1563.678, 1558.712, 1553.0, 1548.43, 1534.828, 1528.013, 1533.069, 1513.239, 1507.115, 1507.649, 1504.208, 1493.335, 1491.732, 1478.782, 1482.74, 1484.752, 1473.855, 1467.764, 1463.938, 1462.175, 1461.254, 1453.275, 1449.757, 1443.642, 1448.755, 1441.416, 1452.112, 1447.332, 1443.021, 1442.498, 1440.756, 1427.242, 1424.074, 1428.013, 1415.039, 1437.056, 1428.454, 1418.785, 1419.565, 1415.376, 1422.251, 1414.955, 1407.587, 1408.774, 1411.392, 1409.903, 1406.32, 1401.285, 1412.07, 1405.778, 1407.934, 1413.935, 1410.697, 1402.097, 1396.743, 1407.562, 1402.134, 1408.64, 1399.701, 1414.358, 1399.754, 1399.685, 1388.811, 1397.472, 1397.262, 1383.972, 1384.586, 1382.968, 1390.152, 1380.144, 1378.982, 1386.134, 1386.58, 1390.018, 1387.398, 1389.413, 1396.456, 1374.345, 1379.299]}	0	100	True
