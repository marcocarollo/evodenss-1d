id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:29 epochs:100	100	1000	True	32418.44336		625826	15	-1	274.50492095947266	{'train_loss': [436896.344, 354645.25, 331330.188, 317730.781, 309611.188, 302549.469, 297763.219, 292160.25, 288605.156, 285037.031, 283234.75, 280125.125, 277079.594, 275442.531, 272272.375, 271433.562, 270174.75, 267611.531, 265804.594, 265732.062, 264253.812, 262565.844, 262075.0, 260097.609, 259528.391, 258831.219, 257590.844, 257007.453, 254816.156, 255204.156, 254673.969, 253030.641, 252882.203, 251157.219, 250975.391, 249588.172, 248503.25, 249882.328, 248025.328, 246962.094, 247182.0, 245858.062, 244970.609, 244656.875, 244766.062, 244123.141, 243469.766, 242784.734, 242132.172, 242397.297, 242075.516, 240606.969, 240013.734, 240266.141, 239822.844, 239624.312, 238598.984, 238847.062, 238102.922, 237485.688, 236558.359, 235925.891, 236737.141, 236404.625, 235685.641, 234332.109, 234707.656, 233646.672, 233913.203, 234040.25, 233064.672, 232912.891, 231851.047, 232395.281, 231140.891, 231429.297, 230839.0, 231952.625, 229863.75, 229820.422, 229318.406, 230083.375, 228137.562, 228848.828, 228585.609, 228507.141, 227561.234, 228262.594, 226495.766, 226755.203, 226504.109, 225842.016, 225841.625, 225378.266, 225335.266, 225363.734, 225338.891, 224813.906, 223554.641, 224378.281], 'val_loss': [2913.389, 2609.204, 2506.749, 2441.589, 2403.571, 2366.448, 2372.934, 2307.428, 2295.905, 2308.643, 2291.15, 2271.19, 2234.947, 2227.975, 2224.244, 2200.794, 2216.626, 2199.885, 2186.906, 2175.631, 2186.074, 2188.532, 2153.27, 2170.907, 2154.493, 2172.665, 2163.822, 2144.449, 2137.414, 2115.117, 2130.16, 2130.037, 2141.161, 2128.29, 2120.048, 2103.285, 2104.499, 2104.617, 2107.511, 2119.219, 2147.248, 2117.978, 2110.99, 2092.271, 2119.594, 2103.716, 2085.946, 2078.656, 2085.775, 2074.85, 2098.382, 2097.408, 2074.65, 2066.392, 2064.938, 2046.958, 2062.81, 2068.548, 2054.912, 2092.637, 2048.33, 2058.445, 2058.399, 2048.344, 2052.635, 2014.752, 2054.135, 2051.7, 2031.545, 2043.169, 2053.387, 2040.233, 2040.809, 2029.689, 2035.646, 2020.551, 2013.102, 2014.213, 2041.803, 2027.551, 2033.028, 2032.112, 2017.775, 2023.778, 2012.611, 2016.711, 2005.604, 2018.65, 1999.797, 2011.411, 1999.366, 2002.131, 2015.615, 2013.793, 1978.611, 2000.292, 1992.888, 1991.404, 1997.458, 1980.852]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:10 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:11 layer:deconv1d out_channels:10 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:rmsprop lr:0.2103871844621022 alpha:0.9030508662295601 weight_decay:5.0454952757116117e-05 batch_size:29 epochs:100	100	1000	True	137307.53125		3571680	15	-1	277.87276124954224	{'train_loss': [1297290.0, 3785523.75, 1066921.0, 2064480.625, 1066921.0, 2735074.0, 1088490.5, 7308346.0, 1066921.0, 3720792.5, 1066921.0, 3471629.5, 1066921.0, 15417471.0, 1066921.0, 1066921.0, 1656135.125, 1066921.0, 3025360.5, 1066921.0, 20021358.0, 1066921.0, 22255026.0, 2564778.25, 3765895.25, 1066921.0, 13583793.0, 1067056.875, 3137284.25, 1485864.5, 1121325.875, 5931373.5, 1079181.375, 2236207.0, 1145002.375, 2013326.125, 7974851.5, 1067141.0, 8537627.0, 1082203.375, 1393688.625, 2657924.75, 1066927.5, 2332992.0, 1123981.75, 1655584.375, 1927201.25, 1366799.125, 2471522.75, 1595305.5, 1461587.0, 2542073.5, 1322772.0, 1746161.5, 1219383.125, 1475608.125, 1838598.25, 1540445.875, 1522581.75, 1559397.125, 1366730.625, 1777694.25, 2253453.75, 1524679.0, 1842384.125, 1578211.375, 1538624.375, 1301173.625, 1557627.375, 1824981.625, 1284381.875, 1857816.0, 1299131.0, 1408449.0, 1645167.125, 1822560.375, 1262707.5, 1850609.625, 1472484.125, 1616173.75, 2307731.5, 2158660.5, 2115790.25, 2328296.25, 2334896.5, 1378477.75, 1852525.25, 1919643.0, 2172371.25, 2103817.0, 1874823.875, 1656005.25, 1556194.625, 1557122.25, 2582369.0, 1410213.375, 1880751.875, 2136533.75, 2475546.0, 1236836.125], 'val_loss': [9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 87762.266, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 739822.0, 9074.758, 9074.758, 9074.758, 9074.758, 37933.961, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 10007.209, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 67973.906, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 9074.758, 28332.309, 9074.758, 9074.758, 9074.758, 9074.758, 24385.357, 9074.758, 9074.758, 9074.758, 42094.363, 9074.536, 9074.758, 9074.758, 9074.758]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	1000	True	31614.28711		675818	15	-1	311.34643721580505	{'train_loss': [430470.281, 349324.219, 327086.344, 314821.219, 306659.594, 300022.469, 294598.906, 290981.906, 287531.469, 284779.781, 282763.125, 281172.625, 277856.031, 275705.625, 273712.062, 271270.969, 270449.0, 269735.594, 267149.0, 265858.125, 264333.094, 262500.031, 261638.266, 260287.75, 258168.906, 257416.156, 256276.141, 255259.406, 254451.016, 253377.516, 254385.391, 251540.688, 250954.25, 250069.844, 249618.297, 247190.109, 246782.359, 246830.484, 246129.688, 244798.719, 243368.312, 243597.125, 242869.125, 242258.859, 241814.328, 241549.609, 241175.438, 239597.047, 239685.828, 238686.25, 238276.031, 237652.328, 236937.953, 237434.016, 236137.547, 235982.875, 234719.219, 234528.875, 233603.297, 234349.594, 233716.656, 232984.125, 232382.609, 232216.703, 232062.641, 230302.562, 231567.938, 230384.578, 230833.328, 229638.891, 228745.141, 228687.125, 228862.25, 228355.438, 227974.953, 227586.281, 227253.688, 226795.094, 225918.172, 226509.75, 226334.297, 226029.438, 226629.531, 225006.0, 224018.656, 223779.406, 224031.969, 223003.938, 223195.828, 222843.203, 223449.031, 223131.781, 222571.672, 221824.328, 222132.234, 221937.438, 220772.484, 219744.719, 220550.531, 220851.25], 'val_loss': [2569.682, 2293.55, 2192.607, 2116.423, 2095.016, 2070.648, 2066.072, 2037.945, 2044.108, 2012.816, 1992.499, 1986.625, 1964.74, 1964.433, 1945.266, 1951.476, 1927.777, 1900.812, 1898.391, 1882.17, 1887.112, 1860.728, 1879.248, 1844.263, 1849.762, 1846.427, 1854.15, 1832.256, 1839.596, 1822.624, 1815.125, 1793.449, 1806.331, 1796.006, 1791.773, 1795.667, 1785.458, 1772.054, 1768.459, 1772.018, 1770.016, 1760.573, 1755.461, 1749.385, 1741.759, 1740.856, 1751.061, 1741.439, 1732.188, 1744.169, 1719.604, 1713.366, 1731.608, 1702.962, 1704.65, 1729.442, 1707.699, 1696.259, 1717.471, 1702.076, 1713.894, 1692.759, 1698.665, 1687.033, 1686.656, 1687.308, 1712.618, 1665.822, 1680.198, 1676.329, 1672.351, 1672.5, 1676.408, 1674.28, 1666.014, 1683.662, 1665.631, 1661.416, 1668.638, 1667.162, 1658.136, 1661.46, 1670.983, 1657.361, 1667.282, 1659.373, 1669.358, 1655.033, 1657.744, 1653.801, 1663.021, 1661.476, 1654.956, 1649.572, 1641.144, 1654.135, 1647.418, 1647.77, 1645.157, 1666.608]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:deconv1d out_channels:58 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.2103871844621022 batch_size:29 epochs:100	100	1000	True	32777.23438		566780	14	-1	252.91055560112	{'train_loss': [452401.656, 348325.094, 325095.781, 312372.562, 303794.688, 298561.469, 293191.688, 289928.688, 285352.125, 282675.906, 280528.688, 277303.25, 275206.781, 273389.781, 271319.031, 268490.344, 267508.625, 266487.406, 264656.281, 263483.375, 262277.062, 261550.484, 260573.078, 259573.422, 258229.578, 257237.781, 257260.031, 255507.172, 254721.109, 253876.062, 253794.547, 252524.625, 252386.234, 251934.531, 251113.266, 250471.656, 249653.125, 249325.906, 248549.688, 248461.031, 248231.297, 247507.25, 247544.797, 246370.609, 245997.922, 245233.531, 245384.156, 244841.406, 245438.969, 244130.984, 244325.453, 243416.734, 243658.797, 242906.0, 242419.797, 242720.062, 242399.516, 241781.078, 240786.016, 241336.625, 240966.781, 239560.578, 240463.531, 239544.156, 239741.297, 239160.438, 238742.953, 238813.891, 238109.656, 238489.375, 237746.75, 237785.391, 237696.359, 237112.359, 236687.203, 236936.75, 235872.688, 235910.594, 235818.359, 235166.031, 235622.75, 234881.562, 235005.875, 234053.062, 235276.75, 233990.922, 233749.844, 234080.094, 233090.156, 233666.172, 232810.984, 233025.125, 231772.719, 232881.734, 231548.516, 232249.75, 230726.422, 231281.656, 231162.547, 231174.609], 'val_loss': [2809.212, 2639.528, 2485.927, 2445.816, 2412.443, 2396.258, 2362.907, 2345.455, 2341.373, 2299.744, 2313.547, 2290.52, 2262.083, 2269.289, 2233.016, 2239.518, 2239.791, 2215.571, 2206.354, 2198.854, 2195.192, 2186.469, 2170.033, 2162.964, 2160.333, 2143.304, 2165.555, 2135.514, 2140.985, 2148.509, 2125.948, 2115.751, 2118.202, 2117.881, 2104.798, 2098.623, 2092.325, 2083.923, 2097.22, 2079.542, 2076.326, 2086.18, 2086.762, 2082.208, 2071.555, 2078.464, 2060.207, 2080.469, 2081.109, 2063.723, 2059.238, 2057.434, 2065.505, 2051.975, 2059.252, 2051.504, 2033.257, 2056.199, 2046.961, 2048.167, 2039.229, 2041.65, 2021.984, 2035.102, 2023.096, 2025.585, 2027.043, 2018.743, 2031.118, 2016.963, 2011.216, 2022.686, 2021.485, 2015.493, 2007.698, 2014.37, 2010.634, 2008.159, 2001.053, 2005.632, 1997.874, 1999.298, 2002.604, 2008.563, 1989.142, 2006.009, 1994.343, 1991.455, 1982.75, 1992.161, 1984.098, 1979.155, 1996.558, 1990.784, 1979.38, 1986.404, 1993.076, 1973.535, 1986.22, 1980.045]}	100	100	True
