id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	3000	True	30969.06836		675818	15	-1	317.39478516578674	{'train_loss': [445633.625, 365313.375, 332242.719, 315668.0, 306218.531, 299985.75, 296035.312, 291602.219, 287783.688, 284600.531, 280913.5, 277950.562, 275187.094, 273212.781, 272055.281, 268920.531, 266800.156, 265080.219, 263550.156, 261457.375, 260949.219, 259709.156, 259139.188, 257414.75, 255711.359, 255350.484, 254060.641, 253092.031, 252124.75, 251469.094, 251166.594, 248642.844, 248800.578, 248468.453, 246538.984, 246365.828, 245704.469, 245589.969, 244573.891, 243255.266, 242789.203, 242528.234, 241054.391, 240496.062, 240152.781, 239763.797, 239476.5, 237747.359, 237959.219, 236959.766, 236847.391, 235924.328, 236048.875, 234634.641, 234737.016, 234083.406, 232460.109, 231539.234, 232218.797, 232053.328, 230538.016, 229910.344, 229599.359, 229364.75, 228937.047, 228276.266, 227763.609, 226955.453, 227808.781, 227065.938, 225948.438, 226102.688, 225657.0, 224464.297, 224471.812, 224820.922, 225071.234, 223741.469, 222738.188, 223421.25, 223447.688, 221476.828, 221413.125, 221090.719, 221011.797, 221052.688, 219904.734, 219932.031, 218644.703, 219191.578, 218647.172, 217630.141, 217966.812, 218075.594, 218317.375, 216689.266, 216170.156, 217692.578, 215321.203, 216302.547], 'val_loss': [2607.431, 2333.809, 2235.711, 2171.408, 2142.241, 2113.213, 2111.101, 2061.431, 2028.867, 2008.899, 1991.871, 1984.756, 1936.937, 1931.808, 1935.962, 1879.935, 1875.625, 1877.527, 1869.445, 1852.772, 1851.65, 1848.131, 1835.899, 1821.541, 1828.455, 1813.999, 1805.511, 1794.341, 1801.602, 1789.192, 1799.067, 1778.273, 1789.417, 1773.473, 1791.86, 1774.008, 1768.72, 1759.91, 1772.559, 1752.882, 1768.456, 1765.358, 1751.187, 1753.502, 1742.15, 1738.878, 1731.075, 1729.502, 1737.1, 1746.436, 1721.308, 1735.953, 1722.473, 1723.21, 1710.044, 1728.865, 1695.091, 1691.975, 1690.365, 1690.825, 1709.723, 1695.818, 1683.65, 1701.533, 1692.817, 1680.823, 1694.568, 1687.623, 1667.136, 1676.944, 1684.014, 1687.05, 1683.461, 1677.314, 1693.682, 1667.785, 1675.736, 1678.863, 1675.703, 1676.654, 1670.284, 1681.209, 1661.59, 1664.46, 1659.848, 1656.138, 1680.225, 1666.152, 1654.068, 1646.643, 1663.495, 1638.513, 1652.023, 1640.71, 1649.441, 1646.105, 1652.726, 1649.06, 1659.917, 1648.631]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:75 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:79 kernel_size:6 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:31 epochs:100	100	3000	True	32300.30664		634797	15	-1	279.9824206829071	{'train_loss': [435169.906, 371735.219, 329271.156, 312279.438, 303993.812, 298732.125, 293329.938, 289642.125, 285930.312, 281700.812, 279201.438, 275439.031, 273252.812, 271505.125, 269329.25, 266922.812, 265720.719, 263780.719, 261223.219, 261184.531, 257492.484, 258419.297, 257600.188, 256626.0, 254680.484, 253785.516, 252747.766, 252631.188, 251386.344, 250691.109, 250278.953, 249093.812, 248434.203, 247485.875, 248345.344, 245886.766, 246812.984, 246562.391, 245366.781, 245584.812, 244761.297, 244245.766, 243128.609, 241960.719, 242849.875, 242315.734, 242025.969, 241301.953, 241086.469, 239948.469, 240408.641, 239513.016, 239356.75, 238356.797, 238189.688, 238175.203, 238255.906, 236829.203, 238315.266, 237271.484, 236547.828, 236066.797, 235338.188, 235611.422, 236257.812, 235117.219, 234811.953, 234019.906, 234277.375, 233897.828, 233340.094, 233387.953, 232513.734, 231782.062, 232114.984, 232789.688, 232128.062, 231638.844, 231797.375, 231011.781, 231762.484, 231401.75, 231722.688, 231192.109, 229637.672, 229801.953, 230139.547, 228815.531, 229599.391, 228337.719, 229064.891, 229040.516, 228824.703, 227969.344, 227350.375, 228242.797, 228164.562, 226450.688, 227896.234, 227574.406], 'val_loss': [3360.243, 2854.429, 2720.903, 2648.803, 2626.837, 2556.456, 2576.678, 2501.339, 2492.459, 2466.456, 2441.829, 2438.173, 2411.174, 2372.678, 2372.455, 2382.993, 2333.487, 2324.335, 2327.175, 2297.869, 2281.382, 2293.34, 2298.871, 2259.599, 2273.692, 2250.31, 2258.016, 2243.798, 2229.887, 2228.38, 2232.685, 2232.308, 2212.072, 2214.909, 2199.017, 2225.562, 2184.179, 2183.481, 2182.599, 2208.355, 2187.179, 2178.683, 2166.11, 2165.696, 2178.879, 2156.332, 2158.136, 2152.365, 2137.189, 2165.314, 2159.705, 2160.704, 2149.557, 2134.638, 2142.574, 2129.135, 2131.011, 2151.886, 2133.031, 2142.404, 2139.635, 2142.792, 2127.829, 2139.722, 2127.84, 2143.685, 2121.909, 2119.875, 2121.23, 2105.337, 2115.654, 2109.691, 2113.937, 2109.75, 2123.532, 2116.533, 2110.29, 2104.995, 2114.332, 2120.569, 2106.193, 2115.572, 2091.376, 2091.203, 2100.014, 2099.119, 2091.195, 2096.919, 2082.848, 2087.563, 2083.647, 2084.735, 2093.342, 2103.275, 2091.831, 2092.536, 2071.402, 2091.877, 2081.516, 2087.13]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.2103871844621022 beta1:0.8165575298150016 beta2:0.8430543679905247 weight_decay:8.914988094236803e-05 batch_size:24 epochs:100	100	1000	True	52741.49609		438397	14	-1	294.59374499320984	{'train_loss': [544489.5, 451712.594, 506707.406, 462655.75, 459403.656, 462532.156, 449700.812, 455067.781, 471254.031, 448721.0, 453255.875, 479699.719, 469460.094, 485348.75, 460846.281, 469791.469, 498768.688, 458869.312, 459110.438, 464126.031, 462832.344, 448023.219, 476015.969, 445551.406, 477787.969, 452627.0, 466910.844, 454518.688, 452901.594, 448223.031, 505184.531, 440968.688, 472183.5, 457490.344, 454910.344, 443764.375, 467826.375, 439263.906, 453728.875, 501423.656, 486430.219, 444385.156, 458542.719, 468136.656, 451878.812, 441090.938, 468736.062, 471083.188, 481874.406, 468155.938, 454314.031, 440359.969, 455124.562, 510435.906, 509533.594, 466124.688, 439815.219, 443592.0, 462984.594, 437383.969, 436172.844, 468187.5, 460607.969, 435641.281, 455443.531, 442212.406, 458184.375, 460004.094, 448216.031, 431495.844, 474576.219, 484793.875, 455325.781, 437758.656, 482661.938, 465411.719, 436722.406, 445986.906, 443193.188, 459132.094, 445378.094, 479929.875, 444439.062, 468052.125, 458163.062, 493544.656, 481050.406, 501734.719, 463643.719, 471021.188, 455832.688, 495403.0, 461456.625, 465873.406, 461115.562, 496360.25, 451357.594, 467348.5, 498551.938, 455349.312], 'val_loss': [5006.144, 3071.284, 3449.41, 5698.585, 3722.467, 4136.893, 3229.14, 4340.44, 4245.487, 5093.722, 6455.351, 5142.05, 3782.932, 4021.632, 3800.785, 6578.702, 5234.901, 6136.575, 3077.606, 3974.659, 3952.922, 5559.208, 3655.722, 2809.561, 2762.138, 7115.024, 6797.875, 5952.209, 2673.776, 3131.368, 3104.791, 4823.634, 2981.66, 4242.084, 3059.597, 4472.048, 3341.351, 6360.51, 3260.482, 6455.553, 3723.488, 3876.597, 4390.355, 2789.8, 5946.854, 5704.924, 5205.614, 2670.104, 2983.699, 5243.952, 2818.374, 3861.286, 3915.1, 13897.424, 4928.016, 4010.753, 4533.867, 4252.752, 4161.954, 2622.832, 3881.903, 3776.93, 3838.74, 2688.711, 3137.647, 2873.442, 3538.218, 3972.334, 3656.318, 7314.851, 7696.583, 6089.25, 4014.423, 4974.218, 3705.628, 2728.746, 4241.718, 4034.55, 4801.446, 3601.854, 7859.476, 2992.812, 5213.537, 3354.896, 2593.291, 3097.697, 4058.841, 3234.053, 3471.981, 3793.341, 2960.276, 4047.172, 2964.285, 3784.02, 7550.439, 2947.473, 7173.601, 5420.365, 3046.974, 2790.078]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:72 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:72 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:23 kernel_size:9 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:37 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	1000	True	32653.22461		391907	16	-1	300.4537425041199	{'train_loss': [382963.875, 354443.344, 326487.688, 315972.469, 308356.156, 303917.031, 300159.0, 296616.938, 294148.594, 291229.281, 288809.656, 286609.5, 283056.219, 280673.656, 278615.438, 277024.906, 275493.781, 273648.688, 272522.469, 270816.188, 270448.344, 269663.938, 267866.844, 267240.688, 266176.656, 265567.219, 265007.312, 263982.125, 263366.875, 262420.812, 261174.734, 260760.812, 260507.953, 259173.094, 258820.422, 257913.469, 257986.438, 257675.406, 256776.625, 256723.062, 255666.016, 255534.422, 255399.453, 254611.062, 254102.469, 254495.547, 253629.391, 253550.375, 251884.625, 252234.75, 251607.031, 250637.688, 251281.453, 251147.922, 251075.828, 250305.969, 249710.031, 250805.625, 250205.5, 248799.953, 248874.625, 248234.188, 248260.141, 247551.312, 247665.062, 246971.188, 247380.328, 246436.875, 246825.766, 246162.5, 245629.359, 245886.047, 245419.922, 246068.672, 244713.625, 244828.703, 245096.406, 244725.234, 244341.172, 244755.453, 243716.203, 242885.656, 243912.156, 242667.25, 242996.344, 242797.797, 243331.828, 242050.609, 241544.875, 242271.766, 241707.312, 241506.062, 240803.016, 240736.844, 240700.641, 241649.031, 240744.938, 240062.484, 239075.859, 240214.469], 'val_loss': [2540.69, 2293.871, 2235.498, 2168.473, 2120.991, 2101.732, 2084.622, 2062.256, 2044.348, 2030.775, 2025.601, 2006.095, 1977.231, 1981.905, 1962.308, 1967.359, 1956.111, 1963.649, 1943.619, 1936.332, 1935.467, 1923.3, 1917.32, 1918.754, 1913.63, 1896.068, 1899.73, 1889.611, 1891.176, 1870.292, 1888.218, 1852.407, 1861.265, 1847.317, 1864.44, 1869.736, 1838.121, 1842.6, 1840.839, 1827.434, 1825.735, 1853.442, 1821.596, 1828.418, 1828.02, 1816.582, 1799.431, 1809.44, 1819.362, 1827.115, 1802.903, 1832.42, 1800.063, 1794.405, 1787.881, 1782.709, 1804.615, 1793.894, 1801.514, 1812.247, 1808.322, 1804.48, 1769.224, 1765.043, 1791.261, 1779.565, 1800.786, 1768.767, 1771.527, 1767.345, 1770.731, 1769.118, 1766.986, 1777.858, 1759.997, 1753.142, 1749.018, 1770.574, 1765.732, 1745.713, 1736.825, 1751.775, 1749.131, 1741.447, 1737.172, 1753.459, 1742.402, 1748.167, 1758.174, 1725.304, 1727.194, 1730.843, 1748.93, 1725.923, 1741.996, 1727.205, 1731.361, 1720.067, 1729.844, 1734.657]}	100	100	True
