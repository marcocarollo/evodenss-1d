id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.2103871844621022 batch_size:24 epochs:100	100	3000	True	31763.32617		675818	15	-1	316.9434320926666	{'train_loss': [440103.031, 354077.906, 335077.5, 323830.062, 313306.656, 305694.75, 299473.188, 292290.188, 286239.781, 280991.688, 277629.688, 273932.75, 269852.781, 266737.125, 264820.188, 261844.125, 259437.672, 257194.609, 254705.422, 252489.203, 251998.125, 250567.969, 248276.25, 247335.516, 245665.703, 244628.625, 243514.125, 243577.953, 241380.984, 240928.969, 238973.203, 238625.953, 237994.453, 237893.891, 236369.953, 236395.422, 234759.219, 233937.156, 232961.719, 232741.25, 231715.406, 231544.953, 230862.719, 229860.203, 229582.906, 229051.656, 228959.078, 228302.922, 228133.656, 227626.016, 227103.031, 227437.375, 225787.156, 225083.391, 224984.875, 224720.969, 225544.656, 225287.5, 223879.922, 222991.609, 222036.719, 223239.078, 221754.953, 221700.656, 221937.219, 221311.203, 220982.969, 219767.797, 220733.031, 219463.891, 219623.547, 219417.078, 218804.672, 218815.75, 217989.609, 218387.625, 217535.047, 217117.609, 218182.766, 217743.109, 216155.234, 217845.453, 216894.0, 216659.078, 216115.828, 215441.078, 214411.516, 215425.156, 214404.219, 215220.234, 214173.453, 214836.906, 213663.891, 214193.641, 213958.453, 212947.672, 212702.344, 212716.359, 212756.391, 211667.781], 'val_loss': [2623.64, 2479.633, 2279.956, 2185.775, 2133.174, 2085.226, 2049.319, 2014.389, 1965.653, 1952.729, 1914.182, 1893.359, 1879.111, 1849.96, 1843.131, 1831.37, 1806.935, 1794.474, 1791.36, 1781.153, 1767.971, 1770.985, 1783.946, 1753.803, 1745.839, 1757.55, 1735.338, 1725.818, 1755.75, 1738.821, 1725.901, 1716.083, 1740.009, 1718.666, 1728.904, 1707.96, 1711.81, 1698.66, 1699.037, 1694.073, 1691.417, 1694.635, 1690.913, 1709.695, 1700.659, 1681.153, 1692.826, 1672.801, 1695.471, 1689.215, 1693.98, 1671.016, 1673.318, 1678.427, 1677.63, 1682.737, 1671.925, 1677.183, 1674.183, 1675.985, 1675.876, 1670.997, 1689.393, 1679.716, 1670.624, 1676.597, 1680.779, 1669.62, 1662.693, 1665.857, 1661.949, 1676.278, 1668.11, 1674.352, 1655.799, 1656.591, 1673.286, 1660.866, 1665.889, 1675.72, 1675.061, 1684.141, 1677.92, 1676.313, 1657.143, 1659.195, 1674.305, 1665.453, 1682.2, 1669.239, 1655.855, 1649.224, 1662.913, 1675.034, 1681.329, 1652.895, 1674.026, 1661.713, 1658.86, 1659.275]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:91 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:91 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:101 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:71 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:conv1d out_channels:126 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:83 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.2103871844621022 batch_size:26 epochs:100	100	1000	True	31595.83398		573297	14	-1	326.4264476299286	{'train_loss': [415389.031, 356684.062, 331482.281, 318444.875, 307395.281, 299769.469, 294201.312, 287412.656, 282253.938, 278593.062, 274549.438, 271667.594, 269084.156, 265489.156, 262806.875, 260559.516, 258348.344, 255572.375, 253750.891, 252826.312, 250193.438, 249147.25, 247927.25, 246539.688, 244182.125, 242734.281, 242786.797, 241369.031, 240344.875, 239614.656, 237393.625, 237468.281, 236419.719, 235698.734, 235561.766, 234297.266, 232855.594, 233302.469, 230816.188, 230800.031, 230067.328, 229133.344, 228668.375, 227547.219, 226904.203, 226485.422, 226661.562, 226237.234, 225101.859, 224748.141, 223642.734, 223969.922, 222893.375, 222031.828, 220916.375, 221270.984, 220501.969, 220386.812, 220259.016, 218639.281, 219193.906, 217861.781, 218308.75, 217529.703, 217406.562, 216609.844, 215364.359, 215257.422, 215447.719, 214761.422, 214491.938, 213520.828, 213287.625, 213266.484, 212853.531, 211109.047, 212067.172, 212333.125, 210972.109, 210703.594, 210488.297, 210438.047, 210464.953, 210041.031, 209353.312, 208749.156, 209281.594, 209145.562, 207396.953, 208617.859, 207055.859, 207469.062, 206020.188, 206282.844, 206002.328, 206675.75, 205569.984, 205796.391, 204921.312, 205291.406], 'val_loss': [2608.333, 2480.661, 2332.647, 2282.84, 2219.066, 2197.312, 2155.848, 2101.243, 2106.791, 2077.183, 2073.158, 2056.49, 2028.449, 2023.825, 1997.912, 1986.507, 1996.81, 1959.152, 1942.183, 1937.301, 1941.323, 1930.203, 1937.594, 1923.25, 1922.014, 1904.544, 1912.393, 1904.447, 1875.657, 1884.408, 1876.622, 1861.277, 1857.56, 1864.162, 1854.624, 1825.504, 1827.901, 1834.856, 1832.854, 1820.005, 1818.336, 1836.871, 1824.625, 1822.315, 1807.494, 1828.994, 1814.805, 1829.208, 1806.68, 1793.938, 1815.383, 1811.421, 1805.229, 1816.392, 1807.706, 1814.407, 1798.225, 1802.16, 1804.222, 1808.63, 1800.875, 1805.283, 1794.627, 1791.68, 1800.249, 1797.81, 1819.41, 1790.89, 1783.691, 1776.526, 1794.29, 1778.547, 1775.444, 1783.807, 1771.526, 1783.547, 1766.46, 1777.97, 1768.938, 1762.404, 1748.667, 1772.624, 1782.671, 1789.72, 1774.392, 1789.417, 1780.719, 1768.51, 1785.667, 1788.709, 1775.668, 1795.606, 1780.923, 1797.948, 1814.934, 1786.691, 1817.812, 1787.785, 1790.867, 1792.268]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:50 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:8 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:12 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:13 layer:fc act:selu out_features:200 bias:True input:14 learning:adadelta lr:0.0877064673841627 batch_size:24 epochs:100	100	1000	True	33054.32031		528276	16	-1	306.7942807674408	{'train_loss': [407736.844, 380466.219, 343144.562, 322727.312, 313219.25, 307907.75, 302998.125, 299987.625, 296316.25, 293748.031, 291489.219, 289556.656, 287144.344, 284704.469, 283452.656, 280694.781, 280623.531, 278866.625, 278130.219, 275867.75, 275540.031, 274490.5, 273857.906, 271359.719, 270661.312, 269571.75, 269293.188, 267990.25, 268014.094, 267761.219, 266423.531, 267191.156, 266264.188, 265081.406, 265274.531, 263980.125, 262959.812, 263245.375, 262639.062, 261536.016, 261253.094, 260427.906, 260919.328, 260479.438, 259814.781, 259212.0, 258564.172, 258423.688, 257847.359, 256471.297, 256738.672, 256777.688, 255925.062, 255283.469, 255207.891, 254494.484, 254344.391, 253700.594, 252729.984, 252914.578, 252642.125, 252157.859, 252048.719, 250851.438, 251292.625, 250938.719, 250533.766, 250024.922, 250322.531, 249057.812, 249414.406, 248605.453, 249391.344, 247984.234, 247672.156, 246857.453, 248090.922, 247267.641, 247505.812, 247127.594, 246698.094, 246450.641, 245571.469, 245451.859, 245877.031, 244973.656, 245094.984, 245157.984, 243696.734, 243393.875, 243453.312, 243949.812, 243673.281, 242348.547, 242268.344, 242486.281, 243608.766, 242949.172, 242070.391, 241518.609], 'val_loss': [2661.792, 2536.56, 2256.592, 2199.008, 2135.909, 2120.099, 2109.959, 2089.514, 2082.245, 2079.523, 2051.961, 2026.776, 2023.894, 2004.541, 1996.486, 1989.398, 1982.223, 1966.104, 1965.718, 1953.401, 1949.014, 1945.254, 1939.215, 1920.298, 1921.048, 1916.054, 1910.822, 1900.129, 1888.478, 1901.907, 1891.245, 1897.573, 1895.798, 1901.589, 1889.455, 1894.14, 1873.155, 1892.647, 1886.671, 1879.089, 1865.975, 1870.099, 1861.686, 1859.927, 1871.085, 1858.627, 1879.438, 1857.073, 1840.997, 1837.33, 1857.024, 1857.646, 1851.823, 1854.404, 1847.094, 1832.787, 1828.137, 1840.597, 1841.487, 1820.605, 1842.926, 1824.469, 1836.724, 1827.39, 1829.549, 1828.257, 1822.212, 1825.536, 1815.898, 1833.652, 1810.699, 1808.571, 1798.036, 1808.634, 1799.277, 1802.232, 1807.784, 1793.361, 1786.506, 1794.086, 1791.882, 1791.576, 1805.383, 1785.563, 1779.836, 1816.524, 1794.19, 1779.319, 1786.73, 1775.291, 1767.312, 1758.712, 1772.051, 1778.225, 1768.855, 1767.794, 1769.174, 1757.836, 1759.938, 1762.069]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:63 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:105 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:deconv1d out_channels:7 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.08785243277328159 batch_size:24 epochs:100	100	1000	True	31394.48047		511644	14	-1	288.864164352417	{'train_loss': [399569.188, 363334.125, 334106.156, 326775.562, 321718.219, 318360.219, 316161.156, 313825.0, 311488.531, 308833.719, 305221.938, 301115.625, 297556.281, 293650.125, 290142.625, 287025.938, 283549.438, 281236.906, 278429.781, 276371.219, 273450.844, 271442.844, 268831.438, 266569.969, 264850.781, 263605.062, 262378.938, 259938.0, 258916.125, 257616.938, 256369.875, 254380.578, 254339.188, 252808.875, 250872.984, 250620.656, 250870.031, 249468.531, 248516.359, 247101.125, 247099.875, 246219.125, 245706.422, 244929.984, 244439.172, 243379.609, 243405.391, 241130.203, 240125.359, 241063.531, 239069.625, 239708.672, 238809.953, 238241.719, 237225.547, 237008.828, 235350.859, 236563.5, 235532.062, 234708.453, 234290.688, 234739.828, 234461.688, 234174.5, 233713.984, 233136.875, 232585.625, 233089.266, 231557.312, 231458.781, 230807.547, 231374.469, 230023.625, 230232.859, 229254.016, 229764.672, 228290.562, 227269.031, 228272.062, 226946.125, 227864.25, 227106.234, 226657.266, 226910.875, 226770.938, 225593.047, 225947.047, 225478.672, 224043.484, 224414.781, 225334.891, 223695.75, 224045.281, 221762.219, 223113.344, 223087.672, 221864.188, 221840.188, 222360.406, 221346.406], 'val_loss': [2633.227, 2294.849, 2261.458, 2231.41, 2196.199, 2179.882, 2169.632, 2160.465, 2148.449, 2137.156, 2103.006, 2092.479, 2075.015, 2054.521, 2034.001, 2018.909, 2014.544, 1984.725, 1967.385, 1958.249, 1948.142, 1927.277, 1915.909, 1914.796, 1894.312, 1894.992, 1883.31, 1866.788, 1858.697, 1857.001, 1855.726, 1839.842, 1842.98, 1857.396, 1823.649, 1826.813, 1827.402, 1812.98, 1816.612, 1802.322, 1789.403, 1796.862, 1780.809, 1785.766, 1776.73, 1779.091, 1766.291, 1761.56, 1758.207, 1752.026, 1753.474, 1761.764, 1764.955, 1738.309, 1737.641, 1751.385, 1754.595, 1730.979, 1728.943, 1733.205, 1729.5, 1730.37, 1741.451, 1736.985, 1729.546, 1739.122, 1731.187, 1724.635, 1719.02, 1726.083, 1728.493, 1714.273, 1729.891, 1721.479, 1701.285, 1713.563, 1716.767, 1692.419, 1687.93, 1709.713, 1709.563, 1710.991, 1693.571, 1704.975, 1705.392, 1714.569, 1708.716, 1694.814, 1687.076, 1691.312, 1694.221, 1680.123, 1676.532, 1689.44, 1693.76, 1698.825, 1703.293, 1698.32, 1688.544, 1684.88]}	100	100	True
