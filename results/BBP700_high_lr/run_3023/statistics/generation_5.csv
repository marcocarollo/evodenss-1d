id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.1864603712075097 batch_size:27 epochs:100	100	1000	True	31855.23633		642608	15	-1	306.87126302719116	{'train_loss': [470634.938, 394884.344, 329322.781, 314062.969, 304266.062, 296498.75, 290361.719, 286973.031, 283506.344, 280856.656, 278500.281, 276299.906, 274007.438, 272455.438, 270756.094, 269036.688, 267856.312, 265350.281, 263945.938, 262967.844, 260774.141, 260078.297, 258137.531, 257207.125, 255824.906, 254796.578, 253612.859, 252374.109, 251213.156, 250202.812, 249610.438, 248345.766, 247191.0, 247077.688, 245851.891, 245720.094, 244700.781, 243460.969, 242500.547, 242031.188, 241314.234, 240543.047, 239449.844, 239566.25, 238247.359, 237847.078, 236781.859, 237065.969, 235182.203, 234966.812, 234529.734, 232819.203, 233113.938, 233029.047, 231906.359, 231854.797, 231620.781, 230407.266, 231040.359, 230222.969, 229154.688, 228208.797, 227927.797, 227611.844, 227713.719, 226292.641, 226034.375, 226440.75, 225093.25, 225110.859, 225169.719, 223369.078, 224440.375, 224858.938, 224675.562, 223197.047, 222534.594, 223288.25, 222213.703, 222565.75, 221557.672, 220888.234, 220489.656, 220302.938, 220255.297, 219641.219, 219778.438, 219103.5, 219048.141, 218438.641, 218512.75, 217965.922, 218110.625, 217704.844, 217708.219, 217897.422, 216920.188, 216772.312, 216689.719, 216895.891], 'val_loss': [3290.803, 2515.45, 2448.627, 2339.812, 2281.711, 2265.24, 2268.683, 2207.981, 2199.948, 2215.479, 2152.885, 2135.458, 2117.705, 2126.249, 2107.4, 2083.416, 2078.06, 2074.491, 2067.889, 2056.824, 2050.488, 2038.237, 2024.935, 2010.974, 1995.723, 2006.033, 1988.162, 1973.887, 1962.314, 1967.067, 1948.537, 1946.23, 1943.233, 1943.077, 1932.921, 1937.113, 1927.332, 1931.997, 1924.295, 1908.734, 1910.289, 1895.981, 1915.256, 1890.173, 1893.011, 1891.114, 1881.713, 1874.894, 1885.643, 1877.166, 1865.154, 1880.547, 1879.468, 1862.749, 1867.26, 1858.242, 1845.027, 1854.145, 1851.932, 1855.315, 1839.941, 1837.161, 1832.192, 1849.296, 1829.151, 1844.866, 1826.402, 1835.821, 1841.583, 1844.712, 1830.201, 1836.76, 1831.007, 1834.5, 1837.718, 1839.234, 1829.824, 1827.788, 1828.325, 1819.156, 1833.612, 1817.439, 1826.342, 1805.778, 1829.594, 1810.624, 1813.685, 1814.809, 1816.527, 1827.238, 1798.858, 1807.478, 1811.017, 1814.724, 1821.344, 1805.826, 1813.2, 1797.213, 1806.953, 1802.106]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.1864603712075097 batch_size:27 epochs:100	100	1000	True	32222.36719		484789	15	-1	283.1711974143982	{'train_loss': [413819.125, 343081.656, 320674.406, 311063.531, 302906.406, 296967.188, 291716.906, 288150.281, 283999.5, 282913.562, 279993.812, 277991.844, 276366.781, 275285.75, 273355.469, 272090.562, 270320.531, 268037.438, 267974.0, 266656.031, 265613.531, 264774.125, 262687.188, 261751.297, 259264.25, 257597.531, 257214.703, 255622.062, 254941.25, 254131.516, 252542.219, 251646.281, 251679.156, 250289.547, 250398.672, 249759.812, 248452.719, 248227.438, 247304.734, 246316.438, 245916.984, 246084.125, 245061.766, 244550.406, 243908.078, 243341.828, 242520.25, 242640.109, 241727.562, 242684.531, 240827.156, 240510.578, 241373.766, 240335.172, 238905.219, 238982.078, 238906.297, 238022.672, 238259.609, 238137.406, 238031.922, 236916.141, 236544.297, 236465.766, 235545.906, 235105.188, 235364.203, 235278.5, 234886.922, 234699.5, 234323.703, 233880.125, 233407.656, 232559.25, 233724.094, 233442.172, 231726.547, 231623.906, 232191.562, 231022.422, 232005.578, 230797.734, 230015.797, 230330.766, 230250.359, 229427.719, 230636.422, 229019.5, 229712.625, 228746.422, 228549.25, 228317.125, 228422.594, 227819.953, 227861.0, 227909.906, 227137.781, 227857.922, 226763.484, 226238.172], 'val_loss': [2708.566, 2455.965, 2386.217, 2331.138, 2287.424, 2249.417, 2254.634, 2202.738, 2194.404, 2163.824, 2134.342, 2121.52, 2103.462, 2097.54, 2085.1, 2067.516, 2063.002, 2055.705, 2042.191, 2034.667, 2042.666, 2023.84, 2012.386, 1994.216, 1989.854, 1977.555, 1986.585, 1975.273, 1966.312, 1965.571, 1959.38, 1924.812, 1922.158, 1930.366, 1931.084, 1927.238, 1911.608, 1915.001, 1908.936, 1911.792, 1900.27, 1895.773, 1904.438, 1892.144, 1878.723, 1904.047, 1883.203, 1876.657, 1901.594, 1867.381, 1889.153, 1874.375, 1873.132, 1872.375, 1874.873, 1875.778, 1872.543, 1859.704, 1869.124, 1879.298, 1871.704, 1852.165, 1859.456, 1855.457, 1863.896, 1854.535, 1860.573, 1850.993, 1852.201, 1850.88, 1856.823, 1842.31, 1852.538, 1840.209, 1845.53, 1829.541, 1823.86, 1843.302, 1831.372, 1827.702, 1836.765, 1828.297, 1833.777, 1832.073, 1832.081, 1838.834, 1830.027, 1829.888, 1819.601, 1827.128, 1824.313, 1810.321, 1820.839, 1820.367, 1817.871, 1813.977, 1790.503, 1803.876, 1798.08, 1794.267]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:36 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:22 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.1864603712075097 beta1:0.9079077916972548 beta2:0.9439428011454261 weight_decay:1.2168588807813632e-05 batch_size:27 epochs:100	100	1000	True	143578.3125		476524	14	-1	305.86312794685364	{'train_loss': [1118215.375, 1066921.0, 1067266.125, 1179592.375, 1066921.0, 1066921.0, 1076014.625, 1098688.375, 1066921.0, 1066921.0, 1080756.375, 1091645.75, 1066921.0, 1067705.125, 1073119.625, 1083165.875, 1073814.5, 1068742.75, 1068655.875, 1078116.5, 1088719.5, 1076878.5, 1070930.75, 1069134.625, 1074839.125, 1087468.5, 1079728.25, 1070849.125, 1069431.125, 1074211.875, 1086557.75, 1085978.875, 1078949.25, 1070795.625, 1071055.25, 1074517.625, 1082973.5, 1076508.625, 1074032.5, 1072478.5, 1075701.375, 1082903.25, 1081919.5, 1074923.625, 1073238.875, 1077220.5, 1086251.375, 1088704.75, 1075507.625, 1073551.375, 1073343.75, 1080781.625, 1084201.875, 1084036.625, 1075059.375, 1073546.25, 1076822.875, 1084058.5, 1087503.5, 1073053.125, 1076331.875, 1074132.0, 1087859.25, 1085255.25, 1083492.125, 1073742.75, 1075491.25, 1076683.375, 1086919.125, 1081002.5, 1075231.125, 1074756.875, 1073245.875, 1076708.0, 1082137.125, 1076620.625, 1077697.0, 1074603.125, 1074909.0, 1083981.375, 1083572.0, 1078532.0, 1073022.875, 1075439.75, 1084262.75, 1077783.5, 1080206.125, 1074081.5, 1072159.5, 1081598.0, 1079710.75, 1087940.125, 1074841.375, 1071017.875, 1077817.5, 1083806.25, 1081166.75, 1076653.75, 1075811.0, 1075848.25], 'val_loss': [8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 31687.598, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 11744.047, 8469.773, 8469.773, 8469.773, 8782.344, 8469.773, 8469.773, 8469.773, 8469.773, 8472.859, 8990.827, 8852.894, 8469.773, 8469.773, 8469.773, 9271.018, 8469.687, 8469.773, 8469.773, 8469.773, 9359.646, 8469.773, 8469.773, 8469.773, 8469.773, 9858.289, 9049.127, 8469.773, 8469.773, 8469.773, 9124.729, 8469.773, 8469.773, 8469.773, 8469.771, 8867.869, 9200.693, 8462.562, 8469.771, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 9639.673, 8469.773, 8469.773, 8469.773, 8469.773, 8606.367, 8469.773, 8469.773, 8469.771, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.773, 8469.77, 8469.771, 8694.163, 8469.773, 8469.773, 8469.773, 9363.202, 8469.773, 8469.773, 8879.765, 8469.773, 8469.773, 8469.773, 8469.773, 8469.771, 8469.773, 9186.963, 8469.767, 8469.773, 8469.773, 8891.367]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:121 kernel_size:3 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:50 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:34 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:deconv1d out_channels:52 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.14301095354062782 batch_size:27 epochs:100	100	1000	True	32206.13477		482778	14	-1	297.63219714164734	{'train_loss': [402520.75, 363700.969, 329171.375, 319456.75, 314281.125, 310126.062, 305514.875, 301952.031, 297970.812, 294561.281, 291542.875, 288393.75, 286453.844, 283496.469, 282074.094, 279487.812, 278718.344, 275949.562, 274103.469, 272062.719, 270429.0, 268815.219, 267461.0, 265327.312, 264032.781, 263034.031, 261116.0, 259850.281, 259638.859, 258374.578, 256730.359, 256538.609, 255574.828, 254074.125, 254022.359, 252541.906, 251262.75, 250595.391, 250342.453, 249777.047, 248637.469, 247412.703, 246994.891, 246955.453, 245678.75, 245060.625, 244038.734, 243697.438, 243568.516, 243039.219, 242627.016, 240643.328, 240602.328, 240514.391, 240813.406, 238557.891, 239026.094, 239325.016, 238041.516, 238419.453, 237641.0, 237636.641, 236471.734, 236172.312, 236116.875, 235277.469, 234921.094, 234089.078, 234704.75, 234325.922, 233955.359, 233591.891, 232881.359, 232745.672, 232883.047, 232229.469, 232021.047, 231260.969, 230262.953, 230497.906, 230592.828, 229567.125, 230453.969, 228902.125, 229247.25, 228456.594, 228170.578, 227530.922, 227855.719, 227692.859, 227090.562, 227231.172, 226327.5, 225957.531, 226365.484, 226501.578, 226107.547, 225651.016, 224846.672, 225707.156], 'val_loss': [2895.678, 2498.276, 2397.24, 2357.493, 2334.568, 2306.267, 2264.877, 2241.422, 2222.08, 2216.095, 2199.899, 2184.422, 2184.645, 2166.118, 2148.722, 2135.886, 2113.252, 2107.987, 2093.426, 2087.829, 2070.019, 2054.369, 2041.816, 2041.939, 2014.134, 2009.709, 2017.51, 1999.163, 2001.454, 1996.588, 1968.761, 1981.825, 1957.224, 1953.655, 1957.11, 1949.037, 1945.079, 1933.719, 1942.031, 1931.279, 1924.008, 1919.378, 1922.884, 1920.387, 1911.735, 1908.092, 1892.857, 1910.931, 1903.664, 1895.754, 1892.431, 1904.407, 1891.779, 1884.181, 1897.05, 1887.308, 1889.792, 1862.859, 1885.441, 1863.382, 1863.829, 1878.758, 1864.803, 1853.211, 1851.204, 1845.718, 1847.916, 1850.345, 1863.405, 1861.383, 1849.255, 1857.328, 1831.739, 1843.804, 1833.835, 1847.693, 1844.625, 1838.115, 1847.701, 1821.441, 1833.657, 1824.194, 1837.763, 1829.25, 1821.681, 1823.21, 1814.92, 1819.852, 1828.854, 1826.472, 1816.111, 1819.484, 1818.762, 1818.192, 1818.182, 1823.332, 1834.659, 1824.649, 1806.675, 1814.967]}	100	100	True
