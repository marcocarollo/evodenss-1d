id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	30914.50586		9715062	13	-1	367.59432101249695	{'train_loss': [598827.375, 621449.375, 620265.375, 596561.188, 565158.062, 523851.156, 478196.469, 427645.281, 384196.844, 350983.0, 326772.531, 309476.875, 298213.781, 290391.5, 285759.281, 281941.688, 278430.938, 274769.844, 272579.062, 270329.438, 267861.375, 266236.406, 264412.75, 262372.875, 260790.266, 259468.375, 257444.844, 256826.109, 255468.484, 253368.094, 252480.578, 250919.844, 250327.094, 249533.156, 248425.938, 246511.75, 246133.438, 245602.641, 244944.391, 243295.859, 242901.422, 241890.469, 241083.266, 240137.359, 239387.266, 238968.469, 237693.75, 237739.156, 236965.609, 236908.75, 235524.359, 235289.141, 234468.703, 234424.062, 233336.359, 233937.703, 232436.625, 231358.297, 231262.438, 230875.984, 230810.688, 229115.859, 229917.922, 228707.859, 229390.016, 228051.391, 227383.078, 227391.562, 226611.297, 227079.562, 225994.938, 225083.422, 225203.719, 224061.438, 224881.078, 224333.047, 223630.312, 224606.969, 222875.797, 222153.578, 221579.031, 221719.25, 221509.234, 221447.094, 220542.984, 219992.0, 219530.078, 218346.672, 218834.484, 218377.078, 218082.5, 217983.0, 217858.781, 217242.719, 216881.391, 216967.891, 216403.969, 215547.359, 215846.125, 215460.422], 'val_loss': [2438.778, 2534.673, 2165.908, 2228.874, 2214.566, 1912.906, 1785.716, 1702.415, 1641.009, 1537.622, 1473.042, 1416.986, 1396.034, 1366.39, 1350.488, 1340.206, 1310.328, 1296.988, 1285.553, 1270.48, 1258.654, 1256.899, 1271.754, 1236.922, 1246.688, 1231.688, 1234.162, 1221.47, 1218.351, 1206.64, 1205.267, 1199.814, 1192.479, 1183.819, 1191.482, 1175.619, 1180.41, 1175.98, 1181.44, 1163.542, 1166.919, 1173.539, 1148.812, 1157.174, 1148.529, 1144.337, 1149.948, 1136.076, 1133.521, 1141.34, 1145.977, 1140.563, 1122.08, 1128.957, 1124.969, 1124.727, 1128.753, 1113.593, 1118.541, 1108.111, 1114.079, 1115.232, 1109.839, 1105.008, 1107.187, 1110.164, 1099.125, 1090.653, 1104.265, 1093.904, 1097.064, 1103.094, 1089.515, 1091.497, 1097.185, 1091.457, 1089.283, 1092.4, 1094.214, 1085.874, 1092.012, 1087.185, 1084.462, 1080.845, 1078.183, 1071.048, 1078.378, 1089.654, 1083.118, 1081.468, 1087.782, 1080.72, 1091.001, 1080.688, 1083.105, 1074.238, 1071.515, 1069.176, 1074.824, 1076.302]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	2000	True	31679.64844		9715062	13	-1	365.5447039604187	{'train_loss': [593330.688, 611430.812, 607012.312, 584841.75, 551026.625, 509884.281, 458696.125, 413071.469, 371016.781, 338655.844, 314418.469, 299071.094, 290185.031, 284797.406, 280929.438, 277405.469, 275495.75, 273086.656, 271281.719, 269369.438, 267933.219, 264913.156, 263653.5, 262863.188, 260507.312, 259690.609, 257556.969, 256825.172, 255143.719, 253998.531, 253470.453, 252087.312, 250694.641, 249412.609, 248279.516, 247484.016, 246085.453, 246727.125, 245484.453, 244029.406, 243082.156, 242645.0, 241899.172, 240757.594, 240866.594, 240205.516, 239826.516, 238253.391, 237713.547, 237019.828, 237519.172, 237258.016, 235351.812, 234564.75, 234889.047, 233797.016, 233824.094, 232130.125, 231851.125, 231190.875, 230851.922, 230946.172, 230849.688, 230227.516, 229290.562, 228152.953, 227251.922, 228233.266, 227175.562, 226472.281, 226294.609, 226107.266, 225650.172, 224722.312, 223781.391, 224971.531, 223409.484, 223169.516, 222943.781, 222604.078, 222102.391, 221538.609, 221901.156, 221573.656, 221376.484, 220891.719, 219919.672, 219958.234, 219026.656, 219073.062, 219258.234, 218219.266, 217786.562, 218207.453, 217130.406, 217548.0, 216473.875, 217024.047, 216082.203, 214945.312], 'val_loss': [2328.862, 2482.403, 2476.857, 2071.784, 2047.326, 1883.467, 1773.959, 1696.773, 1535.263, 1510.935, 1455.059, 1429.841, 1388.862, 1354.716, 1328.998, 1313.855, 1308.902, 1306.583, 1286.136, 1286.628, 1264.993, 1268.822, 1247.661, 1261.011, 1248.533, 1242.043, 1235.169, 1235.308, 1221.931, 1223.155, 1223.737, 1228.435, 1208.315, 1215.981, 1202.73, 1201.843, 1201.9, 1197.928, 1193.154, 1197.173, 1178.891, 1191.199, 1188.518, 1182.932, 1187.195, 1176.393, 1176.084, 1176.878, 1180.578, 1180.622, 1154.316, 1148.229, 1162.197, 1153.742, 1146.516, 1148.771, 1146.842, 1145.794, 1140.282, 1151.598, 1153.938, 1146.39, 1144.426, 1135.329, 1143.727, 1139.357, 1142.043, 1137.503, 1144.658, 1143.592, 1135.301, 1138.072, 1133.425, 1129.302, 1123.664, 1138.477, 1130.033, 1125.61, 1129.46, 1120.325, 1118.621, 1119.445, 1120.253, 1122.649, 1115.271, 1120.288, 1117.986, 1124.256, 1120.219, 1118.016, 1119.93, 1125.561, 1113.089, 1114.057, 1114.662, 1105.784, 1111.566, 1102.215, 1104.927, 1111.444]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:79 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:79 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:79 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1153876851778333 batch_size:16 epochs:100	100	1000	True	34711.92188		51500440	13	-1	710.8445529937744	{'train_loss': [965458.562, 1002444.125, 1031008.812, 1038797.312, 1041749.125, 1040739.188, 1033480.25, 1016681.875, 984765.5, 901498.25, 649341.438, 323495.344, 299265.438, 292523.812, 287488.406, 282256.906, 279359.0, 275682.75, 273293.062, 270092.094, 267950.281, 265766.281, 263535.875, 261242.062, 259823.453, 258254.062, 256326.344, 255094.422, 253791.359, 252039.141, 249831.406, 248325.25, 247545.562, 246015.25, 244284.094, 242969.297, 241828.391, 239893.969, 238891.766, 237830.188, 236286.922, 235492.938, 233661.344, 233022.594, 232643.391, 231037.203, 229854.312, 228060.188, 227166.719, 226753.578, 225074.109, 224886.828, 223861.734, 222990.844, 221347.891, 220844.797, 220526.469, 219133.594, 219097.484, 216539.625, 217316.844, 215818.156, 214171.297, 213174.953, 212795.812, 211749.609, 210784.578, 209554.938, 209660.078, 209285.688, 207787.219, 206692.875, 206744.578, 206411.5, 205861.5, 204285.562, 204040.125, 203685.172, 202572.016, 201190.75, 200625.844, 199964.906, 200312.984, 199295.062, 198990.984, 198561.531, 197853.188, 197000.766, 197225.766, 196527.844, 195465.359, 195283.859, 194855.094, 193941.812, 192067.234, 191660.047, 191679.141, 191043.047, 189890.078, 189813.875], 'val_loss': [4923.964, 5055.022, 5207.168, 5224.164, 5223.734, 5123.375, 5154.275, 5090.237, 4629.133, 3689.992, 1797.094, 1446.005, 1420.132, 1386.636, 1366.599, 1359.8, 1337.843, 1339.147, 1333.372, 1311.587, 1314.568, 1294.727, 1283.24, 1281.917, 1276.965, 1267.04, 1280.189, 1276.308, 1275.511, 1259.374, 1264.656, 1254.961, 1257.143, 1247.119, 1248.345, 1232.632, 1246.342, 1241.507, 1242.812, 1237.77, 1247.421, 1253.223, 1238.233, 1229.84, 1239.348, 1236.037, 1229.013, 1224.294, 1222.777, 1235.547, 1233.772, 1223.245, 1234.704, 1225.801, 1246.174, 1238.989, 1239.948, 1239.474, 1246.418, 1266.343, 1239.329, 1234.862, 1240.1, 1234.095, 1235.483, 1243.618, 1241.69, 1248.708, 1253.601, 1239.213, 1226.054, 1241.472, 1245.209, 1238.953, 1244.568, 1222.608, 1229.967, 1231.464, 1219.018, 1223.321, 1228.227, 1234.08, 1238.169, 1244.314, 1230.008, 1224.9, 1233.517, 1234.774, 1224.431, 1236.581, 1218.986, 1236.4, 1223.522, 1257.828, 1247.249, 1246.162, 1240.07, 1238.363, 1240.338, 1241.77]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adadelta lr:0.09023570113171074 batch_size:6 epochs:100	100	1000	True	35115.07422		20700903	11	-1	587.8376843929291	{'train_loss': [860565.5, 828395.875, 392169.844, 351434.375, 335423.0, 327028.094, 323497.719, 313454.75, 313352.969, 306008.25, 303832.656, 301228.188, 300164.531, 297525.75, 295450.938, 293716.375, 293180.781, 291495.469, 290050.938, 288843.031, 287190.75, 286817.969, 285523.75, 284343.938, 284991.812, 282961.594, 281269.094, 281381.406, 279578.188, 279730.906, 278683.0, 277831.438, 276415.625, 275645.688, 273062.062, 274433.938, 272715.562, 273268.281, 270403.281, 270241.125, 270743.75, 270166.562, 267716.0, 268278.156, 267915.562, 266307.438, 266079.438, 266062.031, 265576.375, 264261.812, 264345.625, 262705.219, 263289.094, 262465.0, 261289.922, 261124.406, 259771.547, 260587.891, 261362.797, 259589.969, 259218.516, 259882.016, 259518.625, 258107.688, 257384.578, 257602.828, 257877.562, 257984.984, 256705.312, 256507.969, 255112.922, 256029.125, 255564.578, 254867.641, 254190.016, 254542.484, 254342.953, 253757.156, 254358.797, 253240.344, 253308.078, 253223.766, 253359.578, 251586.516, 250203.562, 252109.906, 251066.969, 250216.031, 250839.953, 250645.25, 250450.594, 250309.938, 249329.125, 249695.219, 248685.531, 249761.797, 249099.469, 248882.5, 248468.359, 247835.156], 'val_loss': [1792.002, 782.417, 637.925, 584.572, 566.591, 555.69, 529.833, 528.179, 525.989, 513.574, 514.779, 510.176, 504.819, 506.333, 509.475, 501.374, 515.0, 496.847, 487.668, 493.842, 498.644, 482.138, 489.181, 491.974, 479.191, 483.682, 480.277, 495.85, 475.956, 475.424, 471.526, 475.147, 474.713, 475.743, 482.231, 470.513, 469.398, 472.743, 469.267, 473.921, 475.737, 465.378, 463.202, 468.943, 471.861, 461.97, 469.313, 459.976, 464.047, 458.382, 452.244, 465.276, 449.813, 458.296, 457.704, 468.608, 459.869, 455.862, 449.486, 458.484, 453.949, 453.285, 448.062, 449.677, 448.258, 453.168, 456.424, 458.522, 455.037, 450.947, 450.001, 459.182, 457.189, 451.544, 451.335, 450.838, 456.306, 450.05, 445.28, 445.404, 442.738, 439.839, 445.541, 444.394, 446.45, 458.408, 448.755, 448.188, 457.329, 454.352, 447.329, 458.203, 444.157, 442.355, 454.297, 455.783, 449.484, 451.418, 439.745, 467.647]}	100	100	True
