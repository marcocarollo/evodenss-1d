id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31534.23828		9715062	13	-1	373.3395302295685	{'train_loss': [593228.125, 611869.312, 611246.625, 589273.375, 553162.625, 509272.625, 462706.531, 419265.688, 374976.312, 342606.062, 320043.25, 301274.656, 291534.938, 284976.719, 280933.75, 277232.906, 274956.344, 272733.125, 270984.812, 268746.938, 266484.75, 264665.188, 263205.812, 261970.891, 260166.562, 258052.891, 257218.328, 255542.203, 255115.328, 253893.375, 252645.391, 251717.422, 250458.484, 250307.984, 249244.219, 248045.5, 247038.953, 246486.875, 245140.438, 245372.094, 244326.688, 243638.766, 242654.5, 241965.078, 241306.25, 240388.312, 239402.766, 239769.188, 238786.406, 237890.953, 237606.391, 237186.688, 236260.188, 235112.875, 235319.562, 234797.156, 233934.297, 232870.344, 232876.656, 232315.219, 231249.125, 232596.766, 230550.984, 230761.938, 229650.734, 229520.5, 229259.562, 228618.562, 228210.594, 227461.375, 226833.844, 226381.062, 227094.328, 225947.234, 225104.125, 225071.188, 225431.797, 223837.188, 224709.406, 223890.797, 223352.484, 223415.469, 222891.203, 222107.297, 222524.453, 221027.828, 220835.125, 220421.141, 219988.906, 219727.656, 219270.312, 218839.422, 219270.609, 218288.391, 217494.266, 218170.297, 216813.531, 217103.125, 216691.516, 216833.594], 'val_loss': [2554.85, 2499.154, 2354.837, 2296.603, 2263.622, 1951.635, 1828.799, 1728.081, 1610.514, 1532.464, 1431.481, 1388.573, 1360.213, 1334.292, 1319.226, 1303.196, 1299.24, 1295.334, 1286.768, 1277.36, 1270.492, 1275.098, 1261.144, 1249.85, 1253.107, 1242.071, 1238.817, 1243.677, 1238.395, 1222.674, 1224.078, 1226.908, 1214.958, 1217.061, 1211.508, 1207.573, 1202.973, 1202.132, 1201.918, 1190.344, 1201.018, 1202.061, 1186.701, 1194.447, 1188.259, 1186.518, 1169.404, 1175.995, 1178.955, 1173.092, 1167.486, 1168.68, 1169.666, 1169.335, 1159.676, 1164.066, 1165.136, 1154.821, 1148.848, 1149.576, 1162.224, 1150.06, 1147.765, 1144.742, 1143.629, 1138.193, 1141.47, 1141.595, 1131.214, 1141.159, 1143.805, 1130.694, 1128.058, 1129.338, 1129.698, 1130.618, 1124.84, 1129.665, 1124.451, 1116.438, 1117.6, 1130.916, 1117.208, 1108.43, 1119.102, 1107.776, 1115.231, 1114.935, 1111.041, 1111.91, 1119.344, 1112.774, 1112.164, 1095.291, 1112.894, 1106.884, 1107.37, 1106.803, 1106.849, 1108.46]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:122 kernel_size:9 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:19 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.09023570113171074 beta1:0.9326410844893527 beta2:0.9273997218039933 weight_decay:6.033201551377925e-05 batch_size:16 epochs:100	100	1000	True	137413.03125		18514639	14	-1	481.2774622440338	{'train_loss': [1112745.125, 1957131.75, 1294097.125, 1933423.125, 1291962.375, 1173717.25, 1244382.25, 1194806.5, 1232558.375, 1125868.0, 1165255.5, 1189234.625, 1105037.25, 1110821.125, 1110367.25, 1112399.0, 1115439.0, 1150374.25, 1148403.125, 1100971.875, 1099589.25, 1126031.75, 1196022.375, 1103859.5, 1107596.625, 1103521.625, 1109112.125, 1109533.875, 1090404.0, 1102728.0, 1095147.25, 1098578.5, 1081465.125, 1109522.375, 1101726.375, 1105292.75, 1093580.25, 1093842.875, 1115889.375, 1129004.25, 1078904.75, 1204520.75, 1085180.375, 1091337.75, 1131531.875, 1170142.5, 1110915.625, 1113267.0, 1107509.875, 1089895.75, 1098558.875, 1134425.5, 1091978.875, 1146341.25, 1099212.625, 1125570.5, 1100635.0, 1108078.875, 1096366.75, 1091057.875, 1134283.375, 1082328.875, 1087940.375, 1089510.375, 1092671.5, 1082392.875, 1138165.25, 1115396.5, 1089749.5, 1084486.375, 1123979.875, 1097709.75, 1091226.625, 1099736.625, 1145444.5, 1110670.0, 1109693.125, 1083459.875, 1123168.5, 1102216.0, 1107046.0, 1123452.75, 1125049.25, 1197047.875, 1085845.5, 1097659.75, 1110973.0, 1100440.0, 1093917.5, 1093014.75, 1106068.625, 1108742.5, 1192539.125, 1094106.75, 1108574.875, 1098116.25, 1122919.25, 1111068.25, 1175220.875, 1101823.5], 'val_loss': [5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:3 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta lr:0.08788525529737203 batch_size:16 epochs:100	100	1000	True	32857.8125		5033768	12	-1	295.34431505203247	{'train_loss': [542132.188, 528864.75, 515496.688, 495256.375, 469470.188, 439300.656, 408200.156, 357510.406, 329949.188, 316529.281, 308703.094, 301284.812, 296428.25, 292028.562, 288410.156, 286724.438, 283381.594, 282394.969, 279682.281, 278531.719, 276589.531, 275601.219, 274760.594, 272988.656, 271607.562, 270087.406, 269511.375, 268395.062, 266993.375, 265107.469, 264979.562, 263965.969, 262746.594, 261352.797, 260748.266, 259599.688, 258778.078, 258390.266, 257892.422, 256713.266, 255889.672, 256384.031, 256402.938, 254620.266, 254423.375, 253352.078, 253885.984, 252813.922, 252527.188, 251778.812, 251366.953, 251889.438, 250664.344, 250365.672, 249449.828, 250125.875, 249451.031, 248969.688, 247416.219, 248645.328, 247980.844, 247487.547, 246718.859, 247518.0, 246683.844, 246122.203, 246199.609, 245620.391, 245242.969, 245441.562, 243679.172, 244005.156, 244368.312, 243957.859, 243856.891, 243347.109, 242441.594, 242588.578, 242365.516, 241814.047, 241799.266, 241210.047, 241923.734, 241098.672, 241499.203, 240772.875, 240557.0, 239977.078, 240025.031, 240319.828, 239341.5, 239601.469, 239777.844, 239044.516, 239423.938, 238914.375, 238304.219, 238320.297, 238183.719, 238326.234], 'val_loss': [1963.888, 2006.903, 2033.37, 1872.538, 1859.785, 1856.907, 1660.14, 1588.185, 1501.509, 1460.829, 1414.043, 1393.568, 1383.231, 1355.66, 1368.211, 1345.591, 1340.481, 1319.419, 1315.886, 1294.26, 1294.774, 1292.113, 1284.18, 1281.649, 1279.176, 1278.089, 1266.672, 1248.264, 1259.716, 1251.342, 1240.279, 1240.745, 1225.383, 1225.012, 1228.849, 1224.507, 1221.521, 1211.238, 1218.628, 1198.129, 1214.99, 1202.034, 1196.329, 1208.995, 1200.072, 1194.794, 1185.67, 1186.077, 1195.256, 1185.034, 1175.906, 1181.1, 1184.615, 1179.752, 1181.787, 1171.117, 1164.361, 1175.074, 1167.854, 1174.478, 1172.248, 1168.506, 1159.319, 1165.031, 1164.98, 1174.303, 1178.865, 1166.055, 1155.065, 1150.219, 1161.468, 1145.195, 1155.523, 1153.881, 1152.611, 1150.336, 1145.798, 1154.56, 1148.301, 1135.305, 1143.311, 1142.626, 1152.094, 1142.483, 1140.43, 1142.726, 1142.514, 1133.189, 1137.609, 1144.422, 1141.696, 1139.526, 1135.162, 1143.978, 1140.601, 1145.465, 1138.109, 1136.286, 1137.854, 1126.836]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:106 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:73 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	32175.55078		4983702	13	-1	315.9080913066864	{'train_loss': [527102.875, 526126.562, 513502.469, 496157.906, 473293.094, 446637.688, 421745.031, 397274.25, 373575.625, 353559.344, 336572.344, 323571.062, 311154.281, 302278.688, 294337.938, 287853.062, 283776.406, 281166.5, 278708.688, 276124.75, 273409.031, 271093.406, 269517.344, 267852.719, 266358.688, 265561.031, 263083.344, 262047.75, 261134.438, 259917.375, 259463.531, 257441.531, 256771.125, 255055.328, 254127.875, 252956.359, 252590.281, 251455.188, 250607.406, 249651.406, 249024.984, 247599.219, 247494.438, 246955.328, 245385.125, 245402.625, 244417.016, 243771.25, 243998.688, 243014.062, 242019.531, 241731.906, 241281.766, 240396.328, 239132.828, 239546.531, 239473.0, 238205.016, 238956.578, 237703.672, 236805.344, 237074.547, 235690.016, 235684.031, 235341.172, 235274.094, 234249.875, 234531.047, 233502.297, 232845.719, 233259.0, 232342.875, 231451.891, 231902.656, 230807.453, 231312.016, 230311.484, 229837.109, 230273.25, 230099.797, 229589.422, 229145.234, 228880.562, 228439.547, 228460.656, 227821.562, 227231.859, 226929.688, 226634.297, 226752.391, 225413.688, 226082.766, 226160.688, 225052.25, 225506.75, 224618.531, 224257.234, 223332.203, 224379.969, 223273.625], 'val_loss': [2050.978, 2030.088, 1985.086, 1958.223, 1850.986, 1791.306, 1711.68, 1709.843, 1624.51, 1559.76, 1510.015, 1453.573, 1431.511, 1387.023, 1373.786, 1358.946, 1335.804, 1308.597, 1306.862, 1301.137, 1295.661, 1286.797, 1276.272, 1284.796, 1288.85, 1256.582, 1259.464, 1254.533, 1253.039, 1239.156, 1235.729, 1234.603, 1235.12, 1218.526, 1220.415, 1212.383, 1203.524, 1206.273, 1208.679, 1215.98, 1201.604, 1200.439, 1193.925, 1179.274, 1189.765, 1189.149, 1180.141, 1183.984, 1179.568, 1176.737, 1170.533, 1171.006, 1183.162, 1175.175, 1167.287, 1162.263, 1168.225, 1169.141, 1163.88, 1170.237, 1164.146, 1166.129, 1169.179, 1155.117, 1152.594, 1155.39, 1156.364, 1152.321, 1148.132, 1154.581, 1148.972, 1153.789, 1138.612, 1149.836, 1150.711, 1138.213, 1145.148, 1141.358, 1144.254, 1136.79, 1137.279, 1139.204, 1141.946, 1133.379, 1147.511, 1148.927, 1136.413, 1135.436, 1139.084, 1144.853, 1132.819, 1131.272, 1137.438, 1128.776, 1138.463, 1139.447, 1126.849, 1138.474, 1130.015, 1129.119]}	100	100	True
