id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	32055.22461		9715062	13	-1	380.8025140762329	{'train_loss': [592379.812, 612787.125, 608253.75, 584729.312, 553674.875, 506085.656, 457819.5, 413473.312, 370079.656, 338600.0, 316101.875, 299727.094, 290199.75, 284040.531, 279201.125, 277448.719, 274128.156, 271812.781, 268688.281, 267615.094, 266142.688, 263869.656, 262026.25, 260161.594, 257825.281, 256346.812, 255385.094, 254380.438, 252964.859, 251482.0, 250186.109, 249487.266, 248128.406, 247681.344, 245454.797, 245206.922, 243353.703, 243764.016, 241625.828, 240728.062, 239569.203, 239643.234, 238488.953, 238182.75, 237513.156, 236283.172, 235723.266, 234551.312, 234081.516, 232929.688, 232267.062, 231962.766, 231434.047, 230772.984, 230662.938, 230108.562, 229289.875, 229223.031, 229256.656, 227654.156, 227252.359, 227271.562, 226441.75, 226466.922, 226112.281, 224668.578, 224107.156, 223858.391, 223814.25, 223258.328, 222959.422, 222256.219, 221802.219, 221645.031, 221441.047, 220116.109, 220547.422, 220424.266, 219152.828, 219972.953, 219624.922, 218456.469, 218405.812, 217888.484, 217863.75, 217499.531, 216968.703, 216836.672, 216352.234, 216097.25, 215709.5, 215136.703, 215860.391, 215011.375, 215630.25, 214000.297, 213525.953, 214209.703, 213251.531, 213385.703], 'val_loss': [2451.675, 2364.9, 2434.319, 2267.619, 2100.028, 1961.093, 1807.437, 1709.242, 1531.342, 1488.021, 1438.55, 1380.607, 1365.689, 1342.977, 1316.741, 1313.185, 1302.452, 1290.436, 1275.936, 1266.945, 1274.942, 1250.444, 1241.427, 1247.008, 1236.773, 1236.556, 1230.769, 1223.562, 1216.178, 1226.428, 1227.49, 1203.747, 1208.981, 1204.008, 1201.715, 1180.49, 1184.228, 1180.718, 1179.602, 1172.473, 1164.336, 1160.309, 1164.24, 1158.31, 1154.993, 1159.061, 1147.778, 1140.288, 1146.813, 1135.417, 1147.279, 1144.862, 1139.694, 1134.155, 1144.535, 1129.928, 1131.931, 1126.919, 1130.612, 1133.708, 1128.803, 1130.687, 1135.523, 1133.185, 1137.612, 1123.708, 1115.507, 1111.696, 1123.129, 1111.246, 1127.566, 1114.454, 1116.341, 1113.27, 1102.658, 1112.567, 1114.689, 1110.812, 1113.764, 1108.224, 1122.278, 1130.356, 1109.717, 1102.956, 1110.122, 1109.004, 1111.535, 1107.033, 1110.651, 1104.203, 1118.01, 1126.144, 1107.635, 1106.003, 1101.426, 1107.803, 1109.332, 1111.512, 1114.45, 1115.801]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:48 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.09023570113171074 beta1:0.8015312068234807 beta2:0.8570085711957904 weight_decay:3.28439895827622e-05 batch_size:16 epochs:100	100	1000	True	137348.3125		9796938	13	-1	385.4787802696228	{'train_loss': [2494648.25, 1821613.5, 1725613.125, 2033374.0, 2404109.5, 2420903.25, 1637732.875, 1744033.875, 1784134.625, 2230123.75, 2581825.75, 1907695.625, 1866727.75, 2138430.75, 1794922.25, 1408107.625, 2073940.875, 1653708.5, 1375645.125, 1245393.625, 1185371.875, 1215465.125, 1288774.375, 1394571.625, 1186906.375, 1130377.5, 1140203.375, 1157683.5, 1156715.625, 1170163.75, 1147859.875, 1146647.5, 1124898.875, 1169280.5, 1133611.625, 1152457.75, 1180193.5, 1180229.875, 1131200.25, 1136164.875, 1144770.75, 1168313.125, 1152647.375, 1136032.875, 1144113.375, 1163020.5, 1136126.0, 1136169.125, 1144271.625, 1148437.625, 1139063.875, 1159587.875, 1145097.875, 1181058.375, 1157555.125, 1175049.75, 1168017.875, 1134927.875, 1142564.625, 1169630.25, 1151765.75, 1163677.75, 1147441.375, 1160239.0, 1136997.0, 1145448.75, 1137722.75, 1144479.375, 1140507.0, 1169785.625, 1151264.75, 1143752.375, 1150742.875, 1182796.625, 1152924.625, 1136445.875, 1141501.625, 1155755.375, 1130739.0, 1138193.625, 1150859.5, 1156788.75, 1163417.5, 1139808.375, 1161274.875, 1183129.625, 1211945.5, 1165913.5, 1144563.5, 1150711.625, 1165076.75, 1151389.5, 1166085.75, 1154485.75, 1162055.75, 1173393.75, 1188330.75, 1138051.125, 1164071.75, 1164837.0], 'val_loss': [5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 11632.557, 261749.188, 5293.608, 5293.608, 5293.608, 5293.608, 9474.996, 5978.407, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 6658.514, 5293.608, 8005.621, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 6434.089, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 10360.527, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5490.861, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 9636.928, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5706.776, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 7204.139, 5293.608, 5293.608, 5293.608, 5293.608, 6699.296, 5293.608, 5293.608, 5293.608]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:92 kernel_size:8 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:66 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:66 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.13554128341216293 batch_size:16 epochs:100	100	1000	True	31116.7793		5724402	13	-1	372.9803900718689	{'train_loss': [616418.062, 612934.125, 613107.75, 590857.438, 555293.312, 515281.531, 470653.688, 427862.875, 386698.344, 350063.344, 322525.688, 300347.719, 289480.625, 281833.844, 277488.969, 273478.062, 270573.156, 267828.75, 265016.438, 262890.688, 261193.625, 258950.203, 256779.891, 255260.422, 253595.953, 252461.359, 250680.156, 249500.062, 248040.891, 246331.0, 245324.688, 244619.188, 243274.406, 242132.531, 240769.984, 240544.234, 240896.266, 237865.422, 237632.609, 237237.375, 235911.156, 235567.281, 234213.594, 234312.234, 233994.703, 233057.844, 232327.859, 230979.188, 230356.594, 229884.031, 229473.141, 228854.438, 228258.375, 227586.188, 226957.75, 226442.781, 226828.906, 226494.25, 225028.547, 224738.516, 224379.906, 223462.406, 223550.828, 221968.062, 221895.625, 222104.734, 220877.938, 220728.625, 220035.422, 219698.781, 220998.609, 219549.688, 218689.422, 219100.719, 217896.203, 218602.656, 217688.203, 218246.734, 216665.188, 217137.547, 215420.344, 216083.281, 215800.031, 214285.641, 215308.375, 214478.531, 214268.047, 213678.438, 213112.891, 213104.609, 213080.719, 212739.828, 211587.219, 211424.812, 211076.281, 211180.156, 210259.25, 210687.234, 209480.578, 209455.547], 'val_loss': [2464.319, 2766.83, 2428.977, 2090.249, 1970.738, 1855.634, 1746.215, 1671.975, 1612.696, 1514.037, 1435.13, 1382.649, 1342.742, 1320.704, 1302.02, 1296.439, 1266.147, 1271.046, 1257.147, 1237.893, 1239.706, 1230.664, 1225.833, 1220.77, 1215.504, 1215.724, 1205.539, 1201.339, 1191.603, 1203.386, 1185.389, 1193.634, 1177.122, 1175.849, 1176.467, 1181.99, 1162.771, 1163.55, 1171.786, 1162.621, 1168.37, 1159.686, 1158.874, 1148.431, 1145.753, 1139.217, 1157.118, 1149.416, 1138.031, 1138.706, 1141.797, 1135.763, 1117.607, 1132.417, 1127.727, 1143.243, 1125.263, 1126.626, 1123.463, 1120.022, 1118.47, 1127.001, 1117.714, 1125.852, 1122.41, 1134.889, 1128.027, 1131.681, 1113.309, 1104.585, 1119.675, 1115.611, 1113.184, 1121.138, 1109.733, 1119.369, 1139.77, 1111.38, 1113.31, 1118.511, 1110.615, 1118.729, 1120.741, 1124.158, 1118.025, 1109.714, 1114.264, 1114.99, 1109.78, 1124.152, 1114.393, 1119.9, 1107.278, 1118.349, 1111.654, 1106.764, 1100.068, 1115.912, 1106.728, 1108.726]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31072.30273		9702186	13	-1	382.6287546157837	{'train_loss': [601359.125, 625291.812, 620981.688, 597549.562, 562511.375, 521826.094, 471597.188, 425144.688, 383073.469, 348000.75, 324449.781, 308170.781, 296697.219, 290839.594, 284771.188, 280747.688, 277589.562, 275039.219, 271718.344, 270629.719, 267317.156, 265959.906, 264796.656, 263423.312, 261871.625, 259607.766, 259133.109, 257602.469, 255905.453, 255655.875, 253782.891, 253206.109, 252293.047, 250815.141, 250087.219, 249767.094, 248368.391, 246482.672, 247380.766, 246252.938, 244998.172, 245100.344, 244081.016, 242887.641, 242377.391, 241689.516, 240983.438, 239505.938, 240421.234, 239375.25, 238711.109, 237735.812, 237191.047, 236702.219, 236489.75, 234808.141, 235013.016, 234897.766, 234009.031, 233633.797, 233233.047, 232523.984, 231890.906, 231811.125, 231332.906, 231094.75, 230041.938, 229545.406, 230021.406, 228999.797, 228317.609, 228861.484, 228003.953, 227223.906, 227655.578, 226943.891, 226539.422, 226041.062, 225367.172, 225494.5, 224691.609, 225298.5, 224456.562, 224741.609, 223974.844, 223214.906, 222349.203, 222395.375, 222667.516, 222213.203, 221693.453, 222080.641, 220685.797, 220920.828, 220354.156, 220388.953, 220013.953, 219313.156, 219665.5, 219416.516], 'val_loss': [2427.977, 2510.084, 2407.593, 2272.38, 2083.561, 2101.114, 1918.984, 1796.283, 1671.459, 1497.808, 1470.446, 1424.291, 1386.978, 1365.325, 1334.096, 1312.209, 1306.058, 1298.676, 1281.882, 1283.346, 1274.855, 1269.974, 1256.799, 1249.093, 1243.739, 1244.868, 1234.729, 1229.533, 1226.607, 1230.18, 1219.045, 1216.579, 1213.122, 1202.205, 1209.219, 1189.2, 1196.145, 1195.9, 1202.173, 1194.828, 1190.671, 1168.738, 1167.775, 1171.188, 1171.77, 1177.325, 1169.785, 1166.072, 1163.967, 1164.836, 1155.106, 1151.893, 1160.87, 1156.129, 1143.911, 1149.099, 1143.174, 1136.825, 1138.783, 1144.115, 1125.604, 1136.176, 1127.592, 1131.941, 1143.007, 1127.907, 1128.896, 1123.365, 1128.996, 1124.43, 1125.487, 1123.887, 1121.263, 1113.936, 1112.643, 1119.407, 1129.219, 1117.898, 1116.117, 1109.711, 1110.33, 1104.099, 1103.157, 1109.598, 1110.207, 1114.032, 1110.68, 1116.555, 1120.689, 1107.762, 1108.637, 1100.123, 1106.429, 1108.049, 1103.953, 1103.546, 1107.05, 1099.556, 1097.178, 1098.776]}	100	100	True
