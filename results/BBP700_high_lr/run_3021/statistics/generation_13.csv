id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31384.97656		9715062	13	-1	374.2862706184387	{'train_loss': [593785.375, 616296.938, 613602.438, 592185.062, 557217.375, 515208.562, 467682.25, 425289.125, 381170.438, 349091.469, 322030.188, 304539.094, 294270.062, 286710.75, 282714.812, 277846.531, 275552.219, 272262.0, 270590.875, 268210.938, 265314.812, 264211.25, 261962.578, 259713.312, 258649.844, 258429.219, 255749.953, 255275.609, 253407.469, 252045.609, 251253.906, 249655.984, 249008.812, 247074.641, 247091.203, 245741.578, 245578.406, 244864.0, 243994.328, 242922.547, 241915.891, 241270.422, 240259.125, 239283.453, 238855.531, 238886.672, 238096.938, 237138.188, 236350.0, 236978.359, 234722.125, 235427.453, 234858.25, 234333.906, 233000.297, 233269.891, 232868.219, 231841.422, 231491.938, 231239.109, 230740.984, 229762.484, 230615.953, 230351.234, 229065.688, 228804.516, 228473.656, 227908.656, 227321.344, 226929.75, 226201.703, 225946.641, 226227.547, 224551.391, 224978.938, 224035.047, 225639.438, 223797.609, 223516.344, 224119.641, 222789.703, 222706.703, 222329.031, 222525.734, 221135.359, 220498.0, 220983.188, 220311.719, 220938.047, 219288.469, 219143.281, 218655.188, 219014.781, 218595.578, 218119.438, 217462.047, 217600.875, 217391.438, 216946.016, 216209.047], 'val_loss': [2394.943, 2424.24, 2332.981, 2208.985, 1961.469, 1901.905, 1840.191, 1625.976, 1620.43, 1501.875, 1433.131, 1386.294, 1350.07, 1338.972, 1315.937, 1300.848, 1294.211, 1283.602, 1278.326, 1264.19, 1250.532, 1257.523, 1250.495, 1243.133, 1246.491, 1245.137, 1223.362, 1229.119, 1225.268, 1218.629, 1208.553, 1209.71, 1212.975, 1202.939, 1200.113, 1195.133, 1201.344, 1189.679, 1194.435, 1180.313, 1195.752, 1187.213, 1185.237, 1173.923, 1177.276, 1180.187, 1163.915, 1163.479, 1159.906, 1163.179, 1157.754, 1157.371, 1150.941, 1149.473, 1148.912, 1158.541, 1148.32, 1142.315, 1139.321, 1139.54, 1144.611, 1142.503, 1139.736, 1143.111, 1146.875, 1136.702, 1129.417, 1128.943, 1140.965, 1128.758, 1125.669, 1130.237, 1132.324, 1125.723, 1129.528, 1129.026, 1122.354, 1125.164, 1122.56, 1126.095, 1122.331, 1121.173, 1115.602, 1124.646, 1121.081, 1119.089, 1104.665, 1121.844, 1119.372, 1119.593, 1124.057, 1114.383, 1103.246, 1103.678, 1106.881, 1108.692, 1103.404, 1105.398, 1102.038, 1113.953]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:111 kernel_size:6 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:5 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:90 kernel_size:9 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.14299595889512706 batch_size:16 epochs:100	100	1000	True	32260.70508		2616625	13	-1	301.82157492637634	{'train_loss': [537397.562, 491511.906, 456677.781, 422299.656, 391887.875, 364856.062, 342407.188, 322999.438, 310081.312, 300029.0, 293528.5, 287392.812, 284101.25, 279389.0, 278174.312, 275776.0, 273400.25, 270528.594, 269499.75, 268623.0, 267309.281, 266094.75, 264098.906, 263350.031, 261931.906, 262302.375, 260545.703, 259513.219, 257520.047, 256821.125, 256401.781, 256380.203, 255386.719, 255491.312, 253791.547, 252868.344, 252301.078, 251880.703, 251518.031, 250741.891, 249647.422, 250167.984, 248642.594, 248877.406, 248025.984, 246755.156, 247872.422, 247328.25, 246180.422, 245892.312, 244644.625, 244220.719, 244515.094, 244816.594, 244915.203, 242646.484, 242863.344, 242681.703, 242043.125, 241744.672, 240851.141, 241113.891, 241994.016, 240723.047, 240007.812, 239198.734, 239195.859, 238812.031, 239325.125, 239512.125, 238908.422, 237393.219, 237927.812, 237532.984, 236943.156, 237447.812, 235768.594, 236599.188, 236390.812, 235718.984, 236499.047, 235254.875, 235747.031, 235101.047, 234579.516, 234568.672, 234062.922, 233605.125, 233686.141, 233050.156, 232856.297, 233941.828, 232359.656, 232451.781, 231867.547, 231914.312, 232640.438, 232105.484, 231236.422, 230853.0], 'val_loss': [1970.014, 1871.028, 1814.12, 1795.481, 1668.304, 1583.236, 1511.975, 1443.687, 1423.453, 1382.124, 1352.483, 1342.675, 1336.139, 1329.715, 1316.106, 1297.427, 1292.877, 1289.369, 1282.728, 1293.246, 1270.617, 1247.857, 1274.941, 1278.663, 1263.059, 1260.494, 1249.949, 1240.537, 1227.548, 1229.689, 1227.424, 1232.606, 1234.611, 1229.295, 1220.74, 1218.743, 1216.101, 1209.814, 1211.413, 1209.136, 1211.568, 1213.82, 1201.602, 1208.874, 1197.63, 1200.813, 1193.635, 1199.629, 1192.609, 1193.398, 1180.39, 1185.744, 1184.614, 1190.445, 1174.083, 1180.544, 1183.647, 1174.239, 1182.935, 1169.788, 1168.504, 1168.632, 1182.579, 1178.898, 1177.276, 1165.937, 1168.52, 1172.24, 1168.844, 1162.502, 1167.504, 1157.964, 1157.392, 1151.624, 1154.288, 1150.307, 1157.577, 1159.278, 1152.299, 1150.45, 1146.394, 1147.753, 1151.17, 1152.108, 1146.099, 1138.84, 1148.647, 1145.865, 1148.514, 1143.369, 1139.207, 1143.123, 1136.228, 1143.903, 1151.135, 1138.858, 1137.472, 1145.2, 1134.622, 1140.067]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:14 kernel_size:5 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:47 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:47 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.13080922327389816 batch_size:16 epochs:100	100	1000	True	31767.33789		7943600	14	-1	437.8381154537201	{'train_loss': [630673.5, 650126.5, 636500.562, 605044.25, 551808.75, 487718.812, 424347.75, 370012.406, 329601.312, 305159.75, 295325.969, 287887.406, 282420.969, 278116.938, 274132.125, 270647.75, 267821.531, 264839.969, 262467.031, 259241.234, 257509.375, 255516.766, 253555.219, 251700.547, 249722.609, 248841.516, 248379.984, 245783.172, 245820.688, 243796.609, 242182.031, 240986.453, 239868.578, 238226.938, 237743.594, 236010.219, 236104.703, 234535.984, 233812.438, 232873.062, 232583.609, 231714.562, 230379.094, 229608.281, 229112.016, 228742.922, 227499.062, 227333.594, 227515.406, 225474.156, 224572.922, 223755.844, 224756.547, 224036.562, 222650.219, 222555.75, 222494.047, 221129.609, 221389.234, 219756.234, 220043.141, 217734.328, 218915.766, 217848.938, 218353.375, 218495.016, 217386.812, 216180.5, 215916.469, 216222.703, 215260.422, 214479.859, 213803.656, 214026.781, 213482.672, 213186.719, 212743.078, 211443.0, 211988.312, 211821.172, 210421.047, 210237.156, 210366.406, 208920.734, 209465.641, 209694.938, 208790.156, 208233.328, 208309.125, 207123.641, 206423.031, 206904.031, 207552.547, 205403.422, 205088.531, 204976.625, 204328.641, 204021.188, 203785.25, 203250.969], 'val_loss': [3077.436, 2799.628, 2619.31, 2589.218, 2282.134, 1923.331, 1811.047, 1639.242, 1517.145, 1464.981, 1412.929, 1390.849, 1350.962, 1350.226, 1337.587, 1311.939, 1284.795, 1283.287, 1268.898, 1260.104, 1239.216, 1241.107, 1216.761, 1226.224, 1223.472, 1210.256, 1204.816, 1198.617, 1175.567, 1186.558, 1184.814, 1178.748, 1187.628, 1176.114, 1168.344, 1172.686, 1166.661, 1161.303, 1162.713, 1161.446, 1140.51, 1166.642, 1151.887, 1139.309, 1151.225, 1160.788, 1142.838, 1136.136, 1137.432, 1145.706, 1134.602, 1138.587, 1134.055, 1132.706, 1134.757, 1145.3, 1122.855, 1125.482, 1138.792, 1114.685, 1127.946, 1121.057, 1137.051, 1146.753, 1129.866, 1129.456, 1131.493, 1114.62, 1127.318, 1120.269, 1128.007, 1118.281, 1135.252, 1123.24, 1127.893, 1117.803, 1124.9, 1119.88, 1113.85, 1136.781, 1114.786, 1127.566, 1126.824, 1124.291, 1114.229, 1110.772, 1125.894, 1123.366, 1122.632, 1108.559, 1109.215, 1112.084, 1106.23, 1116.771, 1118.99, 1112.112, 1107.607, 1117.539, 1106.97, 1114.531]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:26 kernel_size:9 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.09023570113171074 beta1:0.972194013992717 beta2:0.9616086793689241 weight_decay:5.114178170957228e-05 batch_size:16 epochs:100	100	1000	True	137652.98438		19139788	13	-1	483.2284846305847	{'train_loss': [1548108.5, 1066921.25, 1192574.375, 1965626.0, 1067774.75, 1164774.75, 1458326.25, 1141943.5, 1313766.875, 1453312.875, 1086697.0, 1325356.625, 1311142.375, 1078379.0, 1154054.875, 1326321.875, 1101627.75, 1081075.25, 1113580.75, 1108567.25, 1074489.75, 1101163.875, 1088689.875, 1084571.875, 1076444.25, 1089709.75, 1091714.875, 1078235.25, 1079201.25, 1101008.25, 1079363.625, 1076611.375, 1088118.875, 1091871.625, 1092509.5, 1074849.125, 1084548.0, 1086685.125, 1075324.625, 1078840.375, 1091853.5, 1103341.5, 1075994.125, 1075681.625, 1086268.75, 1087188.875, 1080560.375, 1098102.625, 1105006.0, 1076542.0, 1082021.625, 1113144.0, 1081264.0, 1087735.875, 1082314.25, 1087963.75, 1081474.125, 1084043.0, 1102347.625, 1099815.5, 1078328.5, 1089082.625, 1116255.625, 1080962.5, 1085993.75, 1083065.0, 1087455.625, 1086489.125, 1094706.75, 1079016.125, 1089952.25, 1076945.625, 1094993.375, 1093793.5, 1081892.0, 1101959.25, 1079834.25, 1099001.125, 1088401.625, 1084927.25, 1081014.75, 1078383.125, 1076905.0, 1092691.625, 1095774.625, 1084573.5, 1089757.125, 1082265.75, 1082471.375, 1080325.375, 1083427.125, 1094502.125, 1092107.0, 1072070.25, 1105320.5, 1086076.0, 1080593.25, 1078614.125, 1110045.0, 1090933.25], 'val_loss': [5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 20998.336, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 9768.434, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608]}	100	100	True
