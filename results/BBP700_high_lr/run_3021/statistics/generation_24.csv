id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31509.32227		9702186	13	-1	385.68332028388977	{'train_loss': [587964.812, 615627.25, 613468.188, 593979.375, 562560.25, 522705.438, 470074.562, 422594.562, 382994.688, 348211.781, 324796.562, 308212.594, 297330.031, 290565.781, 285046.281, 281526.438, 278396.812, 274323.312, 272021.844, 269947.938, 267220.062, 266155.781, 264234.375, 261558.672, 259952.781, 258217.734, 258173.391, 255429.5, 254393.188, 252745.219, 251727.359, 250086.734, 249253.5, 248420.531, 246980.062, 246085.016, 245614.203, 244539.953, 244364.531, 243298.203, 241939.625, 241651.188, 239334.312, 239351.344, 239424.859, 238271.953, 236426.75, 237109.109, 236432.484, 235043.281, 235243.484, 234859.516, 233755.938, 233131.562, 232047.5, 232787.531, 231885.625, 231409.047, 230222.062, 229064.609, 229502.109, 228132.812, 227455.062, 226789.359, 226735.656, 225813.641, 225304.672, 225528.828, 225920.25, 224866.203, 224440.609, 223057.828, 223780.828, 222656.844, 222341.438, 221811.547, 221338.734, 220885.562, 221113.688, 220476.172, 219767.484, 220283.469, 218706.656, 218753.906, 219096.906, 218007.984, 217279.547, 217684.453, 216940.5, 216415.156, 216451.391, 216075.219, 214877.297, 215652.438, 215250.516, 214379.812, 213736.656, 213321.328, 213940.156, 212528.5], 'val_loss': [2377.364, 2531.581, 2433.36, 2224.8, 2108.837, 1984.24, 1831.724, 1771.765, 1622.013, 1570.781, 1481.98, 1425.767, 1401.217, 1366.483, 1349.92, 1319.318, 1322.149, 1307.832, 1297.942, 1302.399, 1283.988, 1280.517, 1278.227, 1290.84, 1277.21, 1270.59, 1267.378, 1252.563, 1246.865, 1244.796, 1233.572, 1220.152, 1215.107, 1228.552, 1217.423, 1209.759, 1200.025, 1206.398, 1208.598, 1194.647, 1199.619, 1184.569, 1186.162, 1194.424, 1175.618, 1177.551, 1182.553, 1180.868, 1171.871, 1174.335, 1170.129, 1158.778, 1152.352, 1150.148, 1155.992, 1159.156, 1153.775, 1158.292, 1152.593, 1148.437, 1146.72, 1146.553, 1144.966, 1156.919, 1139.526, 1132.292, 1140.48, 1136.829, 1136.305, 1150.081, 1137.964, 1138.063, 1139.628, 1136.286, 1133.13, 1141.677, 1130.595, 1130.93, 1144.582, 1134.053, 1134.103, 1125.365, 1146.143, 1126.745, 1131.757, 1130.423, 1148.776, 1125.432, 1126.949, 1130.959, 1133.048, 1127.001, 1124.388, 1129.469, 1127.151, 1124.308, 1121.026, 1125.288, 1124.449, 1129.991]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:14 epochs:100	100	1000	True	31263.66406		9702186	13	-1	419.5991749763489	{'train_loss': [611551.75, 628859.5, 622797.625, 595760.375, 555935.562, 505205.688, 453281.688, 406162.438, 364713.125, 334294.219, 315312.344, 301767.469, 294370.594, 288206.375, 284095.375, 279297.625, 276806.438, 273476.219, 270634.156, 269006.375, 266037.844, 263632.594, 261402.859, 259422.109, 257802.344, 255622.5, 253475.297, 252005.531, 250864.688, 249472.812, 248305.094, 247471.031, 245789.438, 243941.109, 244648.203, 242924.609, 241460.938, 240853.75, 240368.594, 238803.656, 239533.391, 237976.734, 236949.969, 236953.297, 235496.969, 234673.047, 233932.328, 233926.625, 233581.484, 231852.141, 231714.094, 231089.781, 230754.5, 230549.266, 230508.938, 229137.016, 228480.297, 226653.375, 226944.719, 226888.172, 225935.234, 225770.812, 225230.422, 224744.688, 224184.406, 224038.281, 224023.266, 223226.516, 222573.891, 221890.531, 221850.141, 221330.938, 220853.766, 219625.375, 219909.375, 219302.141, 218834.422, 218872.266, 218044.016, 218017.578, 218278.703, 217190.562, 216684.266, 216495.234, 216333.281, 215195.156, 215407.047, 214631.422, 214332.219, 213854.297, 214333.219, 212976.562, 212257.156, 212346.75, 212378.281, 212100.922, 212313.328, 211369.609, 210422.641, 211028.438], 'val_loss': [2028.265, 2075.067, 2031.763, 1849.465, 1691.393, 1569.974, 1558.503, 1378.317, 1322.014, 1268.706, 1228.37, 1215.03, 1188.419, 1158.171, 1131.974, 1120.295, 1109.45, 1106.791, 1086.181, 1084.758, 1081.468, 1071.05, 1070.603, 1060.64, 1053.274, 1045.401, 1044.565, 1038.323, 1032.448, 1022.347, 1019.534, 1026.967, 1023.19, 1014.455, 1014.809, 1010.279, 1007.029, 1006.298, 996.823, 999.417, 993.997, 990.368, 996.99, 992.764, 997.429, 992.896, 990.253, 992.88, 982.981, 987.514, 978.422, 981.169, 979.64, 980.284, 969.387, 972.362, 969.75, 975.605, 967.595, 959.335, 962.44, 962.257, 963.116, 961.753, 963.32, 957.073, 952.017, 964.043, 959.813, 955.029, 956.561, 961.865, 947.064, 959.047, 953.283, 958.679, 955.054, 950.072, 954.702, 951.89, 956.061, 949.198, 947.158, 946.648, 956.718, 946.846, 948.545, 963.826, 953.101, 954.977, 955.714, 954.012, 947.126, 951.655, 953.62, 945.226, 944.138, 949.158, 944.86, 946.127]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:15 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:57 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:82 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:82 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.09023570113171074 batch_size:6 epochs:100	100	1000	True	32722.97852		13523778	14	-1	841.6853051185608	{'train_loss': [742659.625, 798641.312, 788714.188, 741930.438, 660470.5, 546371.312, 428338.531, 339376.844, 300481.031, 290507.125, 284970.812, 280851.656, 277485.938, 273585.188, 270715.469, 268144.719, 266120.562, 263671.031, 261270.078, 259857.484, 257778.609, 255986.719, 254847.359, 253086.375, 251929.734, 249996.812, 248969.328, 247410.578, 246441.312, 245085.094, 243747.812, 243054.156, 241754.234, 240280.562, 239837.234, 239450.906, 238517.547, 236446.75, 235568.031, 234282.562, 233078.234, 232561.344, 230836.375, 230384.906, 229685.047, 228359.438, 227987.375, 227254.047, 226756.688, 225534.25, 224857.531, 223997.047, 223255.297, 222546.609, 221262.656, 221160.328, 220701.984, 219631.562, 219183.453, 218710.312, 217484.859, 217092.766, 216251.25, 215451.5, 215084.141, 215108.25, 214116.391, 213910.719, 213195.703, 211808.109, 211474.922, 210789.453, 211094.641, 210361.828, 209289.625, 209501.109, 209278.516, 208693.562, 208338.594, 206666.781, 207776.219, 206557.141, 205929.609, 205974.203, 206057.234, 204837.422, 204697.359, 203852.609, 203668.438, 203680.375, 204139.297, 202849.344, 202500.969, 201880.938, 202573.344, 202647.344, 201396.859, 201297.734, 200623.969, 200295.906], 'val_loss': [1363.131, 1334.231, 1418.317, 1139.021, 925.839, 716.158, 599.953, 552.281, 527.698, 516.256, 512.521, 503.748, 500.93, 496.171, 491.93, 487.795, 479.129, 479.768, 475.409, 472.156, 471.624, 469.998, 466.532, 469.225, 463.294, 463.075, 459.961, 467.356, 465.333, 462.028, 460.564, 454.897, 453.178, 453.13, 451.987, 454.269, 453.781, 455.439, 450.126, 453.83, 452.294, 449.971, 448.787, 451.988, 453.172, 447.119, 447.246, 445.439, 441.581, 443.127, 445.162, 444.695, 447.231, 443.877, 444.826, 440.028, 448.767, 444.645, 446.593, 444.234, 448.525, 444.911, 444.605, 445.931, 445.65, 445.929, 443.554, 445.896, 446.515, 448.814, 445.188, 450.65, 444.086, 444.496, 445.997, 448.144, 441.074, 444.896, 446.062, 447.04, 449.934, 449.239, 445.633, 447.175, 446.213, 446.823, 449.723, 441.213, 442.232, 442.774, 446.588, 447.825, 449.629, 446.154, 440.162, 440.687, 450.868, 441.326, 443.077, 445.822]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	2000	True	31885.86719		9702186	13	-1	395.66082239151	{'train_loss': [594094.688, 617303.875, 611382.0, 590713.0, 557158.625, 515678.844, 468563.375, 419753.0, 379402.875, 347268.344, 321213.75, 305669.656, 296206.344, 289665.844, 285353.75, 282027.938, 279240.656, 276475.531, 274092.156, 271580.062, 268914.188, 266922.406, 264879.438, 263524.344, 260434.5, 259198.562, 257198.984, 256554.438, 256330.344, 254574.969, 252579.828, 252249.594, 251061.406, 249516.047, 248770.312, 247781.25, 246286.219, 246067.734, 244889.656, 244229.469, 243306.156, 242280.562, 241395.312, 241145.828, 240437.141, 240297.312, 238990.016, 238858.5, 237831.891, 236998.094, 236393.859, 235322.0, 234376.781, 234882.188, 233682.406, 232721.047, 232244.688, 232075.203, 231207.828, 230361.297, 230389.641, 228873.453, 229463.328, 228540.906, 228669.953, 228302.453, 228117.0, 227636.531, 226880.109, 226703.953, 225961.391, 225416.531, 225148.047, 224584.578, 224797.5, 223907.906, 223255.562, 222685.531, 223370.75, 222940.016, 222466.375, 221060.891, 221128.438, 219622.156, 220389.375, 219834.062, 220065.766, 219437.578, 219014.266, 218161.906, 218773.281, 217646.188, 217191.359, 217996.375, 216756.062, 217354.594, 216183.562, 216110.156, 215957.781, 215326.297], 'val_loss': [2393.991, 2407.815, 2439.573, 2265.77, 2077.148, 1928.54, 1852.496, 1699.538, 1621.498, 1489.786, 1457.917, 1420.641, 1393.466, 1370.596, 1350.357, 1330.002, 1315.712, 1332.118, 1305.734, 1300.058, 1309.147, 1304.166, 1284.361, 1274.126, 1262.816, 1257.07, 1265.166, 1254.574, 1228.209, 1229.805, 1225.928, 1223.709, 1225.88, 1210.011, 1210.516, 1213.825, 1197.743, 1208.142, 1192.259, 1194.471, 1202.577, 1176.952, 1176.208, 1175.984, 1179.684, 1186.533, 1169.309, 1166.83, 1173.104, 1158.698, 1158.614, 1163.098, 1158.847, 1159.369, 1169.212, 1167.102, 1149.944, 1156.745, 1146.333, 1151.323, 1145.634, 1154.774, 1159.654, 1145.925, 1146.787, 1151.779, 1147.482, 1142.629, 1141.987, 1132.388, 1134.696, 1140.689, 1141.866, 1131.599, 1132.328, 1134.444, 1141.605, 1132.976, 1143.403, 1139.123, 1133.107, 1130.731, 1135.125, 1137.329, 1135.911, 1127.16, 1141.981, 1137.587, 1138.936, 1128.861, 1129.119, 1131.773, 1127.758, 1130.084, 1141.149, 1128.358, 1117.349, 1134.22, 1119.838, 1126.086]}	0	100	True
