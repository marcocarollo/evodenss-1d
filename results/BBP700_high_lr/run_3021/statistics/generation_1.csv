id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1262432328377348 batch_size:32 epochs:100	100	1000	True	32049.9668		452251	14	-1	209.41993594169617	{'train_loss': [386265.125, 364393.344, 335480.5, 324718.156, 319492.625, 315607.344, 313225.688, 311498.75, 310123.375, 308579.5, 307141.188, 305837.094, 304198.25, 302849.312, 301410.875, 299918.406, 298492.156, 296495.844, 294633.562, 291610.281, 289379.188, 287542.406, 285441.781, 283502.469, 282296.312, 280482.938, 278784.344, 277072.344, 275885.156, 273956.094, 272406.625, 272404.438, 270613.125, 268249.219, 267630.188, 265515.938, 264789.625, 264062.875, 262277.469, 261867.156, 260501.062, 260430.594, 258118.359, 259005.75, 257541.953, 256824.594, 256877.891, 256052.453, 255394.859, 254792.438, 253586.375, 253217.078, 252894.234, 251971.062, 250712.891, 250181.797, 251278.484, 249293.203, 249654.344, 248953.516, 248484.328, 247543.75, 247211.172, 247048.875, 246690.25, 245527.281, 245377.062, 245106.688, 245081.984, 244057.844, 244214.594, 243430.859, 242187.438, 242577.422, 241898.438, 241197.25, 240533.641, 240932.672, 240065.188, 240194.188, 240311.031, 238616.594, 239012.031, 238911.344, 237522.172, 237547.797, 238122.547, 237134.531, 236538.469, 236502.547, 235731.188, 236365.766, 234497.078, 235056.922, 234859.062, 234520.172, 234726.344, 234682.281, 234498.156, 234823.656], 'val_loss': [3398.903, 3138.976, 2996.137, 2969.936, 2943.936, 2923.653, 2908.235, 2897.566, 2886.768, 2879.634, 2870.89, 2860.437, 2850.638, 2841.156, 2830.732, 2820.328, 2803.909, 2783.178, 2768.92, 2746.586, 2729.159, 2717.85, 2704.113, 2697.33, 2689.006, 2673.4, 2658.508, 2643.409, 2636.631, 2607.836, 2613.251, 2603.093, 2595.76, 2591.486, 2565.914, 2562.764, 2532.017, 2543.225, 2530.684, 2521.678, 2523.328, 2506.825, 2529.626, 2502.081, 2501.477, 2507.147, 2498.916, 2475.254, 2504.173, 2498.795, 2469.032, 2457.706, 2447.365, 2445.201, 2451.741, 2459.495, 2415.912, 2422.445, 2421.137, 2419.975, 2395.611, 2387.372, 2372.206, 2375.531, 2371.917, 2388.098, 2371.927, 2363.126, 2392.252, 2385.823, 2362.04, 2349.502, 2368.281, 2347.435, 2325.771, 2334.54, 2330.201, 2332.556, 2318.383, 2309.208, 2314.402, 2318.583, 2325.156, 2318.2, 2310.735, 2296.363, 2306.449, 2296.839, 2284.308, 2298.684, 2302.827, 2295.283, 2272.869, 2282.57, 2303.631, 2285.865, 2274.329, 2275.196, 2266.877, 2273.732]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:32 epochs:100	100	1000	True	31962.83203		545661	13	-1	213.74594116210938	{'train_loss': [399724.188, 356509.75, 320191.656, 311260.656, 307016.969, 303412.25, 300971.188, 298916.406, 296147.031, 294357.062, 291082.406, 288760.969, 287293.062, 284684.938, 283150.75, 280855.531, 279102.688, 277268.438, 276019.469, 274769.844, 273748.75, 272154.75, 270369.594, 268260.25, 268885.406, 267570.562, 265883.688, 264767.625, 264327.75, 263268.75, 261296.516, 260996.297, 260221.469, 259794.75, 258367.469, 258382.75, 256219.219, 255800.469, 255134.609, 253993.266, 253875.875, 252439.172, 251854.578, 251788.781, 250474.172, 249236.406, 249934.312, 248627.312, 248021.922, 247332.469, 246736.516, 247308.797, 245528.062, 245389.188, 245865.344, 245513.641, 243758.234, 244445.625, 243283.422, 243500.172, 242140.688, 242504.344, 241474.016, 240838.25, 240629.891, 240357.312, 239909.484, 239910.016, 238645.0, 238487.156, 237228.578, 237432.547, 237574.781, 237126.625, 236596.266, 237106.5, 236342.125, 235459.484, 235791.875, 235276.109, 234488.453, 234708.266, 234255.219, 234284.359, 234273.719, 234019.234, 232987.891, 233502.219, 232117.453, 231856.828, 231862.625, 231474.578, 231644.906, 230707.172, 230528.891, 230460.344, 229702.469, 229765.953, 230050.062, 229512.391], 'val_loss': [3536.785, 2983.721, 2912.229, 2867.738, 2851.978, 2835.534, 2806.695, 2798.717, 2834.545, 2781.833, 2755.012, 2793.522, 2716.308, 2708.45, 2692.05, 2653.25, 2658.553, 2641.28, 2629.593, 2621.882, 2603.017, 2587.347, 2568.346, 2562.102, 2540.882, 2546.713, 2534.532, 2517.996, 2523.323, 2498.432, 2485.935, 2487.426, 2485.493, 2467.421, 2455.683, 2467.682, 2432.747, 2440.686, 2438.079, 2429.351, 2429.096, 2414.829, 2408.444, 2413.169, 2409.421, 2416.158, 2396.948, 2393.644, 2387.801, 2381.201, 2398.896, 2384.378, 2371.713, 2362.601, 2365.186, 2360.118, 2377.328, 2357.122, 2339.879, 2343.723, 2360.059, 2331.688, 2349.605, 2326.628, 2341.479, 2324.676, 2324.106, 2320.423, 2320.152, 2304.832, 2295.358, 2284.371, 2297.162, 2286.613, 2275.729, 2293.085, 2277.863, 2293.847, 2285.094, 2284.453, 2279.714, 2260.047, 2267.432, 2263.295, 2269.35, 2267.913, 2262.506, 2252.219, 2249.049, 2253.199, 2256.756, 2240.161, 2247.97, 2241.259, 2247.703, 2232.958, 2232.95, 2241.007, 2246.424, 2229.681]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:34 epochs:100	100	1000	True	33044.44141		386731	13	-1	196.29811692237854	{'train_loss': [383843.719, 356889.031, 323872.156, 314402.125, 309825.375, 306692.0, 304269.094, 301715.312, 300029.188, 298528.0, 296294.812, 294000.5, 292521.969, 290402.812, 288325.438, 286071.875, 283509.281, 280911.938, 278856.75, 276975.875, 274975.312, 273844.188, 272502.25, 270963.469, 269814.125, 268652.406, 267820.281, 266187.281, 265953.031, 264880.125, 264296.406, 263703.438, 262838.406, 262033.359, 261119.078, 261141.844, 260292.672, 259386.188, 258615.406, 258097.734, 258543.344, 257099.953, 257307.734, 256688.469, 255964.688, 255205.453, 254566.734, 254035.188, 254143.25, 253730.453, 253438.641, 252482.75, 252032.031, 251601.531, 251418.25, 251091.5, 250333.109, 250548.031, 250328.328, 249689.422, 249326.859, 248679.297, 248835.984, 247987.875, 248583.453, 247796.109, 247620.344, 246898.312, 247531.203, 246333.391, 246296.031, 246617.547, 245984.25, 245363.766, 245574.406, 244449.562, 244193.891, 244340.688, 244065.203, 243750.266, 244094.875, 243429.0, 243404.406, 242443.625, 243146.906, 242388.906, 241449.062, 242099.359, 241281.734, 241408.047, 241099.094, 241219.328, 240837.484, 239925.312, 240414.203, 240026.953, 239646.422, 239294.047, 239658.359, 239537.984], 'val_loss': [3415.691, 3099.596, 2930.865, 2903.366, 2887.462, 2861.901, 2858.937, 2835.95, 2827.298, 2818.328, 2802.351, 2793.648, 2784.105, 2760.662, 2745.027, 2736.385, 2702.195, 2689.823, 2658.39, 2650.675, 2628.583, 2626.799, 2594.308, 2586.6, 2572.785, 2558.175, 2545.976, 2533.844, 2533.065, 2528.177, 2513.385, 2514.618, 2501.816, 2499.77, 2498.924, 2499.539, 2484.658, 2483.183, 2474.313, 2478.703, 2466.461, 2460.976, 2457.863, 2456.483, 2450.054, 2454.46, 2450.495, 2442.238, 2440.396, 2432.53, 2432.984, 2427.248, 2415.901, 2411.203, 2408.258, 2407.067, 2413.602, 2397.039, 2407.557, 2415.978, 2393.34, 2402.237, 2393.778, 2397.419, 2384.022, 2380.315, 2386.396, 2386.306, 2374.668, 2397.327, 2374.259, 2356.028, 2379.155, 2398.674, 2359.262, 2382.699, 2401.833, 2365.757, 2370.216, 2390.399, 2374.39, 2383.435, 2357.896, 2357.243, 2366.033, 2358.727, 2348.364, 2361.233, 2355.618, 2339.287, 2338.835, 2361.558, 2360.958, 2358.617, 2333.185, 2349.781, 2355.573, 2341.148, 2342.178, 2322.326]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:72 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:32 epochs:100	100	1000	True	32452.34766		387075	13	-1	196.93829941749573	{'train_loss': [387739.906, 337134.156, 315769.125, 309592.094, 305826.438, 302757.469, 299965.156, 296834.625, 293558.594, 290601.906, 287680.875, 285082.688, 282558.156, 280361.375, 278047.219, 277441.719, 275752.969, 274126.125, 273647.719, 272600.469, 271597.062, 270269.719, 269603.688, 268252.344, 268082.5, 267457.125, 266746.844, 265996.094, 265129.688, 263751.875, 263316.062, 261861.422, 260998.859, 261233.562, 259728.062, 259459.672, 259516.641, 258579.125, 257755.328, 257603.625, 256642.828, 256525.453, 255505.766, 255213.453, 255399.344, 254273.625, 254616.031, 253375.156, 253763.172, 252188.125, 251745.578, 250583.828, 251430.812, 249511.766, 250073.078, 248580.484, 248302.562, 247757.422, 247923.141, 247525.672, 247698.297, 247142.156, 246192.062, 246176.969, 245678.203, 245826.156, 245283.969, 243897.438, 243935.438, 244030.438, 243506.484, 243335.516, 243118.875, 242403.75, 241720.828, 241445.625, 241708.0, 241267.0, 241323.547, 239713.156, 240273.812, 239700.672, 238895.188, 238690.125, 238778.641, 238850.781, 237390.406, 238152.344, 236838.375, 237440.219, 236443.188, 236437.641, 237221.359, 235215.109, 235218.781, 235944.219, 234959.781, 234792.719, 234826.203, 234147.031], 'val_loss': [3405.878, 3015.59, 2891.363, 2864.688, 2857.795, 2844.358, 2810.797, 2781.634, 2759.657, 2722.586, 2728.279, 2682.902, 2670.514, 2655.609, 2628.313, 2637.144, 2627.534, 2634.346, 2623.956, 2625.953, 2603.44, 2602.62, 2607.265, 2591.982, 2585.019, 2577.955, 2596.764, 2578.173, 2561.649, 2558.825, 2546.578, 2531.029, 2530.013, 2533.663, 2528.946, 2515.068, 2521.514, 2522.979, 2509.638, 2492.299, 2491.519, 2484.246, 2491.836, 2485.669, 2452.541, 2475.479, 2464.281, 2471.28, 2461.604, 2446.388, 2433.422, 2429.22, 2446.957, 2410.571, 2417.737, 2398.231, 2385.447, 2410.584, 2418.068, 2387.311, 2372.687, 2374.167, 2379.505, 2369.635, 2369.753, 2356.991, 2368.04, 2377.504, 2345.022, 2362.418, 2368.189, 2347.779, 2340.198, 2327.692, 2320.882, 2333.952, 2334.198, 2324.819, 2315.544, 2320.216, 2313.244, 2315.326, 2304.243, 2308.125, 2304.635, 2303.219, 2304.312, 2308.404, 2290.022, 2287.43, 2286.133, 2294.758, 2275.793, 2270.513, 2290.46, 2276.566, 2291.834, 2268.049, 2283.312, 2281.896]}	100	100	True
