id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1262432328377348 batch_size:16 epochs:100	100	1000	True	32743.18359		409230	14	-1	312.47374391555786	{'train_loss': [375334.781, 328298.656, 316839.625, 309829.562, 303990.906, 299360.094, 295821.312, 292469.688, 289795.594, 287186.688, 284769.75, 282006.938, 280415.938, 278031.75, 275825.781, 274144.5, 273312.531, 271210.594, 268891.781, 267872.75, 266200.156, 264722.406, 262377.031, 261442.125, 261150.656, 258777.672, 257863.078, 257565.359, 256418.406, 256250.844, 255429.375, 253598.297, 253449.484, 251250.406, 250879.609, 250202.844, 249924.703, 249411.484, 248799.719, 247312.344, 246552.734, 246469.766, 246043.891, 245767.125, 244763.812, 243854.125, 243383.844, 244309.016, 242239.125, 242754.406, 241189.016, 240537.344, 240238.047, 240629.5, 240458.312, 239678.703, 238996.234, 239632.078, 237955.328, 238271.906, 236811.547, 236708.844, 236810.203, 235717.406, 238067.453, 236262.766, 235521.578, 235294.969, 234535.094, 234912.984, 234196.422, 234160.5, 232890.203, 234268.312, 232884.703, 231881.031, 232013.906, 232007.875, 231331.812, 230909.562, 231008.391, 230048.297, 230553.281, 230410.328, 229818.234, 229150.969, 229404.625, 229072.375, 228925.172, 228718.125, 228254.797, 228187.266, 226860.234, 227331.594, 226967.656, 226049.234, 226386.219, 226070.656, 226419.922, 226189.953], 'val_loss': [1550.085, 1497.655, 1479.826, 1445.937, 1412.601, 1396.523, 1383.061, 1376.624, 1357.823, 1353.249, 1340.506, 1325.097, 1330.404, 1318.03, 1315.314, 1308.542, 1300.893, 1299.765, 1281.239, 1263.75, 1262.936, 1259.937, 1266.499, 1261.695, 1245.098, 1234.975, 1235.387, 1230.906, 1228.461, 1223.96, 1212.226, 1207.337, 1204.04, 1198.052, 1222.667, 1220.823, 1197.286, 1204.548, 1198.375, 1180.95, 1194.085, 1199.249, 1181.007, 1195.968, 1185.335, 1187.395, 1185.312, 1198.801, 1186.721, 1173.333, 1177.033, 1173.984, 1167.002, 1173.28, 1171.421, 1174.37, 1179.71, 1181.929, 1180.688, 1169.372, 1165.922, 1174.47, 1158.327, 1163.968, 1164.054, 1162.303, 1156.469, 1156.889, 1147.912, 1157.317, 1160.125, 1149.646, 1154.579, 1158.065, 1155.931, 1159.751, 1148.39, 1139.843, 1150.142, 1144.46, 1150.869, 1144.101, 1140.843, 1141.965, 1139.314, 1138.719, 1139.449, 1140.308, 1136.889, 1132.314, 1132.64, 1132.755, 1144.134, 1138.021, 1136.004, 1133.256, 1134.592, 1138.819, 1131.296, 1138.917]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:86 kernel_size:6 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.1262432328377348 beta1:0.9829500492896798 beta2:0.844059754675888 weight_decay:4.040713247389102e-05 batch_size:16 epochs:100	100	1000	True	1036171008.0		512938	14	-1	327.107727766037	{'train_loss': [1083565.75, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1405364.625, 1945425.0, 2698654.75, 3630501.5, 3803235.75, 2522366.25, 1245534.875, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1448815.5, 102078784.0, 1066921.25, 212236672.0, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25], 'val_loss': [5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5668696752128.0, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 1031876247552.0, 5293.608, 9325587202048.0, 5293.608, 842188062720.0, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:96 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:8 layer:conv1d out_channels:50 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.1262432328377348 beta1:0.9824816475575915 beta2:0.927806061313556 weight_decay:1.8847510023835076e-05 batch_size:16 epochs:100	100	1000	True	50344.41797		415202	14	-1	301.8203926086426	{'train_loss': [479682.656, 398505.75, 392580.219, 405990.219, 398054.375, 393128.0, 384129.406, 384124.094, 393639.719, 393400.688, 402713.125, 387907.688, 400224.344, 400378.969, 391035.781, 395537.0, 399245.75, 386444.875, 409280.062, 388374.594, 400731.688, 388777.062, 399491.969, 387488.656, 398242.438, 397776.188, 402831.438, 391716.406, 393133.406, 393271.812, 388727.75, 422615.594, 396150.469, 384173.0, 383228.531, 386004.125, 398141.344, 404656.562, 383297.25, 398179.031, 387190.781, 386880.656, 394322.531, 409203.969, 396398.156, 390176.375, 386999.406, 398686.281, 398923.812, 384582.375, 384085.5, 390185.031, 399951.188, 406481.219, 385241.531, 376954.031, 377214.5, 400362.031, 396343.438, 385903.156, 381789.625, 388731.406, 385604.406, 408464.406, 383743.625, 387321.5, 387594.562, 381420.0, 386108.25, 381407.938, 389645.5, 386635.688, 393131.281, 392551.406, 384872.375, 408048.281, 400136.094, 389895.688, 388316.469, 389845.312, 393623.469, 386352.125, 398828.562, 397705.406, 394354.281, 391144.281, 393073.719, 397070.188, 398413.344, 393718.656, 390416.75, 404376.562, 385624.406, 395993.312, 410571.375, 419036.812, 386427.469, 377429.875, 384945.5, 405626.406], 'val_loss': [2231.56, 1733.645, 2009.093, 2012.366, 1872.336, 1961.072, 1857.02, 1892.312, 1906.276, 1879.316, 1831.888, 2089.29, 2273.696, 2013.191, 1936.664, 1845.509, 2170.993, 1787.218, 1732.997, 1831.725, 2036.77, 1811.526, 1913.693, 1908.012, 1879.774, 1788.605, 1906.356, 2118.55, 2061.699, 1977.425, 1778.798, 1751.669, 1808.08, 1858.594, 1787.999, 1760.438, 1827.906, 1852.502, 1942.344, 1754.158, 1961.717, 1770.021, 1867.293, 2081.239, 1743.292, 1804.255, 1878.778, 2069.156, 1983.088, 1760.81, 1936.193, 1951.036, 2148.948, 1867.056, 1777.603, 1807.692, 1731.238, 1803.937, 1796.397, 1741.441, 1724.797, 1743.302, 2270.557, 1779.947, 1885.649, 1829.444, 1860.356, 1954.304, 1743.422, 1876.067, 1853.965, 1940.104, 1904.471, 2097.676, 1784.398, 1874.151, 1869.803, 2032.19, 1787.398, 1785.135, 1827.283, 1999.15, 1943.268, 2291.934, 1947.37, 1834.379, 2012.826, 1971.195, 1952.822, 1770.135, 2003.238, 1772.028, 1886.231, 1833.337, 2834.546, 1924.83, 1824.011, 1805.644, 1874.458, 1759.818]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:115 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:72 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:76 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:115 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:16 epochs:100	100	1000	True	137696.10938		74380710	13	-1	667.947722196579	{'train_loss': [1081033.0, 1066921.25, 1070921.5, 1066921.25, 1067874.875, 1067184.625, 1067176.875, 1066915.375, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25], 'val_loss': [5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608]}	100	100	True
