id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31243.13672		9715062	13	-1	375.07995986938477	{'train_loss': [591214.5, 615136.25, 608309.312, 587242.812, 555813.688, 512683.688, 464462.344, 421329.812, 379072.906, 348529.281, 324688.625, 308107.125, 296820.344, 290284.906, 287001.531, 283917.0, 279885.25, 277896.156, 274997.75, 272138.5, 270792.25, 267627.062, 265707.938, 263922.0, 262243.438, 260349.578, 258705.922, 256576.375, 254573.234, 253678.266, 252356.375, 251219.078, 249884.938, 247888.094, 247615.875, 246252.5, 245654.109, 244459.953, 243196.469, 242302.141, 241156.641, 240720.25, 239539.391, 239805.031, 238594.625, 237603.703, 237469.922, 235566.5, 234906.062, 234698.047, 234702.781, 233164.109, 231998.578, 231642.75, 231735.859, 231516.234, 231394.453, 230066.766, 229646.797, 227893.891, 228072.297, 228133.844, 226787.75, 226687.469, 225476.109, 226551.297, 224939.062, 224203.781, 223935.031, 223398.938, 223713.141, 222660.109, 222278.344, 221213.094, 220502.5, 220595.453, 220743.891, 220077.953, 219455.531, 219126.922, 218353.906, 217707.422, 217466.609, 217633.156, 216294.109, 217209.75, 217096.016, 216264.641, 215261.078, 215231.016, 215288.297, 214304.328, 214728.594, 213842.344, 214272.641, 213549.984, 212886.766, 212872.344, 212541.438, 211697.75], 'val_loss': [2290.683, 2675.299, 2304.09, 2273.871, 2092.217, 1975.448, 1877.437, 1713.838, 1599.227, 1557.584, 1488.273, 1418.76, 1393.453, 1362.82, 1367.846, 1351.234, 1335.346, 1332.909, 1309.913, 1292.09, 1283.815, 1284.434, 1251.844, 1258.491, 1258.058, 1242.247, 1245.518, 1235.365, 1226.755, 1225.857, 1210.986, 1211.269, 1214.467, 1208.01, 1192.781, 1188.013, 1194.522, 1191.168, 1183.977, 1177.906, 1177.109, 1170.731, 1166.741, 1170.36, 1163.799, 1162.347, 1155.342, 1167.452, 1156.929, 1170.686, 1152.57, 1158.255, 1155.227, 1154.447, 1147.774, 1148.509, 1145.299, 1150.559, 1151.383, 1151.507, 1145.433, 1135.452, 1146.079, 1137.897, 1145.279, 1140.696, 1128.708, 1134.216, 1135.993, 1139.669, 1132.2, 1130.502, 1126.166, 1138.674, 1132.223, 1135.976, 1129.162, 1119.947, 1123.145, 1124.079, 1124.452, 1121.07, 1141.276, 1128.25, 1123.715, 1119.506, 1127.426, 1121.03, 1132.467, 1115.228, 1124.684, 1119.962, 1117.553, 1109.497, 1119.991, 1123.509, 1125.066, 1121.274, 1123.302, 1121.758]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	2000	True	31534.23828		9715062	13	-1	374.5779092311859	{'train_loss': [593228.125, 611869.312, 611246.625, 589273.375, 553162.625, 509272.625, 462706.531, 419265.688, 374976.312, 342606.062, 320043.25, 301274.656, 291534.938, 284976.719, 280933.75, 277232.906, 274956.344, 272733.125, 270984.812, 268746.938, 266484.75, 264665.188, 263205.812, 261970.891, 260166.562, 258052.891, 257218.328, 255542.203, 255115.328, 253893.375, 252645.391, 251717.422, 250458.484, 250307.984, 249244.219, 248045.5, 247038.953, 246486.875, 245140.438, 245372.094, 244326.688, 243638.766, 242654.5, 241965.078, 241306.25, 240388.312, 239402.766, 239769.188, 238786.406, 237890.953, 237606.391, 237186.688, 236260.188, 235112.875, 235319.562, 234797.156, 233934.297, 232870.344, 232876.656, 232315.219, 231249.125, 232596.766, 230550.984, 230761.938, 229650.734, 229520.5, 229259.562, 228618.562, 228210.594, 227461.375, 226833.844, 226381.062, 227094.328, 225947.234, 225104.125, 225071.188, 225431.797, 223837.188, 224709.406, 223890.797, 223352.484, 223415.469, 222891.203, 222107.297, 222524.453, 221027.828, 220835.125, 220421.141, 219988.906, 219727.656, 219270.312, 218839.422, 219270.609, 218288.391, 217494.266, 218170.297, 216813.531, 217103.125, 216691.516, 216833.594], 'val_loss': [2554.85, 2499.154, 2354.837, 2296.603, 2263.622, 1951.635, 1828.799, 1728.081, 1610.514, 1532.464, 1431.481, 1388.573, 1360.213, 1334.292, 1319.226, 1303.196, 1299.24, 1295.334, 1286.768, 1277.36, 1270.492, 1275.098, 1261.144, 1249.85, 1253.107, 1242.071, 1238.817, 1243.677, 1238.395, 1222.674, 1224.078, 1226.908, 1214.958, 1217.061, 1211.508, 1207.573, 1202.973, 1202.132, 1201.918, 1190.344, 1201.018, 1202.061, 1186.701, 1194.447, 1188.259, 1186.518, 1169.404, 1175.995, 1178.955, 1173.092, 1167.486, 1168.68, 1169.666, 1169.335, 1159.676, 1164.066, 1165.136, 1154.821, 1148.848, 1149.576, 1162.224, 1150.06, 1147.765, 1144.742, 1143.629, 1138.193, 1141.47, 1141.595, 1131.214, 1141.159, 1143.805, 1130.694, 1128.058, 1129.338, 1129.698, 1130.618, 1124.84, 1129.665, 1124.451, 1116.438, 1117.6, 1130.916, 1117.208, 1108.43, 1119.102, 1107.776, 1115.231, 1114.935, 1111.041, 1111.91, 1119.344, 1112.774, 1112.164, 1095.291, 1112.894, 1106.884, 1107.37, 1106.803, 1106.849, 1108.46]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:74 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:84 kernel_size:8 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31715.24805		18867546	13	-1	462.1183178424835	{'train_loss': [684629.5, 729911.438, 740745.938, 724464.75, 675022.562, 600265.875, 500689.375, 393993.812, 326111.188, 301164.5, 289694.75, 283986.969, 279427.094, 275697.438, 273413.219, 270907.062, 267143.688, 264743.969, 263462.594, 261361.047, 259553.219, 256747.641, 255704.922, 253083.094, 252485.062, 250015.922, 249638.484, 247563.875, 246852.938, 244825.141, 244356.188, 243274.812, 242267.484, 241207.656, 239858.438, 239385.203, 237718.812, 237107.75, 236662.484, 235560.719, 235429.172, 234011.219, 233293.359, 232616.25, 231449.906, 230786.25, 230862.25, 229238.625, 228759.781, 228317.578, 226852.562, 226153.797, 226087.891, 225295.766, 224577.547, 224057.969, 223436.391, 223119.906, 222507.078, 221292.406, 221594.906, 220842.266, 219849.891, 219591.734, 218366.766, 218301.828, 217547.469, 216818.844, 215752.719, 216251.5, 215319.922, 214742.516, 214930.078, 213478.266, 213901.047, 212808.531, 213524.625, 212410.172, 211432.875, 211551.484, 211077.203, 209797.734, 210412.219, 209558.609, 208820.719, 208419.656, 208516.141, 208804.344, 207725.516, 207755.797, 205979.234, 206232.047, 205275.531, 205410.75, 205284.703, 204189.734, 204407.203, 203751.047, 202997.484, 203435.172], 'val_loss': [3088.269, 3222.607, 3139.466, 2899.418, 2555.604, 2153.055, 1832.759, 1552.47, 1459.221, 1423.818, 1373.405, 1377.757, 1344.93, 1319.698, 1299.875, 1294.118, 1283.692, 1272.698, 1257.359, 1255.07, 1237.876, 1230.392, 1221.766, 1232.496, 1214.643, 1211.844, 1204.322, 1206.868, 1187.875, 1185.183, 1187.587, 1182.951, 1177.349, 1174.614, 1171.175, 1174.953, 1164.071, 1165.464, 1163.065, 1167.723, 1158.806, 1157.308, 1154.519, 1148.599, 1140.935, 1149.127, 1143.164, 1130.073, 1146.864, 1131.191, 1131.435, 1132.217, 1135.278, 1126.916, 1126.408, 1120.414, 1126.199, 1118.603, 1123.098, 1117.635, 1126.69, 1109.871, 1122.236, 1114.331, 1116.958, 1112.622, 1104.692, 1112.396, 1118.951, 1109.683, 1108.641, 1111.008, 1103.417, 1127.554, 1112.997, 1107.132, 1107.627, 1117.891, 1103.839, 1103.384, 1102.211, 1093.094, 1112.924, 1101.664, 1111.429, 1095.733, 1094.195, 1097.524, 1095.136, 1085.837, 1095.075, 1094.135, 1084.194, 1081.986, 1100.807, 1092.61, 1091.285, 1090.781, 1087.791, 1102.55]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	2000	True	31534.23828		9715062	13	-1	374.46806812286377	{'train_loss': [593228.125, 611869.312, 611246.625, 589273.375, 553162.625, 509272.625, 462706.531, 419265.688, 374976.312, 342606.062, 320043.25, 301274.656, 291534.938, 284976.719, 280933.75, 277232.906, 274956.344, 272733.125, 270984.812, 268746.938, 266484.75, 264665.188, 263205.812, 261970.891, 260166.562, 258052.891, 257218.328, 255542.203, 255115.328, 253893.375, 252645.391, 251717.422, 250458.484, 250307.984, 249244.219, 248045.5, 247038.953, 246486.875, 245140.438, 245372.094, 244326.688, 243638.766, 242654.5, 241965.078, 241306.25, 240388.312, 239402.766, 239769.188, 238786.406, 237890.953, 237606.391, 237186.688, 236260.188, 235112.875, 235319.562, 234797.156, 233934.297, 232870.344, 232876.656, 232315.219, 231249.125, 232596.766, 230550.984, 230761.938, 229650.734, 229520.5, 229259.562, 228618.562, 228210.594, 227461.375, 226833.844, 226381.062, 227094.328, 225947.234, 225104.125, 225071.188, 225431.797, 223837.188, 224709.406, 223890.797, 223352.484, 223415.469, 222891.203, 222107.297, 222524.453, 221027.828, 220835.125, 220421.141, 219988.906, 219727.656, 219270.312, 218839.422, 219270.609, 218288.391, 217494.266, 218170.297, 216813.531, 217103.125, 216691.516, 216833.594], 'val_loss': [2554.85, 2499.154, 2354.837, 2296.603, 2263.622, 1951.635, 1828.799, 1728.081, 1610.514, 1532.464, 1431.481, 1388.573, 1360.213, 1334.292, 1319.226, 1303.196, 1299.24, 1295.334, 1286.768, 1277.36, 1270.492, 1275.098, 1261.144, 1249.85, 1253.107, 1242.071, 1238.817, 1243.677, 1238.395, 1222.674, 1224.078, 1226.908, 1214.958, 1217.061, 1211.508, 1207.573, 1202.973, 1202.132, 1201.918, 1190.344, 1201.018, 1202.061, 1186.701, 1194.447, 1188.259, 1186.518, 1169.404, 1175.995, 1178.955, 1173.092, 1167.486, 1168.68, 1169.666, 1169.335, 1159.676, 1164.066, 1165.136, 1154.821, 1148.848, 1149.576, 1162.224, 1150.06, 1147.765, 1144.742, 1143.629, 1138.193, 1141.47, 1141.595, 1131.214, 1141.159, 1143.805, 1130.694, 1128.058, 1129.338, 1129.698, 1130.618, 1124.84, 1129.665, 1124.451, 1116.438, 1117.6, 1130.916, 1117.208, 1108.43, 1119.102, 1107.776, 1115.231, 1114.935, 1111.041, 1111.91, 1119.344, 1112.774, 1112.164, 1095.291, 1112.894, 1106.884, 1107.37, 1106.803, 1106.849, 1108.46]}	0	100	True
