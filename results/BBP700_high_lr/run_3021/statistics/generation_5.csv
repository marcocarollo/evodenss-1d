id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:16 epochs:100	100	1000	True	32356.75		545661	13	-1	309.4499866962433	{'train_loss': [395682.594, 335857.844, 317913.5, 310427.844, 305449.562, 300225.188, 296371.5, 292945.688, 289111.312, 285290.594, 282917.344, 280271.969, 278191.469, 277248.844, 274549.312, 274174.531, 272490.469, 270921.906, 269178.781, 267644.562, 267699.531, 265892.125, 265114.562, 264187.062, 263624.094, 262114.781, 261601.375, 259655.172, 259995.969, 258900.328, 258296.094, 257027.656, 256227.172, 255338.047, 254247.422, 255056.156, 253515.828, 252861.141, 251686.234, 251825.125, 251337.906, 250141.656, 250036.438, 249074.266, 248636.938, 247560.047, 247958.422, 246898.438, 246231.828, 245876.797, 245201.781, 244567.328, 245458.391, 243296.906, 243525.641, 242740.203, 242598.203, 241964.547, 241600.422, 241820.266, 240765.297, 240835.078, 239669.859, 239553.141, 238888.906, 237970.141, 237581.766, 237961.969, 237621.562, 236757.5, 236545.828, 236204.922, 236266.875, 236091.906, 235476.438, 236093.828, 234549.938, 234760.75, 234511.188, 233637.453, 233215.922, 232973.516, 233053.828, 233158.047, 231906.062, 231851.641, 230782.391, 232140.594, 231331.469, 230789.016, 230191.109, 229626.438, 229804.172, 228871.844, 229793.609, 228459.375, 228750.266, 227522.5, 228316.109, 227211.297], 'val_loss': [1761.796, 1492.649, 1460.752, 1429.907, 1423.104, 1419.551, 1392.166, 1379.648, 1355.19, 1342.563, 1339.757, 1322.405, 1305.256, 1302.097, 1308.703, 1306.228, 1294.97, 1293.62, 1288.998, 1286.057, 1276.714, 1271.096, 1266.591, 1274.845, 1259.682, 1265.728, 1270.313, 1251.508, 1257.235, 1250.154, 1238.598, 1234.741, 1238.694, 1233.313, 1228.881, 1235.238, 1220.17, 1218.652, 1222.328, 1223.825, 1208.718, 1203.862, 1202.635, 1207.313, 1203.922, 1201.528, 1198.18, 1198.849, 1190.708, 1186.552, 1183.833, 1188.274, 1173.411, 1175.304, 1185.797, 1170.99, 1174.417, 1180.589, 1169.889, 1162.62, 1173.083, 1173.692, 1174.793, 1158.087, 1172.503, 1170.829, 1172.612, 1162.358, 1164.358, 1166.692, 1149.606, 1156.422, 1153.089, 1147.849, 1152.527, 1145.106, 1156.875, 1141.704, 1144.589, 1150.272, 1160.016, 1149.321, 1146.76, 1145.495, 1145.488, 1146.106, 1142.392, 1135.043, 1136.988, 1137.286, 1134.548, 1144.797, 1141.531, 1130.603, 1129.473, 1137.733, 1130.081, 1138.666, 1129.038, 1134.585]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:70 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.1262432328377348 alpha:0.9022379644734494 weight_decay:3.627873670406919e-05 batch_size:16 epochs:100	100	1000	True	136827.60938		483727	14	-1	290.3121380805969	{'train_loss': [1309647.5, 1342485.0, 1227268.625, 1188528.125, 1207091.625, 1260685.125, 1183368.75, 1219641.0, 1186134.875, 1197575.75, 1217830.375, 1190746.5, 1224549.875, 1142703.125, 1162954.0, 1166589.75, 1208238.125, 1151239.75, 1130917.625, 1241021.375, 1170355.25, 1122783.875, 1106595.25, 1174827.25, 1180714.75, 1137429.875, 1203216.125, 1158793.5, 1100420.375, 1181517.0, 1139596.625, 1115210.0, 1130373.125, 1116246.625, 1183244.625, 1112785.0, 1128704.375, 1192815.625, 1126957.625, 1190934.5, 1128963.0, 1131720.5, 1114734.25, 1144026.25, 1110751.75, 1127370.875, 1111442.875, 1132506.375, 1188814.5, 1128170.125, 1135049.25, 1258050.625, 1134439.125, 1112438.875, 1148224.25, 1118353.125, 1113358.125, 1164720.625, 1191504.375, 1129459.625, 1121570.5, 1135547.25, 1125149.625, 1127750.125, 1124471.75, 1160845.875, 1147216.375, 1135120.75, 1147173.625, 1121491.375, 1203226.75, 1125037.25, 1133532.75, 1153544.875, 1136433.5, 1156507.625, 1183150.375, 1128253.625, 1183014.25, 1143945.25, 1126967.625, 1157808.875, 1181724.125, 1121981.125, 1118916.125, 1118582.875, 1256158.875, 1105124.25, 1218072.0, 1151837.375, 1152122.75, 1142562.5, 1159741.625, 1103829.75, 1120306.375, 1201577.875, 1111076.5, 1238335.125, 1128959.125, 1190794.5], 'val_loss': [5293.608, 5617.038, 5293.438, 5293.472, 5224.542, 5260.727, 5291.567, 5247.068, 5465.662, 5293.01, 5261.32, 5317.0, 5154.564, 5293.498, 5277.98, 5283.339, 7281.603, 5293.362, 5293.522, 5269.234, 5282.125, 5225.566, 5288.986, 5383.11, 7088.211, 5293.339, 5275.656, 5177.435, 5293.595, 5290.337, 6684.358, 5292.765, 5217.278, 5372.164, 5293.517, 5291.111, 5293.597, 5289.571, 5257.38, 5287.923, 5292.67, 5287.522, 5280.647, 5237.157, 5283.768, 5297.823, 5280.717, 5269.776, 8201.568, 5249.868, 5364.324, 5206.447, 5582.381, 5293.125, 5813.432, 5289.321, 5293.353, 5293.531, 5203.884, 5875.122, 5536.947, 5292.558, 5288.381, 5272.852, 5275.906, 5289.374, 5290.765, 5281.881, 5239.674, 5292.412, 5338.755, 5280.075, 5293.398, 5258.507, 5293.31, 5221.317, 5194.896, 5246.958, 5329.651, 5431.725, 5271.036, 5287.121, 5210.731, 5292.597, 5293.539, 5280.729, 5288.913, 5293.602, 5270.004, 5196.672, 5184.566, 5293.499, 6120.22, 5293.608, 5293.077, 5285.847, 5292.767, 5276.12, 5290.381, 5274.942]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1262432328377348 batch_size:11 epochs:100	100	1000	True	31987.17969		666858	14	-1	406.79829120635986	{'train_loss': [400518.094, 336833.656, 322763.562, 313633.438, 305608.5, 299705.594, 294929.125, 290641.5, 287250.344, 284870.0, 281476.781, 279389.5, 276702.531, 274543.844, 272491.469, 270787.188, 269640.719, 268134.75, 265952.688, 264332.031, 261676.453, 259956.109, 259687.891, 257398.516, 255350.953, 254895.031, 252822.891, 250699.984, 249732.062, 248547.406, 246778.516, 245179.078, 245030.328, 243196.734, 241983.641, 241753.266, 240962.422, 240076.438, 238728.219, 238461.672, 237142.234, 237155.281, 235995.609, 235030.0, 234358.438, 234397.891, 233995.812, 233139.969, 232328.797, 231179.359, 230882.281, 230367.688, 229507.391, 229075.5, 227928.625, 229526.781, 228258.328, 227306.859, 226382.938, 226327.547, 225513.094, 225199.484, 223879.641, 224283.516, 223431.391, 223844.656, 222537.938, 222767.562, 222089.156, 221826.188, 221390.391, 220529.719, 219685.906, 219646.531, 219014.5, 219121.453, 218516.203, 218130.672, 217372.203, 217321.641, 217546.594, 216326.531, 215523.188, 215842.594, 214873.438, 214665.656, 215109.0, 213327.906, 214265.656, 214202.734, 213326.875, 213152.922, 212861.906, 211999.094, 211488.75, 211533.266, 211763.516, 211439.719, 210986.062, 209714.641], 'val_loss': [1088.86, 1041.912, 1006.466, 972.631, 959.861, 952.938, 945.584, 935.057, 926.181, 924.926, 909.68, 906.843, 903.409, 898.302, 893.901, 887.4, 879.248, 875.154, 876.339, 874.508, 865.492, 874.824, 860.2, 861.123, 855.471, 855.216, 862.457, 855.794, 850.288, 845.852, 845.972, 850.564, 837.239, 838.278, 836.287, 826.523, 825.41, 824.134, 824.155, 827.004, 828.656, 834.366, 818.923, 836.387, 825.729, 822.819, 825.411, 827.097, 821.689, 833.746, 833.274, 830.743, 810.034, 814.593, 823.128, 823.856, 821.453, 812.761, 810.374, 813.315, 818.248, 810.394, 823.78, 807.822, 814.294, 804.405, 805.715, 802.124, 817.361, 802.482, 807.764, 808.7, 817.786, 816.387, 813.144, 811.279, 807.515, 809.278, 804.52, 799.047, 806.352, 806.747, 798.815, 806.214, 805.558, 802.001, 811.932, 799.299, 819.685, 798.862, 804.119, 808.38, 807.204, 819.358, 805.52, 798.945, 813.204, 816.628, 799.851, 814.312]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:125 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.14340826072923146 batch_size:16 epochs:100	100	1000	True	32904.05469		569688	13	-1	290.21642804145813	{'train_loss': [367379.156, 325980.344, 317553.75, 313154.531, 308956.062, 302438.5, 295645.156, 290051.312, 285673.344, 282006.688, 279832.25, 277603.688, 275651.344, 273246.125, 271528.5, 269996.438, 268398.25, 266303.812, 265400.594, 263904.375, 263155.812, 261846.703, 261114.297, 259589.281, 258757.812, 258180.016, 257709.531, 256430.984, 255316.625, 254862.688, 254111.797, 253092.453, 252690.094, 251637.359, 251382.141, 250431.156, 250130.0, 249267.172, 248432.891, 248030.766, 247530.656, 247191.078, 245913.078, 245723.828, 244739.984, 243702.766, 244061.094, 244868.984, 243276.547, 242648.875, 242777.281, 242015.656, 242028.516, 241274.344, 240920.797, 240983.141, 240145.516, 239958.281, 239155.297, 239333.797, 238615.188, 238895.625, 237836.438, 237478.703, 237424.703, 237379.031, 236470.156, 236187.938, 235375.156, 236180.469, 235843.047, 234727.734, 234348.172, 234060.062, 235036.031, 233897.281, 233788.984, 233579.984, 232044.016, 232421.094, 232518.922, 232790.109, 232277.188, 231376.594, 231484.609, 230269.734, 231370.547, 230585.156, 229877.234, 230523.656, 230519.219, 229355.484, 229084.0, 229699.234, 228884.547, 228893.453, 228679.438, 227930.875, 228722.656, 227552.5], 'val_loss': [1549.47, 1482.568, 1457.499, 1443.745, 1425.027, 1401.744, 1395.925, 1363.399, 1338.743, 1341.744, 1334.634, 1324.776, 1317.634, 1308.358, 1301.735, 1295.805, 1297.013, 1289.519, 1284.721, 1279.242, 1277.127, 1263.114, 1267.828, 1263.379, 1260.16, 1254.584, 1245.136, 1242.363, 1235.93, 1239.928, 1231.522, 1222.933, 1219.343, 1218.651, 1217.854, 1207.572, 1205.266, 1206.667, 1215.983, 1213.151, 1218.493, 1204.725, 1204.331, 1203.203, 1201.716, 1200.506, 1203.136, 1194.202, 1195.245, 1196.83, 1199.723, 1202.721, 1193.862, 1182.622, 1186.63, 1186.136, 1185.246, 1186.054, 1183.976, 1183.671, 1187.417, 1188.503, 1183.563, 1175.802, 1185.908, 1156.836, 1179.227, 1162.643, 1168.142, 1175.035, 1173.804, 1159.124, 1166.187, 1158.937, 1161.389, 1166.434, 1151.267, 1145.764, 1155.775, 1156.343, 1155.994, 1151.331, 1162.768, 1139.833, 1152.61, 1160.851, 1137.904, 1155.713, 1156.07, 1154.635, 1145.666, 1139.616, 1148.165, 1128.129, 1149.454, 1133.442, 1130.769, 1140.247, 1141.827, 1142.675]}	100	100	True
