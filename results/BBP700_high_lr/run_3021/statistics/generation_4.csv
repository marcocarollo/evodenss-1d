id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:20 epochs:100	100	1000	True	32316.54688		545661	13	-1	270.80719113349915	{'train_loss': [386451.562, 327884.812, 315742.969, 311211.531, 307665.75, 304371.25, 301628.312, 298559.844, 296252.781, 293191.469, 290601.375, 288764.312, 285251.938, 283164.812, 280112.625, 278000.688, 275701.844, 273900.906, 272115.125, 269836.188, 267919.094, 266425.5, 264959.688, 264000.875, 262401.656, 261033.984, 260883.969, 259266.578, 258119.344, 257307.031, 257750.938, 256257.938, 255417.938, 254747.453, 253503.297, 252509.797, 251602.438, 251698.297, 250951.219, 249635.5, 249175.422, 249401.672, 248762.266, 247872.547, 247277.172, 246518.312, 246121.344, 246438.953, 244560.078, 244706.844, 244419.844, 244068.516, 243239.078, 242659.344, 242322.688, 241984.125, 242265.484, 241270.266, 241149.094, 240410.609, 240288.344, 240053.594, 239662.828, 238468.078, 238078.406, 237812.469, 237305.109, 236957.234, 236938.234, 236814.016, 235587.562, 235316.438, 235824.797, 235774.906, 234441.938, 233633.469, 233879.0, 233323.25, 233301.859, 232539.188, 232533.172, 232007.688, 232444.375, 231484.453, 230992.031, 230929.406, 230606.828, 230584.609, 231079.672, 229669.156, 229472.984, 229134.203, 229539.172, 229092.359, 228015.391, 227859.562, 228175.672, 228145.844, 227263.375, 227453.797], 'val_loss': [1990.547, 1859.698, 1821.74, 1812.654, 1799.681, 1790.747, 1781.722, 1770.646, 1749.21, 1733.963, 1735.019, 1702.849, 1702.362, 1680.392, 1684.133, 1662.671, 1654.578, 1640.586, 1625.844, 1619.626, 1629.058, 1609.547, 1600.157, 1601.353, 1596.417, 1580.931, 1584.747, 1576.92, 1573.813, 1588.258, 1562.225, 1561.152, 1566.848, 1557.601, 1552.746, 1545.591, 1537.311, 1550.345, 1535.305, 1528.992, 1539.096, 1520.101, 1531.098, 1517.047, 1519.292, 1522.058, 1522.864, 1517.463, 1520.597, 1503.858, 1506.817, 1517.146, 1495.956, 1495.691, 1493.465, 1494.939, 1494.196, 1493.07, 1492.697, 1480.229, 1471.755, 1482.073, 1491.737, 1475.004, 1492.033, 1474.968, 1471.576, 1469.837, 1467.522, 1481.347, 1469.727, 1461.585, 1464.868, 1461.47, 1463.659, 1464.648, 1453.309, 1458.385, 1458.079, 1459.035, 1446.269, 1462.222, 1453.664, 1448.074, 1464.834, 1444.089, 1458.578, 1458.229, 1452.567, 1445.619, 1441.02, 1446.048, 1447.155, 1443.277, 1441.708, 1435.195, 1435.631, 1437.57, 1435.866, 1434.013]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:30 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:20 epochs:100	100	1000	True	32211.90234		19143799	13	-1	390.9259090423584	{'train_loss': [812480.312, 823730.625, 841480.688, 822647.875, 776468.188, 680657.25, 517128.719, 344642.312, 296732.844, 288648.719, 282857.344, 278873.281, 276104.406, 273209.406, 271159.469, 268915.719, 267391.844, 264417.594, 262651.906, 261095.609, 259140.547, 256842.266, 255495.25, 253967.844, 252798.578, 251241.938, 250312.641, 249526.797, 248783.266, 247748.109, 246288.953, 245687.359, 244592.625, 243499.625, 243789.297, 242337.469, 241044.391, 240425.641, 239667.312, 238564.094, 237452.672, 237711.156, 235711.484, 235046.078, 234728.125, 233672.078, 232941.891, 231928.984, 232099.641, 230867.375, 231440.703, 230489.391, 229701.641, 228670.969, 227339.172, 227556.891, 226679.281, 225713.312, 226241.484, 225096.094, 224680.797, 224591.938, 224360.969, 223034.969, 223595.359, 222300.25, 221171.438, 220626.812, 220328.812, 220530.672, 220437.125, 219479.281, 218770.219, 217654.969, 218390.297, 217586.141, 217930.312, 217054.141, 216583.344, 215297.688, 216102.75, 215190.531, 214940.0, 214966.062, 215020.453, 214553.781, 213387.422, 212928.531, 212663.141, 213238.297, 212659.656, 211857.703, 211796.328, 211075.781, 210727.406, 211090.266, 210722.297, 211661.172, 210019.969, 209317.141], 'val_loss': [4892.796, 5122.141, 4833.972, 4449.202, 3974.183, 3153.791, 2201.332, 1782.829, 1739.811, 1699.171, 1680.292, 1657.338, 1642.087, 1639.045, 1621.128, 1612.242, 1592.183, 1586.727, 1580.238, 1582.913, 1562.316, 1560.205, 1553.5, 1523.258, 1532.759, 1518.36, 1512.296, 1508.92, 1509.079, 1500.269, 1500.78, 1506.815, 1495.096, 1498.426, 1492.269, 1503.329, 1479.111, 1487.725, 1489.344, 1485.206, 1473.288, 1470.501, 1467.576, 1469.342, 1468.002, 1475.398, 1476.135, 1462.3, 1459.643, 1455.327, 1462.34, 1455.87, 1481.239, 1454.972, 1453.378, 1467.537, 1465.581, 1455.589, 1451.797, 1452.418, 1466.64, 1456.67, 1440.209, 1463.675, 1457.609, 1463.075, 1451.513, 1438.161, 1434.851, 1439.349, 1420.72, 1446.113, 1441.022, 1437.852, 1449.984, 1439.453, 1439.045, 1438.278, 1437.574, 1439.214, 1443.661, 1440.983, 1451.733, 1450.91, 1441.272, 1450.92, 1468.66, 1442.842, 1425.431, 1428.787, 1442.487, 1434.714, 1430.078, 1443.512, 1431.521, 1430.686, 1451.448, 1435.749, 1439.532, 1441.591]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:16 epochs:100	100	1000	True	31723.8125		545661	13	-1	299.11116552352905	{'train_loss': [389866.719, 332102.219, 318072.406, 310361.031, 304165.312, 298981.188, 294051.281, 290339.0, 286270.281, 284072.469, 280850.719, 278179.094, 275906.281, 274248.375, 271673.719, 270276.906, 268705.188, 266619.0, 265510.469, 264781.594, 262807.75, 261883.812, 258338.078, 259412.016, 257474.703, 256456.594, 255463.484, 255272.25, 254241.703, 252815.391, 251350.266, 250909.391, 250196.969, 248862.156, 248512.719, 247714.969, 247599.312, 246410.969, 245621.297, 245765.031, 244888.828, 244675.391, 243365.906, 243292.625, 242309.125, 241946.656, 241872.812, 239895.141, 240088.797, 239598.828, 239106.438, 237534.969, 237515.984, 237767.281, 237558.812, 236428.953, 236470.172, 235435.25, 235150.688, 235033.625, 234662.5, 233561.75, 233664.438, 233008.969, 233303.547, 231994.422, 231989.938, 231549.766, 231792.453, 232324.844, 231323.312, 230405.797, 229108.344, 229968.5, 229529.156, 229091.5, 229037.984, 227661.672, 227385.625, 227307.812, 226952.688, 227214.922, 227019.844, 226529.375, 226264.547, 226227.375, 225003.703, 225022.891, 225120.625, 224267.156, 224386.609, 223455.094, 223508.109, 223026.25, 222857.797, 222347.859, 223159.75, 223385.156, 222996.734, 221993.203], 'val_loss': [1583.687, 1472.229, 1443.6, 1426.247, 1408.881, 1389.197, 1381.144, 1362.064, 1351.049, 1332.411, 1326.712, 1309.891, 1304.626, 1297.893, 1292.677, 1282.953, 1274.908, 1270.064, 1260.109, 1254.726, 1247.851, 1252.737, 1247.49, 1239.042, 1233.893, 1232.285, 1233.612, 1232.712, 1206.971, 1218.955, 1217.697, 1214.072, 1203.858, 1204.411, 1204.325, 1194.053, 1199.566, 1194.852, 1200.291, 1206.32, 1188.157, 1185.881, 1185.554, 1188.011, 1176.182, 1180.335, 1171.626, 1191.829, 1183.975, 1173.06, 1181.24, 1173.005, 1166.076, 1163.4, 1182.977, 1166.057, 1161.752, 1163.171, 1175.402, 1160.496, 1172.84, 1168.129, 1166.848, 1160.005, 1163.304, 1155.876, 1161.714, 1161.632, 1157.79, 1136.502, 1151.74, 1152.993, 1150.293, 1146.872, 1155.787, 1136.25, 1140.154, 1128.76, 1137.476, 1136.3, 1129.526, 1134.491, 1126.606, 1126.233, 1139.524, 1126.868, 1122.684, 1126.124, 1127.7, 1122.767, 1126.462, 1121.239, 1121.231, 1117.734, 1129.738, 1117.36, 1119.708, 1117.946, 1110.562, 1124.531]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:69 kernel_size:2 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:20 epochs:100	100	1000	True	31947.89258		985292	13	-1	350.274790763855	{'train_loss': [440894.438, 343410.156, 321020.625, 311341.531, 304724.594, 299441.188, 294273.625, 290643.438, 287650.312, 284789.5, 282822.062, 279710.562, 277573.844, 275915.562, 273667.656, 271849.812, 270541.0, 268951.25, 267094.219, 265768.438, 264742.562, 262881.906, 261208.016, 259707.016, 258407.266, 257429.312, 256608.859, 255153.219, 254222.766, 254206.719, 252291.156, 251359.688, 251125.547, 249093.438, 248402.203, 247927.688, 247446.812, 247288.344, 245864.359, 245210.75, 244741.609, 243700.234, 243365.156, 242545.078, 242145.484, 241983.391, 241269.141, 240511.562, 239926.547, 239652.578, 238123.469, 238156.938, 237484.344, 237029.219, 236407.234, 235787.297, 235564.719, 235131.953, 233706.75, 234171.219, 234064.984, 232921.359, 232909.062, 231765.875, 232125.531, 231059.484, 230757.0, 230539.406, 229632.266, 229264.312, 228904.25, 229295.438, 228392.438, 228751.219, 228327.766, 227425.406, 227464.469, 226838.234, 226554.469, 226763.312, 225542.312, 224667.281, 225441.125, 223932.016, 224128.297, 224449.891, 224213.25, 223103.203, 223239.938, 222406.844, 222724.75, 222553.906, 220369.547, 220916.828, 221410.203, 221179.641, 221400.75, 219053.797, 220093.922, 220117.828], 'val_loss': [2182.067, 1962.059, 1876.482, 1821.023, 1775.113, 1748.329, 1714.545, 1687.234, 1686.527, 1684.95, 1668.285, 1669.027, 1641.439, 1636.397, 1628.995, 1624.409, 1617.85, 1614.225, 1590.383, 1577.831, 1583.627, 1574.465, 1565.275, 1575.624, 1567.836, 1544.193, 1539.442, 1551.067, 1554.748, 1536.789, 1546.653, 1531.112, 1537.259, 1527.12, 1520.287, 1516.151, 1513.477, 1511.34, 1513.005, 1504.119, 1505.206, 1491.463, 1487.524, 1506.732, 1512.32, 1501.404, 1491.223, 1496.466, 1499.713, 1499.58, 1494.13, 1494.967, 1484.173, 1489.785, 1493.331, 1496.378, 1482.91, 1482.332, 1497.065, 1490.727, 1484.73, 1481.011, 1476.33, 1485.959, 1472.973, 1478.571, 1485.388, 1476.248, 1476.537, 1474.33, 1465.475, 1473.534, 1456.208, 1469.526, 1470.698, 1475.416, 1484.467, 1474.164, 1482.245, 1453.51, 1454.731, 1464.445, 1447.083, 1449.279, 1454.778, 1464.359, 1451.363, 1468.596, 1455.609, 1457.125, 1456.693, 1460.504, 1450.021, 1456.506, 1449.687, 1454.106, 1437.912, 1450.902, 1449.091, 1447.791]}	100	100	True
