id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31885.86719		9702186	13	-1	394.5179908275604	{'train_loss': [594094.688, 617303.875, 611382.0, 590713.0, 557158.625, 515678.844, 468563.375, 419753.0, 379402.875, 347268.344, 321213.75, 305669.656, 296206.344, 289665.844, 285353.75, 282027.938, 279240.656, 276475.531, 274092.156, 271580.062, 268914.188, 266922.406, 264879.438, 263524.344, 260434.5, 259198.562, 257198.984, 256554.438, 256330.344, 254574.969, 252579.828, 252249.594, 251061.406, 249516.047, 248770.312, 247781.25, 246286.219, 246067.734, 244889.656, 244229.469, 243306.156, 242280.562, 241395.312, 241145.828, 240437.141, 240297.312, 238990.016, 238858.5, 237831.891, 236998.094, 236393.859, 235322.0, 234376.781, 234882.188, 233682.406, 232721.047, 232244.688, 232075.203, 231207.828, 230361.297, 230389.641, 228873.453, 229463.328, 228540.906, 228669.953, 228302.453, 228117.0, 227636.531, 226880.109, 226703.953, 225961.391, 225416.531, 225148.047, 224584.578, 224797.5, 223907.906, 223255.562, 222685.531, 223370.75, 222940.016, 222466.375, 221060.891, 221128.438, 219622.156, 220389.375, 219834.062, 220065.766, 219437.578, 219014.266, 218161.906, 218773.281, 217646.188, 217191.359, 217996.375, 216756.062, 217354.594, 216183.562, 216110.156, 215957.781, 215326.297], 'val_loss': [2393.991, 2407.815, 2439.573, 2265.77, 2077.148, 1928.54, 1852.496, 1699.538, 1621.498, 1489.786, 1457.917, 1420.641, 1393.466, 1370.596, 1350.357, 1330.002, 1315.712, 1332.118, 1305.734, 1300.058, 1309.147, 1304.166, 1284.361, 1274.126, 1262.816, 1257.07, 1265.166, 1254.574, 1228.209, 1229.805, 1225.928, 1223.709, 1225.88, 1210.011, 1210.516, 1213.825, 1197.743, 1208.142, 1192.259, 1194.471, 1202.577, 1176.952, 1176.208, 1175.984, 1179.684, 1186.533, 1169.309, 1166.83, 1173.104, 1158.698, 1158.614, 1163.098, 1158.847, 1159.369, 1169.212, 1167.102, 1149.944, 1156.745, 1146.333, 1151.323, 1145.634, 1154.774, 1159.654, 1145.925, 1146.787, 1151.779, 1147.482, 1142.629, 1141.987, 1132.388, 1134.696, 1140.689, 1141.866, 1131.599, 1132.328, 1134.444, 1141.605, 1132.976, 1143.403, 1139.123, 1133.107, 1130.731, 1135.125, 1137.329, 1135.911, 1127.16, 1141.981, 1137.587, 1138.936, 1128.861, 1129.119, 1131.773, 1127.758, 1130.084, 1141.149, 1128.358, 1117.349, 1134.22, 1119.838, 1126.086]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:40 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:102 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:40 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta lr:0.10122985789045114 batch_size:16 epochs:100	100	1000	True	32133.8418		1961986	12	-1	324.5891897678375	{'train_loss': [499501.0, 456316.812, 430598.75, 409037.688, 391333.656, 375708.031, 359274.5, 345145.156, 329463.281, 315606.938, 305411.188, 298289.812, 294380.531, 289397.469, 285377.594, 282545.031, 279213.812, 276981.875, 273601.125, 272229.688, 270739.125, 268978.656, 268035.0, 266719.781, 264558.25, 263310.938, 261462.547, 261797.094, 260145.234, 259854.5, 258575.281, 257438.0, 256326.672, 255788.078, 255836.719, 255594.047, 254031.406, 252889.297, 253833.156, 252590.734, 252490.859, 252084.297, 250384.641, 250161.859, 249553.484, 249045.516, 247811.328, 247901.328, 247056.109, 246577.094, 246380.312, 246021.844, 245657.938, 245207.266, 244995.938, 244828.547, 243749.078, 243862.375, 242932.25, 242907.656, 242925.203, 241908.234, 242127.125, 242590.531, 241565.688, 241283.234, 240910.125, 240237.094, 240247.812, 240340.625, 239292.891, 239852.875, 237721.125, 238393.828, 239166.172, 237630.281, 236957.219, 237259.828, 237181.75, 236609.344, 236347.562, 236530.0, 236501.672, 235686.562, 235256.906, 235277.062, 234664.953, 234413.469, 234967.25, 233967.406, 234204.016, 234976.953, 233153.094, 233871.078, 232158.078, 232519.625, 232887.859, 232735.5, 232015.328, 232145.562], 'val_loss': [2020.196, 1899.146, 1850.062, 1778.975, 1719.9, 1620.986, 1537.125, 1533.737, 1428.653, 1392.074, 1357.595, 1352.29, 1361.612, 1338.241, 1314.802, 1304.913, 1306.997, 1285.091, 1272.443, 1260.552, 1261.755, 1251.008, 1262.49, 1238.822, 1251.345, 1236.389, 1236.614, 1244.716, 1247.143, 1217.827, 1221.831, 1216.29, 1217.122, 1216.97, 1207.395, 1211.537, 1208.056, 1204.79, 1195.509, 1193.011, 1204.932, 1194.566, 1194.288, 1186.648, 1180.42, 1183.716, 1184.672, 1179.981, 1170.545, 1176.805, 1176.748, 1163.693, 1175.941, 1180.206, 1170.139, 1159.932, 1162.959, 1170.885, 1167.136, 1165.423, 1159.148, 1165.595, 1156.981, 1167.391, 1160.688, 1154.451, 1154.377, 1159.753, 1172.433, 1153.413, 1149.523, 1153.267, 1157.144, 1144.956, 1151.47, 1145.638, 1142.459, 1142.979, 1142.5, 1144.645, 1143.53, 1148.348, 1154.436, 1143.723, 1129.394, 1140.773, 1142.14, 1136.13, 1131.883, 1136.531, 1134.8, 1130.997, 1139.879, 1138.088, 1138.166, 1131.795, 1130.686, 1134.541, 1132.518, 1129.289]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:76 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:84 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:76 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.09023570113171074 beta1:0.8203608390260786 beta2:0.8593662671872886 weight_decay:3.641135250156975e-05 batch_size:16 epochs:100	100	1000	True	141095.40625		6542240	13	-1	361.48502349853516	{'train_loss': [1993501.0, 1387992.0, 1406101.625, 1352133.375, 1583762.875, 1383560.375, 1240418.625, 1372335.625, 1421328.625, 1485606.5, 1833988.25, 1695468.5, 1315017.375, 1403376.0, 1288031.75, 1177874.375, 1107819.875, 1165749.75, 1100530.75, 1139577.75, 1145294.75, 1101977.375, 1121260.625, 1147199.375, 1105944.5, 1097324.375, 1099209.125, 1107695.75, 1122311.75, 1109036.25, 1116112.125, 1103724.25, 1108380.75, 1116758.5, 1109111.25, 1123558.0, 1096748.375, 1102838.875, 1107905.125, 1108556.0, 1106659.25, 1114473.375, 1107953.625, 1120065.5, 1124946.75, 1114625.25, 1115547.375, 1117318.25, 1107290.25, 1108495.5, 1114205.375, 1102235.0, 1112904.5, 1106694.5, 1107996.375, 1113196.25, 1120958.875, 1120269.875, 1108855.25, 1114456.25, 1109425.5, 1115159.5, 1107262.5, 1127859.625, 1107867.625, 1115354.25, 1105205.625, 1103240.25, 1116574.25, 1104319.25, 1105707.875, 1118964.25, 1115281.0, 1103532.375, 1109156.0, 1118056.75, 1117862.125, 1114725.25, 1105077.625, 1102586.375, 1103351.75, 1103093.125, 1106919.125, 1109511.25, 1110264.0, 1120992.75, 1100997.5, 1105275.75, 1108575.625, 1100728.0, 1109116.5, 1115656.5, 1114770.125, 1115701.125, 1106504.375, 1111987.375, 1112462.375, 1107705.75, 1094767.75, 1118905.0], 'val_loss': [5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 6344.227, 20815.672, 6005.922, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5533.238, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 6827.827, 5293.608, 6301.473, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5342.8, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5693.151, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.607, 5293.608, 5293.608, 7892.887, 5969.696, 5583.228, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 6479.375, 5293.608, 5293.608, 5293.608, 5293.608, 6297.008, 5293.608, 5449.68]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:96 kernel_size:8 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:5 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.09023570113171074 alpha:0.8735333706568141 weight_decay:5.979366822424135e-05 batch_size:16 epochs:100	100	1000	True	137304.73438		3502884	12	-1	325.0700480937958	{'train_loss': [2306951.25, 1711410.25, 2932048.5, 2543781.75, 2577952.75, 2432754.75, 2040964.0, 2345529.5, 2039993.25, 1921770.25, 1897919.0, 3068112.0, 1574145.125, 2406907.75, 1655104.75, 1877386.375, 1787842.125, 2011757.875, 1485524.5, 1364133.625, 1305090.5, 1282179.0, 1305595.5, 1356325.375, 1361448.875, 1280773.375, 1344317.125, 1291616.75, 1351199.5, 1289417.375, 1210095.0, 1345152.625, 1282387.875, 1273320.875, 1213089.375, 1380291.625, 1413878.875, 1252326.5, 1312439.125, 1236184.375, 1267221.875, 1293249.5, 1215934.25, 1260787.625, 1187728.25, 1263871.25, 1278278.75, 1279897.75, 1268579.875, 1272256.25, 1338795.0, 1274917.25, 1254257.125, 1346885.125, 1328414.875, 1212794.875, 1301061.75, 1234472.25, 1198966.0, 1284420.875, 1245562.75, 1220148.625, 1275809.75, 1422205.625, 1277056.0, 1288362.25, 1277403.0, 1201548.75, 1259609.625, 1259672.5, 1285352.625, 1251130.5, 1258810.625, 1223768.25, 1221319.375, 1254017.25, 1288491.75, 1200147.5, 1234780.625, 1314877.375, 1300838.625, 1402746.875, 1213412.875, 1310962.125, 1254668.75, 1289423.5, 1235490.0, 1390224.875, 1231004.0, 1291033.125, 1257434.0, 1217429.25, 1286215.875, 1296535.875, 1292555.875, 1233689.0, 1337972.875, 1269061.25, 1205016.25, 1327624.0], 'val_loss': [5293.608, 5293.608, 5293.608, 14513.23, 5293.608, 5293.608, 5293.608, 5293.608, 8364.79, 6928.28, 5292.485, 5293.608, 69398.008, 5293.608, 5872.359, 5293.608, 5278.856, 12801.681, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5283.965, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 6147.055, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 6038.346, 5293.608, 7203.124, 5293.608, 5293.608, 6756.223, 5293.608, 5293.608, 5293.608, 13867.237, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 9507.172, 5293.608, 6986.76, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608]}	100	100	True
