id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31094.90039		9702186	13	-1	401.2616422176361	{'train_loss': [586669.0, 616191.0, 612527.812, 591467.625, 559822.062, 516485.281, 470290.719, 420980.438, 379787.625, 343943.219, 320248.031, 302473.844, 291790.688, 285250.156, 281778.562, 278797.25, 276308.438, 274338.812, 272577.688, 270527.844, 269371.281, 267788.594, 265785.469, 264294.25, 262362.094, 261007.984, 259460.375, 258407.531, 256941.297, 255607.266, 254474.938, 253261.688, 252627.891, 251378.516, 250140.312, 250166.531, 248536.328, 247740.5, 247191.25, 246223.953, 245808.109, 244497.141, 244237.875, 243347.219, 242421.594, 242704.672, 241417.328, 240975.125, 240555.359, 238668.656, 238613.156, 237984.609, 238231.875, 236340.266, 236278.859, 235433.078, 235668.453, 234682.844, 234400.828, 234215.406, 233742.344, 232550.625, 232365.438, 231932.688, 231875.281, 231139.484, 230046.656, 229986.297, 229170.781, 228758.062, 228566.375, 227894.234, 227363.797, 227040.109, 227136.656, 225861.578, 225883.094, 225646.141, 224750.812, 223712.578, 223778.031, 223442.797, 223970.906, 222947.234, 222883.047, 222654.203, 221688.031, 221213.016, 220918.688, 220280.234, 219862.969, 220181.719, 219331.891, 219520.859, 218382.25, 218453.25, 217608.703, 217929.656, 217418.781, 216407.266], 'val_loss': [2432.871, 2283.949, 2383.844, 2219.657, 2155.846, 2108.247, 1894.446, 1751.174, 1580.398, 1512.871, 1440.896, 1389.082, 1367.12, 1344.61, 1330.24, 1314.325, 1313.772, 1296.198, 1302.842, 1288.899, 1286.443, 1283.796, 1282.962, 1267.527, 1255.174, 1259.522, 1271.784, 1244.675, 1240.166, 1235.163, 1236.125, 1238.285, 1224.522, 1210.754, 1219.171, 1214.453, 1206.92, 1210.311, 1199.31, 1205.496, 1188.721, 1188.221, 1194.781, 1186.095, 1189.131, 1177.143, 1175.318, 1169.197, 1172.616, 1168.815, 1160.323, 1168.357, 1164.747, 1166.122, 1155.198, 1157.992, 1154.237, 1154.201, 1158.334, 1151.059, 1149.952, 1151.054, 1150.91, 1143.413, 1154.903, 1135.485, 1150.092, 1137.167, 1146.718, 1130.403, 1129.321, 1137.251, 1135.586, 1133.186, 1126.175, 1125.775, 1129.585, 1134.887, 1132.47, 1129.617, 1121.07, 1134.239, 1121.368, 1126.736, 1124.529, 1132.208, 1122.401, 1121.327, 1125.259, 1129.278, 1120.851, 1115.91, 1116.233, 1113.12, 1113.25, 1112.481, 1120.02, 1126.485, 1119.45, 1118.045]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:77 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:79 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:65 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:14 epochs:100	100	1000	True	33171.97266		2999135	13	-1	365.72865629196167	{'train_loss': [519935.062, 495488.125, 477146.5, 406045.75, 337378.406, 322277.688, 314461.375, 311258.281, 306695.906, 302819.094, 298976.531, 298345.469, 296355.625, 293831.0, 291414.344, 288177.531, 287149.25, 285165.625, 282465.25, 281356.25, 281547.844, 279454.719, 278777.469, 276821.438, 275595.906, 274782.594, 274080.375, 272893.375, 270973.625, 271953.531, 270200.0, 269599.562, 268216.625, 267774.219, 267020.0, 266535.594, 266433.312, 265009.656, 263687.719, 264192.062, 263363.094, 263592.281, 263275.969, 261562.516, 261978.812, 260494.719, 259693.078, 259522.125, 260172.578, 257965.781, 259161.375, 257310.391, 257483.938, 256245.891, 257741.047, 256139.078, 256326.406, 254903.438, 255094.75, 254211.766, 255478.031, 253686.172, 254998.062, 254201.703, 252615.422, 252693.906, 252897.625, 252524.125, 252302.297, 251252.969, 251625.469, 250594.969, 249968.734, 248381.406, 251087.453, 249551.516, 249257.141, 249167.609, 248466.719, 247877.641, 247068.656, 247512.641, 246926.688, 246431.109, 246285.062, 246451.625, 245611.906, 245479.062, 245861.0, 244467.875, 245201.016, 245165.594, 244237.062, 244313.047, 244544.25, 244636.422, 243883.516, 243315.672, 243169.234, 242201.828], 'val_loss': [1755.85, 1719.339, 1717.542, 1392.234, 1298.863, 1252.156, 1238.417, 1209.956, 1198.861, 1172.077, 1173.767, 1170.491, 1149.683, 1152.391, 1148.505, 1131.555, 1136.453, 1128.249, 1128.902, 1107.099, 1108.985, 1108.815, 1116.479, 1101.783, 1110.616, 1098.31, 1105.111, 1101.024, 1078.845, 1076.337, 1072.165, 1070.976, 1072.008, 1060.899, 1066.63, 1070.666, 1069.433, 1069.251, 1047.976, 1037.639, 1042.331, 1043.074, 1037.026, 1040.013, 1036.511, 1043.533, 1023.397, 1037.197, 1029.996, 1026.325, 1029.647, 1045.738, 1030.433, 1024.943, 1034.045, 1025.339, 1027.249, 1015.679, 1029.966, 1028.971, 1028.784, 1033.729, 1027.213, 1012.695, 1015.855, 1022.395, 1028.303, 1026.013, 1025.5, 1029.811, 1020.536, 1023.715, 1021.48, 1024.943, 1013.196, 1026.487, 1012.988, 1024.358, 1007.257, 1020.654, 1020.056, 1010.345, 1002.052, 1013.448, 1009.78, 1018.111, 1003.12, 1014.817, 1016.919, 1002.939, 1010.496, 1002.626, 1003.75, 1010.03, 1001.904, 1010.815, 1013.652, 999.183, 1005.367, 996.39]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:115 kernel_size:5 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta lr:0.14803270441862185 batch_size:16 epochs:100	100	1000	True	32267.97656		9949472	12	-1	346.7684669494629	{'train_loss': [747359.438, 778000.125, 737373.688, 618940.625, 441269.281, 318851.719, 298759.594, 292498.344, 287297.281, 283321.625, 280450.75, 278268.781, 275988.25, 273279.906, 272380.25, 270345.812, 268556.031, 266934.5, 265723.656, 264027.844, 262507.188, 261450.359, 260061.344, 259301.844, 257796.609, 255571.797, 254447.188, 254191.281, 252469.203, 251319.812, 250924.859, 249279.375, 248490.547, 246710.609, 245908.0, 245439.406, 244598.547, 243378.406, 242723.266, 242516.75, 241053.812, 240462.156, 239723.672, 239315.75, 239195.969, 237635.984, 236458.672, 236443.25, 235617.516, 234820.109, 234541.672, 233356.266, 233289.281, 233352.047, 231359.172, 232598.562, 231525.031, 231293.734, 230172.047, 230599.859, 230404.406, 228856.562, 228538.469, 227398.141, 228771.234, 226814.938, 227433.922, 227264.531, 225815.281, 225896.969, 226535.422, 226145.953, 226070.234, 224086.078, 223116.891, 223211.219, 222669.141, 223020.312, 223135.297, 221870.875, 221115.422, 221889.109, 221312.953, 221239.781, 220785.422, 220513.531, 219313.172, 218791.562, 220322.156, 219379.891, 218517.844, 217787.469, 217515.828, 217854.531, 217589.469, 217734.531, 216152.281, 216654.875, 216886.625, 216566.672], 'val_loss': [3044.657, 3067.007, 2717.193, 1984.282, 1608.949, 1440.188, 1400.032, 1384.451, 1353.527, 1338.584, 1341.639, 1319.21, 1307.101, 1308.062, 1302.463, 1295.309, 1283.052, 1282.891, 1269.402, 1268.15, 1258.961, 1261.209, 1249.193, 1259.64, 1244.852, 1246.732, 1247.93, 1236.034, 1229.817, 1232.388, 1228.943, 1226.752, 1209.728, 1211.349, 1218.722, 1209.131, 1213.151, 1206.563, 1205.116, 1221.911, 1207.367, 1213.811, 1203.437, 1210.322, 1199.347, 1199.332, 1188.596, 1181.215, 1196.181, 1180.555, 1184.257, 1175.458, 1179.625, 1181.796, 1199.542, 1183.216, 1184.141, 1195.781, 1177.102, 1161.551, 1170.896, 1170.493, 1184.033, 1169.386, 1173.619, 1187.366, 1166.362, 1170.773, 1171.146, 1162.669, 1161.068, 1156.076, 1156.096, 1156.317, 1165.837, 1172.311, 1159.249, 1165.646, 1147.605, 1172.846, 1146.73, 1155.609, 1158.922, 1165.107, 1151.296, 1162.135, 1154.042, 1161.68, 1163.75, 1157.367, 1149.177, 1164.214, 1166.143, 1141.258, 1154.041, 1155.225, 1141.02, 1139.503, 1144.103, 1138.731]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:36 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:36 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta lr:0.09023570113171074 batch_size:5 epochs:100	100	1000	True	32905.60547		6081124	12	-1	689.1556355953217	{'train_loss': [596308.875, 598654.5, 554830.875, 485720.781, 410890.656, 341147.656, 310235.781, 300389.594, 292523.938, 287996.0, 284929.969, 281145.375, 278116.188, 275566.438, 272981.625, 272459.594, 270526.781, 268656.406, 266996.938, 264236.375, 263826.625, 262375.812, 261078.0, 259707.125, 258683.344, 257497.141, 255634.094, 254917.875, 254259.875, 253212.953, 252025.047, 250737.547, 250520.906, 249291.188, 247962.75, 248599.406, 246627.953, 245711.188, 244840.047, 244612.062, 244066.469, 242630.734, 242462.562, 240759.219, 240478.141, 240362.219, 239912.312, 239350.797, 237611.109, 237975.094, 237193.812, 236216.078, 236139.891, 234980.141, 234893.859, 233973.188, 234141.766, 233619.812, 232324.781, 231305.172, 231819.672, 231658.578, 230805.938, 230447.328, 230486.453, 229048.234, 228961.953, 228839.688, 227611.5, 228067.062, 227528.516, 227407.312, 225990.531, 225072.578, 226177.984, 224917.078, 224893.266, 224402.078, 222913.734, 223752.375, 222664.156, 221183.016, 222259.453, 222055.141, 221248.969, 220430.578, 220842.578, 219748.219, 220082.25, 219621.031, 219395.312, 219326.219, 218550.094, 218689.953, 216475.156, 216282.953, 216625.578, 216192.844, 216230.547, 215798.891], 'val_loss': [722.09, 652.851, 565.454, 520.538, 488.179, 458.587, 439.503, 428.7, 423.673, 418.692, 415.236, 412.166, 408.372, 404.869, 401.877, 399.622, 397.103, 395.689, 395.036, 392.067, 390.904, 387.778, 385.524, 384.869, 383.307, 380.355, 379.944, 377.143, 376.393, 375.572, 375.662, 378.246, 374.561, 373.752, 374.332, 373.778, 372.625, 374.79, 374.395, 372.327, 370.494, 371.356, 370.148, 370.33, 369.577, 369.434, 366.938, 367.537, 367.933, 366.422, 365.953, 366.505, 367.752, 366.612, 366.53, 365.656, 363.57, 365.679, 366.314, 361.398, 366.544, 363.498, 366.767, 363.204, 363.915, 364.14, 366.952, 366.032, 364.672, 363.329, 365.311, 367.273, 366.141, 365.49, 365.591, 365.901, 364.438, 367.744, 366.312, 368.824, 365.591, 363.892, 365.309, 369.636, 369.488, 369.3, 368.404, 369.548, 365.863, 365.018, 371.495, 366.975, 371.149, 365.497, 368.422, 365.533, 367.703, 364.257, 365.83, 371.93]}	100	100	True
