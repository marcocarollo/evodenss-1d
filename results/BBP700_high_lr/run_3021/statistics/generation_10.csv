id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31108.53516		9715062	13	-1	362.7038516998291	{'train_loss': [597515.0, 614790.75, 610649.0, 587377.625, 551542.312, 510508.281, 463230.156, 414819.0, 375374.625, 340837.188, 317765.406, 300502.062, 290606.5, 285067.094, 281147.531, 277368.062, 275338.781, 271509.031, 270154.469, 267645.25, 265860.875, 264504.688, 262339.969, 259694.516, 258864.031, 257167.031, 256443.703, 255075.203, 253427.516, 251762.672, 250872.438, 250132.672, 248423.281, 248462.578, 246944.672, 244991.734, 244604.125, 244175.219, 243344.078, 241661.531, 241959.719, 241490.625, 240952.234, 239680.234, 238338.031, 238388.953, 237624.469, 237981.188, 236308.719, 235307.781, 234363.672, 234003.703, 234171.922, 232586.016, 232964.812, 232524.938, 230786.125, 231523.203, 230472.188, 229992.672, 229648.922, 229346.531, 228457.312, 227949.109, 227931.906, 226793.703, 226644.078, 226058.969, 225272.406, 225459.109, 225065.484, 224766.531, 223517.953, 223200.547, 224301.281, 222936.188, 222281.344, 222847.281, 220685.422, 222103.609, 220871.812, 221708.094, 219775.984, 219720.859, 219962.953, 218814.438, 219199.328, 219083.375, 217801.578, 217763.219, 216776.719, 216248.188, 217624.781, 217000.266, 215595.734, 215767.844, 215838.516, 213943.266, 215107.906, 214883.094], 'val_loss': [2439.749, 2423.95, 2407.639, 2180.823, 2019.858, 1866.105, 1792.759, 1690.458, 1580.426, 1538.255, 1454.466, 1394.776, 1360.547, 1335.534, 1319.59, 1315.628, 1305.663, 1292.835, 1282.821, 1269.18, 1267.817, 1251.811, 1241.167, 1237.606, 1220.491, 1228.963, 1214.55, 1205.164, 1204.854, 1189.361, 1198.416, 1193.712, 1189.648, 1180.618, 1173.92, 1171.063, 1170.92, 1162.379, 1159.99, 1157.363, 1160.471, 1159.619, 1150.479, 1142.8, 1143.38, 1142.008, 1142.133, 1139.781, 1140.504, 1135.207, 1128.793, 1125.547, 1124.232, 1130.246, 1119.694, 1114.961, 1120.924, 1113.915, 1104.274, 1116.949, 1110.241, 1111.881, 1111.008, 1107.205, 1109.661, 1099.389, 1101.98, 1101.248, 1095.769, 1092.447, 1092.045, 1090.28, 1082.355, 1090.582, 1086.278, 1082.413, 1088.648, 1079.103, 1089.993, 1079.321, 1083.575, 1079.4, 1077.844, 1090.456, 1079.187, 1084.57, 1079.331, 1076.268, 1076.075, 1076.547, 1081.419, 1079.351, 1078.54, 1068.101, 1076.484, 1063.399, 1082.954, 1072.729, 1072.829, 1069.965]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:97 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:85 kernel_size:7 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:36 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta lr:0.08858187504975472 batch_size:16 epochs:100	100	1000	True	32378.21875		1791481	12	-1	288.0761089324951	{'train_loss': [448953.438, 326533.938, 315248.469, 309377.344, 303942.344, 300319.906, 296483.875, 293710.781, 290653.844, 288496.938, 286089.75, 283294.219, 281486.281, 278838.312, 277116.281, 276099.219, 274587.062, 271532.688, 271743.656, 269383.625, 268146.062, 267011.688, 265292.406, 265916.875, 264372.344, 263377.094, 262466.156, 262115.5, 261533.609, 260501.672, 258630.922, 258512.703, 257867.969, 256852.203, 258449.969, 255656.625, 255138.078, 253633.0, 254710.906, 254137.125, 252215.578, 253382.953, 251686.031, 251510.891, 251144.75, 249829.453, 249685.859, 250580.906, 248487.578, 249453.906, 248564.594, 247907.469, 248796.812, 246553.0, 246695.5, 246629.391, 245718.75, 245888.328, 244927.078, 245835.781, 244990.859, 245127.188, 244005.719, 244251.0, 243598.531, 244297.328, 243449.219, 242950.594, 242269.094, 241626.0, 242132.844, 241158.781, 241576.188, 240806.984, 240509.562, 241214.766, 241746.484, 240225.328, 240366.531, 239187.828, 239508.562, 239049.094, 239953.453, 239496.359, 238124.656, 239788.141, 238817.453, 239055.703, 238304.312, 237481.797, 237517.297, 237394.859, 237816.125, 237346.656, 237251.297, 237374.828, 237769.562, 237099.438, 235244.297, 236602.531], 'val_loss': [1580.18, 1476.506, 1442.066, 1418.057, 1408.121, 1394.19, 1371.559, 1374.688, 1365.32, 1345.128, 1320.634, 1328.956, 1316.239, 1302.686, 1286.173, 1284.148, 1280.08, 1273.043, 1260.229, 1267.046, 1268.713, 1252.644, 1262.611, 1260.361, 1240.778, 1244.261, 1245.161, 1228.027, 1241.221, 1239.555, 1231.661, 1224.889, 1235.001, 1213.973, 1228.283, 1206.408, 1216.803, 1207.68, 1216.531, 1208.879, 1202.502, 1201.634, 1198.576, 1208.386, 1203.31, 1198.761, 1196.096, 1196.409, 1193.84, 1194.564, 1202.855, 1192.54, 1181.267, 1190.116, 1177.087, 1178.425, 1181.463, 1187.853, 1175.337, 1175.73, 1164.825, 1182.345, 1174.777, 1180.541, 1170.844, 1179.334, 1173.998, 1167.643, 1183.439, 1177.863, 1169.259, 1161.398, 1175.164, 1170.398, 1165.534, 1157.821, 1158.972, 1164.111, 1168.091, 1164.772, 1158.215, 1155.841, 1162.915, 1154.295, 1155.744, 1162.203, 1161.224, 1152.821, 1153.898, 1154.846, 1149.546, 1158.556, 1153.046, 1147.934, 1152.78, 1147.216, 1145.82, 1145.068, 1151.33, 1136.781]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:29 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	32456.11523		2648460	12	-1	311.4054934978485	{'train_loss': [493089.375, 464563.469, 444895.75, 426336.938, 399095.625, 349706.344, 322968.062, 314012.469, 307269.0, 302310.312, 295824.812, 291791.781, 288891.031, 285337.969, 283187.938, 280968.125, 279116.969, 276600.375, 275376.062, 273215.562, 272724.375, 271805.969, 269064.062, 268025.375, 266463.062, 265867.719, 264097.875, 263596.781, 262835.875, 262014.125, 260979.141, 260137.281, 259685.359, 259195.656, 258567.266, 258291.484, 257681.719, 255691.578, 256018.719, 255731.203, 254991.766, 254012.5, 254024.422, 252502.984, 251243.422, 252081.094, 251994.234, 251072.328, 250977.734, 249688.672, 248535.188, 250002.141, 249114.359, 247976.266, 247649.312, 247980.594, 246893.812, 246983.656, 246296.906, 245801.328, 246076.766, 245519.219, 244586.172, 244636.297, 243888.672, 244610.656, 243137.094, 243736.281, 242650.656, 242697.453, 242604.094, 241334.469, 242143.953, 240870.953, 241904.234, 241092.25, 240514.625, 240243.547, 239342.859, 239095.828, 238718.0, 238866.641, 237869.297, 238685.609, 237943.172, 238501.453, 237994.516, 237715.875, 237182.844, 237275.297, 237851.047, 237240.922, 236482.281, 236411.562, 235740.766, 236649.516, 236203.734, 236136.406, 234702.656, 235506.5], 'val_loss': [1951.149, 1779.539, 1759.153, 1802.307, 1682.646, 1533.973, 1482.629, 1437.684, 1427.566, 1399.82, 1374.71, 1338.275, 1325.655, 1304.151, 1286.495, 1295.816, 1284.562, 1281.987, 1280.199, 1272.886, 1263.846, 1264.983, 1255.533, 1244.798, 1245.902, 1253.698, 1240.114, 1223.697, 1237.257, 1236.715, 1230.783, 1235.04, 1225.319, 1235.798, 1226.979, 1232.064, 1208.374, 1221.88, 1216.305, 1221.975, 1207.86, 1208.438, 1215.343, 1188.278, 1197.973, 1200.467, 1206.351, 1197.615, 1189.936, 1202.187, 1191.727, 1182.59, 1194.216, 1183.075, 1185.286, 1185.838, 1174.176, 1177.39, 1183.424, 1175.18, 1172.024, 1172.863, 1172.884, 1175.185, 1179.695, 1164.874, 1170.439, 1175.07, 1166.015, 1175.724, 1174.822, 1166.133, 1170.488, 1164.87, 1166.46, 1161.461, 1167.367, 1157.864, 1156.683, 1155.121, 1164.002, 1151.158, 1155.673, 1159.049, 1151.659, 1154.561, 1153.344, 1145.598, 1159.428, 1148.592, 1153.415, 1154.527, 1152.081, 1154.601, 1144.793, 1142.879, 1158.097, 1145.334, 1142.889, 1145.536]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:86 kernel_size:10 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:43 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:91 kernel_size:6 stride:1 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:11 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:12 layer:fc act:selu out_features:200 bias:True input:13 learning:adadelta lr:0.10903913505074114 batch_size:16 epochs:100	100	1000	True	32542.79102		19409709	15	-1	599.1016175746918	{'train_loss': [720265.75, 774451.25, 779482.562, 742733.938, 665478.938, 535601.438, 383379.562, 308654.094, 291455.094, 282597.031, 277036.75, 273078.625, 269882.188, 266214.469, 263686.719, 260862.422, 258849.875, 257060.047, 254612.609, 252978.953, 250988.734, 250123.844, 248191.531, 246941.906, 245592.266, 244644.047, 242584.922, 241775.625, 240608.516, 239247.516, 238609.484, 237166.312, 236256.172, 235581.734, 234368.922, 232681.719, 232601.359, 231032.312, 230658.312, 228912.891, 228381.656, 227190.859, 226230.688, 225665.656, 225131.469, 224184.141, 223000.734, 222893.812, 221591.469, 220174.406, 220935.875, 219012.328, 218563.766, 218218.594, 216860.281, 216026.766, 216164.312, 215669.359, 214309.344, 213278.266, 213681.828, 212004.828, 211881.922, 211212.188, 210344.016, 210458.594, 208499.094, 208645.406, 207971.469, 207777.109, 205925.219, 206432.188, 204985.047, 205332.375, 204596.969, 204308.062, 204032.344, 202541.484, 202206.281, 202153.828, 200601.078, 200821.531, 199766.984, 199536.438, 198615.875, 197969.047, 197324.797, 197123.031, 196510.75, 196284.281, 196103.422, 195278.328, 193498.125, 194208.5, 193429.422, 194283.516, 192820.531, 192320.219, 192231.094, 191236.234], 'val_loss': [3639.718, 3759.435, 3554.71, 3334.798, 2618.34, 2136.766, 1588.774, 1450.014, 1404.98, 1374.673, 1348.958, 1331.276, 1312.001, 1301.682, 1290.034, 1277.957, 1263.048, 1267.098, 1247.078, 1250.748, 1237.887, 1222.016, 1211.679, 1213.715, 1216.049, 1199.514, 1214.186, 1213.357, 1192.701, 1192.098, 1191.948, 1183.758, 1192.178, 1189.784, 1189.576, 1190.219, 1180.924, 1179.796, 1175.714, 1180.745, 1171.599, 1178.755, 1185.054, 1183.914, 1177.001, 1175.784, 1176.619, 1172.069, 1176.915, 1166.059, 1173.87, 1154.861, 1176.159, 1162.296, 1161.496, 1154.855, 1158.878, 1159.85, 1152.762, 1164.297, 1162.54, 1159.927, 1163.208, 1162.732, 1170.644, 1159.997, 1158.136, 1169.658, 1155.158, 1160.04, 1151.199, 1143.881, 1152.278, 1172.983, 1149.67, 1153.247, 1163.765, 1152.492, 1161.467, 1168.764, 1163.213, 1172.949, 1158.21, 1169.759, 1167.16, 1165.717, 1164.043, 1158.244, 1159.961, 1161.981, 1154.01, 1159.252, 1163.241, 1162.282, 1159.683, 1162.635, 1156.615, 1163.421, 1153.658, 1161.881]}	100	100	True
