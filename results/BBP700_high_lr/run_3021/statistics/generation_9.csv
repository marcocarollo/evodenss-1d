id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1262432328377348 batch_size:16 epochs:100	100	1000	True	32422.23047		409230	14	-1	319.1616566181183	{'train_loss': [378405.812, 330995.094, 319306.219, 311536.562, 306033.188, 302600.188, 299741.188, 296744.469, 293672.188, 290957.594, 288242.688, 285640.656, 282811.094, 279937.281, 276970.906, 274451.5, 272189.125, 270192.75, 268145.344, 265714.344, 263693.531, 262768.625, 261548.062, 259754.938, 259047.312, 258021.703, 257112.469, 255326.188, 254091.797, 253819.453, 252691.812, 251226.438, 251276.297, 250577.578, 249472.688, 249235.109, 247429.859, 247305.438, 246213.719, 245784.484, 244264.578, 244172.875, 243637.703, 242968.891, 242651.969, 241600.375, 241640.266, 240673.406, 240142.875, 240029.328, 238994.188, 238927.5, 239145.438, 238778.547, 238509.0, 237845.062, 237238.406, 237413.703, 235971.672, 235315.641, 235414.25, 234494.484, 234127.391, 234079.203, 234173.938, 233342.953, 233335.906, 233380.688, 232691.766, 232055.172, 231458.547, 231546.188, 231869.234, 231499.891, 231360.109, 230862.141, 230696.219, 229543.609, 229249.719, 229168.578, 229460.797, 229125.422, 229116.484, 228453.828, 228904.125, 228254.266, 227426.938, 227485.703, 227283.484, 227457.516, 227413.922, 226591.859, 227292.859, 226113.844, 225619.219, 226432.781, 225881.828, 225583.484, 225734.516, 224024.422], 'val_loss': [1641.424, 1500.661, 1470.742, 1449.187, 1435.754, 1423.894, 1416.731, 1410.056, 1394.888, 1380.95, 1372.764, 1359.301, 1346.898, 1336.874, 1324.112, 1309.136, 1299.596, 1292.6, 1286.088, 1277.342, 1268.391, 1278.638, 1266.112, 1270.796, 1261.166, 1253.191, 1248.875, 1241.887, 1242.651, 1233.055, 1231.617, 1249.698, 1225.905, 1226.008, 1235.315, 1220.462, 1220.628, 1214.712, 1201.31, 1205.775, 1205.114, 1215.305, 1193.55, 1199.624, 1203.469, 1181.237, 1190.768, 1184.982, 1199.084, 1183.296, 1181.358, 1203.24, 1182.844, 1166.921, 1186.052, 1174.674, 1174.473, 1174.534, 1169.224, 1171.272, 1175.79, 1166.922, 1173.623, 1179.379, 1162.553, 1160.266, 1168.358, 1168.591, 1156.817, 1168.851, 1174.566, 1174.01, 1170.299, 1174.379, 1168.6, 1175.487, 1159.927, 1157.778, 1165.054, 1167.303, 1161.036, 1177.137, 1168.773, 1173.207, 1173.877, 1175.917, 1159.063, 1167.762, 1153.448, 1155.302, 1156.206, 1158.475, 1156.527, 1154.233, 1157.57, 1143.571, 1144.795, 1152.391, 1150.566, 1150.194]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:98 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:conv1d out_channels:71 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:98 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.1262432328377348 beta1:0.9175450589544871 beta2:0.806833109236426 weight_decay:4.33111487667686e-05 batch_size:16 epochs:100	100	1000	True	57090.37109		458014	14	-1	308.95502829551697	{'train_loss': [1133735.5, 1130417.75, 1081653.875, 1074199.625, 1018916.625, 908473.0, 953666.125, 957105.75, 849050.188, 715148.375, 724174.312, 622086.312, 533300.688, 532413.562, 543181.875, 496291.75, 519368.594, 493090.5, 468874.594, 466010.812, 492565.906, 445580.25, 449892.469, 461686.062, 457981.125, 453383.625, 413645.344, 414071.906, 424868.156, 409825.719, 409500.719, 428937.188, 422018.75, 421435.906, 422129.219, 425455.625, 405062.156, 424824.469, 449635.281, 410194.312, 419043.0, 410531.312, 412367.656, 412181.5, 416309.25, 415743.25, 423315.125, 413677.25, 422408.156, 439492.094, 411422.906, 408774.469, 416608.312, 417127.938, 414177.531, 413353.031, 426990.719, 417047.844, 397083.156, 404756.0, 412916.75, 413502.281, 421168.0, 431188.969, 422254.938, 417432.562, 427464.531, 442912.688, 402838.188, 414625.812, 414524.812, 403514.5, 414890.5, 423897.031, 418471.531, 416737.125, 408446.188, 412431.031, 425478.688, 405920.188, 410943.562, 421620.406, 405976.625, 407571.125, 411264.438, 416128.156, 438064.781, 419771.625, 430756.656, 421571.156, 415207.406, 421883.938, 406060.969, 421680.219, 412871.094, 411107.281, 424392.25, 441728.062, 421504.688, 412327.469], 'val_loss': [2941269.75, 10585.778, 5270.452, 5285.314, 5027.943, 6648.264, 4961.657, 4535.112, 5276.609, 3909.271, 3936.812, 3270.792, 2600.191, 2253.878, 3120.031, 4566.323, 2817.269, 2323.835, 2695.778, 2202.844, 2920.267, 2062.369, 4564.475, 2548.124, 2112.21, 2916.79, 2146.244, 1760.527, 2242.743, 4748.435, 1846.299, 3125.225, 1804.778, 2694.878, 2051.643, 2752.285, 2545.658, 1785.188, 2322.325, 1793.422, 1757.959, 2917.43, 1801.636, 1902.563, 1785.69, 1837.159, 2019.775, 4093.468, 1976.94, 2117.579, 2199.444, 1862.791, 1902.424, 2646.542, 3124.884, 2450.323, 2060.974, 1999.972, 2300.592, 2308.326, 3201.954, 1782.537, 1950.28, 2319.493, 1919.793, 2267.988, 3513.762, 2066.102, 2167.202, 2327.579, 1756.972, 2012.105, 1893.45, 1984.681, 2649.035, 2618.656, 1787.378, 1827.781, 1980.169, 1786.411, 1779.356, 1985.837, 1764.366, 1901.748, 2118.625, 2236.777, 2602.385, 1987.281, 1758.533, 1935.434, 1847.882, 2404.95, 2437.005, 1876.387, 2115.088, 1899.007, 1858.442, 2847.439, 1803.055, 2102.81]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:90 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:110 kernel_size:2 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.11211841076624371 batch_size:16 epochs:100	100	1000	True	31427.53516		616194	14	-1	488.4054346084595	{'train_loss': [417536.5, 344379.156, 321881.812, 314381.5, 310237.5, 306840.281, 303326.344, 299648.094, 296017.562, 292738.656, 289998.938, 286254.062, 283900.469, 280525.5, 276821.281, 274784.125, 271794.312, 268599.594, 266601.062, 265264.281, 262908.031, 261431.219, 258902.094, 256828.891, 255919.828, 254559.156, 252404.188, 251010.062, 250427.516, 249042.734, 247147.359, 245751.094, 245606.844, 244618.859, 243505.266, 243146.922, 241635.078, 241275.406, 239497.656, 239139.156, 238424.172, 237236.922, 236605.578, 235029.172, 235445.188, 234417.969, 233678.641, 232692.781, 232932.609, 232174.453, 231563.969, 231501.344, 230034.859, 229966.953, 229730.5, 230541.562, 227985.5, 228610.531, 227315.734, 226716.219, 225926.406, 225773.406, 225695.781, 224936.859, 224011.078, 223718.125, 224342.828, 223739.156, 223235.312, 221796.078, 221332.406, 221358.641, 221798.969, 220309.219, 219463.297, 218971.938, 218813.0, 219315.25, 218266.266, 218103.906, 217516.828, 216424.844, 215911.141, 216338.984, 216326.422, 216191.312, 214350.625, 214650.188, 214391.266, 213789.734, 213698.125, 213695.438, 212471.547, 213598.766, 212114.078, 212403.062, 211709.484, 211014.125, 210770.391, 210474.297], 'val_loss': [1760.742, 1526.363, 1477.651, 1454.501, 1443.407, 1424.146, 1417.769, 1400.784, 1378.03, 1357.031, 1348.266, 1342.15, 1319.633, 1302.74, 1283.253, 1283.03, 1277.058, 1269.047, 1273.025, 1246.167, 1252.583, 1230.689, 1229.79, 1213.56, 1221.123, 1208.484, 1212.884, 1231.298, 1191.424, 1192.558, 1185.461, 1187.829, 1183.917, 1162.84, 1184.342, 1160.458, 1154.802, 1161.048, 1148.338, 1161.754, 1158.637, 1149.624, 1150.295, 1145.626, 1152.825, 1139.628, 1133.293, 1147.765, 1136.866, 1141.825, 1147.103, 1160.357, 1140.12, 1127.638, 1153.785, 1140.356, 1125.259, 1142.802, 1128.486, 1145.446, 1124.178, 1140.995, 1127.57, 1126.745, 1129.758, 1121.021, 1118.966, 1119.565, 1124.21, 1108.936, 1118.825, 1125.386, 1120.915, 1119.14, 1114.085, 1122.585, 1119.719, 1113.965, 1129.496, 1117.272, 1106.361, 1106.34, 1100.7, 1102.899, 1095.414, 1100.954, 1115.075, 1101.291, 1099.76, 1101.452, 1111.781, 1102.724, 1115.178, 1095.649, 1101.817, 1101.875, 1108.07, 1099.597, 1100.422, 1103.629]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	30981.57031		9715062	13	-1	365.95795488357544	{'train_loss': [595129.438, 612741.062, 608897.188, 593676.0, 556776.625, 516797.594, 469802.406, 423138.812, 383811.625, 347979.125, 324178.188, 307924.375, 296537.875, 288804.406, 284693.0, 280716.281, 277491.125, 274954.844, 272097.969, 270763.719, 268264.656, 266229.906, 264523.281, 262442.219, 260764.766, 258576.75, 258374.438, 256658.203, 255467.078, 254145.953, 253609.578, 252010.25, 251740.828, 248665.516, 248303.953, 248707.734, 247606.766, 246218.625, 245244.266, 244701.516, 243403.359, 242798.125, 242984.938, 241139.125, 240227.359, 239560.281, 239962.984, 238023.172, 237999.906, 237243.109, 236742.516, 236236.297, 235266.406, 234914.922, 233739.422, 233771.578, 234012.281, 232667.266, 231960.594, 231680.172, 230438.031, 230317.562, 230089.766, 229003.516, 228927.688, 228971.828, 227570.828, 227167.25, 227046.094, 226119.766, 225700.453, 226321.156, 225513.172, 225770.031, 225638.031, 225022.703, 224273.219, 224064.156, 222876.844, 223059.578, 222091.062, 222347.938, 221529.938, 221618.297, 220809.047, 220979.0, 221100.016, 219303.641, 219400.375, 219026.953, 218358.891, 218035.953, 218089.406, 218063.281, 217592.797, 217388.391, 216430.641, 216124.531, 216027.969, 215804.828], 'val_loss': [2463.553, 2300.551, 2348.541, 2200.645, 2007.277, 1883.614, 1799.199, 1686.86, 1595.639, 1506.616, 1451.681, 1411.427, 1379.204, 1358.772, 1337.806, 1317.088, 1318.821, 1304.238, 1295.894, 1288.601, 1282.155, 1267.235, 1256.754, 1248.409, 1242.224, 1240.954, 1246.782, 1234.802, 1224.631, 1223.177, 1224.802, 1212.043, 1210.768, 1209.89, 1201.707, 1210.972, 1197.355, 1190.166, 1184.401, 1188.521, 1191.879, 1170.834, 1178.604, 1174.882, 1180.979, 1163.855, 1162.946, 1160.375, 1159.763, 1160.06, 1147.521, 1157.536, 1154.889, 1145.937, 1151.195, 1150.168, 1148.783, 1146.221, 1149.147, 1136.756, 1141.646, 1136.672, 1140.738, 1129.033, 1140.66, 1137.091, 1135.772, 1131.487, 1127.719, 1127.709, 1146.407, 1119.929, 1126.341, 1125.03, 1124.64, 1117.134, 1119.556, 1112.726, 1118.025, 1109.169, 1113.528, 1108.123, 1112.346, 1118.622, 1110.126, 1099.662, 1105.809, 1101.575, 1109.567, 1095.79, 1108.584, 1103.3, 1101.228, 1097.413, 1092.07, 1087.796, 1097.643, 1090.026, 1090.551, 1094.403]}	100	100	True
