id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:32 epochs:100	100	2000	True	32706.53906		545661	13	-1	216.80106854438782	{'train_loss': [407232.625, 364813.312, 325671.062, 314001.656, 308151.562, 303550.531, 299384.906, 297045.438, 293905.688, 291620.312, 289886.688, 288314.094, 285664.062, 284190.844, 282728.938, 280879.031, 279825.969, 278479.688, 276687.781, 274476.438, 273920.938, 272371.688, 271776.188, 271446.875, 269739.719, 269085.281, 267715.969, 266629.5, 266043.812, 264468.219, 264180.844, 264363.375, 262113.781, 261736.859, 260779.938, 260688.531, 258808.734, 259265.938, 257800.281, 257478.219, 255846.344, 255945.922, 255568.719, 254181.5, 254042.812, 254039.25, 253215.094, 253482.484, 252249.906, 252021.047, 250796.734, 250337.391, 250263.094, 250002.016, 248945.609, 248708.219, 248615.484, 248010.281, 248884.516, 247454.031, 247018.938, 246260.547, 245936.828, 245627.938, 245348.219, 244512.422, 244326.953, 243790.422, 243472.219, 243838.547, 242990.406, 242507.781, 241869.156, 241563.531, 242100.109, 242062.469, 241361.984, 241161.875, 240273.453, 239781.234, 239994.312, 238728.062, 239196.812, 237928.828, 238918.203, 238706.969, 238712.031, 237464.359, 237378.203, 237069.766, 237250.203, 236418.297, 236167.031, 236015.109, 235712.25, 235487.141, 236153.75, 235557.062, 233953.0, 234303.406], 'val_loss': [3578.774, 3059.72, 2925.239, 2940.733, 2869.027, 2833.39, 2825.287, 2820.283, 2780.519, 2820.026, 2796.414, 2742.397, 2805.503, 2769.844, 2751.065, 2765.985, 2743.227, 2678.089, 2723.492, 2691.343, 2687.393, 2638.836, 2627.223, 2611.167, 2607.426, 2596.97, 2613.072, 2585.816, 2592.367, 2597.892, 2557.42, 2563.366, 2558.969, 2543.762, 2520.733, 2510.074, 2525.653, 2505.653, 2497.48, 2481.062, 2496.673, 2497.423, 2479.174, 2475.549, 2474.172, 2461.577, 2474.526, 2466.129, 2456.327, 2437.017, 2440.302, 2443.364, 2440.364, 2407.581, 2427.437, 2422.832, 2418.353, 2432.623, 2444.019, 2406.698, 2383.101, 2394.573, 2379.741, 2368.089, 2378.383, 2369.44, 2363.707, 2364.264, 2378.787, 2385.243, 2366.106, 2346.881, 2360.587, 2355.128, 2367.297, 2336.223, 2336.253, 2331.227, 2327.86, 2324.668, 2342.611, 2337.181, 2335.503, 2333.593, 2323.083, 2327.107, 2327.325, 2332.303, 2302.41, 2311.787, 2304.223, 2327.048, 2325.908, 2316.838, 2319.316, 2314.494, 2304.914, 2299.414, 2304.857, 2289.865]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.14327599707704447 batch_size:32 epochs:100	100	1000	True	32361.94727		545661	13	-1	217.22741985321045	{'train_loss': [402229.906, 335237.156, 318861.062, 310922.125, 306068.875, 302780.031, 299196.062, 296022.156, 293183.719, 289014.656, 285294.125, 281778.5, 279357.688, 278058.125, 275376.094, 274154.531, 272787.406, 271837.031, 270238.469, 269672.344, 268271.625, 266820.531, 266063.125, 265424.0, 264004.719, 263550.031, 262523.969, 261686.812, 261666.859, 259811.0, 259592.031, 258853.375, 258425.656, 257097.469, 257316.312, 256688.25, 255855.188, 255373.719, 254327.172, 253194.188, 253553.844, 252272.031, 251653.5, 250948.891, 250658.5, 250438.25, 249858.297, 249615.047, 247902.516, 248219.219, 247847.062, 247515.703, 246933.391, 246667.688, 246850.312, 245449.578, 245362.438, 244570.781, 244902.5, 244058.406, 243331.672, 243161.203, 243215.672, 242798.891, 241601.594, 241575.359, 241757.891, 240780.312, 240188.75, 240018.062, 241154.109, 239460.766, 239605.172, 238283.859, 238845.516, 238304.344, 238933.672, 237625.25, 237560.0, 236586.188, 237168.406, 235848.547, 236103.078, 235989.859, 235566.203, 235548.984, 235423.234, 234762.422, 234190.172, 233943.234, 234316.469, 233828.938, 232849.484, 233207.719, 233275.859, 232553.469, 232482.969, 232325.141, 231420.266, 230405.016], 'val_loss': [3444.539, 3009.054, 2894.329, 2873.529, 2867.728, 2836.987, 2820.202, 2787.135, 2770.202, 2714.192, 2688.569, 2670.09, 2639.814, 2633.123, 2612.924, 2605.238, 2580.711, 2582.482, 2560.013, 2554.697, 2560.211, 2565.044, 2546.305, 2533.186, 2538.888, 2515.47, 2510.052, 2518.5, 2505.116, 2493.526, 2480.537, 2489.387, 2467.407, 2464.025, 2459.177, 2444.286, 2441.565, 2433.562, 2429.483, 2437.626, 2422.069, 2409.183, 2410.71, 2393.839, 2409.758, 2395.613, 2401.499, 2390.113, 2384.372, 2394.023, 2374.037, 2373.946, 2373.775, 2365.489, 2364.094, 2365.874, 2349.792, 2339.91, 2386.425, 2339.488, 2344.437, 2345.813, 2342.684, 2341.15, 2332.749, 2337.578, 2345.827, 2326.745, 2315.404, 2318.939, 2318.041, 2328.338, 2330.384, 2338.331, 2307.469, 2331.778, 2314.15, 2296.878, 2321.015, 2301.623, 2293.443, 2305.601, 2313.301, 2305.896, 2286.963, 2284.902, 2308.66, 2293.615, 2304.051, 2274.508, 2282.849, 2287.319, 2280.465, 2285.274, 2281.2, 2276.489, 2261.837, 2270.589, 2272.432, 2241.527]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:118 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.10125925970883529 batch_size:32 epochs:100	100	1000	True	33606.82422		621591	13	-1	210.00740218162537	{'train_loss': [383186.75, 334545.438, 318937.656, 311132.938, 307023.531, 303264.25, 299864.5, 295829.344, 292640.75, 289841.781, 288060.344, 285419.094, 283766.531, 282737.5, 281331.344, 279867.094, 279534.688, 278433.781, 277356.781, 276172.344, 275287.625, 275048.094, 273975.375, 273133.312, 271639.688, 270657.156, 270626.344, 269781.719, 268578.781, 268001.438, 266990.75, 266605.062, 266202.5, 265894.875, 265328.75, 264827.5, 264555.25, 263742.406, 263517.562, 262829.125, 262345.938, 262134.219, 261413.562, 261048.844, 261050.234, 260431.141, 259798.781, 259163.25, 259359.031, 259106.938, 257824.875, 257248.094, 256995.062, 256899.438, 256432.812, 256387.906, 255243.938, 255245.078, 254989.266, 254082.109, 254042.719, 253884.766, 253551.562, 252471.281, 252602.25, 252253.609, 251949.406, 251976.016, 251790.422, 250742.328, 250462.234, 250144.875, 250406.406, 250401.266, 250164.469, 249906.547, 249771.047, 248586.656, 248832.734, 248084.406, 248272.938, 248447.203, 248416.688, 247618.891, 247779.125, 246598.516, 247588.547, 246501.062, 246186.625, 246363.672, 246077.906, 245383.719, 245274.25, 245261.453, 245348.625, 244751.188, 245409.484, 245200.578, 244187.344, 244377.891], 'val_loss': [3284.026, 2999.165, 2929.359, 2890.277, 2854.993, 2837.502, 2788.783, 2780.709, 2799.117, 2798.45, 2741.623, 2750.372, 2730.945, 2688.948, 2702.578, 2687.484, 2663.846, 2653.708, 2646.508, 2637.157, 2636.138, 2623.987, 2620.631, 2616.221, 2619.963, 2592.32, 2611.109, 2581.158, 2572.26, 2574.585, 2578.637, 2548.731, 2554.099, 2550.105, 2541.793, 2529.488, 2525.138, 2529.988, 2515.802, 2525.053, 2506.623, 2511.64, 2514.935, 2516.291, 2512.581, 2492.8, 2485.408, 2502.047, 2483.526, 2478.833, 2488.148, 2479.663, 2460.717, 2480.108, 2462.147, 2462.549, 2442.628, 2445.598, 2433.824, 2448.129, 2430.495, 2424.908, 2431.407, 2426.631, 2412.195, 2404.538, 2425.137, 2410.817, 2403.993, 2399.117, 2403.203, 2397.918, 2399.587, 2385.571, 2384.0, 2387.027, 2389.903, 2382.034, 2375.569, 2394.045, 2377.824, 2381.483, 2367.741, 2378.799, 2380.817, 2367.247, 2382.332, 2355.443, 2367.53, 2368.957, 2388.281, 2368.844, 2373.825, 2348.786, 2366.951, 2366.714, 2383.439, 2354.22, 2359.942, 2363.041]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:20 epochs:100	100	1000	True	31891.35156		545661	13	-1	266.62813568115234	{'train_loss': [371680.906, 325547.375, 313925.875, 308260.781, 303413.375, 300450.938, 296989.875, 293899.906, 292178.406, 290408.031, 288469.219, 286457.406, 285165.5, 283357.812, 282278.344, 281177.625, 279924.531, 279063.0, 277333.375, 276593.844, 275125.625, 274776.594, 273950.156, 273015.406, 272079.906, 270913.25, 270794.344, 269888.25, 268796.719, 267818.0, 266436.938, 265160.938, 264881.406, 263278.781, 262286.625, 261257.969, 259853.688, 259416.219, 259073.891, 258333.875, 256806.562, 255825.438, 254838.812, 254107.672, 253238.672, 252892.734, 252696.75, 251466.906, 251166.906, 250045.781, 250290.438, 248794.734, 248679.703, 247861.641, 247543.062, 247560.641, 245842.344, 245736.359, 244752.859, 244550.375, 243506.062, 243550.781, 242856.656, 242682.938, 242380.016, 241191.328, 240985.422, 241123.719, 240376.016, 239966.094, 239216.703, 239051.453, 238862.734, 237504.297, 237605.516, 237938.094, 237699.75, 237290.766, 236655.109, 235700.516, 235731.641, 235253.141, 235415.547, 235343.109, 234397.078, 234543.25, 233326.0, 233803.562, 233279.953, 233188.125, 233056.438, 232067.719, 231593.672, 231372.094, 231883.375, 230431.953, 231519.312, 229837.234, 230126.609, 229795.375], 'val_loss': [1964.782, 1878.883, 1824.629, 1799.289, 1784.394, 1776.937, 1749.967, 1732.839, 1727.334, 1712.759, 1711.195, 1688.845, 1687.803, 1680.195, 1665.484, 1677.482, 1655.904, 1658.346, 1648.141, 1644.84, 1645.626, 1643.162, 1634.974, 1617.744, 1621.372, 1626.138, 1620.996, 1607.035, 1620.647, 1600.43, 1597.936, 1607.14, 1593.125, 1591.238, 1583.451, 1583.307, 1570.144, 1570.522, 1555.668, 1556.925, 1554.568, 1557.671, 1556.236, 1550.735, 1535.46, 1531.554, 1528.883, 1537.42, 1516.23, 1524.547, 1513.713, 1508.521, 1519.763, 1504.461, 1493.874, 1493.374, 1502.652, 1488.978, 1488.677, 1490.341, 1481.873, 1485.416, 1475.856, 1475.246, 1475.552, 1480.812, 1481.744, 1481.799, 1469.882, 1485.101, 1473.592, 1478.205, 1466.664, 1474.025, 1462.868, 1464.657, 1469.811, 1463.776, 1460.587, 1458.985, 1453.091, 1450.352, 1458.972, 1442.943, 1455.823, 1460.895, 1449.196, 1454.957, 1447.031, 1445.043, 1444.489, 1448.654, 1445.844, 1456.526, 1447.179, 1440.447, 1443.827, 1438.133, 1435.714, 1432.436]}	100	100	True
