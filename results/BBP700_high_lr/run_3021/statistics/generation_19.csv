id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31899.39453		9715062	13	-1	382.5007712841034	{'train_loss': [599645.188, 623272.375, 616892.812, 594553.812, 558110.375, 516310.906, 469378.781, 423188.75, 381010.125, 348775.5, 323185.375, 305684.25, 294475.594, 286557.281, 281988.219, 278936.688, 275167.719, 272927.594, 270408.188, 268000.281, 266213.125, 264883.125, 263047.469, 261834.891, 259509.375, 258201.484, 257138.516, 256033.562, 254884.0, 253228.75, 253056.531, 251758.547, 249907.938, 249415.406, 248533.047, 248077.797, 247178.438, 247157.812, 244451.156, 244280.266, 243958.422, 243376.625, 242109.641, 241179.484, 240393.969, 239979.828, 239438.859, 238725.547, 238079.25, 237505.047, 236654.047, 236682.766, 235624.109, 235583.375, 234603.688, 234257.75, 234626.625, 232962.25, 232072.766, 231414.422, 231659.25, 230723.578, 230765.234, 230183.781, 230140.516, 228963.734, 228425.969, 228787.375, 227521.391, 226841.766, 227220.391, 226360.203, 226722.891, 225907.125, 225580.109, 225623.844, 225573.766, 225059.188, 223891.719, 223798.734, 223420.438, 223303.328, 222963.469, 221939.891, 222595.938, 222099.047, 222279.344, 221527.297, 220712.984, 221205.688, 219667.703, 219271.906, 219842.734, 218880.297, 217976.688, 218032.672, 218448.219, 217376.688, 217538.234, 217731.422], 'val_loss': [2453.283, 2302.167, 2319.524, 2184.062, 2106.698, 2024.404, 1855.143, 1752.491, 1631.701, 1510.46, 1453.188, 1421.652, 1373.671, 1351.029, 1333.869, 1316.783, 1304.766, 1291.035, 1287.02, 1277.664, 1271.574, 1256.497, 1259.481, 1252.337, 1239.79, 1241.6, 1239.717, 1218.9, 1229.426, 1225.203, 1227.43, 1211.455, 1210.661, 1214.187, 1201.339, 1207.736, 1203.661, 1195.185, 1192.417, 1196.723, 1182.643, 1186.452, 1177.345, 1181.775, 1179.144, 1178.803, 1179.607, 1177.29, 1181.228, 1171.731, 1168.03, 1162.551, 1169.573, 1169.711, 1163.61, 1167.732, 1163.32, 1152.664, 1156.407, 1146.99, 1152.41, 1150.014, 1160.132, 1147.369, 1148.405, 1146.445, 1142.665, 1144.047, 1140.228, 1146.322, 1146.426, 1152.86, 1141.796, 1142.407, 1138.264, 1142.101, 1135.189, 1126.573, 1139.1, 1137.652, 1132.095, 1129.219, 1134.898, 1132.706, 1133.354, 1131.38, 1132.692, 1130.963, 1131.261, 1132.317, 1124.042, 1128.033, 1124.109, 1120.642, 1128.907, 1120.125, 1126.819, 1130.536, 1124.128, 1118.981]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:119 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:127 kernel_size:9 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.09023570113171074 batch_size:5 epochs:100	100	1000	True	32823.84375		2883033	14	-1	746.0657062530518	{'train_loss': [526735.062, 485137.438, 438685.062, 394493.5, 358017.094, 330221.875, 311801.062, 300271.219, 291215.594, 284949.25, 280093.594, 277342.75, 274267.625, 272239.219, 270419.75, 267215.094, 266803.25, 265335.281, 263704.875, 262256.562, 262568.75, 259767.938, 258649.344, 257928.656, 257102.406, 256826.859, 254539.859, 253597.422, 253455.203, 252898.094, 251067.812, 250727.156, 250121.359, 249687.828, 249458.359, 248371.578, 246933.266, 247324.016, 246552.203, 245844.094, 245819.0, 244965.797, 244743.859, 244440.031, 243403.656, 243077.344, 242950.641, 241581.734, 242023.453, 241460.969, 241383.781, 241311.422, 240921.125, 240044.266, 239788.797, 239949.578, 239060.375, 239059.984, 238423.531, 237949.969, 236991.062, 237808.125, 236184.391, 236658.234, 235956.141, 234906.031, 234961.266, 235706.531, 235761.391, 235401.906, 233482.797, 233820.078, 233904.922, 233053.859, 233054.391, 232839.906, 232781.594, 231793.516, 231321.469, 232369.062, 231246.531, 231946.797, 230862.531, 230718.094, 230782.141, 230087.422, 230463.016, 230555.312, 229410.844, 229225.0, 229289.141, 229009.734, 228627.281, 229237.969, 228181.438, 228015.688, 228050.328, 228597.25, 226960.109, 226527.406], 'val_loss': [683.29, 642.714, 607.194, 541.316, 499.976, 468.563, 449.793, 430.806, 422.4, 415.7, 413.051, 412.95, 411.056, 406.474, 401.625, 403.298, 398.374, 397.6, 394.844, 392.776, 392.001, 389.562, 392.717, 390.111, 387.521, 386.743, 385.59, 384.732, 383.054, 384.846, 382.244, 382.534, 383.179, 379.401, 379.953, 380.198, 380.804, 381.323, 381.427, 377.412, 382.217, 381.188, 379.41, 376.549, 380.435, 378.536, 380.166, 377.248, 378.154, 376.236, 376.716, 375.445, 374.307, 374.805, 375.509, 372.872, 374.789, 374.215, 372.221, 373.067, 374.019, 373.117, 372.652, 374.697, 370.527, 368.818, 372.079, 371.95, 369.543, 370.062, 370.399, 366.55, 368.868, 368.093, 368.452, 368.942, 370.049, 370.665, 368.142, 369.099, 368.815, 368.416, 367.287, 369.876, 365.568, 367.463, 367.626, 366.113, 364.603, 369.342, 366.161, 365.208, 365.506, 364.67, 365.351, 367.074, 366.96, 366.702, 365.234, 365.264]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:64 kernel_size:7 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adam lr:0.09023570113171074 beta1:0.9268459640934508 beta2:0.8057758917993553 weight_decay:0.0009061470578319065 batch_size:16 epochs:100	100	1000	True	137383.25		4988694	14	-1	344.8252742290497	{'train_loss': [3015087.75, 16587417.0, 27899142.0, 15322447.0, 14694012.0, 2143671.75, 1600675.0, 1748498.875, 1502966.75, 1578666.25, 1528044.0, 1548565.75, 1341685.0, 1376616.125, 1370922.875, 1430816.0, 1363150.5, 1410455.375, 1482569.125, 1420474.625, 1282708.125, 1433677.75, 1427916.125, 1427688.625, 1555261.5, 1433981.25, 1337910.875, 1344512.125, 1344410.625, 1329719.25, 1274511.125, 1342765.5, 1364148.625, 1306207.125, 1281345.375, 1345237.125, 1379514.625, 1326637.375, 1245715.5, 1245734.75, 1278859.5, 1258132.0, 1354543.5, 1243353.625, 1283676.125, 1223707.25, 1292655.5, 1414930.25, 1245868.625, 1241415.5, 1411430.625, 1331978.0, 1309768.75, 1223215.25, 1267225.5, 1167743.625, 1246563.375, 1224935.5, 1308549.25, 1198586.875, 1247596.0, 1246023.75, 1218441.75, 1189754.875, 1222452.625, 1322769.5, 1245257.0, 1302538.75, 1285290.375, 1220589.125, 1287542.75, 1213969.625, 1278600.375, 1313423.0, 1282020.5, 1229654.25, 1208811.125, 1188305.875, 1290079.625, 1197653.5, 1190589.625, 1295172.625, 1248895.625, 1191892.375, 1286965.625, 1204126.125, 1288778.125, 1382740.0, 1282112.125, 1166894.875, 1183776.25, 1274596.875, 1278949.125, 1356311.75, 1265649.625, 1262093.625, 1230564.375, 1223058.125, 1221895.5, 1177842.5], 'val_loss': [5293.608, 8405025.0, 710457.688, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:114 kernel_size:7 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31962.50391		9815398	13	-1	371.56699204444885	{'train_loss': [608950.625, 629229.562, 624938.188, 598506.5, 557893.688, 509585.562, 453859.5, 406034.031, 362023.812, 328769.156, 307659.844, 295611.344, 287910.312, 282410.344, 278264.094, 276238.812, 273536.406, 271399.469, 269706.344, 267818.281, 266427.844, 264031.062, 263042.875, 261067.781, 259833.422, 258601.938, 257000.844, 256187.234, 254772.031, 253611.453, 253360.625, 252224.516, 251720.641, 250387.25, 249110.719, 248132.812, 247137.094, 246309.172, 245278.578, 245105.281, 244711.922, 244089.453, 243612.953, 242600.375, 241691.672, 241424.297, 240180.875, 239555.328, 239415.703, 239022.766, 239144.438, 237630.516, 237067.469, 236202.594, 236293.094, 236242.906, 234624.516, 234611.812, 233954.828, 233848.578, 233487.641, 233067.219, 233014.703, 231499.812, 231189.969, 230920.594, 231672.516, 230381.328, 229399.422, 229892.391, 227872.031, 228544.594, 228926.422, 227663.406, 227613.438, 226742.219, 226762.688, 225499.641, 226518.906, 226162.891, 225714.859, 224936.0, 224345.328, 224367.812, 224805.297, 223892.328, 223810.391, 223196.266, 222571.234, 222408.984, 222423.438, 221500.609, 221710.922, 221427.781, 220995.625, 220585.359, 220596.75, 220772.875, 219463.734, 220357.453], 'val_loss': [2337.274, 2516.356, 2362.379, 2173.473, 2127.856, 1923.653, 1774.033, 1647.841, 1588.683, 1499.67, 1447.871, 1378.334, 1352.848, 1328.572, 1310.278, 1303.153, 1289.601, 1288.827, 1261.18, 1270.758, 1265.904, 1255.003, 1242.594, 1235.583, 1220.375, 1222.114, 1221.852, 1216.351, 1201.104, 1200.563, 1192.13, 1192.889, 1198.962, 1199.027, 1198.381, 1184.287, 1187.518, 1172.154, 1189.587, 1186.31, 1182.529, 1188.178, 1183.733, 1176.164, 1167.221, 1172.706, 1163.125, 1159.973, 1165.912, 1161.424, 1162.089, 1166.287, 1170.352, 1161.248, 1152.658, 1150.276, 1168.391, 1158.845, 1153.487, 1153.323, 1146.372, 1152.987, 1151.158, 1144.724, 1145.868, 1146.557, 1142.467, 1147.605, 1152.278, 1132.175, 1145.653, 1145.178, 1142.258, 1141.764, 1129.913, 1126.028, 1132.659, 1145.966, 1135.333, 1131.676, 1126.671, 1131.022, 1141.198, 1124.538, 1124.316, 1124.985, 1122.8, 1133.848, 1128.86, 1111.77, 1122.394, 1121.234, 1115.939, 1114.6, 1111.598, 1114.567, 1109.077, 1109.007, 1121.369, 1107.868]}	100	100	True
