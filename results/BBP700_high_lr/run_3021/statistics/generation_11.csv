id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31679.64844		9715062	13	-1	364.3686988353729	{'train_loss': [593330.688, 611430.812, 607012.312, 584841.75, 551026.625, 509884.281, 458696.125, 413071.469, 371016.781, 338655.844, 314418.469, 299071.094, 290185.031, 284797.406, 280929.438, 277405.469, 275495.75, 273086.656, 271281.719, 269369.438, 267933.219, 264913.156, 263653.5, 262863.188, 260507.312, 259690.609, 257556.969, 256825.172, 255143.719, 253998.531, 253470.453, 252087.312, 250694.641, 249412.609, 248279.516, 247484.016, 246085.453, 246727.125, 245484.453, 244029.406, 243082.156, 242645.0, 241899.172, 240757.594, 240866.594, 240205.516, 239826.516, 238253.391, 237713.547, 237019.828, 237519.172, 237258.016, 235351.812, 234564.75, 234889.047, 233797.016, 233824.094, 232130.125, 231851.125, 231190.875, 230851.922, 230946.172, 230849.688, 230227.516, 229290.562, 228152.953, 227251.922, 228233.266, 227175.562, 226472.281, 226294.609, 226107.266, 225650.172, 224722.312, 223781.391, 224971.531, 223409.484, 223169.516, 222943.781, 222604.078, 222102.391, 221538.609, 221901.156, 221573.656, 221376.484, 220891.719, 219919.672, 219958.234, 219026.656, 219073.062, 219258.234, 218219.266, 217786.562, 218207.453, 217130.406, 217548.0, 216473.875, 217024.047, 216082.203, 214945.312], 'val_loss': [2328.862, 2482.403, 2476.857, 2071.784, 2047.326, 1883.467, 1773.959, 1696.773, 1535.263, 1510.935, 1455.059, 1429.841, 1388.862, 1354.716, 1328.998, 1313.855, 1308.902, 1306.583, 1286.136, 1286.628, 1264.993, 1268.822, 1247.661, 1261.011, 1248.533, 1242.043, 1235.169, 1235.308, 1221.931, 1223.155, 1223.737, 1228.435, 1208.315, 1215.981, 1202.73, 1201.843, 1201.9, 1197.928, 1193.154, 1197.173, 1178.891, 1191.199, 1188.518, 1182.932, 1187.195, 1176.393, 1176.084, 1176.878, 1180.578, 1180.622, 1154.316, 1148.229, 1162.197, 1153.742, 1146.516, 1148.771, 1146.842, 1145.794, 1140.282, 1151.598, 1153.938, 1146.39, 1144.426, 1135.329, 1143.727, 1139.357, 1142.043, 1137.503, 1144.658, 1143.592, 1135.301, 1138.072, 1133.425, 1129.302, 1123.664, 1138.477, 1130.033, 1125.61, 1129.46, 1120.325, 1118.621, 1119.445, 1120.253, 1122.649, 1115.271, 1120.288, 1117.986, 1124.256, 1120.219, 1118.016, 1119.93, 1125.561, 1113.089, 1114.057, 1114.662, 1105.784, 1111.566, 1102.215, 1104.927, 1111.444]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:59 kernel_size:10 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:4 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:4 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:29 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:29 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1260646679515705 batch_size:16 epochs:100	100	1000	True	38232.11719		74817205	14	-1	951.8956649303436	{'train_loss': [954661.25, 1000504.812, 1023470.0, 1027476.188, 1022861.125, 1005310.312, 961672.938, 794784.688, 347266.812, 296625.438, 287679.531, 281686.906, 276716.531, 272645.594, 268404.812, 265955.969, 262510.25, 260045.953, 257524.266, 254495.531, 252437.844, 250067.594, 248133.078, 245954.938, 243924.906, 241289.547, 240568.531, 238217.344, 236998.078, 234003.812, 232698.031, 230535.844, 227643.906, 226639.406, 224780.188, 223620.719, 221352.672, 220351.5, 217656.406, 216437.203, 215844.609, 213664.438, 212290.688, 211313.766, 209564.203, 207902.859, 206366.641, 204562.844, 203053.531, 202164.781, 200922.031, 201039.672, 199444.969, 198821.516, 196889.031, 196244.062, 195265.078, 192515.188, 192754.281, 191649.625, 189535.219, 191044.203, 189190.594, 188185.906, 187477.234, 186606.969, 186087.609, 185049.109, 185351.094, 184295.859, 182591.984, 183883.906, 183591.766, 180183.078, 180614.484, 180548.922, 179841.953, 176895.688, 178390.062, 177106.25, 175890.5, 175371.688, 175495.469, 174584.516, 174056.938, 174028.391, 173953.516, 172619.203, 172303.625, 171186.312, 171174.25, 169976.703, 168793.0, 168757.656, 169351.188, 169151.859, 168044.656, 166482.734, 167544.344, 166676.5], 'val_loss': [4942.427, 5064.996, 5080.163, 5009.847, 4938.867, 4726.307, 4100.008, 2156.384, 1458.677, 1468.007, 1449.3, 1419.755, 1428.485, 1404.404, 1389.95, 1373.246, 1347.762, 1325.359, 1300.67, 1320.035, 1305.631, 1320.928, 1311.611, 1330.074, 1300.422, 1235.0, 1281.122, 1258.502, 1253.953, 1253.416, 1326.779, 1263.348, 1345.044, 1280.179, 1271.197, 1287.752, 1310.799, 1262.14, 1262.82, 1290.287, 1278.577, 1313.037, 1320.18, 1297.151, 1314.363, 1294.225, 1312.735, 1287.815, 1317.502, 1306.79, 1314.658, 1303.186, 1363.334, 1359.991, 1360.687, 1330.5, 1333.027, 1351.107, 1354.42, 1292.642, 1337.916, 1394.709, 1351.861, 1340.788, 1309.031, 1341.515, 1389.19, 1342.347, 1352.913, 1346.611, 1395.038, 1390.444, 1407.903, 1374.795, 1396.743, 1364.508, 1379.988, 1406.225, 1385.349, 1389.104, 1376.118, 1391.401, 1398.417, 1390.392, 1386.955, 1406.039, 1392.598, 1417.732, 1378.235, 1416.426, 1463.681, 1429.101, 1345.331, 1412.876, 1401.543, 1436.597, 1430.438, 1419.579, 1452.547, 1420.716]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.09023570113171074 alpha:0.9555946251427133 weight_decay:5.365392178709508e-05 batch_size:16 epochs:100	100	1000	True	137377.53125		9562918	12	-1	331.52650690078735	{'train_loss': [1185184.0, 1066921.25, 11243455.0, 1067545.5, 6651057.5, 1066921.25, 1461223.5, 3923478.25, 1066921.25, 6414312.0, 1066921.25, 3138157.0, 6167713.5, 1205204.25, 4131565.25, 3384704.25, 1124330.5, 4974211.5, 1072847.0, 3195088.25, 1879219.875, 1305283.75, 7185183.0, 1162886.875, 3459746.0, 3850778.0, 1234358.875, 2319034.25, 2782328.0, 1948526.375, 1943867.0, 1832552.75, 2781626.75, 1844612.125, 1654404.25, 2325751.0, 1506646.75, 1600266.5, 2080746.75, 1450034.5, 1666649.375, 1998378.0, 1919956.875, 1231259.5, 1525329.125, 1630488.375, 1405902.375, 1603417.625, 1952131.5, 1559206.125, 1891214.625, 1783163.875, 1511414.375, 1808692.125, 1411217.0, 1844450.875, 2001726.875, 1266464.5, 1518915.625, 1520432.875, 1557373.125, 1369009.875, 1423556.375, 1749958.125, 1573470.125, 1295532.25, 1705805.875, 1621542.375, 1526709.75, 1582557.125, 1541724.125, 1533661.125, 1598752.125, 1409597.125, 1515236.625, 1647553.0, 1410192.0, 1806204.125, 1884681.625, 1724903.25, 1356932.625, 1464899.5, 1951544.875, 1594072.75, 1460453.25, 1498694.5, 1891640.25, 1476791.625, 1639096.5, 1398512.375, 1849328.75, 1495634.625, 1554159.5, 1449527.25, 1364467.125, 1814585.375, 1440918.0, 1466268.0, 1625005.625, 1880562.625], 'val_loss': [5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 493013.156, 5293.608, 936921.438, 435719.906, 387696.688, 27110.428, 72478.227, 27858.016, 5293.608, 5293.608, 1250661.875, 5293.608, 1606220.625, 5293.608, 878378.875, 1582625.75, 1761190.25, 1414931.375, 2004850.0, 1459643.625, 2167381.0, 1635635.25, 128959.539, 1268202.5, 3787816.25, 227389.203, 5293.608, 481571.0, 211960.203, 227315.016, 507870.188, 177661.938, 18799.672, 9406.054, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 319888.094, 46128.797, 8723.164, 6044.971, 28250.137, 5666.648, 5293.608, 31860.709, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 11260.718, 53179.617, 32725.256, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:46 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:45 kernel_size:4 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.09023570113171074 batch_size:11 epochs:100	100	1000	True	32278.14258		19037984	14	-1	555.4496054649353	{'train_loss': [725156.5, 771665.125, 744183.5, 677512.438, 569174.812, 437012.312, 342704.406, 307748.188, 296374.188, 289757.156, 285181.594, 281753.969, 278190.469, 275512.094, 273192.562, 271030.594, 269209.938, 266905.312, 266214.312, 263485.719, 262803.562, 261523.656, 260851.297, 258065.266, 257589.047, 256435.516, 255388.469, 254090.609, 253607.0, 252404.891, 251785.062, 251060.703, 249890.094, 248908.766, 247498.25, 248256.078, 246463.812, 245938.953, 245569.547, 244234.969, 243784.672, 243413.656, 243666.891, 242429.078, 241427.516, 241023.328, 240926.484, 240079.578, 238831.641, 238139.281, 237969.406, 238238.688, 236901.688, 237790.781, 236525.219, 234555.922, 235333.312, 233561.672, 234096.516, 233533.109, 232771.828, 232450.375, 232445.938, 231210.094, 231591.109, 231237.797, 230485.188, 228979.453, 229506.688, 228950.453, 228729.391, 227942.312, 228964.453, 227689.125, 227676.281, 226831.828, 225277.078, 225915.234, 225417.969, 224761.859, 225065.609, 223567.547, 224161.141, 223946.172, 223058.141, 223066.812, 222530.828, 222335.969, 220477.891, 220769.641, 220642.797, 219271.219, 221556.797, 219558.938, 219475.766, 219297.516, 219302.453, 218107.031, 218537.484, 218195.453], 'val_loss': [2260.525, 2301.124, 2281.398, 1798.88, 1419.532, 1182.697, 1041.224, 988.757, 963.516, 948.702, 935.485, 909.372, 918.27, 903.931, 898.585, 883.282, 880.734, 870.048, 870.67, 871.141, 868.707, 856.91, 847.871, 867.022, 844.383, 847.304, 848.313, 842.779, 845.672, 831.333, 834.089, 830.035, 824.839, 829.159, 819.811, 825.93, 816.866, 816.305, 815.197, 824.423, 816.35, 814.762, 808.121, 813.134, 812.688, 800.616, 796.851, 798.347, 799.322, 796.973, 798.379, 788.592, 790.677, 799.749, 790.044, 806.053, 789.435, 782.825, 780.42, 785.523, 784.282, 776.452, 788.579, 796.902, 790.418, 789.505, 786.929, 791.151, 775.86, 777.538, 786.93, 779.476, 799.117, 773.486, 776.156, 783.569, 776.375, 791.272, 791.841, 783.058, 780.829, 775.319, 779.851, 778.65, 779.335, 779.533, 789.678, 770.448, 791.928, 776.994, 783.219, 781.63, 784.262, 784.612, 784.088, 785.262, 770.806, 779.53, 782.543, 780.497]}	100	100	True
