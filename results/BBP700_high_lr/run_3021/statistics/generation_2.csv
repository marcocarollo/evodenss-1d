id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:32 epochs:100	100	2000	True	31982.2832		545661	13	-1	217.22470116615295	{'train_loss': [398928.094, 372194.094, 336443.875, 318698.625, 311724.938, 307669.219, 304373.344, 301239.719, 299116.812, 296052.969, 293374.906, 290569.188, 287922.562, 285402.844, 283378.719, 281577.844, 279793.719, 278529.812, 276891.344, 275511.688, 273516.281, 272169.406, 271511.5, 268959.75, 267840.188, 266247.125, 265861.5, 264714.312, 262792.781, 261520.578, 261150.828, 260279.203, 259563.531, 258061.672, 257278.656, 257125.844, 255771.766, 254916.219, 255212.781, 253467.266, 253103.109, 252629.734, 251770.656, 250645.078, 251307.547, 250133.484, 249664.844, 249129.609, 248604.766, 248740.656, 246877.109, 246082.359, 245914.922, 245525.141, 245919.203, 244940.219, 244990.125, 244195.109, 243511.547, 244333.031, 243388.422, 242624.359, 242019.625, 241480.016, 241813.094, 240944.359, 240487.391, 240457.594, 240127.703, 239461.328, 240106.203, 239604.141, 239062.5, 237551.578, 237653.062, 237902.922, 237749.156, 236769.969, 237090.422, 236479.688, 236350.266, 235668.297, 235365.703, 235400.781, 235363.656, 234162.5, 233784.922, 234401.828, 233210.656, 234645.312, 233107.188, 232987.547, 233231.109, 231852.141, 232928.422, 232263.219, 231633.578, 231019.562, 230883.328, 231663.188], 'val_loss': [3492.304, 3139.197, 2968.545, 2900.056, 2874.256, 2861.037, 2829.075, 2827.945, 2807.412, 2773.718, 2732.099, 2699.981, 2695.545, 2656.56, 2653.063, 2651.087, 2649.331, 2609.888, 2617.929, 2609.674, 2580.535, 2596.093, 2562.536, 2554.331, 2549.726, 2531.652, 2517.39, 2509.98, 2478.822, 2488.197, 2477.254, 2478.906, 2483.68, 2453.332, 2446.301, 2441.756, 2452.402, 2433.1, 2428.195, 2415.785, 2399.06, 2411.812, 2389.021, 2390.988, 2382.294, 2402.4, 2384.216, 2387.722, 2372.759, 2373.115, 2361.734, 2361.828, 2372.364, 2374.372, 2344.09, 2337.996, 2364.237, 2346.163, 2345.669, 2352.144, 2369.338, 2340.89, 2335.369, 2341.524, 2349.966, 2315.83, 2324.088, 2315.374, 2316.841, 2329.684, 2331.309, 2348.315, 2307.91, 2313.667, 2301.768, 2313.992, 2313.277, 2332.737, 2302.156, 2284.229, 2338.62, 2298.152, 2309.614, 2308.672, 2288.966, 2301.074, 2299.315, 2292.672, 2288.639, 2300.713, 2259.856, 2282.865, 2254.256, 2278.89, 2270.439, 2260.136, 2281.368, 2246.962, 2262.961, 2251.911]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta lr:0.1262432328377348 batch_size:32 epochs:100	100	1000	True	32132.79102		481341	12	-1	226.67082118988037	{'train_loss': [400360.219, 358408.969, 325230.625, 314838.531, 308636.281, 305228.531, 302462.625, 299987.781, 297673.5, 294654.781, 291670.188, 289463.812, 286850.875, 284263.656, 282458.344, 279970.844, 277986.656, 275529.375, 274111.406, 272081.656, 271586.312, 269490.625, 268521.656, 266341.0, 264998.406, 264345.938, 263664.031, 262244.938, 260864.422, 260561.828, 259328.562, 258289.797, 257577.812, 257277.812, 256045.562, 255109.703, 254300.078, 253273.094, 253658.188, 252519.0, 251435.484, 251018.5, 250309.234, 250545.469, 249558.281, 248551.484, 247642.359, 247712.078, 247825.281, 247246.516, 246677.062, 245325.578, 245414.672, 245526.5, 244250.75, 244495.562, 242987.672, 243120.812, 243053.469, 242181.672, 241954.375, 241332.922, 241224.016, 241585.281, 240139.234, 240509.078, 239484.719, 238735.172, 238578.547, 238929.031, 238104.578, 238103.234, 237380.859, 237773.141, 237116.75, 236950.109, 235949.359, 236431.516, 234497.266, 235628.172, 234757.0, 235168.812, 234740.797, 234383.328, 233887.719, 233749.203, 233562.906, 233728.031, 232520.688, 233197.219, 232581.438, 231943.0, 231798.766, 231503.703, 231673.141, 231096.578, 232043.047, 231345.328, 231035.406, 230739.906], 'val_loss': [3485.929, 3049.182, 2963.978, 2901.124, 2873.757, 2854.878, 2828.22, 2823.309, 2794.842, 2774.714, 2838.276, 2748.032, 2744.468, 2727.618, 2682.156, 2675.708, 2652.146, 2635.669, 2653.734, 2621.239, 2605.212, 2600.229, 2599.413, 2586.849, 2540.952, 2539.825, 2537.408, 2527.731, 2568.233, 2492.46, 2500.586, 2496.458, 2484.663, 2470.425, 2487.767, 2467.914, 2441.848, 2433.121, 2428.969, 2442.083, 2424.25, 2421.374, 2422.502, 2401.042, 2409.966, 2400.876, 2389.677, 2389.476, 2385.545, 2383.932, 2371.558, 2384.543, 2376.144, 2361.408, 2348.579, 2361.262, 2354.765, 2368.923, 2341.359, 2347.407, 2364.115, 2350.123, 2365.365, 2338.071, 2352.743, 2331.306, 2322.617, 2347.063, 2341.527, 2319.372, 2326.777, 2317.298, 2321.921, 2330.456, 2305.006, 2321.386, 2331.188, 2313.183, 2310.816, 2299.357, 2298.944, 2323.268, 2308.375, 2300.316, 2306.63, 2310.266, 2288.266, 2310.781, 2314.581, 2289.339, 2290.719, 2291.322, 2292.339, 2281.237, 2284.249, 2293.312, 2286.39, 2284.01, 2277.833, 2255.308]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:19 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:77 kernel_size:3 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:32 epochs:100	100	1000	True	32962.3125		508288	13	-1	225.30043482780457	{'train_loss': [402627.125, 359334.406, 323931.219, 313241.312, 308621.938, 305121.156, 301149.625, 297860.031, 294050.969, 290595.906, 287670.062, 284987.531, 282081.906, 281236.812, 279363.719, 277868.625, 276158.438, 275268.219, 273722.688, 272892.406, 271790.938, 270896.0, 269492.188, 268794.219, 267892.219, 267325.844, 266867.031, 265051.844, 265429.562, 264084.844, 263473.688, 263602.719, 263317.812, 261588.609, 260913.0, 260366.859, 259789.922, 258572.344, 258998.391, 258784.875, 257125.625, 256814.953, 256770.344, 256335.188, 255273.109, 255873.344, 255294.484, 254926.859, 253763.781, 253465.391, 252531.203, 252640.531, 252219.609, 251301.859, 250526.641, 250579.781, 249656.469, 249745.203, 250161.828, 248965.828, 248331.766, 248522.078, 248130.625, 247567.641, 247732.156, 247208.969, 246268.188, 245957.75, 246596.938, 245605.359, 245315.672, 246032.031, 245102.25, 244385.156, 244297.188, 243759.375, 243676.625, 243186.297, 243583.812, 242584.281, 242071.156, 242055.031, 240872.0, 241460.781, 241679.922, 240688.641, 240764.375, 240783.609, 239655.797, 239811.688, 240439.734, 239014.391, 239026.797, 240057.203, 238383.922, 238634.703, 237735.297, 238686.672, 236737.484, 237093.344], 'val_loss': [3544.018, 3348.452, 3003.838, 2971.133, 2886.339, 2870.318, 2837.627, 2815.596, 2793.47, 2782.538, 2767.548, 2742.807, 2698.515, 2682.334, 2665.549, 2649.799, 2662.684, 2632.685, 2621.824, 2609.54, 2597.854, 2579.629, 2581.084, 2563.37, 2572.847, 2550.217, 2537.242, 2527.152, 2531.579, 2523.813, 2533.553, 2531.91, 2540.608, 2537.748, 2502.345, 2506.63, 2492.558, 2493.134, 2498.481, 2483.148, 2481.823, 2476.74, 2486.827, 2479.882, 2472.241, 2480.994, 2484.016, 2484.724, 2481.232, 2452.263, 2470.732, 2462.271, 2449.572, 2450.444, 2444.527, 2457.229, 2445.478, 2451.451, 2450.133, 2434.52, 2450.787, 2438.405, 2438.693, 2430.599, 2432.375, 2438.448, 2435.702, 2417.738, 2414.09, 2414.593, 2414.696, 2408.269, 2417.189, 2404.029, 2402.071, 2407.361, 2397.355, 2383.89, 2391.374, 2383.852, 2390.499, 2377.175, 2378.516, 2387.144, 2368.012, 2373.196, 2362.074, 2364.455, 2371.206, 2359.282, 2363.825, 2362.402, 2352.376, 2353.785, 2360.766, 2332.303, 2347.749, 2346.57, 2346.983, 2336.277]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:107 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1262432328377348 batch_size:32 epochs:100	100	2000	True	31962.83203		545661	13	-1	214.4513063430786	{'train_loss': [399724.188, 356509.75, 320191.656, 311260.656, 307016.969, 303412.25, 300971.188, 298916.406, 296147.031, 294357.062, 291082.406, 288760.969, 287293.062, 284684.938, 283150.75, 280855.531, 279102.688, 277268.438, 276019.469, 274769.844, 273748.75, 272154.75, 270369.594, 268260.25, 268885.406, 267570.562, 265883.688, 264767.625, 264327.75, 263268.75, 261296.516, 260996.297, 260221.469, 259794.75, 258367.469, 258382.75, 256219.219, 255800.469, 255134.609, 253993.266, 253875.875, 252439.172, 251854.578, 251788.781, 250474.172, 249236.406, 249934.312, 248627.312, 248021.922, 247332.469, 246736.516, 247308.797, 245528.062, 245389.188, 245865.344, 245513.641, 243758.234, 244445.625, 243283.422, 243500.172, 242140.688, 242504.344, 241474.016, 240838.25, 240629.891, 240357.312, 239909.484, 239910.016, 238645.0, 238487.156, 237228.578, 237432.547, 237574.781, 237126.625, 236596.266, 237106.5, 236342.125, 235459.484, 235791.875, 235276.109, 234488.453, 234708.266, 234255.219, 234284.359, 234273.719, 234019.234, 232987.891, 233502.219, 232117.453, 231856.828, 231862.625, 231474.578, 231644.906, 230707.172, 230528.891, 230460.344, 229702.469, 229765.953, 230050.062, 229512.391], 'val_loss': [3536.785, 2983.721, 2912.229, 2867.738, 2851.978, 2835.534, 2806.695, 2798.717, 2834.545, 2781.833, 2755.012, 2793.522, 2716.308, 2708.45, 2692.05, 2653.25, 2658.553, 2641.28, 2629.593, 2621.882, 2603.017, 2587.347, 2568.346, 2562.102, 2540.882, 2546.713, 2534.532, 2517.996, 2523.323, 2498.432, 2485.935, 2487.426, 2485.493, 2467.421, 2455.683, 2467.682, 2432.747, 2440.686, 2438.079, 2429.351, 2429.096, 2414.829, 2408.444, 2413.169, 2409.421, 2416.158, 2396.948, 2393.644, 2387.801, 2381.201, 2398.896, 2384.378, 2371.713, 2362.601, 2365.186, 2360.118, 2377.328, 2357.122, 2339.879, 2343.723, 2360.059, 2331.688, 2349.605, 2326.628, 2341.479, 2324.676, 2324.106, 2320.423, 2320.152, 2304.832, 2295.358, 2284.371, 2297.162, 2286.613, 2275.729, 2293.085, 2277.863, 2293.847, 2285.094, 2284.453, 2279.714, 2260.047, 2267.432, 2263.295, 2269.35, 2267.913, 2262.506, 2252.219, 2249.049, 2253.199, 2256.756, 2240.161, 2247.97, 2241.259, 2247.703, 2232.958, 2232.95, 2241.007, 2246.424, 2229.681]}	0	100	True
