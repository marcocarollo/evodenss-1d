id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31226.33594		9715062	13	-1	371.43916940689087	{'train_loss': [599080.875, 618242.75, 614340.0, 596714.125, 562830.938, 522837.219, 474942.5, 427657.812, 383676.0, 352057.781, 326303.594, 308338.062, 295865.062, 288137.719, 283408.219, 280095.344, 276752.219, 274415.0, 272456.844, 270290.812, 269350.281, 267075.469, 265407.219, 263588.188, 262539.75, 260898.656, 259621.578, 257947.188, 256647.281, 255110.141, 254215.312, 253593.75, 252074.344, 251810.078, 249615.875, 248780.609, 248544.766, 247152.0, 246241.578, 245632.719, 244534.281, 243910.906, 242218.5, 241689.281, 241163.469, 241230.797, 240172.188, 239854.984, 239094.531, 238149.125, 237030.969, 236108.281, 235915.688, 235289.172, 235129.156, 234709.375, 233876.094, 233335.266, 232738.453, 232379.016, 231903.812, 231983.219, 231563.141, 230417.156, 229805.422, 228952.266, 230003.062, 229325.781, 227889.328, 227405.109, 227650.016, 227557.266, 226876.484, 226022.016, 226356.234, 224838.25, 225913.969, 224707.906, 223569.531, 223963.297, 222607.328, 222535.891, 222739.844, 222461.344, 221685.625, 221980.375, 221465.203, 220518.281, 219785.734, 220447.359, 220121.719, 219395.094, 219220.328, 218558.844, 218547.859, 218388.062, 218200.047, 217416.641, 217324.734, 215738.266], 'val_loss': [2419.112, 2500.569, 2457.869, 2268.404, 2241.055, 2081.875, 1793.646, 1681.46, 1588.731, 1522.162, 1454.181, 1422.333, 1389.531, 1358.535, 1336.95, 1310.382, 1304.403, 1285.724, 1293.044, 1276.412, 1285.601, 1269.894, 1268.818, 1264.002, 1249.111, 1249.641, 1239.709, 1247.154, 1226.944, 1220.785, 1236.321, 1214.345, 1206.22, 1204.612, 1206.168, 1205.311, 1197.342, 1188.415, 1192.207, 1184.375, 1178.348, 1181.497, 1179.697, 1166.936, 1175.396, 1158.947, 1160.704, 1163.593, 1159.14, 1157.569, 1148.403, 1156.906, 1143.594, 1141.642, 1147.207, 1137.513, 1139.869, 1138.277, 1137.377, 1127.591, 1131.28, 1114.921, 1126.635, 1117.379, 1114.808, 1114.896, 1108.029, 1112.296, 1118.484, 1108.26, 1111.941, 1100.358, 1098.625, 1100.07, 1099.141, 1106.809, 1093.215, 1102.892, 1103.146, 1102.524, 1094.833, 1089.945, 1091.163, 1093.027, 1090.602, 1096.387, 1102.739, 1089.872, 1084.162, 1083.696, 1084.668, 1087.123, 1079.491, 1083.766, 1080.178, 1094.25, 1083.454, 1090.016, 1082.644, 1076.822]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:79 kernel_size:4 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:99 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:10 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.09023570113171074 batch_size:9 epochs:100	100	1000	True	32563.90234		9820470	14	-1	513.69056391716	{'train_loss': [635203.562, 653076.438, 620008.438, 561171.875, 479882.438, 403108.812, 344854.281, 307680.719, 293334.75, 286610.344, 282514.281, 278145.594, 274966.812, 272131.031, 269566.594, 266864.656, 264828.688, 263211.125, 261828.516, 260093.266, 258941.406, 256857.844, 256055.625, 254931.016, 252602.5, 251951.688, 250657.672, 249339.375, 247789.953, 246453.0, 246086.906, 245367.516, 244455.922, 243014.109, 243028.203, 241695.156, 239953.844, 239213.891, 239603.734, 238038.891, 238158.141, 237109.875, 235820.406, 235729.844, 234816.781, 234682.891, 233235.688, 233075.969, 232529.219, 231082.078, 230880.547, 231409.812, 230662.891, 229261.891, 228419.094, 228219.578, 227219.781, 227097.531, 227454.438, 226501.281, 225498.609, 224675.109, 225074.25, 223929.062, 223856.656, 223850.266, 223580.406, 223009.234, 221928.922, 221642.672, 221343.859, 220676.812, 219970.391, 220865.375, 220028.078, 219343.062, 218792.219, 218383.562, 218351.891, 218041.156, 217991.719, 215834.453, 217139.609, 216947.578, 216721.141, 215515.375, 215465.266, 215978.219, 214847.344, 214708.516, 215218.703, 214342.641, 213890.344, 213835.531, 214528.094, 213388.172, 213328.719, 213059.641, 211896.016, 213194.516], 'val_loss': [1477.02, 1548.816, 1281.582, 1177.955, 1033.324, 947.173, 818.477, 778.843, 758.596, 757.9, 746.514, 731.065, 719.858, 716.317, 705.044, 711.014, 701.816, 701.761, 692.653, 688.777, 691.171, 683.715, 685.087, 684.579, 688.466, 677.512, 676.751, 676.956, 669.849, 666.759, 673.739, 667.475, 666.995, 670.651, 661.994, 663.558, 654.676, 656.961, 655.675, 658.194, 651.562, 652.913, 661.492, 653.523, 649.686, 651.08, 654.201, 650.912, 648.094, 639.66, 646.181, 644.363, 645.846, 638.427, 642.923, 633.806, 639.135, 641.974, 638.237, 632.104, 636.746, 639.387, 634.704, 639.394, 640.672, 635.719, 635.493, 637.003, 639.732, 639.352, 631.962, 631.704, 635.2, 638.38, 630.605, 632.501, 634.203, 636.108, 626.042, 628.965, 632.508, 634.68, 624.603, 628.858, 626.692, 634.467, 634.127, 629.953, 627.901, 628.865, 633.694, 630.038, 633.172, 623.94, 629.07, 624.975, 621.343, 628.745, 630.98, 627.634]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:112 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.09023570113171074 alpha:0.8461095232899474 weight_decay:0.0004905708926631085 batch_size:16 epochs:100	100	1000	True	193006.95312		4861030	12	-1	305.1826047897339	{'train_loss': [1745329.25, 1188607.75, 2040833.5, 3170738.5, 1813574.75, 2143131.25, 1702650.25, 2145576.0, 2384267.25, 2158989.75, 2063666.5, 2154542.25, 2319159.0, 2105479.0, 1706424.5, 1931442.125, 1964991.5, 2138659.5, 1999562.25, 1916362.25, 2140937.75, 1850555.625, 2271477.25, 2017841.625, 1983355.625, 1761354.75, 2057132.875, 1923483.0, 1897491.375, 2168190.0, 1995727.375, 2116026.5, 1785964.875, 2053583.125, 2093473.75, 1855000.0, 1873409.625, 2136240.5, 1891378.0, 1856488.375, 2058206.5, 1839614.0, 2051446.125, 1871945.25, 2459149.5, 1887002.5, 1981939.5, 1879948.375, 1763339.0, 2282489.75, 1826871.75, 1932158.625, 1910577.125, 1904376.625, 1954837.375, 1825407.25, 2632943.5, 1712070.0, 1824352.0, 1903991.25, 1834124.875, 1907959.875, 2069241.0, 1936858.75, 1840304.125, 1949427.0, 2004730.875, 2089677.625, 1883591.75, 1931021.25, 2105488.0, 2004021.5, 1897884.125, 1899274.75, 1903151.75, 2178027.5, 1764373.875, 1863224.5, 1856847.5, 1953878.375, 1992483.5, 1926312.625, 2028361.25, 2058222.75, 1750822.25, 2134986.5, 1827088.0, 1877995.0, 2035746.875, 1887396.375, 1869223.25, 1983645.0, 1946671.75, 2167848.25, 2167994.5, 2147159.25, 1976003.0, 2064801.75, 2514500.5, 1897865.875], 'val_loss': [6742.826, 5293.608, 5293.608, 5293.608, 6955.757, 5293.608, 5293.608, 5293.608, 5293.608, 5540.755, 5988.656, 5293.608, 5293.608, 5293.608, 6150.147, 5293.608, 5293.343, 6207.619, 5293.585, 10676.782, 5293.608, 5293.608, 5293.608, 5293.608, 5293.573, 5293.608, 6316.021, 5434.092, 5293.608, 5292.308, 7559.829, 5293.608, 9612.339, 12857.987, 5293.608, 5357.018, 15026.683, 5291.274, 5293.608, 8826.276, 5620.552, 7578.863, 6935.001, 5526.812, 7389.704, 8448.872, 5293.608, 7633.21, 5293.608, 5293.608, 8153.391, 5563.275, 5293.608, 5293.607, 8601.229, 6755.131, 7395.412, 5293.608, 5293.608, 13661.161, 5372.2, 6127.331, 5269.39, 6086.252, 26000.406, 5448.686, 5293.608, 10553.273, 8955.087, 5293.604, 5291.77, 5293.608, 7356.597, 5288.773, 5746.72, 5293.608, 6947.879, 5293.608, 10943.299, 9018.493, 6911.533, 5293.608, 12489.361, 5293.589, 30936.596, 5293.608, 5293.608, 5543.013, 7287.396, 9125.955, 10521.414, 10197.625, 5293.608, 5293.608, 5293.608, 7543.092, 5293.608, 8107.09, 8481.674, 7569.576]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:8 epochs:100	100	1000	True	32318.98438		9715062	13	-1	545.7707986831665	{'train_loss': [635597.25, 655011.25, 620702.5, 558063.312, 480165.938, 405929.562, 339564.219, 309394.5, 297235.969, 290787.281, 285639.094, 282074.469, 277860.25, 274915.875, 273049.25, 270227.312, 268108.938, 266039.125, 264578.031, 262835.031, 260656.016, 258440.578, 258546.75, 256888.047, 255538.812, 254775.484, 252248.016, 252971.203, 251306.219, 250639.953, 249450.734, 249240.562, 248855.969, 247865.688, 246909.281, 245631.656, 244301.391, 243321.078, 243142.688, 242036.422, 242013.547, 241877.125, 240703.031, 239857.156, 239813.547, 238881.266, 237796.734, 238521.875, 237165.281, 235636.594, 235476.422, 235313.484, 234796.562, 234110.094, 233230.312, 233594.422, 233213.75, 232826.469, 232410.875, 231477.047, 230745.891, 230369.969, 230089.562, 231058.141, 228976.75, 229344.422, 228790.672, 227715.859, 226758.078, 227460.188, 226756.516, 226046.719, 225820.891, 225313.141, 226170.594, 224671.344, 224286.578, 223865.156, 222542.297, 222712.031, 222928.516, 223106.906, 222569.516, 222057.969, 221115.953, 220974.312, 219963.047, 220109.938, 220327.766, 219207.656, 218916.812, 218323.172, 218610.641, 217861.344, 217447.781, 217656.812, 217570.875, 216546.469, 216426.516, 215707.828], 'val_loss': [1319.044, 1163.822, 1117.154, 1008.498, 905.663, 802.753, 737.8, 709.308, 692.469, 678.496, 665.123, 665.248, 653.754, 647.138, 644.991, 634.431, 634.552, 624.643, 621.879, 622.379, 618.668, 614.249, 615.461, 610.796, 611.47, 603.568, 606.637, 602.051, 602.81, 601.633, 600.616, 597.41, 594.416, 595.17, 593.478, 597.74, 597.195, 592.857, 591.431, 590.599, 592.585, 591.691, 591.628, 590.752, 589.008, 593.304, 590.4, 587.71, 589.602, 592.788, 592.545, 588.813, 586.149, 585.273, 588.2, 585.036, 591.511, 589.242, 587.947, 584.346, 589.061, 589.132, 585.311, 588.264, 586.665, 580.575, 578.824, 581.465, 581.09, 579.292, 584.155, 580.919, 582.664, 585.736, 583.906, 583.489, 582.088, 582.276, 584.562, 576.221, 584.063, 580.014, 589.825, 571.6, 581.952, 575.317, 573.56, 586.855, 573.509, 579.141, 582.734, 578.672, 577.659, 581.115, 577.617, 576.366, 580.445, 577.716, 579.92, 584.193]}	100	100	True
