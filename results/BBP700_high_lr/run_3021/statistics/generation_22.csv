id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31878.34375		9702186	13	-1	383.29559230804443	{'train_loss': [587252.5, 614932.0, 612030.188, 592684.938, 558074.0, 512743.75, 461949.156, 413475.688, 371671.5, 338624.188, 316539.031, 300151.344, 290843.906, 285190.656, 281088.656, 277509.125, 275927.375, 272949.5, 270731.219, 268099.844, 267092.406, 265442.562, 262640.094, 261477.5, 259222.547, 257843.625, 256023.906, 254800.312, 253498.672, 252789.578, 252503.781, 251162.094, 249457.922, 248215.219, 247831.219, 246044.547, 246436.5, 245258.656, 244953.719, 244436.078, 243349.031, 242519.891, 241330.938, 240380.312, 239761.281, 238886.969, 238033.609, 237606.094, 237588.766, 236897.188, 236888.078, 235069.562, 235710.938, 234798.047, 233761.062, 233703.125, 233183.594, 233197.812, 232281.531, 230680.625, 231466.188, 230722.812, 230553.578, 229079.875, 229476.0, 229082.547, 228609.828, 228079.047, 227662.0, 227427.359, 226712.594, 226221.188, 226213.828, 226338.562, 224576.531, 225617.047, 224950.016, 223789.609, 223879.594, 223385.672, 222462.703, 222808.656, 222716.766, 221891.047, 221923.578, 221796.406, 220626.531, 220625.0, 220954.797, 220007.312, 219721.766, 219446.078, 219763.719, 218759.188, 218398.719, 218530.766, 217194.203, 217836.719, 216913.203, 216434.109], 'val_loss': [2441.986, 2486.558, 2423.317, 2168.523, 1981.127, 1907.032, 1711.426, 1697.826, 1583.358, 1522.941, 1442.59, 1397.004, 1361.254, 1341.778, 1318.597, 1326.645, 1301.558, 1294.191, 1285.735, 1295.516, 1270.146, 1256.077, 1254.708, 1263.73, 1258.188, 1256.979, 1244.563, 1241.856, 1231.774, 1223.071, 1230.26, 1214.1, 1215.164, 1205.388, 1203.189, 1205.683, 1198.073, 1196.666, 1192.221, 1201.253, 1180.725, 1185.507, 1180.896, 1174.072, 1178.48, 1180.078, 1175.856, 1167.754, 1176.755, 1163.942, 1163.737, 1159.593, 1160.718, 1158.892, 1159.271, 1160.849, 1167.287, 1152.983, 1152.42, 1142.801, 1145.234, 1137.259, 1140.155, 1143.668, 1140.154, 1136.095, 1142.752, 1135.795, 1139.378, 1133.153, 1130.307, 1126.907, 1130.354, 1125.351, 1123.647, 1124.065, 1128.138, 1116.821, 1119.684, 1121.067, 1114.352, 1111.217, 1118.633, 1119.538, 1111.592, 1103.708, 1124.824, 1109.435, 1113.388, 1098.819, 1104.927, 1112.942, 1107.445, 1112.974, 1111.258, 1105.847, 1113.789, 1093.3, 1101.147, 1095.686]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:111 kernel_size:8 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta lr:0.1313035062189972 batch_size:16 epochs:100	100	1000	True	33813.63672		3180564	12	-1	348.62692975997925	{'train_loss': [556443.188, 535160.625, 489376.0, 361503.906, 331963.938, 319810.906, 314343.406, 308270.625, 304434.5, 300857.062, 298625.562, 295713.188, 293686.438, 291739.719, 288664.375, 288505.5, 285574.094, 284647.375, 283013.906, 281229.5, 280522.469, 279552.719, 277614.531, 276241.344, 274651.125, 274000.219, 273819.969, 273623.281, 272369.562, 271884.5, 271700.812, 270370.938, 269762.094, 268299.344, 267630.406, 268181.156, 266184.906, 265882.844, 265644.188, 264209.188, 264485.188, 262492.531, 263598.469, 263337.469, 262288.531, 262165.5, 262274.281, 260577.391, 261022.047, 261682.484, 260349.922, 259290.078, 258974.953, 258828.438, 258353.672, 257973.562, 257247.25, 257021.281, 257477.203, 257026.859, 256072.609, 255953.672, 257083.484, 255826.672, 254055.453, 254982.172, 255358.469, 253618.078, 253408.938, 253944.125, 253818.188, 254157.5, 253683.047, 252602.234, 251737.047, 251403.828, 250962.109, 250837.188, 251029.172, 250201.516, 249406.797, 250010.969, 249463.625, 248718.859, 248672.469, 247857.531, 248208.438, 248086.016, 246717.828, 247531.469, 247214.359, 246679.344, 246105.5, 246097.656, 245509.266, 245658.812, 244059.031, 244521.188, 244661.469, 243980.422], 'val_loss': [2306.765, 2240.413, 1854.388, 1576.072, 1553.962, 1534.143, 1463.858, 1437.073, 1463.061, 1388.061, 1407.151, 1383.448, 1350.688, 1333.558, 1323.586, 1316.991, 1337.49, 1300.185, 1308.388, 1315.26, 1304.673, 1286.224, 1298.328, 1273.75, 1271.181, 1275.474, 1271.87, 1258.227, 1256.31, 1274.341, 1270.883, 1267.314, 1268.297, 1261.209, 1263.306, 1264.735, 1262.154, 1242.093, 1253.341, 1253.865, 1243.076, 1247.438, 1235.499, 1243.446, 1247.749, 1238.834, 1224.414, 1231.4, 1221.22, 1240.642, 1225.182, 1221.517, 1218.775, 1228.931, 1218.205, 1227.632, 1231.027, 1217.884, 1213.131, 1224.22, 1210.936, 1229.492, 1223.509, 1212.365, 1202.219, 1201.926, 1194.166, 1203.393, 1195.425, 1207.117, 1210.208, 1192.635, 1199.484, 1211.757, 1191.298, 1189.259, 1190.874, 1185.374, 1190.815, 1193.898, 1191.722, 1190.514, 1194.072, 1181.686, 1195.507, 1180.941, 1182.303, 1193.485, 1182.039, 1176.729, 1178.14, 1173.521, 1178.192, 1176.91, 1166.937, 1181.788, 1174.851, 1173.386, 1167.445, 1177.589]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:74 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:71 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:True bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.09023570113171074 beta1:0.8877348511836665 beta2:0.8216341174438956 weight_decay:0.0009275603922372945 batch_size:16 epochs:100	100	1000	True	137339.42188		9731629	13	-1	408.3296811580658	{'train_loss': [1709487.0, 2569734.0, 2673378.25, 2055699.375, 1801743.75, 1878682.625, 1424947.25, 1206099.625, 1147941.375, 1130897.375, 1111468.125, 1126923.75, 1147040.0, 1118209.5, 1136505.875, 1110931.5, 1118708.25, 1123843.5, 1138323.875, 1136264.125, 1142182.25, 1112993.375, 1123798.375, 1122691.25, 1126130.5, 1126063.5, 1125147.5, 1139963.75, 1120261.375, 1149719.25, 1159025.5, 1131755.5, 1155162.375, 1126738.125, 1139248.75, 1137846.125, 1135361.375, 1114061.625, 1123416.875, 1119254.0, 1215863.25, 1125454.375, 1134412.75, 1117799.0, 1210886.25, 1197386.75, 1138053.125, 1208788.0, 1118084.375, 1124843.75, 1144378.125, 1113415.125, 1185089.375, 1171123.125, 1167903.625, 1138805.0, 1111125.5, 1109870.0, 1148906.75, 1155070.5, 1131733.25, 1118966.125, 1135310.25, 1112951.875, 1156759.875, 1113622.0, 1144106.5, 1127050.875, 1133204.375, 1122309.375, 1105376.5, 1230968.75, 1114965.0, 1240681.25, 1109513.75, 1132031.125, 1117451.5, 1131593.75, 1118352.375, 1123364.125, 1107134.375, 1165050.0, 1131052.75, 1131745.75, 1188674.0, 1120268.75, 1128864.125, 1112861.875, 1125976.125, 1134317.125, 1266751.25, 1186744.625, 1165010.75, 1126158.25, 1168978.25, 1332850.375, 1146968.5, 1319080.25, 1110273.25, 1391959.625], 'val_loss': [5293.608, 9012.709, 27382.572, 5293.608, 8894.811, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.605, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:102 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.1404837209892806 batch_size:16 epochs:100	100	1000	True	32443.66406		4861856	13	-1	323.5889358520508	{'train_loss': [605553.312, 592952.75, 560971.562, 507937.469, 448898.844, 390122.156, 345260.438, 316805.344, 299948.406, 292376.188, 288106.469, 284207.75, 281930.938, 278461.906, 276166.219, 274674.219, 271733.344, 269618.375, 268173.625, 266111.375, 264505.312, 262484.625, 261449.234, 260934.328, 259007.359, 258008.703, 256802.188, 257279.391, 254857.859, 253685.719, 253467.266, 252484.469, 250904.672, 250367.375, 249550.469, 249513.422, 247939.078, 247457.469, 246843.875, 246277.266, 245760.078, 245641.328, 244583.828, 243175.703, 243916.109, 243150.469, 242946.938, 241703.203, 241200.906, 240488.875, 239905.328, 239086.75, 238397.656, 238819.562, 238418.266, 238045.844, 237576.875, 236592.484, 236180.484, 235666.672, 235421.141, 235339.516, 234843.547, 234481.203, 233221.453, 233249.125, 232599.703, 233276.812, 232305.047, 232405.562, 230985.484, 230659.969, 230704.781, 229785.484, 229375.844, 229734.969, 228745.547, 229679.734, 228331.984, 227854.688, 227991.812, 226972.484, 227634.062, 226471.766, 225874.906, 225666.344, 225707.484, 225727.016, 224807.406, 224770.812, 224454.484, 223099.156, 223833.375, 223700.0, 223146.266, 223231.406, 223552.922, 222946.609, 222428.922, 222571.891], 'val_loss': [2432.169, 2206.931, 1941.121, 1879.075, 1680.999, 1554.976, 1500.167, 1429.008, 1372.522, 1350.266, 1346.776, 1336.568, 1314.821, 1296.168, 1296.28, 1296.419, 1301.572, 1291.397, 1275.154, 1266.333, 1257.272, 1240.202, 1239.602, 1234.958, 1224.447, 1225.495, 1233.527, 1223.972, 1229.854, 1214.589, 1218.052, 1209.826, 1209.856, 1211.006, 1213.758, 1206.369, 1202.24, 1204.571, 1205.619, 1205.151, 1198.529, 1206.423, 1194.806, 1198.234, 1195.234, 1189.78, 1196.942, 1197.598, 1199.587, 1194.325, 1197.19, 1203.266, 1193.066, 1194.398, 1191.553, 1195.672, 1181.132, 1180.206, 1183.529, 1181.159, 1176.933, 1173.616, 1190.203, 1183.169, 1182.233, 1181.293, 1169.551, 1173.185, 1176.292, 1174.731, 1173.173, 1167.128, 1176.099, 1166.168, 1166.521, 1154.458, 1156.457, 1160.361, 1161.176, 1162.395, 1160.033, 1154.509, 1147.138, 1160.127, 1146.432, 1158.414, 1155.185, 1157.487, 1152.018, 1158.259, 1151.277, 1148.231, 1145.175, 1144.174, 1153.018, 1139.892, 1145.81, 1139.599, 1136.236, 1141.678]}	100	100	True
