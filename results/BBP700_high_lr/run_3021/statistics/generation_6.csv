id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1262432328377348 batch_size:11 epochs:100	100	2000	True	33134.44531		666858	14	-1	413.25454545021057	{'train_loss': [387264.094, 330809.906, 315226.094, 305129.062, 297051.594, 292153.875, 288379.656, 285839.781, 283341.312, 280189.406, 278171.5, 275685.375, 274433.594, 272724.938, 270577.688, 269742.688, 266837.906, 265464.969, 265354.781, 263840.781, 262741.75, 261188.047, 261005.625, 259700.922, 258728.219, 257566.562, 256677.875, 255476.719, 254999.578, 253747.844, 253525.719, 252840.922, 251088.109, 250800.062, 250665.047, 248887.438, 249026.094, 247819.609, 247032.359, 246427.719, 246339.031, 245777.031, 245609.25, 244971.594, 244749.984, 243819.203, 244437.406, 243280.5, 243008.812, 242064.516, 242438.734, 241211.047, 241125.594, 240251.062, 240106.219, 239512.859, 239744.234, 238929.422, 238468.984, 238239.984, 237194.234, 237391.609, 236788.062, 237028.625, 236157.891, 236770.609, 234637.797, 235604.672, 234646.672, 235087.188, 234724.703, 232775.141, 233285.0, 232870.562, 231813.125, 231783.359, 231249.188, 232038.703, 231231.297, 231014.406, 230681.922, 229933.406, 229922.047, 228610.391, 228695.0, 228623.953, 228231.719, 228218.469, 227827.469, 226740.922, 226575.797, 226700.641, 226534.297, 226476.516, 225602.438, 224646.641, 225137.891, 225096.406, 223872.672, 224107.688], 'val_loss': [1081.135, 1015.3, 975.684, 948.824, 940.858, 931.664, 923.926, 911.149, 902.519, 890.917, 886.739, 885.18, 876.516, 878.125, 876.508, 864.633, 861.93, 865.987, 859.157, 860.426, 849.443, 856.546, 853.39, 849.565, 847.766, 843.585, 847.9, 840.785, 840.333, 834.481, 837.313, 839.332, 835.954, 836.671, 832.734, 837.501, 835.201, 833.284, 836.037, 839.542, 833.419, 832.48, 828.069, 832.488, 830.604, 827.896, 825.969, 831.973, 823.337, 826.32, 828.293, 828.689, 822.606, 823.585, 822.754, 825.341, 825.706, 821.948, 824.627, 825.209, 825.052, 822.706, 823.682, 823.888, 823.487, 821.168, 822.818, 824.125, 821.074, 820.671, 820.679, 818.437, 826.458, 815.466, 819.537, 812.875, 813.221, 807.42, 810.525, 816.667, 807.514, 807.429, 810.293, 811.874, 810.346, 815.687, 803.876, 808.521, 804.21, 806.964, 804.019, 809.146, 804.024, 808.957, 802.72, 804.703, 801.294, 805.328, 800.532, 811.99]}	0	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1262432328377348 batch_size:11 epochs:100	100	2000	True	31987.17969		666858	14	-1	407.6518681049347	{'train_loss': [400518.094, 336833.656, 322763.562, 313633.438, 305608.5, 299705.594, 294929.125, 290641.5, 287250.344, 284870.0, 281476.781, 279389.5, 276702.531, 274543.844, 272491.469, 270787.188, 269640.719, 268134.75, 265952.688, 264332.031, 261676.453, 259956.109, 259687.891, 257398.516, 255350.953, 254895.031, 252822.891, 250699.984, 249732.062, 248547.406, 246778.516, 245179.078, 245030.328, 243196.734, 241983.641, 241753.266, 240962.422, 240076.438, 238728.219, 238461.672, 237142.234, 237155.281, 235995.609, 235030.0, 234358.438, 234397.891, 233995.812, 233139.969, 232328.797, 231179.359, 230882.281, 230367.688, 229507.391, 229075.5, 227928.625, 229526.781, 228258.328, 227306.859, 226382.938, 226327.547, 225513.094, 225199.484, 223879.641, 224283.516, 223431.391, 223844.656, 222537.938, 222767.562, 222089.156, 221826.188, 221390.391, 220529.719, 219685.906, 219646.531, 219014.5, 219121.453, 218516.203, 218130.672, 217372.203, 217321.641, 217546.594, 216326.531, 215523.188, 215842.594, 214873.438, 214665.656, 215109.0, 213327.906, 214265.656, 214202.734, 213326.875, 213152.922, 212861.906, 211999.094, 211488.75, 211533.266, 211763.516, 211439.719, 210986.062, 209714.641], 'val_loss': [1088.86, 1041.912, 1006.466, 972.631, 959.861, 952.938, 945.584, 935.057, 926.181, 924.926, 909.68, 906.843, 903.409, 898.302, 893.901, 887.4, 879.248, 875.154, 876.339, 874.508, 865.492, 874.824, 860.2, 861.123, 855.471, 855.216, 862.457, 855.794, 850.288, 845.852, 845.972, 850.564, 837.239, 838.278, 836.287, 826.523, 825.41, 824.134, 824.155, 827.004, 828.656, 834.366, 818.923, 836.387, 825.729, 822.819, 825.411, 827.097, 821.689, 833.746, 833.274, 830.743, 810.034, 814.593, 823.128, 823.856, 821.453, 812.761, 810.374, 813.315, 818.248, 810.394, 823.78, 807.822, 814.294, 804.405, 805.715, 802.124, 817.361, 802.482, 807.764, 808.7, 817.786, 816.387, 813.144, 811.279, 807.515, 809.278, 804.52, 799.047, 806.352, 806.747, 798.815, 806.214, 805.558, 802.001, 811.932, 799.299, 819.685, 798.862, 804.119, 808.38, 807.204, 819.358, 805.52, 798.945, 813.204, 816.628, 799.851, 814.312]}	0	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:15 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:15 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.1262432328377348 beta1:0.829522786104283 beta2:0.8589859409243664 weight_decay:0.0004933646428282743 batch_size:11 epochs:100	100	1000	True	137380.17188		10125982	13	-1	455.2363214492798	{'train_loss': [2513873.5, 2395818.75, 1908816.25, 1423814.875, 1487966.25, 1298180.5, 1364640.625, 1257999.375, 1363588.625, 1227133.625, 1289721.25, 1274224.25, 1286707.375, 1252801.25, 1237935.625, 1270705.875, 1219465.5, 1258927.625, 1221644.875, 1260844.0, 1262182.25, 1245473.625, 1237816.875, 1226650.0, 1254571.125, 1248711.75, 1276675.375, 1227214.5, 1238677.5, 1217884.125, 1247371.5, 1291449.0, 1252216.625, 1262332.875, 1226524.0, 1271292.75, 1279825.875, 1246564.75, 1369953.125, 1224854.25, 1227056.375, 1261462.875, 1286303.125, 1284080.625, 1254045.125, 1203604.125, 1284238.875, 1247473.75, 1196410.0, 1322013.0, 1301669.0, 1213734.625, 1249322.5, 1209463.625, 1296870.625, 1305765.125, 1258190.25, 1272362.375, 1254732.0, 1359794.375, 1323309.625, 1263914.25, 1270548.625, 1270049.75, 1250484.375, 1268878.75, 1242986.0, 1250636.5, 1294756.25, 1283296.0, 1271452.5, 1201790.375, 1256602.75, 1241774.75, 1229190.75, 1276411.0, 1197079.625, 1219905.5, 1243603.0, 1263342.125, 1250184.75, 1237550.375, 1224046.25, 1255082.5, 1208677.5, 1254309.0, 1272748.0, 1301035.875, 1311523.75, 1241374.0, 1254451.0, 1325089.75, 1298325.5, 1241865.5, 1263155.75, 1260565.625, 1305891.25, 1280012.75, 1296123.375, 1276373.75], 'val_loss': [3629.903, 16874.193, 3629.903, 90994.008, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3845.314, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 5086.863, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 4938.528, 3629.903, 3629.903, 3629.903, 5011.346, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 7548.333, 3629.903, 3629.903, 3629.903, 3629.903, 4783.897, 3629.903, 3629.903, 3629.903, 7797.168, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 10530.546, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 5501.912, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903, 3629.903]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:40 kernel_size:8 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:95 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:60 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.1262432328377348 beta1:0.8446571967695493 beta2:0.9381886713940483 weight_decay:0.0001220354146115189 batch_size:11 epochs:100	100	1000	True	57096.90625		334420	13	-1	352.9263095855713	{'train_loss': [499537.062, 446119.781, 456138.938, 449048.562, 458746.656, 447453.594, 457291.688, 459422.094, 456620.0, 451035.5, 438185.562, 447132.625, 451537.281, 456575.062, 463675.344, 452009.312, 446553.375, 465985.5, 433045.219, 436571.0, 436710.781, 454800.781, 444819.656, 435633.156, 422233.031, 433649.781, 439635.438, 429359.094, 429766.844, 435503.031, 432701.688, 422027.312, 430578.562, 452040.969, 423336.25, 434624.625, 426665.531, 441715.875, 434012.0, 445858.531, 550468.688, 1087482.875, 1081241.75, 541267.062, 432208.719, 429320.0, 428925.156, 439561.844, 436422.125, 442631.219, 431638.531, 432257.781, 419705.188, 430771.312, 433816.812, 435604.719, 417765.844, 439250.062, 432223.906, 429785.375, 437734.5, 452835.156, 460476.969, 437512.625, 430301.906, 430034.594, 447638.094, 426312.938, 432455.312, 438397.75, 424198.344, 433724.719, 437467.031, 431626.875, 435719.156, 435184.531, 430833.562, 433445.406, 430052.688, 428153.75, 435440.406, 438550.531, 432450.719, 453464.812, 426462.625, 429674.125, 434232.844, 448460.375, 427899.281, 453956.188, 432202.906, 426323.312, 436725.062, 460301.125, 429024.438, 443973.125, 432646.844, 432153.469, 433157.156, 431221.031], 'val_loss': [1524.424, 2161.487, 2336.206, 2143.704, 1280.708, 1217.61, 1420.964, 1451.642, 1230.313, 1577.78, 1231.147, 1386.745, 1943.287, 1195.504, 2777.645, 1374.247, 3536.112, 2172.703, 1403.182, 1527.331, 2713.256, 1960.274, 1896.716, 1233.207, 1644.441, 2446.768, 1891.29, 1376.182, 1247.002, 1253.192, 1849.026, 1431.783, 1609.035, 1631.322, 1209.257, 1874.471, 1246.985, 1705.916, 1552.789, 1435.38, 3629.902, 3629.902, 3394.667, 2448.877, 1503.061, 1187.015, 1318.42, 2050.221, 1389.324, 1334.296, 1887.134, 1632.225, 3021.212, 1784.369, 1240.269, 1379.524, 1361.883, 2508.162, 1270.247, 1409.056, 1363.887, 4388.442, 1401.332, 1466.786, 1200.73, 1329.433, 1485.083, 1644.96, 1380.029, 1213.335, 1232.656, 1897.179, 1396.679, 1981.095, 1183.688, 1610.895, 2043.894, 2515.945, 1623.815, 1631.339, 1342.048, 2549.594, 2352.254, 1764.334, 1225.232, 1371.328, 2612.508, 1341.719, 1749.124, 1585.449, 1192.369, 1542.9, 1531.074, 1495.972, 1491.507, 1586.573, 1595.631, 1643.496, 3040.169, 1385.34]}	100	100	True
