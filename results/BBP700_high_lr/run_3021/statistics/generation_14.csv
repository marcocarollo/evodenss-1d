id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31014.76562		9715062	13	-1	365.2159969806671	{'train_loss': [588309.75, 615856.688, 611778.562, 590324.062, 558242.25, 514986.844, 470920.094, 424578.531, 385297.438, 351764.25, 325486.281, 307710.781, 296006.094, 288357.219, 283335.438, 279548.812, 275904.219, 272895.562, 270876.281, 267865.656, 265170.719, 263200.875, 261020.0, 259226.5, 259063.266, 256019.203, 255070.453, 253710.672, 252338.859, 250592.219, 249899.344, 248823.453, 247989.125, 246919.656, 246756.578, 245722.562, 243761.547, 243227.0, 242751.156, 241115.219, 240566.75, 240429.656, 239738.578, 238995.656, 238806.734, 237297.953, 237345.875, 235870.188, 235804.922, 235158.672, 234781.719, 234031.609, 233301.672, 232056.578, 231372.453, 230869.719, 230719.172, 230065.969, 230178.109, 229508.141, 229243.703, 228919.594, 228924.0, 227467.156, 227841.125, 227000.688, 226010.938, 224645.281, 225668.547, 223754.562, 223580.609, 224256.375, 224187.203, 222379.375, 222244.062, 221608.703, 222066.25, 221891.609, 221413.984, 221119.25, 220164.0, 219634.594, 219198.625, 219583.969, 219087.0, 218970.984, 217840.188, 218002.141, 218047.609, 217586.188, 217427.594, 215829.375, 215833.75, 215367.641, 215355.734, 215800.797, 214300.469, 214679.484, 213918.219, 213488.094], 'val_loss': [2393.069, 2454.509, 2232.178, 2262.388, 1984.393, 1909.208, 1783.505, 1694.641, 1610.792, 1523.907, 1445.091, 1407.863, 1388.2, 1348.278, 1325.657, 1312.755, 1306.162, 1287.855, 1281.949, 1274.439, 1252.458, 1244.685, 1239.883, 1231.634, 1217.772, 1225.362, 1207.091, 1211.63, 1207.793, 1198.0, 1201.614, 1194.912, 1195.414, 1198.834, 1186.006, 1181.435, 1192.62, 1185.078, 1183.842, 1185.107, 1165.13, 1175.866, 1171.314, 1169.553, 1176.488, 1165.662, 1171.062, 1165.986, 1158.685, 1165.776, 1164.307, 1157.112, 1151.562, 1170.197, 1164.092, 1154.974, 1152.828, 1149.126, 1142.892, 1157.131, 1150.743, 1152.135, 1149.91, 1157.592, 1137.152, 1149.515, 1140.235, 1151.713, 1132.025, 1152.069, 1147.847, 1136.924, 1137.651, 1146.233, 1132.493, 1138.621, 1127.366, 1142.221, 1117.174, 1137.207, 1128.247, 1129.206, 1138.87, 1129.513, 1116.097, 1140.341, 1131.765, 1124.134, 1114.535, 1127.291, 1112.344, 1120.412, 1120.599, 1117.46, 1132.327, 1117.306, 1123.382, 1121.022, 1112.602, 1115.147]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:66 kernel_size:10 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:15 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:15 kernel_size:10 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.14028707708543894 batch_size:16 epochs:100	100	1000	True	32455.49023		619768	13	-1	300.56944012641907	{'train_loss': [432553.625, 361603.719, 337610.594, 324007.594, 314909.062, 308523.812, 304006.25, 300520.719, 297202.469, 294232.594, 291960.094, 290331.094, 287887.688, 285237.781, 284408.156, 281621.938, 280665.438, 279612.312, 277153.906, 275955.406, 275205.781, 273435.844, 272975.469, 272433.062, 271457.188, 271409.656, 269673.906, 268849.938, 268190.219, 267336.812, 267015.531, 266568.844, 265863.188, 265336.812, 266035.312, 264411.0, 262552.031, 261580.766, 261968.297, 261624.047, 260976.828, 260716.844, 259813.594, 258896.609, 258797.906, 257788.141, 258212.672, 256511.938, 256064.391, 255931.781, 255570.141, 255231.781, 253187.922, 253697.516, 253751.422, 251834.312, 252317.031, 251393.969, 250550.438, 250899.453, 250732.953, 249978.016, 250641.328, 249087.406, 248528.422, 248773.203, 247124.438, 247263.156, 247641.234, 247234.125, 246575.766, 246385.188, 244872.172, 246053.25, 245116.484, 244203.891, 243759.594, 243771.0, 244182.172, 243629.984, 243625.109, 242412.688, 242489.688, 242035.141, 240751.969, 241337.844, 240872.469, 240151.062, 240112.906, 240312.219, 240232.641, 240429.719, 239336.922, 239817.281, 239085.875, 238561.547, 237985.672, 237551.375, 237233.562, 237756.297], 'val_loss': [1660.037, 1585.833, 1468.779, 1438.305, 1417.536, 1420.335, 1395.399, 1369.547, 1385.392, 1363.912, 1356.993, 1355.43, 1359.475, 1319.677, 1318.352, 1313.006, 1313.382, 1303.252, 1292.756, 1294.444, 1286.66, 1278.357, 1283.41, 1268.824, 1275.644, 1261.531, 1269.151, 1273.315, 1257.577, 1260.484, 1254.775, 1256.486, 1252.683, 1242.642, 1238.781, 1242.637, 1235.74, 1246.592, 1231.292, 1228.214, 1229.908, 1235.651, 1229.207, 1223.814, 1224.767, 1219.279, 1213.726, 1212.948, 1213.023, 1214.396, 1206.492, 1207.827, 1209.944, 1199.001, 1203.94, 1202.189, 1202.188, 1195.178, 1196.763, 1197.714, 1189.397, 1189.712, 1188.544, 1185.416, 1191.55, 1179.261, 1184.823, 1174.809, 1183.487, 1186.694, 1191.199, 1175.333, 1182.208, 1176.298, 1163.767, 1169.866, 1167.575, 1169.142, 1164.412, 1170.422, 1163.301, 1166.341, 1159.117, 1158.494, 1158.534, 1158.14, 1168.468, 1156.228, 1163.837, 1158.128, 1146.507, 1156.66, 1149.007, 1165.626, 1152.865, 1148.734, 1155.721, 1162.652, 1139.275, 1153.272]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:104 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:98 kernel_size:6 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.12251640432699351 batch_size:16 epochs:100	100	1000	True	32186.24023		18997538	14	-1	542.7915341854095	{'train_loss': [747374.438, 793410.688, 791254.188, 742440.688, 639380.75, 477090.375, 330919.812, 297487.625, 288889.062, 283010.594, 278406.031, 273336.906, 269992.875, 267124.344, 264884.406, 262043.5, 260286.047, 257613.812, 256200.859, 254647.938, 252719.594, 250436.422, 249285.25, 247614.562, 246145.922, 244445.734, 243637.922, 242470.922, 241696.844, 239227.438, 238855.953, 237985.297, 236854.609, 235315.797, 234742.75, 234030.672, 232731.156, 232101.266, 231842.141, 230659.312, 229376.594, 228919.781, 227447.047, 227439.609, 226683.0, 226363.797, 224793.734, 225348.188, 224174.625, 223615.156, 223498.438, 221947.688, 222800.625, 221574.188, 220955.297, 220378.938, 219728.891, 219577.297, 219439.219, 219384.859, 217955.703, 217671.016, 217458.281, 217111.547, 216103.828, 215577.797, 215158.672, 215132.031, 214015.516, 214308.578, 213785.844, 213145.938, 213803.062, 213487.156, 211959.531, 212614.141, 211309.234, 210618.156, 210899.719, 211123.859, 210209.219, 209484.828, 208767.25, 208963.375, 207940.328, 208028.375, 208010.062, 206698.656, 205567.234, 205205.25, 204140.641, 204113.953, 203339.859, 203049.328, 203874.266, 202778.172, 201694.156, 201859.672, 201363.938, 200247.719], 'val_loss': [3552.345, 3595.884, 3436.727, 2990.837, 2383.533, 1726.25, 1468.781, 1392.831, 1357.653, 1335.155, 1315.136, 1316.704, 1307.813, 1288.954, 1290.099, 1280.865, 1270.723, 1262.767, 1246.304, 1240.482, 1227.971, 1231.892, 1227.705, 1241.799, 1225.712, 1204.225, 1206.713, 1210.996, 1201.595, 1204.019, 1215.115, 1212.867, 1199.91, 1205.774, 1214.358, 1206.929, 1203.81, 1206.277, 1197.24, 1202.431, 1205.295, 1194.159, 1201.485, 1203.553, 1196.389, 1199.477, 1188.465, 1188.304, 1176.693, 1190.493, 1184.467, 1181.273, 1173.543, 1167.717, 1168.72, 1177.1, 1178.096, 1171.865, 1162.029, 1172.08, 1176.801, 1165.676, 1162.642, 1182.243, 1163.723, 1164.28, 1163.813, 1154.94, 1162.888, 1157.943, 1155.892, 1157.567, 1158.694, 1150.364, 1169.078, 1139.192, 1147.82, 1160.729, 1161.233, 1154.697, 1159.731, 1141.347, 1176.058, 1152.328, 1159.809, 1155.986, 1148.368, 1156.291, 1158.595, 1143.385, 1160.964, 1162.926, 1154.344, 1159.968, 1165.314, 1160.997, 1151.879, 1147.511, 1151.78, 1167.325]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:54 kernel_size:3 stride:2 padding_deconv:3 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:rmsprop lr:0.09023570113171074 alpha:0.957110469281841 weight_decay:0.0009067840485743055 batch_size:16 epochs:100	100	1000	True	137409.42188		19069684	14	-1	446.58287382125854	{'train_loss': [1296508.75, 8335476.5, 1066921.25, 27697078.0, 1066921.25, 16297650.0, 2066006.375, 5497113.5, 1066921.25, 13348366.0, 1174559.125, 13065952.0, 1900932.875, 2541505.75, 17897358.0, 1674452.0, 10936764.0, 1725272.125, 2434272.75, 15802973.0, 1066921.25, 12018583.0, 1066921.25, 5033036.5, 2004867.125, 9758616.0, 1066921.25, 5026076.0, 1134627.25, 6552120.0, 1345316.5, 5989726.5, 1684089.625, 3009027.5, 1881581.875, 2982983.25, 2793261.75, 2300794.5, 3535195.5, 2665011.5, 2822811.75, 2478506.5, 2674460.25, 2783393.75, 2420627.5, 2752679.5, 3043956.0, 2882831.25, 2577580.0, 2514890.5, 2969533.0, 2452964.75, 2975093.5, 2490084.25, 2941534.75, 3353976.5, 2879084.0, 1924552.0, 3249493.25, 1855838.25, 3176892.0, 1750991.125, 3206308.75, 1847521.875, 3547428.0, 2558738.5, 3666827.75, 2333266.5, 2624901.0, 2139238.0, 3031426.75, 2864051.0, 1920081.5, 3783204.75, 2140334.75, 3024554.0, 2276391.0, 2582076.25, 2710114.25, 2865255.25, 2885211.5, 3031150.75, 2202093.25, 2481925.0, 2558815.0, 2805234.25, 2850138.5, 2430503.25, 2784158.25, 2582993.25, 2915231.75, 2852667.5, 2911005.5, 1891669.875, 3919786.0, 1681905.0, 3089834.0, 2324969.5, 2912253.5, 2137247.0], 'val_loss': [5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 7129.783, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 16283.443, 5293.608, 20727.623, 12624.738, 1704462.25, 75270.211, 5281.688, 976357.75, 994990.812, 84925.164, 105279.648, 6146.107, 94416.797, 81335.82, 181955.016, 69125.969, 514879.031, 38208.832, 307734.438, 382534.75, 89213.703, 1015403.562, 503404.938, 41649.781, 145226.172, 943055.438, 20281.268, 187719.734, 48846.75, 55362.473, 64429.797, 1284315.625, 50125.289, 69705.797, 60457.953, 1302191.375, 13420.187, 11483.057, 5293.605, 360368.062, 220581.312, 535779.625, 550606.0, 289066.812, 52752.645, 267152.75, 1683534.5, 870818.938, 288317.5, 258572.938, 577586.5, 410415.938, 6660.856, 17242.211, 35287.113, 182877.969, 268852.25, 158027.156, 15776.434, 35132.551, 44484.645, 5292.753, 12566.875, 13886.74, 39454.949, 5655.833, 220900.766, 10645.787, 5293.608, 472585.375, 87733.172, 5293.528, 120598.062, 184843.906, 32339.504, 6526.96, 5293.608]}	100	100	True
