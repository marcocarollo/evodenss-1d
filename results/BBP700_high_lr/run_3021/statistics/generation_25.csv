id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:14 epochs:100	100	1000	True	31093.05859		9702186	13	-1	418.20586371421814	{'train_loss': [608382.75, 627898.75, 620544.125, 594854.0, 560478.062, 509373.719, 456659.219, 408413.656, 365259.344, 333906.531, 312307.094, 298547.875, 290264.969, 284812.875, 280923.031, 277197.594, 273867.625, 271329.25, 268083.531, 265540.312, 263624.75, 261741.859, 258415.188, 257511.203, 254801.453, 253685.812, 251868.984, 251795.859, 249694.734, 248810.219, 248083.953, 246675.953, 245418.047, 245099.156, 243546.422, 242762.734, 243246.609, 240980.875, 241085.406, 240643.547, 239914.0, 239043.781, 238620.641, 238240.078, 237621.75, 236167.203, 235743.359, 236161.406, 235123.969, 233405.469, 234032.328, 232291.891, 232623.547, 232021.859, 231405.0, 229760.219, 229031.531, 229414.688, 229734.75, 228589.844, 227193.969, 227461.406, 227122.391, 226900.922, 226100.594, 225714.25, 225786.203, 225139.078, 223815.016, 223833.625, 224269.484, 223211.438, 222825.875, 222584.141, 221278.859, 221020.484, 220956.938, 220796.062, 221098.531, 219501.625, 219014.453, 219158.094, 217919.938, 218064.359, 218617.078, 217419.594, 216623.953, 216854.734, 216117.203, 216301.547, 216247.031, 216425.625, 215072.469, 215249.203, 214524.609, 214442.594, 213046.297, 213758.031, 213049.188, 213285.562], 'val_loss': [2208.529, 2170.309, 2424.586, 1851.999, 1824.745, 1657.391, 1463.24, 1410.206, 1306.06, 1262.867, 1214.053, 1194.826, 1161.078, 1130.852, 1117.352, 1113.025, 1092.649, 1089.047, 1079.318, 1069.942, 1066.919, 1062.382, 1052.872, 1052.72, 1047.837, 1038.915, 1032.999, 1033.902, 1025.681, 1024.584, 1029.249, 1023.695, 1020.156, 1012.852, 1019.651, 1019.482, 1015.409, 1005.281, 998.327, 995.763, 996.941, 999.948, 997.707, 994.427, 990.989, 986.663, 986.435, 982.6, 988.75, 984.992, 983.332, 978.684, 983.294, 983.057, 979.45, 979.355, 974.454, 983.784, 973.521, 972.124, 968.369, 973.097, 971.078, 977.629, 974.921, 975.239, 974.41, 975.766, 967.703, 960.644, 970.151, 961.715, 966.424, 962.049, 971.506, 971.172, 970.956, 959.858, 966.048, 960.438, 950.396, 955.408, 960.549, 952.138, 958.565, 945.679, 953.917, 948.966, 956.581, 958.727, 956.889, 948.618, 951.472, 944.938, 958.043, 954.962, 952.395, 950.712, 947.879, 951.962]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:12 epochs:100	100	1000	True	32008.65039		9702186	13	-1	449.5296576023102	{'train_loss': [599237.875, 631734.188, 620255.5, 585004.625, 533938.875, 474605.062, 416944.625, 370183.625, 332757.625, 309558.406, 296148.188, 289477.688, 283766.719, 280397.031, 276669.312, 272960.406, 271738.562, 268486.594, 266260.406, 264234.5, 262217.219, 261104.188, 258848.828, 257689.406, 255645.25, 254716.812, 253767.031, 252169.141, 250533.766, 250142.156, 248471.109, 247567.375, 246157.859, 245221.469, 244295.438, 243334.531, 241757.859, 241153.672, 240048.922, 239184.984, 238438.438, 236589.594, 237045.906, 236024.109, 235343.047, 235479.094, 233391.016, 233220.75, 232161.797, 232238.188, 230826.609, 230613.531, 229270.359, 228755.734, 228795.391, 227999.531, 228060.859, 226213.016, 225123.188, 225480.0, 225373.984, 224574.719, 224668.438, 222261.531, 223327.469, 222489.344, 221568.562, 221209.797, 220970.984, 221519.25, 220537.203, 220499.594, 219991.484, 219477.969, 219155.562, 218701.172, 217863.812, 217585.234, 217271.375, 215816.688, 215718.141, 215807.984, 215387.609, 215247.125, 214865.375, 214181.438, 213445.656, 213388.25, 213576.609, 213033.344, 212652.312, 212760.406, 211724.266, 210945.359, 211126.875, 210658.297, 210453.531, 209910.516, 209272.375, 209739.25], 'val_loss': [1897.529, 1792.528, 1717.167, 1562.925, 1439.192, 1323.365, 1239.665, 1172.115, 1092.863, 1038.577, 1023.115, 1006.615, 997.922, 988.84, 985.041, 967.895, 968.144, 948.631, 941.258, 938.135, 937.375, 936.228, 929.119, 931.112, 928.717, 920.199, 916.468, 922.149, 918.886, 926.945, 920.935, 903.259, 906.804, 901.664, 894.924, 902.397, 890.295, 892.243, 894.792, 893.351, 880.422, 878.692, 882.167, 877.836, 875.944, 866.781, 871.465, 862.026, 869.955, 868.623, 871.818, 866.93, 864.195, 865.969, 864.509, 865.15, 864.378, 860.362, 859.625, 853.062, 857.341, 858.88, 857.189, 852.919, 863.482, 866.591, 849.496, 854.705, 850.973, 843.375, 849.464, 849.634, 856.143, 851.951, 853.477, 849.817, 849.756, 848.384, 842.771, 856.169, 852.417, 856.329, 850.332, 852.462, 852.389, 845.124, 847.762, 846.715, 847.752, 846.569, 850.504, 847.39, 837.802, 837.983, 840.078, 844.73, 850.268, 842.652, 845.328, 842.683]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:rmsprop lr:0.09023570113171074 alpha:0.9377732665827545 weight_decay:0.00030841894684567904 batch_size:14 epochs:100	100	1000	True	52112464.0		9834826	13	-1	432.96178221702576	{'train_loss': [1140892.875, 6279341.5, 5800949.5, 3922077.0, 1067577.125, 3876302.75, 6059623.0, 1904558.5, 3651723.75, 3308436.0, 3340026.25, 1379245.375, 4042220.0, 1741528.625, 5049108.5, 4612458.5, 1454816.625, 4126285.5, 2336938.0, 2115601.5, 3180662.0, 2193696.25, 2667747.0, 3503292.0, 1587408.5, 2656169.0, 1760975.875, 3119031.75, 2489929.25, 2671798.5, 1695423.375, 2503248.0, 2249589.75, 4122960.5, 2264065.25, 3645482.25, 1979654.75, 2362128.25, 2543993.25, 1965289.75, 2239127.5, 2137001.75, 2631204.75, 2066907.75, 1922802.5, 2299948.0, 1963712.625, 1960612.125, 2266327.25, 2355506.25, 1970965.5, 1782738.25, 2099783.5, 1794979.75, 2144710.75, 1949231.0, 2287743.0, 1984582.125, 2005770.75, 1993122.75, 2359794.0, 2017908.625, 2075802.0, 3375983.0, 2222177.25, 1973853.875, 2256627.75, 2282961.5, 2093850.875, 2046533.75, 2461443.5, 2065747.25, 2151247.75, 1944716.375, 2336883.25, 2338398.75, 2034091.75, 2497318.5, 2078120.125, 2275746.75, 2519435.0, 2142612.5, 2189253.25, 2259838.25, 2244584.5, 2002194.75, 2298209.0, 2284195.75, 2213977.5, 1588515.5, 2552083.25, 2426196.25, 2278589.0, 2344322.5, 2025288.875, 2507232.0, 1973895.375, 2061862.5, 2151579.25, 2254411.5], 'val_loss': [4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 4537.378, 485052.781, 4537.378, 18242.334, 14908.583, 4537.378, 12561.175, 4537.378, 608333.875, 4537.378, 11961.689, 867942.25, 1055454.5, 635995.938, 100539.438, 1732437.5, 617643.75, 940928.125, 2046801.25, 1272624.0, 891816.312, 765068.75, 773462.688, 1170866.375, 560576.375, 1265007.375, 1243405.25, 1301435.625, 763767.812, 963764.062, 31125.873, 1054946.25, 486068.25, 1712577.375, 1137026.875, 709665.0, 1471130.5, 1530811.5, 595696.688, 454630.375, 1477735.0, 1125983.875, 1259529.0, 1388540.0, 1484285.375, 1299441.0, 379097.25, 890459.188, 1643061.0, 1641691.625, 255333.719, 1007759.375, 1384251.25, 1280540.125, 1139192.375, 412851.438, 420292.875, 674032.688, 841746.0, 940660.688, 1087519.125, 391000.969, 1511511.125, 1064804.25, 1221877.875, 112874.719, 1091243.25, 244489.0, 1001929.75, 921261.875, 1261192.625, 1116227.0, 837403.688, 920267.938, 148735.344, 1332379.25, 1025168.75, 494808.5, 1156395.0, 1682338.0, 413261.875, 657315.062, 1092928.25, 961798.188, 968730.125, 1478119.375, 1678599.125, 1015111.312, 961469.5, 1133906.625, 1847676.0]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:126 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:8 layer:conv1d out_channels:77 kernel_size:9 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	33018.76172		12691731	12	-1	377.5899522304535	{'train_loss': [682381.562, 731306.75, 683499.188, 414936.094, 358647.0, 341686.031, 332751.781, 325692.781, 319149.156, 320065.375, 311454.094, 306606.75, 304817.625, 301292.656, 298928.062, 297200.781, 295307.656, 292933.25, 291820.812, 289255.875, 287349.125, 287433.375, 286004.812, 282555.188, 283228.969, 281681.094, 281132.812, 279403.938, 278370.531, 278149.031, 278225.188, 275840.75, 280306.188, 274058.156, 274628.625, 271559.094, 272101.531, 271120.219, 271136.438, 269299.688, 267962.375, 268247.844, 267950.25, 267710.688, 266255.062, 266249.0, 265035.688, 265371.156, 263024.625, 264127.625, 277126.844, 263463.469, 261087.078, 262101.062, 259343.359, 259870.156, 261122.016, 259811.672, 259701.547, 257998.641, 257674.281, 257495.859, 258423.828, 255869.891, 257261.031, 254549.766, 254588.203, 254476.453, 253624.938, 254120.234, 252409.734, 252069.094, 253026.375, 251987.109, 252328.094, 254327.5, 250087.656, 250218.25, 249605.875, 250465.703, 249922.594, 250348.047, 248166.578, 248877.797, 248654.672, 248481.312, 248050.453, 246254.609, 246641.969, 247652.531, 246720.109, 245997.969, 245140.031, 244936.719, 243834.297, 245473.469, 245418.156, 243570.031, 244946.031, 243196.734], 'val_loss': [2719.675, 3345.198, 2017.686, 1820.328, 1777.135, 1561.212, 1608.995, 1506.028, 1491.031, 1438.404, 1439.531, 1419.269, 1403.64, 1405.192, 1393.828, 1387.565, 1391.959, 1367.51, 1371.652, 1329.02, 1347.548, 1363.18, 1311.401, 1355.466, 1329.819, 1296.761, 1309.086, 1287.835, 1289.95, 1288.516, 1277.92, 1280.038, 1288.853, 1285.834, 1264.175, 1264.334, 1272.052, 1241.844, 1256.814, 1266.251, 1241.442, 1244.265, 1235.867, 1237.218, 1221.31, 1232.982, 1236.39, 1223.25, 1234.372, 1230.124, 1291.154, 1235.426, 1215.254, 1231.806, 1210.065, 1245.018, 1231.659, 1216.419, 1209.72, 1206.664, 1215.766, 1220.191, 1202.309, 1221.179, 1202.192, 1187.322, 1205.927, 1205.321, 1219.697, 1207.499, 1192.965, 1197.924, 1190.484, 1189.086, 1206.537, 1187.345, 1193.558, 1195.694, 1176.833, 1191.717, 1188.384, 1179.235, 1187.708, 1170.881, 1174.215, 1180.682, 1167.244, 1185.539, 1163.615, 1180.886, 1173.593, 1195.102, 1172.259, 1163.748, 1166.206, 1167.071, 1179.086, 1173.417, 1165.057, 1163.926]}	100	100	True
