id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adadelta lr:0.09023570113171074 batch_size:16 epochs:100	100	1000	True	31019.26367		9715062	13	-1	376.4626245498657	{'train_loss': [595687.25, 616475.875, 611702.438, 596546.75, 558630.688, 516057.938, 468218.312, 422005.844, 382509.719, 348677.531, 323180.312, 307021.312, 295914.594, 289221.531, 285084.469, 281107.531, 278382.594, 275643.281, 272903.031, 270443.188, 267445.656, 265591.469, 264072.5, 262651.781, 260654.734, 258867.781, 256491.016, 255737.312, 254274.859, 252899.938, 251246.672, 250099.25, 250577.656, 248045.953, 247327.094, 246449.562, 245248.953, 244858.281, 242958.469, 242441.797, 241778.5, 240416.766, 239822.875, 239114.547, 238126.484, 238048.688, 237098.953, 236505.016, 235572.281, 236344.016, 234671.859, 233801.375, 233224.109, 233612.422, 231960.453, 231612.344, 230248.203, 230554.688, 229632.75, 228796.547, 228533.859, 228996.188, 227544.328, 226994.484, 226760.5, 225891.281, 225871.984, 225359.422, 225475.094, 224647.516, 223899.172, 223334.625, 224233.094, 222903.203, 222889.891, 222175.484, 221943.266, 220947.359, 220906.703, 220906.906, 220619.094, 219370.719, 219442.344, 218395.156, 218851.641, 218810.922, 217584.375, 216715.844, 217231.219, 216752.875, 215828.281, 215374.578, 216101.047, 215326.797, 215315.703, 214998.359, 214793.062, 214196.797, 214111.062, 212620.266], 'val_loss': [2587.323, 2499.726, 2514.975, 2188.455, 1979.467, 1950.312, 1831.364, 1689.301, 1614.608, 1529.999, 1462.809, 1455.2, 1387.63, 1375.125, 1351.049, 1332.151, 1328.887, 1302.11, 1292.218, 1290.729, 1273.562, 1270.427, 1264.157, 1255.579, 1246.351, 1253.824, 1235.855, 1232.837, 1224.265, 1226.872, 1219.123, 1214.513, 1197.018, 1210.126, 1202.252, 1206.391, 1191.422, 1185.893, 1195.992, 1181.01, 1194.719, 1175.674, 1171.991, 1169.929, 1166.592, 1159.592, 1169.291, 1170.048, 1172.227, 1162.895, 1156.369, 1160.735, 1165.982, 1161.428, 1163.949, 1160.781, 1158.503, 1138.812, 1152.374, 1155.526, 1152.979, 1156.895, 1140.908, 1148.13, 1144.018, 1154.143, 1144.422, 1145.203, 1143.098, 1146.572, 1142.217, 1142.674, 1141.403, 1139.577, 1147.983, 1133.737, 1137.388, 1124.959, 1136.719, 1127.975, 1138.881, 1116.691, 1126.891, 1141.88, 1127.514, 1120.794, 1135.956, 1128.479, 1134.115, 1123.84, 1123.696, 1124.493, 1130.69, 1118.07, 1122.107, 1121.99, 1115.87, 1121.599, 1128.475, 1132.285]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:78 kernel_size:9 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:rmsprop lr:0.09023570113171074 alpha:0.9010158875246181 weight_decay:0.0008728715476025363 batch_size:16 epochs:100	100	1000	True	139624.6875		5017858	13	-1	343.64159297943115	{'train_loss': [2247911.5, 2818179.25, 2978368.75, 2910140.75, 2907877.25, 3205828.75, 2303280.75, 2350313.0, 2647868.25, 3176529.5, 1772147.125, 1967739.75, 2227692.0, 2066516.125, 2413002.5, 2034187.0, 1968159.375, 2092925.5, 2002401.375, 1968121.625, 2135476.5, 2065171.625, 2248676.5, 1986840.0, 1934634.0, 1811939.75, 1937589.125, 2061280.5, 1915184.875, 1954565.875, 1925893.125, 2117896.25, 1983498.875, 1868922.5, 1979721.375, 2098476.75, 2269158.25, 2045622.5, 1763226.875, 1995407.25, 2019836.625, 1940590.375, 1991781.375, 1846071.0, 1829521.375, 2009901.125, 1952333.5, 2040904.625, 1783185.25, 1873008.5, 1796218.75, 1927206.25, 1866528.125, 1753565.0, 1982732.875, 2067741.375, 2192606.5, 1832383.125, 1801109.875, 1816367.25, 1850294.25, 1701698.875, 1798956.5, 1852741.75, 1955852.75, 1966630.0, 1865312.625, 2129266.25, 2142871.75, 1830532.75, 2021092.0, 1809684.0, 2332963.25, 1937639.0, 1942577.75, 1959284.375, 1878157.25, 1893485.125, 2017992.875, 1888863.375, 1987126.875, 1913589.875, 1821887.125, 2214351.0, 1881978.75, 1965938.125, 2033103.625, 2023049.5, 1926142.0, 1945993.25, 1891760.625, 1938894.5, 1815435.375, 2086555.75, 1905576.75, 1815040.625, 1991366.75, 2018943.75, 2000320.25, 2156001.25], 'val_loss': [5293.608, 5293.608, 5293.608, 5293.608, 5293.607, 5293.595, 5293.607, 5293.608, 5293.608, 7465.885, 6140.367, 5293.411, 5293.608, 226428.281, 5293.608, 5302.313, 7366.304, 6458.634, 7039.288, 9241.744, 5292.928, 7167.841, 15514.578, 9415.043, 14402.107, 10649.884, 42236.855, 12536.969, 6196.204, 38959.156, 13587.961, 653004.562, 9792.382, 5293.607, 8027.018, 5897.439, 8437.518, 8027.566, 19023.721, 5765.292, 10544.765, 5628.804, 37585.027, 9547.231, 12463.114, 16807.469, 10653.99, 35842.344, 11045.692, 152129.547, 7912.554, 10106.475, 23395.596, 6960.805, 8164.843, 21528.582, 19641.717, 5549.337, 5293.184, 33141.031, 7763.123, 5343.493, 5293.608, 17114.488, 6837.331, 7937.914, 29865.246, 8772.11, 6214.404, 5510.075, 16886.408, 9891.863, 5652.498, 8892.932, 61136.703, 15840.729, 6130.858, 5293.607, 25545.049, 7729.495, 24428.293, 26774.693, 5293.432, 21711.934, 5545.264, 11802.425, 8929.9, 17717.551, 13272.817, 9753.206, 5399.237, 14644.13, 105549.672, 182646.078, 215653.672, 7552.552, 12069.698, 202358.391, 27957.078, 5391.25]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:adam lr:0.09023570113171074 beta1:0.848575005261354 beta2:0.9963857438120455 weight_decay:5.5275614895889646e-05 batch_size:16 epochs:100	100	1000	True	137596.90625		9715062	13	-1	372.36393570899963	{'train_loss': [1092206.875, 1066921.25, 1066923.875, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 2817698.0, 1066921.25, 1066921.25, 1077891.0, 1937112.875, 1209503.0, 1066921.25, 1066921.25, 1066921.25, 1066921.25, 1067126.875, 1066921.25, 1068204.5, 1124994.625, 1066921.25, 1066921.25, 1068720.875, 1363891.0, 1066921.25, 1094820.375, 1066921.25, 1066921.25, 2616891.75, 2128401.25, 1067467.875, 1068431.5, 1070335.5, 1066921.25, 1078190.75, 1700126.125, 1066921.25, 1678763.375, 1603517.5, 1196234.375, 1362697.125, 1669738.375, 1988666.875, 1099574.625, 1074102.875, 1112583.25, 1117328.125, 1066921.25, 1066921.25, 1068890.375, 1138162.75, 1066982.625, 3149823.75, 1319325.375, 1067230.375, 1251303.5, 1227651.875, 1720948.25, 1191081.0, 1102977.75, 1167816.25, 1067053.5, 1066921.25, 1117789.625, 1071788.875, 1172628.25, 1646464.5, 2045291.375, 1113974.125, 1082255.25, 1066974.0, 1085631.25, 1071866.375, 1132081.625, 1066921.25, 2949472.0, 1513765.75, 1293656.125, 1265770.875, 1259351.625, 1068195.0, 1486201.5, 1153258.625, 1066921.25, 1076905.625, 1094344.0, 1196463.625, 1066921.25, 1084454.875, 1214106.0, 1972744.75, 1379999.75], 'val_loss': [5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 152963.688, 5293.608, 5293.519, 5284.631, 5293.608, 23494.252, 23294.02, 22584.088, 9889.27, 5293.608, 5293.607, 5293.608, 5293.607, 5293.608, 5293.607, 5293.608, 5293.608, 5293.604, 5293.607, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 8627.764, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.557, 52277.23, 37743.969, 5293.608, 5293.608, 5293.601, 5293.608, 5293.608, 5293.608, 5594.683, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5293.608, 5292.783]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:89 kernel_size:6 stride:2 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:72 kernel_size:2 stride:1 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:72 kernel_size:8 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:83 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:deconv1d out_channels:29 kernel_size:5 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:linear internal_batch_norm:False bias:True input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.10130657491723512 batch_size:16 epochs:100	100	1000	True	32512.6875		2663229	14	-1	359.80451917648315	{'train_loss': [513422.0, 475501.062, 445207.75, 420651.281, 399316.688, 382348.688, 364151.625, 348593.5, 338716.719, 326748.406, 317380.594, 309661.844, 303509.688, 295637.188, 290917.469, 287762.344, 283836.719, 281374.594, 276352.844, 275304.438, 272385.969, 271690.906, 269334.156, 267437.531, 266219.844, 264587.375, 262847.5, 261185.047, 261159.875, 259361.359, 257751.672, 256744.422, 255745.906, 255191.953, 253733.625, 252636.062, 251663.188, 250868.641, 250187.531, 248255.969, 248717.141, 248099.562, 246588.359, 246664.484, 244838.109, 244833.062, 244669.969, 243707.453, 242979.516, 242932.781, 241964.484, 241324.656, 240395.0, 240932.656, 239448.578, 238917.969, 238683.656, 238530.938, 237856.156, 237425.062, 237145.25, 236837.531, 237025.812, 235823.062, 235528.078, 235831.688, 235752.703, 235197.219, 233995.578, 233581.094, 234082.719, 233960.719, 232798.188, 232602.609, 232186.25, 231517.312, 231143.328, 231910.141, 230745.719, 230263.984, 229842.125, 230197.625, 229632.344, 230170.031, 229054.219, 229604.0, 229109.828, 228705.016, 229591.219, 228484.375, 228361.812, 227250.469, 227822.5, 228255.922, 225713.25, 226782.641, 226240.953, 226916.203, 225932.812, 225790.859], 'val_loss': [2022.03, 2095.764, 2021.155, 1872.694, 1830.1, 1694.152, 1671.231, 1611.458, 1550.171, 1543.979, 1508.418, 1478.385, 1428.319, 1403.265, 1408.87, 1355.309, 1337.592, 1326.339, 1305.287, 1321.172, 1306.432, 1267.905, 1268.256, 1273.687, 1261.155, 1251.807, 1275.994, 1230.167, 1224.448, 1236.896, 1210.092, 1208.436, 1198.176, 1208.825, 1205.823, 1196.558, 1206.643, 1186.53, 1188.002, 1189.351, 1188.664, 1176.177, 1179.73, 1186.809, 1176.433, 1172.623, 1171.128, 1163.219, 1162.042, 1166.579, 1156.807, 1166.031, 1159.604, 1154.156, 1153.766, 1143.388, 1151.416, 1155.062, 1143.651, 1144.069, 1152.742, 1165.396, 1142.231, 1139.251, 1143.019, 1141.059, 1130.845, 1131.404, 1145.134, 1133.001, 1138.906, 1133.602, 1132.917, 1134.354, 1134.584, 1131.575, 1130.545, 1120.834, 1122.38, 1128.854, 1127.765, 1128.871, 1132.276, 1127.654, 1129.161, 1122.867, 1127.751, 1119.588, 1135.168, 1123.618, 1115.731, 1117.848, 1112.376, 1123.499, 1118.29, 1115.711, 1111.18, 1113.99, 1114.009, 1111.272]}	100	100	True
