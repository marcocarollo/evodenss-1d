id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1262432328377348 batch_size:11 epochs:100	100	2000	True	32411.79102		666858	14	-1	413.7947175502777	{'train_loss': [405001.812, 333337.125, 318898.562, 310914.0, 304821.469, 299482.031, 294659.812, 290557.656, 286173.5, 282188.375, 280474.5, 276946.188, 274800.625, 272076.531, 269048.969, 267480.969, 265262.031, 262583.344, 260696.516, 257863.188, 256609.906, 255177.312, 253028.203, 251917.5, 250400.375, 249527.656, 247374.109, 246116.422, 244965.672, 244247.422, 243260.625, 242905.656, 241568.281, 240271.047, 239503.797, 239125.906, 238410.938, 237946.188, 237239.953, 236540.625, 234891.75, 234582.734, 233831.938, 233604.328, 231949.281, 231649.156, 231134.547, 230311.641, 230524.297, 229239.125, 229648.25, 229000.312, 227593.5, 227084.094, 226387.484, 226329.688, 226669.984, 225846.797, 225087.828, 224910.484, 224554.625, 224366.516, 223517.391, 223365.219, 222243.516, 222321.875, 221978.453, 221581.719, 221028.844, 220929.734, 219731.531, 219409.609, 218903.719, 219185.391, 218935.25, 217506.688, 218649.047, 217474.641, 217502.438, 217402.953, 216796.641, 216857.516, 215923.469, 215880.5, 215312.344, 215399.422, 215198.047, 214944.766, 214846.438, 214033.078, 213460.438, 212765.828, 214090.203, 213891.703, 212863.703, 212344.469, 212421.031, 211957.656, 211231.562, 211142.859], 'val_loss': [1101.386, 1026.714, 1000.244, 977.342, 966.607, 957.97, 946.862, 926.525, 924.754, 917.112, 900.79, 895.962, 881.706, 889.471, 876.28, 874.724, 866.921, 856.596, 860.282, 857.281, 855.234, 855.616, 849.11, 848.808, 838.479, 834.655, 834.788, 832.894, 848.248, 837.953, 830.763, 827.391, 826.854, 813.697, 808.438, 823.204, 806.132, 811.06, 808.192, 820.86, 801.565, 806.205, 812.136, 803.33, 806.445, 795.418, 813.633, 809.098, 803.594, 795.999, 795.125, 797.752, 800.246, 799.47, 798.816, 795.45, 793.599, 794.886, 796.84, 792.057, 798.594, 799.988, 792.882, 784.41, 789.962, 785.363, 793.178, 792.776, 795.074, 789.931, 787.12, 791.141, 791.511, 778.319, 788.435, 785.762, 784.208, 787.354, 786.543, 789.599, 782.057, 782.583, 789.474, 785.11, 781.216, 784.911, 783.407, 785.427, 779.345, 787.004, 779.294, 783.236, 786.767, 777.774, 782.731, 780.815, 775.701, 779.033, 779.007, 778.386]}	100	100	True
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:48 kernel_size:5 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:conv1d out_channels:70 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:deconv1d out_channels:84 kernel_size:9 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1262432328377348 batch_size:11 epochs:100	100	1000	True	32840.54297		416270	14	-1	353.09703612327576	{'train_loss': [368526.781, 325597.625, 320130.031, 316404.906, 313865.031, 311669.0, 310109.156, 308430.938, 307142.188, 305127.031, 303130.5, 301432.906, 299084.594, 296040.812, 293098.125, 289637.938, 287246.188, 282930.375, 281192.969, 277682.781, 275074.062, 272733.781, 270721.094, 268166.844, 266374.844, 264964.906, 263223.781, 261135.0, 259669.484, 257863.156, 257956.75, 256816.094, 255743.391, 254671.812, 252989.594, 253720.438, 251728.25, 251418.609, 251209.031, 250273.906, 249918.688, 248643.375, 248661.062, 247862.344, 247156.234, 246185.312, 246247.922, 244842.734, 244639.484, 245293.562, 242953.469, 242757.375, 242401.922, 241463.938, 240578.156, 240769.031, 240215.594, 240249.859, 239308.078, 239382.953, 237633.875, 238202.984, 237947.234, 236601.922, 235877.781, 236290.375, 235219.078, 235224.016, 235150.641, 234263.516, 233904.812, 234070.812, 233524.141, 232741.969, 232749.953, 231464.438, 231699.625, 231255.578, 231745.188, 230800.547, 230382.875, 230080.75, 229999.547, 230281.25, 229273.953, 228888.906, 229847.953, 228214.109, 228485.047, 228213.719, 227869.875, 227331.141, 228346.938, 226698.547, 226788.922, 226380.688, 226237.438, 226426.484, 225113.516, 225648.328], 'val_loss': [1046.921, 1025.118, 1013.211, 1002.418, 997.358, 993.042, 989.497, 988.254, 983.261, 980.654, 977.366, 971.464, 966.618, 961.334, 945.024, 934.879, 926.992, 922.934, 914.356, 904.526, 897.723, 888.472, 886.854, 886.479, 870.249, 865.575, 876.116, 861.814, 856.21, 849.96, 848.404, 855.572, 847.235, 835.731, 841.679, 824.687, 822.011, 819.815, 819.655, 819.334, 816.651, 834.111, 811.697, 830.67, 812.06, 805.309, 807.556, 807.72, 806.165, 805.425, 826.204, 798.364, 788.075, 789.942, 795.757, 795.989, 794.884, 790.661, 785.84, 787.841, 808.703, 782.239, 786.585, 775.69, 777.363, 792.254, 779.609, 778.37, 765.952, 782.387, 778.331, 794.367, 781.565, 795.285, 780.358, 762.142, 776.251, 779.594, 765.869, 753.618, 792.01, 785.955, 788.828, 783.185, 776.958, 770.346, 772.577, 795.983, 778.06, 769.301, 786.361, 777.907, 774.395, 755.919, 764.04, 767.752, 767.605, 771.533, 783.266, 777.649]}	100	100	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:52 kernel_size:4 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:conv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.1262432328377348 batch_size:16 epochs:100	100	1000	True	32073.6543		409230	14	-1	317.8690629005432	{'train_loss': [371584.844, 327924.062, 320144.406, 315839.625, 313837.188, 311916.844, 309464.156, 307872.469, 305909.844, 303429.219, 300959.031, 299497.5, 297594.031, 296242.531, 294669.438, 293450.469, 291590.25, 290802.906, 289822.344, 287802.312, 285955.344, 284652.469, 282523.344, 280223.375, 278379.875, 276944.438, 274783.844, 273478.531, 271736.0, 269606.0, 268064.469, 266144.688, 265402.031, 263681.688, 262207.344, 260587.25, 260122.562, 258600.328, 257324.891, 255739.094, 255457.406, 254688.344, 253756.016, 252313.734, 251890.234, 250813.141, 250499.297, 249941.766, 248039.609, 248407.688, 246856.0, 247035.578, 246610.094, 245002.734, 245463.734, 244379.25, 244098.766, 243347.656, 241917.25, 242512.359, 241819.875, 241442.781, 240675.344, 240492.609, 239985.188, 239309.547, 238975.219, 238699.375, 238367.641, 238148.5, 236825.016, 237091.641, 236412.25, 236532.953, 236625.922, 235409.422, 235057.938, 233633.484, 234609.984, 234200.062, 233619.25, 232585.422, 232292.766, 233035.438, 232323.547, 231898.219, 231608.891, 231173.75, 231538.797, 229996.219, 229877.859, 228435.797, 229078.734, 228398.5, 229310.312, 228533.25, 228641.234, 228178.234, 227977.219, 226902.797], 'val_loss': [1547.23, 1500.206, 1477.773, 1465.357, 1454.741, 1449.479, 1442.079, 1432.331, 1427.797, 1425.734, 1416.96, 1416.824, 1409.153, 1411.147, 1405.839, 1399.545, 1392.13, 1388.751, 1388.587, 1385.275, 1373.015, 1371.639, 1350.153, 1342.94, 1328.625, 1316.656, 1305.672, 1303.003, 1297.734, 1293.258, 1273.275, 1262.257, 1256.524, 1248.775, 1242.557, 1238.845, 1228.185, 1220.271, 1218.812, 1207.211, 1205.307, 1199.752, 1207.572, 1198.839, 1201.069, 1188.762, 1185.459, 1181.747, 1194.137, 1174.281, 1176.523, 1191.191, 1184.821, 1185.907, 1175.752, 1167.035, 1161.895, 1173.919, 1163.608, 1164.967, 1167.977, 1154.541, 1162.037, 1158.363, 1152.327, 1159.26, 1146.091, 1147.344, 1150.546, 1147.171, 1156.92, 1155.655, 1156.414, 1146.492, 1158.818, 1137.855, 1142.306, 1145.054, 1150.334, 1131.299, 1130.58, 1144.465, 1133.592, 1128.679, 1124.788, 1135.173, 1127.454, 1132.21, 1125.974, 1141.411, 1112.558, 1106.18, 1114.007, 1110.686, 1113.584, 1115.49, 1111.476, 1138.296, 1127.102, 1117.918]}	100	100	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:52 kernel_size:9 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:7 layer:conv1d out_channels:100 kernel_size:6 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:72 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta lr:0.11909217547543158 batch_size:11 epochs:100	100	1000	True	32411.62305		397850	14	-1	374.7803382873535	{'train_loss': [367254.688, 325365.5, 318605.781, 314576.938, 311526.281, 309352.438, 307341.25, 305188.969, 302763.344, 299357.875, 294476.312, 289669.125, 286005.469, 282912.094, 280194.344, 277013.5, 274499.094, 272205.344, 269918.938, 267779.594, 265505.625, 264508.406, 262028.031, 260980.016, 259964.641, 258718.094, 257477.609, 256628.969, 254479.938, 254208.359, 253215.484, 251634.969, 251010.594, 249964.5, 248972.172, 249004.375, 248110.094, 247006.062, 246801.234, 246385.312, 246084.609, 245038.234, 244932.828, 243794.828, 243740.922, 243007.625, 242308.594, 242675.672, 241865.938, 241994.203, 241115.156, 240114.219, 240092.047, 239436.594, 239722.969, 239027.531, 239339.469, 238808.891, 237972.016, 238480.281, 238199.438, 237897.516, 236808.094, 236099.578, 236311.875, 235412.719, 235377.516, 236125.75, 234728.578, 234684.141, 234002.922, 234306.453, 234215.531, 233519.312, 233563.359, 233115.516, 232904.891, 232167.906, 232551.125, 232660.266, 230784.672, 231857.047, 231532.328, 230998.578, 230560.484, 230168.469, 230983.375, 230008.859, 229479.031, 229459.391, 228576.391, 229381.688, 228447.828, 228166.047, 228600.828, 228688.969, 228336.875, 227981.234, 227077.5, 227902.188], 'val_loss': [1049.422, 1022.312, 1006.63, 996.666, 988.16, 985.422, 983.653, 977.124, 971.552, 961.061, 950.019, 940.933, 931.2, 927.946, 918.058, 909.666, 902.818, 891.455, 887.197, 873.454, 871.236, 862.483, 861.963, 866.231, 859.345, 859.397, 853.155, 859.556, 851.79, 847.173, 850.877, 840.784, 841.287, 843.018, 847.144, 835.618, 833.475, 836.653, 833.817, 842.648, 838.39, 831.532, 833.09, 833.648, 825.691, 835.947, 829.334, 836.722, 839.288, 827.605, 825.891, 822.422, 820.926, 820.263, 827.502, 828.588, 819.807, 818.619, 816.808, 815.618, 818.661, 806.203, 811.738, 817.477, 815.713, 813.571, 808.825, 803.427, 812.239, 810.216, 805.571, 809.304, 808.584, 804.673, 809.089, 813.835, 808.134, 812.568, 796.23, 800.017, 806.227, 805.153, 791.635, 798.862, 800.06, 797.68, 798.049, 789.783, 792.674, 806.809, 795.16, 791.003, 791.533, 790.885, 799.858, 792.471, 793.181, 778.736, 784.167, 791.954]}	100	100	True
