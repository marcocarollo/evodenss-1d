id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4067.14307		452251	14	-1	184.5121157169342	{'train_loss': [383693.219, 252209.812, 206953.062, 191744.219, 181707.672, 153724.609, 132087.719, 122979.703, 117199.609, 111619.883, 109634.156, 107241.219, 104074.211, 103355.562, 101147.945, 99507.117, 99480.453, 97379.211, 95760.883, 95331.453, 93786.797, 92335.695, 92581.828, 91239.945, 90941.156, 90380.453, 88717.352, 90173.688, 88570.562, 87936.805, 87436.078, 88617.367, 87271.852, 85780.141, 86261.766, 85612.508, 85635.617, 85954.328, 84415.641, 85020.234, 83845.562, 84096.195, 83843.297, 83406.008, 82299.359, 82511.719, 83355.297, 82920.398, 81083.031, 82134.039, 81572.703, 81110.758, 81305.836, 80540.359, 81528.672, 80608.008, 80703.43, 79563.578, 80626.727, 79974.984, 80056.695, 80101.828, 78914.93, 78883.844, 79665.055, 79500.109, 79319.867, 78559.984, 78804.359, 79379.414, 78425.906, 78166.352, 77271.32, 78331.875, 77805.125, 78460.547, 77837.688, 77875.422, 78161.875, 77214.688, 77195.398, 76804.172, 76411.828, 77143.586, 76157.219, 76535.633, 76151.766, 77291.945, 76107.43, 75723.82, 76860.312, 75936.422, 75671.367, 74920.75, 75758.633, 75146.57, 75414.477, 74802.25, 75792.453, 74636.555], 'val_loss': [3413.047, 2930.44, 2779.741, 2755.427, 2719.889, 2328.688, 1888.415, 1767.956, 1753.756, 1835.169, 1653.345, 1587.243, 1496.603, 1497.044, 1575.403, 1681.982, 1451.343, 1365.812, 1330.488, 1264.853, 1248.174, 1280.504, 1206.634, 1387.773, 1250.634, 1167.569, 1320.926, 1200.203, 1169.202, 1096.484, 1106.785, 1134.256, 1131.149, 1153.011, 1133.97, 1220.865, 1166.441, 1187.79, 1098.175, 1080.885, 1096.993, 1069.788, 1077.32, 1078.133, 1260.094, 1067.408, 1052.378, 1043.984, 1054.894, 1061.223, 1016.731, 1113.634, 1068.993, 1038.501, 1111.276, 1041.366, 1005.045, 1014.985, 1028.309, 1050.521, 1012.856, 1079.511, 979.589, 1026.239, 1003.992, 1044.365, 1023.993, 1012.183, 1032.205, 1040.169, 993.874, 1005.483, 996.699, 1010.657, 1022.741, 1081.798, 972.206, 1001.126, 997.544, 1058.251, 1006.028, 973.413, 1014.56, 1014.441, 992.805, 992.257, 998.891, 1024.303, 975.787, 1064.831, 1022.587, 1049.657, 990.289, 990.067, 1026.42, 956.599, 957.835, 1013.672, 991.008, 993.276]}	100	100	True
