id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	8178.79492		452251	14	-1	169.839049577713	{'train_loss': [347554.531, 309445.438, 214030.906, 178017.609, 169690.891, 163730.469, 160503.406, 152904.938, 124999.883, 113497.953, 108542.922, 104831.586, 101261.828, 99687.391, 96606.273, 96734.867, 94577.727, 92802.25, 91264.477, 91494.961, 90382.82, 88760.195, 88299.086, 86801.086, 86630.609, 87077.453, 85242.375, 85852.82, 84829.914, 83873.727, 84130.844, 83534.938, 82530.398, 82387.414, 82022.797, 81616.992, 81064.875, 81142.195, 80666.969, 81228.664, 80103.102, 80100.875, 79225.578, 79847.516, 78732.117, 78866.891, 78763.5, 79223.141, 77710.461, 78039.844, 78386.727, 77701.945, 77391.703, 77469.727, 77067.078, 76340.023, 76826.727, 76928.375, 76927.656, 76270.391, 75528.023, 76153.469, 75912.211, 75867.234, 75163.672, 75308.844, 75173.023, 75155.609, 75269.531, 74259.219, 74082.07, 74467.773, 74184.102, 73712.836, 73334.875, 73773.039, 73477.109, 73982.289, 72683.867, 73952.453, 73614.359, 73036.852, 73578.906, 71732.68, 72530.836, 72435.453, 72696.969, 72305.719, 71421.734, 71937.156, 71294.914, 71511.445, 70387.484, 70966.812, 71850.742, 71161.273, 70420.0, 71538.531, 71689.578, 71405.438], 'val_loss': [5254.012, 3152.716, 2740.577, 2843.601, 2651.764, 2637.323, 2485.983, 2034.127, 1600.433, 1489.593, 1420.315, 1382.588, 1419.845, 1340.469, 1390.745, 1400.211, 1323.073, 1269.622, 1348.468, 1223.563, 1211.006, 1242.183, 1184.386, 1205.629, 1169.625, 1253.121, 1235.096, 1196.865, 1168.597, 1216.534, 1145.127, 1159.459, 1139.843, 1159.978, 1147.284, 1209.387, 1088.09, 1111.736, 1095.448, 1107.008, 1088.706, 1133.511, 1090.378, 1080.627, 1090.106, 1086.28, 1072.397, 1095.955, 1087.16, 1082.398, 1078.882, 1058.065, 1087.659, 1074.858, 1072.137, 1052.816, 1056.915, 1087.248, 1043.566, 1036.556, 1072.193, 1093.93, 1066.139, 1089.868, 1068.638, 1036.243, 1035.993, 1105.169, 1027.087, 1014.36, 1055.452, 1025.459, 1013.389, 1028.904, 1010.734, 1008.773, 1023.816, 1021.566, 1035.504, 1039.557, 1051.648, 1014.99, 999.796, 1024.104, 1007.456, 1046.504, 1049.38, 1024.025, 1005.568, 1061.447, 1008.604, 989.528, 984.317, 992.611, 1046.139, 1016.512, 1027.984, 987.06, 976.882, 1000.222]}	100	100	True
