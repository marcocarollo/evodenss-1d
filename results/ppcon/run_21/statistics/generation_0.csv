id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	7790.95166		452251	14	-1	169.9445924758911	{'train_loss': [344163.719, 327268.125, 243200.922, 187519.891, 171415.578, 164265.141, 161601.5, 158140.203, 154407.391, 141050.188, 120625.914, 110246.117, 103940.977, 101058.438, 97040.586, 95273.797, 92198.531, 91970.531, 91062.008, 88115.562, 87486.617, 86172.688, 85944.078, 85298.266, 83496.719, 83459.117, 82103.156, 82022.914, 81947.25, 80279.094, 80016.828, 80094.281, 78898.5, 78270.969, 78340.539, 77823.93, 77039.602, 77092.805, 76798.758, 75985.086, 74427.633, 74452.469, 74379.773, 74880.641, 74180.812, 73342.32, 73117.719, 72652.336, 72840.164, 72943.398, 72072.609, 71848.852, 71496.352, 72626.828, 70986.859, 71203.789, 70824.898, 71131.852, 70240.977, 70101.469, 69998.977, 70082.203, 69341.211, 69757.664, 69117.867, 69443.422, 69209.586, 69167.344, 69454.281, 69556.531, 68873.992, 68218.086, 68588.508, 68411.867, 68483.188, 68348.469, 67702.164, 67532.32, 67592.227, 67658.789, 67109.125, 67120.852, 67198.297, 67523.883, 67195.531, 66250.164, 67323.078, 66469.531, 66698.0, 66111.625, 66312.359, 66379.953, 66588.906, 66133.188, 65908.773, 66112.141, 66142.039, 66168.961, 65912.094, 65509.625], 'val_loss': [5364.834, 4814.421, 2783.625, 2524.132, 2689.602, 2635.043, 2522.346, 2530.962, 2244.963, 1922.194, 1694.125, 1564.886, 1382.846, 1404.327, 1291.139, 1312.439, 1364.89, 1238.944, 1260.922, 1268.241, 1265.195, 1205.227, 1226.178, 1261.003, 1226.298, 1214.286, 1184.922, 1181.475, 1193.348, 1162.835, 1119.043, 1151.251, 1127.537, 1134.681, 1115.097, 1124.112, 1109.054, 1104.324, 1109.782, 1118.721, 1091.223, 1097.626, 1098.635, 1067.045, 1075.824, 1069.574, 1107.364, 1113.7, 1077.552, 1050.525, 1067.681, 1078.221, 1034.199, 1080.587, 1060.426, 1074.729, 1037.123, 1012.024, 1027.764, 1071.358, 1056.906, 1071.32, 1023.669, 1077.244, 1077.661, 1015.71, 1045.784, 1004.713, 1015.056, 1024.894, 1018.874, 1053.649, 1015.815, 1001.111, 1006.197, 1052.867, 1034.591, 997.866, 1016.889, 1007.562, 1011.266, 986.628, 1061.38, 993.435, 1005.378, 990.053, 984.759, 1043.626, 998.595, 1005.249, 1004.712, 970.611, 1012.846, 1021.052, 1003.725, 968.361, 1008.374, 995.193, 968.819, 953.284]}	100	100	True
