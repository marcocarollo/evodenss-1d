id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	3936.72974		452251	14	-1	183.57375574111938	{'train_loss': [383412.812, 248700.469, 200062.016, 185460.906, 167934.875, 136478.438, 127162.023, 120265.18, 116407.695, 112827.398, 110333.109, 107820.438, 105080.695, 104674.711, 102809.711, 100464.125, 100326.383, 97812.062, 96428.562, 95264.867, 94389.484, 93868.188, 93239.539, 91853.453, 91594.461, 91739.242, 89809.383, 89596.438, 88228.008, 88346.906, 86654.727, 87146.773, 86340.195, 85994.18, 86101.055, 85898.766, 85021.359, 84132.852, 83916.0, 84317.547, 83353.414, 82324.648, 82714.836, 82389.656, 82018.359, 81518.047, 82245.805, 81350.305, 81262.625, 79962.977, 80193.391, 79803.727, 80166.414, 78829.242, 80658.828, 79028.648, 80121.148, 78411.984, 78642.867, 78469.977, 78071.828, 77908.773, 77998.234, 77912.797, 77906.461, 77191.273, 76947.117, 75946.227, 76793.93, 75784.406, 76707.172, 76256.461, 75775.703, 75754.812, 75664.219, 76208.312, 75556.773, 75633.656, 75606.961, 74475.039, 75229.945, 74737.75, 74535.562, 73866.328, 74988.289, 74269.984, 74420.992, 74601.375, 74141.461, 74251.828, 73776.008, 72610.93, 72917.492, 73153.219, 73503.398, 73697.07, 72309.727, 74405.68, 72726.703, 72470.773], 'val_loss': [5332.783, 3035.936, 2563.49, 2597.611, 2301.727, 2061.806, 1823.931, 1701.025, 1632.0, 1631.498, 1700.826, 1574.096, 1545.61, 1717.182, 1579.53, 1603.825, 1533.272, 1376.446, 1543.764, 1433.141, 1445.401, 1455.583, 1281.425, 1315.776, 1293.835, 1284.412, 1262.218, 1245.114, 1154.007, 1195.571, 1176.499, 1184.051, 1222.025, 1131.417, 1194.389, 1140.437, 1180.126, 1181.458, 1110.67, 1123.665, 1150.316, 1093.173, 1119.411, 1133.844, 1176.649, 1109.132, 1119.73, 1072.33, 1080.384, 1079.893, 1088.139, 1048.967, 1061.561, 1040.266, 1060.448, 1085.743, 1080.671, 1065.315, 1056.142, 1079.273, 1031.173, 1106.733, 1044.199, 1072.724, 1056.342, 1070.463, 1111.488, 1014.213, 1040.895, 1028.285, 1065.482, 979.97, 1015.362, 1036.414, 1055.032, 1100.062, 1046.829, 983.669, 1040.467, 1011.987, 1004.218, 1030.978, 972.151, 1070.163, 969.007, 990.331, 980.718, 1013.253, 987.465, 1036.336, 986.802, 1008.874, 966.876, 972.674, 1003.926, 965.796, 963.395, 1054.507, 973.167, 968.199]}	100	100	True
