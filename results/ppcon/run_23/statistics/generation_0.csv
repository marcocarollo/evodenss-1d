id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	7766.65088		452251	14	-1	170.00183820724487	{'train_loss': [344045.031, 323369.719, 205745.062, 178935.812, 162705.766, 153463.594, 134073.516, 118662.219, 111519.734, 105726.984, 102628.484, 99701.789, 98193.164, 96529.195, 93168.641, 92371.922, 91090.492, 89057.945, 88656.375, 86701.672, 86105.867, 85856.305, 84368.555, 83705.805, 84249.109, 82837.133, 82500.539, 82208.883, 80581.648, 80828.258, 79669.477, 79937.977, 79665.938, 79050.414, 78829.758, 78547.016, 78073.672, 77162.93, 77126.336, 77880.367, 77109.734, 75920.32, 76312.273, 75537.547, 75831.602, 75471.273, 75352.797, 74967.008, 74927.531, 74073.336, 74825.156, 73669.969, 74343.961, 73579.57, 73930.875, 72522.172, 72335.938, 72630.727, 72491.258, 71663.609, 72639.828, 72742.07, 71374.758, 71845.984, 71170.945, 71278.062, 71471.883, 71331.227, 70820.234, 69936.852, 71331.664, 71157.703, 70957.18, 70527.531, 69759.422, 69484.898, 69430.156, 69347.984, 69289.906, 69295.133, 69121.734, 68049.477, 69060.703, 68211.297, 68236.914, 68366.086, 68225.953, 69053.945, 68386.047, 67756.391, 67852.297, 66944.992, 66890.109, 66949.719, 67093.25, 66976.391, 67459.039, 66970.648, 67091.695, 66773.508], 'val_loss': [5251.01, 3957.048, 2690.229, 2631.785, 2309.756, 2154.231, 1836.661, 1613.304, 1535.534, 1433.305, 1333.619, 1347.217, 1365.4, 1368.725, 1345.68, 1299.166, 1188.435, 1276.307, 1252.0, 1257.825, 1189.914, 1164.2, 1172.925, 1247.155, 1126.317, 1130.64, 1197.587, 1135.198, 1155.318, 1118.736, 1122.342, 1124.189, 1109.041, 1165.512, 1141.477, 1088.956, 1105.878, 1085.32, 1131.233, 1124.184, 1090.266, 1089.898, 1084.725, 1082.035, 1053.188, 1065.547, 1053.218, 1116.457, 1039.737, 1093.999, 1065.948, 1041.381, 1024.33, 1041.372, 1019.996, 1042.007, 1000.181, 1044.186, 1088.62, 1083.503, 1007.098, 1039.419, 1016.076, 1073.313, 1102.535, 1006.431, 983.155, 1004.093, 1034.154, 976.981, 995.59, 1016.519, 998.003, 976.678, 992.782, 984.119, 1013.98, 983.491, 1033.127, 977.881, 1018.951, 977.002, 996.493, 983.968, 956.855, 983.524, 958.844, 1009.64, 960.537, 998.629, 972.178, 963.162, 1011.076, 969.281, 986.199, 974.278, 984.166, 981.609, 978.742, 969.847]}	100	100	True
