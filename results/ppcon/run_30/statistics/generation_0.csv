id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	32668.47656		452251	14	-1	222.40506505966187	{'train_loss': [398251.594, 331665.812, 315389.219, 303623.5, 296791.75, 291211.812, 285224.312, 281092.594, 276276.812, 272969.969, 270992.438, 269185.188, 266923.344, 265563.25, 263354.156, 262087.516, 260681.562, 260035.812, 258172.547, 257055.5, 256210.094, 255899.484, 254126.328, 252958.828, 252445.547, 251509.594, 250355.781, 250445.953, 250479.453, 249640.734, 249567.859, 248630.219, 247501.938, 246211.531, 247230.609, 246335.406, 245476.516, 245530.422, 244608.359, 245745.516, 244057.906, 244324.25, 244038.469, 243578.531, 242902.516, 242084.062, 242215.812, 241949.203, 240993.75, 241206.641, 240775.828, 240290.266, 239357.953, 238972.172, 239536.219, 238981.188, 238284.812, 238478.781, 237915.5, 237499.516, 237897.656, 236516.547, 236342.156, 236254.484, 235249.844, 235546.844, 235550.453, 235361.547, 234532.609, 234572.5, 234344.438, 234851.234, 233480.75, 233326.953, 232926.438, 233165.906, 233051.953, 232554.219, 232160.594, 232671.516, 232297.172, 231568.594, 231891.484, 231543.312, 231217.859, 230546.891, 230281.453, 230886.219, 231116.609, 230705.75, 229959.828, 229942.656, 229918.125, 230465.75, 229313.156, 229475.297, 229598.219, 229281.516, 228587.062, 229086.0], 'val_loss': [3149.253, 3092.957, 2956.238, 2918.772, 2816.36, 2776.525, 2716.258, 2699.064, 2669.315, 2630.042, 2625.697, 2610.272, 2579.475, 2577.616, 2576.315, 2559.171, 2547.705, 2534.807, 2520.355, 2490.407, 2486.667, 2517.545, 2474.651, 2451.486, 2443.17, 2442.703, 2432.772, 2437.646, 2438.088, 2424.18, 2392.345, 2424.189, 2426.529, 2397.1, 2398.754, 2413.543, 2388.221, 2411.172, 2380.636, 2377.927, 2376.366, 2371.417, 2377.437, 2394.187, 2362.564, 2367.556, 2372.514, 2354.976, 2375.125, 2412.321, 2370.542, 2328.068, 2357.058, 2327.478, 2404.02, 2353.323, 2356.224, 2336.228, 2348.945, 2359.054, 2373.826, 2338.014, 2333.695, 2376.744, 2349.676, 2358.264, 2328.73, 2335.809, 2360.738, 2334.051, 2330.147, 2330.847, 2349.185, 2306.124, 2349.112, 2318.138, 2317.394, 2338.709, 2330.647, 2321.542, 2321.316, 2320.099, 2300.503, 2299.403, 2322.045, 2292.241, 2309.241, 2290.88, 2313.463, 2279.64, 2277.752, 2302.48, 2307.115, 2275.458, 2296.014, 2297.486, 2275.549, 2281.643, 2286.664, 2297.385]}	100	100	True
