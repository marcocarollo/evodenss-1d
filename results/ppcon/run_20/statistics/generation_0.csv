id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4549.88623		452251	14	-1	182.88289999961853	{'train_loss': [389655.844, 315889.406, 159337.625, 144049.469, 134985.281, 126870.289, 121520.312, 118046.82, 114790.734, 112407.727, 110491.297, 108892.656, 107413.344, 105451.031, 104374.797, 103768.336, 103773.719, 100494.25, 101120.156, 100228.555, 99001.148, 98799.195, 98717.039, 97512.992, 97337.195, 96630.82, 96580.797, 94919.328, 94902.984, 95392.961, 94519.391, 93958.531, 94213.719, 93001.312, 92806.172, 92453.203, 92029.062, 91949.484, 91099.195, 91034.375, 89595.43, 90162.727, 90265.945, 89991.289, 89058.461, 88722.359, 87899.906, 88276.781, 87552.375, 87636.164, 86661.367, 87237.586, 86899.75, 85873.234, 85663.477, 85931.273, 85839.828, 85048.891, 85226.93, 85749.078, 84780.281, 84301.992, 84211.602, 83992.312, 84798.414, 84137.523, 84429.789, 83106.266, 83141.352, 82637.508, 83057.992, 83265.438, 82672.438, 82819.578, 81769.992, 82723.734, 82104.172, 81960.43, 81600.25, 81550.961, 82045.867, 81778.719, 81046.07, 81297.398, 80553.742, 80992.508, 81531.703, 80483.008, 80573.031, 80976.547, 79282.719, 80013.102, 80389.461, 79740.711, 79676.812, 79599.273, 78988.414, 78674.5, 78958.164, 78804.289], 'val_loss': [4772.788, 2279.145, 1976.38, 1800.278, 1805.282, 1767.254, 1719.461, 1677.418, 1569.848, 1634.108, 1553.499, 1551.16, 1501.545, 1546.317, 1474.521, 1435.986, 1463.891, 1475.926, 1416.243, 1405.193, 1359.109, 1344.536, 1412.346, 1326.876, 1321.189, 1325.988, 1318.806, 1341.45, 1324.624, 1373.644, 1308.613, 1348.968, 1327.973, 1316.703, 1294.76, 1371.587, 1347.569, 1303.806, 1292.074, 1244.982, 1286.75, 1288.45, 1267.202, 1325.643, 1355.572, 1276.957, 1297.381, 1251.207, 1216.636, 1145.338, 1320.635, 1254.759, 1298.109, 1251.287, 1270.537, 1292.983, 1253.758, 1307.049, 1253.59, 1218.818, 1291.988, 1277.792, 1230.141, 1237.63, 1266.182, 1231.703, 1159.867, 1210.233, 1208.694, 1239.701, 1246.199, 1257.243, 1195.373, 1211.78, 1189.953, 1213.489, 1177.965, 1267.172, 1144.544, 1187.581, 1201.51, 1176.248, 1136.233, 1168.582, 1168.657, 1190.964, 1136.677, 1121.604, 1160.741, 1144.759, 1190.063, 1149.683, 1267.585, 1187.216, 1191.969, 1180.604, 1166.624, 1153.955, 1180.039, 1141.764]}	100	100	True
