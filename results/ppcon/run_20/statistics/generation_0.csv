id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	8285.45508		452251	14	-1	167.96939373016357	{'train_loss': [342405.188, 229887.109, 184615.953, 168528.344, 146477.203, 122484.711, 114099.805, 107100.273, 102721.594, 100836.078, 98359.875, 95674.688, 94558.141, 92760.0, 91522.773, 90330.227, 89397.148, 87869.867, 86665.008, 86711.578, 85456.836, 84291.781, 85380.688, 84170.969, 82458.555, 82254.992, 81263.773, 81209.617, 80153.266, 80258.641, 79609.172, 79548.398, 78352.828, 77565.727, 77709.383, 77667.609, 76589.453, 76890.641, 76093.375, 75802.961, 75775.961, 75271.789, 74504.305, 74444.016, 75073.383, 73829.414, 73823.414, 74356.625, 73226.016, 72880.516, 72536.203, 73159.5, 72211.07, 72742.906, 72110.586, 71928.977, 71402.164, 72213.844, 70821.578, 71693.945, 70644.391, 71024.727, 70846.336, 70268.641, 70169.375, 70421.391, 70116.562, 70663.812, 70152.414, 69482.883, 70044.445, 69470.742, 68811.039, 69234.211, 69353.484, 69513.734, 68520.625, 68734.75, 68818.359, 68286.258, 68374.617, 68028.875, 67806.539, 67399.531, 68433.242, 67557.156, 67851.32, 67773.93, 67410.969, 67450.164, 67707.734, 66775.773, 67349.484, 66771.758, 66893.734, 67044.75, 66441.141, 66538.969, 66652.312, 66218.867], 'val_loss': [4252.358, 2939.809, 2568.866, 2279.739, 1652.625, 1577.847, 1471.234, 1394.581, 1433.094, 1330.517, 1289.893, 1355.991, 1269.602, 1233.259, 1290.99, 1298.57, 1204.142, 1248.243, 1175.267, 1239.338, 1228.518, 1268.702, 1279.989, 1232.452, 1135.598, 1258.883, 1223.571, 1206.012, 1200.487, 1213.954, 1169.081, 1155.552, 1128.031, 1219.041, 1110.487, 1114.095, 1131.843, 1114.527, 1087.929, 1082.753, 1048.757, 1054.071, 1128.284, 1099.57, 1106.71, 1166.36, 1116.061, 1015.606, 1038.138, 1090.652, 1084.344, 1057.043, 1081.564, 1066.229, 977.871, 1141.954, 992.699, 998.97, 1061.847, 986.963, 1017.023, 974.32, 1001.548, 970.945, 1002.501, 984.033, 1054.898, 993.812, 968.198, 1000.663, 984.979, 964.188, 973.2, 1005.901, 1077.318, 966.224, 991.654, 976.625, 963.471, 963.894, 1011.141, 929.877, 981.464, 970.159, 1001.443, 1052.535, 967.482, 1021.503, 958.305, 1000.052, 965.191, 990.192, 1032.792, 982.726, 953.543, 1006.701, 988.77, 966.979, 965.606, 1029.682]}	100	100	True
