id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	32703.56055		452251	14	-1	223.36710906028748	{'train_loss': [420290.75, 368833.625, 327981.688, 315644.469, 309719.562, 305560.469, 301888.5, 298984.875, 296025.938, 290240.562, 283220.125, 277750.469, 274096.688, 271901.094, 268896.812, 267404.75, 266185.188, 263487.062, 261566.547, 259863.812, 258340.531, 257394.203, 256843.781, 255542.938, 254514.516, 254267.484, 253222.953, 252779.062, 252297.141, 250667.906, 249746.062, 249537.141, 250490.016, 248473.797, 249614.203, 248660.594, 248061.078, 246973.281, 247224.953, 246436.797, 245384.484, 246191.031, 245490.312, 244875.266, 244837.531, 244948.984, 244395.266, 243455.844, 243547.906, 243309.75, 243014.859, 242013.281, 241049.625, 241978.172, 241314.234, 241228.344, 240322.375, 239569.344, 240482.297, 239800.625, 239692.953, 239646.75, 239366.844, 239361.297, 238189.578, 237053.938, 238177.266, 237088.609, 236634.688, 236522.906, 235456.25, 236104.641, 235997.875, 236251.297, 234954.375, 234560.266, 234042.375, 233603.281, 233396.484, 233571.719, 232876.688, 232923.562, 232429.781, 231983.0, 232550.609, 232161.047, 231507.156, 231467.219, 231025.828, 230549.797, 229833.359, 230044.016, 230411.906, 229467.766, 229606.359, 229749.109, 229523.172, 229935.594, 229653.812, 228976.844], 'val_loss': [3604.471, 3069.305, 2946.181, 2902.367, 2879.159, 2852.275, 2826.262, 2799.339, 2753.342, 2722.497, 2682.666, 2687.196, 2648.299, 2613.698, 2571.878, 2573.384, 2563.912, 2526.628, 2520.512, 2508.952, 2487.104, 2499.991, 2504.136, 2479.655, 2464.369, 2475.936, 2473.512, 2449.313, 2437.1, 2445.703, 2431.784, 2439.602, 2448.235, 2423.012, 2418.825, 2413.868, 2418.61, 2394.617, 2410.702, 2404.752, 2405.099, 2395.58, 2410.448, 2414.947, 2422.177, 2396.543, 2392.011, 2393.312, 2407.574, 2378.388, 2385.895, 2403.118, 2394.072, 2394.969, 2383.054, 2366.553, 2384.167, 2368.828, 2386.412, 2377.857, 2358.389, 2369.471, 2358.624, 2362.881, 2377.971, 2371.625, 2360.215, 2364.59, 2360.955, 2352.651, 2362.325, 2343.342, 2341.276, 2357.243, 2348.706, 2328.982, 2322.421, 2327.844, 2318.75, 2325.498, 2332.855, 2311.26, 2318.088, 2311.343, 2307.426, 2323.005, 2307.748, 2289.625, 2305.03, 2295.732, 2291.236, 2285.434, 2282.281, 2292.901, 2280.677, 2278.432, 2296.386, 2281.263, 2303.841, 2274.677]}	100	100	True
