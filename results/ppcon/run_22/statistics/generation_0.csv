id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	8001.99463		452251	14	-1	169.40600323677063	{'train_loss': [346266.312, 329980.781, 289249.25, 201821.422, 154617.656, 120601.922, 111491.383, 106803.219, 104463.562, 101635.906, 99795.07, 97254.062, 96285.352, 94604.703, 92314.805, 91941.273, 91027.547, 89960.359, 89300.242, 87909.867, 87406.875, 86899.594, 85813.117, 85475.516, 84797.891, 83964.469, 83082.234, 83319.578, 83274.375, 82679.43, 82077.281, 81676.5, 80913.117, 81311.805, 80692.438, 79836.492, 80665.898, 80029.172, 79929.633, 79269.289, 79308.281, 78601.219, 78370.664, 78848.891, 78363.531, 77780.078, 77206.57, 77403.633, 77711.109, 76164.898, 77117.461, 76471.234, 77098.055, 76021.594, 76147.664, 76375.531, 76186.672, 75886.461, 75300.727, 75577.758, 75614.43, 75601.328, 74632.328, 75284.148, 74875.867, 74251.367, 74475.648, 74482.672, 74446.266, 74141.961, 73958.938, 73355.547, 73574.734, 73040.164, 73814.148, 73097.219, 72980.68, 72625.242, 72600.875, 72982.367, 72452.875, 72979.078, 72208.695, 71994.633, 71925.047, 72211.859, 72005.695, 71974.43, 71287.766, 71394.391, 71496.5, 71249.82, 70945.5, 71658.18, 71201.703, 70564.289, 71365.422, 71034.805, 70782.75, 70243.078], 'val_loss': [5303.265, 5183.055, 3407.157, 2773.796, 1705.096, 1538.825, 1504.211, 1396.066, 1419.606, 1353.7, 1381.44, 1311.902, 1312.528, 1409.189, 1278.75, 1303.737, 1327.375, 1310.497, 1241.682, 1332.459, 1279.684, 1259.493, 1228.908, 1196.435, 1246.411, 1172.628, 1159.023, 1270.962, 1178.122, 1202.178, 1186.787, 1213.863, 1131.169, 1136.588, 1158.207, 1119.413, 1116.503, 1123.462, 1137.883, 1108.774, 1137.187, 1110.724, 1107.201, 1075.802, 1137.258, 1092.928, 1122.096, 1129.634, 1104.886, 1084.271, 1105.146, 1147.763, 1148.572, 1110.771, 1080.335, 1126.364, 1068.834, 1075.413, 1178.36, 1090.245, 1062.861, 1041.211, 1085.856, 1055.75, 1044.097, 1034.736, 1079.622, 1026.841, 1025.917, 1025.82, 1039.944, 989.347, 1015.727, 999.7, 1039.793, 1047.663, 1043.942, 1045.01, 1007.635, 1031.288, 986.907, 1026.656, 1024.433, 1018.999, 1004.737, 1015.205, 1050.01, 1004.166, 1017.553, 992.17, 1009.104, 1010.223, 999.436, 973.599, 1002.655, 1009.064, 984.816, 1001.618, 991.427, 983.329]}	100	100	True
