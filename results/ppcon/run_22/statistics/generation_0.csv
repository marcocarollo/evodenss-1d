id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4258.50635		452251	14	-1	184.89208722114563	{'train_loss': [388020.594, 242339.078, 149111.938, 136836.312, 129443.203, 124543.789, 119804.531, 117538.875, 114127.797, 111816.102, 109984.852, 108667.578, 106641.148, 105671.578, 104423.906, 103234.734, 102119.07, 101609.172, 100810.695, 99845.539, 99769.391, 98435.773, 97440.141, 96887.539, 96326.984, 96676.203, 95019.25, 94005.977, 93884.477, 93566.156, 92779.258, 92830.344, 92558.453, 91769.266, 91797.492, 90684.398, 90278.312, 89999.898, 89970.961, 89461.141, 88819.766, 88394.906, 87861.438, 87233.703, 86715.352, 86866.297, 86651.891, 86603.922, 86114.859, 85834.641, 84698.031, 85602.961, 84400.102, 84846.766, 84522.508, 84195.734, 83724.281, 83751.828, 82880.344, 83238.938, 83262.82, 82945.398, 82907.32, 82171.492, 81129.633, 82136.664, 81850.469, 80550.188, 81311.422, 81216.562, 81151.734, 80934.031, 80335.43, 79890.812, 81140.453, 79941.875, 80720.93, 79924.859, 80397.453, 79553.188, 79727.344, 78154.5, 79531.812, 78818.648, 78998.055, 79076.172, 78251.234, 78403.266, 77615.859, 77960.688, 78246.414, 77802.016, 77609.844, 77085.0, 77456.805, 76760.008, 77711.641, 77201.68, 77952.906, 77026.562], 'val_loss': [4420.485, 1744.968, 1736.685, 1728.527, 1640.572, 1711.536, 1646.802, 1618.374, 1654.797, 1573.272, 1596.244, 1575.658, 1501.792, 1492.397, 1618.131, 1433.125, 1526.561, 1478.508, 1457.347, 1561.983, 1387.554, 1366.303, 1407.963, 1349.645, 1356.899, 1323.391, 1388.157, 1307.336, 1291.099, 1295.968, 1276.885, 1301.729, 1295.993, 1326.53, 1305.868, 1286.486, 1325.294, 1218.409, 1193.61, 1246.952, 1269.887, 1255.052, 1239.524, 1290.446, 1290.144, 1170.334, 1233.785, 1230.264, 1219.794, 1199.77, 1182.144, 1228.001, 1208.335, 1176.846, 1282.881, 1184.445, 1169.556, 1115.696, 1270.177, 1159.376, 1173.468, 1229.307, 1102.104, 1138.157, 1181.256, 1204.307, 1162.242, 1150.295, 1107.248, 1081.234, 1098.237, 1153.86, 1087.899, 1131.0, 1105.961, 1087.702, 1113.723, 1119.119, 1154.023, 1156.647, 1083.379, 1087.079, 1117.281, 1133.335, 1156.695, 1119.773, 1083.13, 1114.554, 1055.964, 1072.828, 1036.64, 1078.093, 1057.587, 1024.588, 1094.084, 1074.405, 1112.274, 1107.074, 1105.364, 1082.515]}	100	100	True
