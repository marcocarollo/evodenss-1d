id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	4374.1416		452251	14	-1	185.8775749206543	{'train_loss': [388937.875, 262430.0, 157842.328, 138974.516, 130951.023, 125071.562, 120410.781, 116702.672, 113373.125, 110822.992, 108236.914, 106579.352, 104714.773, 102609.008, 102264.031, 99881.969, 99108.922, 97990.641, 97395.789, 96437.141, 94802.258, 94595.672, 93494.75, 92946.812, 92107.883, 91865.484, 91054.836, 90023.203, 90386.867, 89001.055, 89490.258, 87871.891, 88487.953, 87283.023, 86367.961, 87054.352, 86479.484, 85551.812, 85614.57, 85755.648, 84340.75, 83744.344, 83833.727, 83403.406, 83322.805, 82848.008, 82269.383, 82615.891, 81856.258, 81778.523, 81490.898, 81456.078, 80904.039, 81420.102, 80393.594, 80482.234, 80684.062, 80941.484, 79320.523, 79185.016, 79893.594, 79536.992, 79624.75, 79215.938, 78837.828, 78520.445, 77899.688, 77563.625, 78123.031, 77911.516, 77497.828, 78461.906, 76534.078, 77140.906, 77362.094, 76083.828, 76363.086, 76501.344, 76955.773, 75955.625, 75471.75, 76376.125, 75283.195, 76400.648, 75688.445, 75507.406, 74764.188, 75531.852, 75276.172, 75184.055, 74704.312, 74479.062, 75120.156, 74056.969, 74075.078, 75144.273, 74531.008, 73918.844, 73346.539, 73716.359], 'val_loss': [4969.131, 2180.784, 1737.135, 1617.571, 1597.639, 1615.058, 1613.645, 1527.94, 1536.531, 1550.952, 1490.739, 1407.693, 1503.727, 1443.463, 1313.501, 1316.201, 1304.988, 1265.063, 1390.876, 1320.967, 1391.706, 1225.195, 1180.935, 1218.117, 1238.515, 1230.059, 1204.898, 1137.785, 1177.98, 1169.877, 1128.111, 1099.949, 1164.421, 1166.052, 1105.022, 1155.255, 1129.322, 1118.203, 1090.794, 1123.477, 1205.98, 1069.694, 1131.745, 1077.516, 1119.671, 1080.785, 1096.055, 1067.523, 1123.108, 1058.446, 1057.72, 1042.643, 1061.892, 1084.261, 1041.218, 1076.729, 1095.148, 1162.524, 1059.93, 1034.018, 1022.522, 1076.432, 1012.553, 1059.508, 998.068, 1067.0, 1056.436, 1072.6, 1061.123, 1055.931, 1024.461, 1043.634, 997.01, 1131.956, 1026.422, 978.975, 1027.09, 1053.029, 1026.571, 1061.353, 1022.724, 1040.561, 983.316, 991.498, 984.948, 986.64, 1080.49, 993.973, 992.603, 1001.896, 1031.177, 1009.404, 1001.374, 969.344, 1043.485, 1018.013, 974.771, 1012.052, 988.695, 1059.37]}	100	100	True
