id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:64 kernel_size:2 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:deconv1d out_channels:64 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:10 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:11 layer:fc act:selu out_features:200 bias:True input:12 learning:adadelta batch_size:32 epochs:100	100	1000	True	8072.771		452251	14	-1	169.68092560768127	{'train_loss': [346680.5, 299060.031, 153955.234, 135676.531, 126881.742, 121859.484, 117582.992, 114174.398, 111113.727, 108145.055, 106084.977, 103533.352, 102005.969, 100598.75, 99128.688, 98541.891, 97257.258, 95947.516, 95364.945, 94758.172, 93824.219, 93613.594, 92761.203, 91837.094, 90976.211, 90381.078, 90034.898, 89350.031, 88588.102, 89196.844, 88036.578, 87604.047, 86722.953, 87076.109, 86676.227, 86289.672, 85340.891, 84970.93, 85036.695, 84456.172, 83857.07, 82273.086, 82656.367, 82374.703, 81580.555, 81763.531, 81721.875, 80588.258, 80021.445, 80648.203, 80003.891, 79127.008, 79790.891, 79141.375, 78893.203, 78016.461, 77463.719, 77440.195, 77540.562, 77621.555, 76813.195, 76361.0, 76147.297, 76893.219, 75804.359, 75674.0, 76070.625, 75204.234, 74964.773, 74459.617, 74853.68, 74501.672, 74881.086, 73382.727, 73860.438, 74271.883, 73397.383, 73386.938, 72820.516, 72898.672, 71834.023, 73426.328, 73251.578, 73048.797, 72259.711, 72350.57, 72057.859, 71523.039, 71628.219, 72179.336, 71579.398, 71668.695, 71320.625, 70924.219, 70019.008, 70880.289, 70600.461, 70984.688, 70341.984, 69171.211], 'val_loss': [5229.61, 2377.977, 1753.71, 1530.653, 1614.295, 1432.865, 1446.29, 1389.331, 1423.455, 1350.702, 1391.005, 1385.39, 1395.148, 1344.621, 1423.2, 1490.74, 1272.837, 1284.302, 1445.77, 1320.749, 1396.504, 1327.02, 1450.801, 1409.638, 1343.9, 1276.295, 1362.394, 1256.299, 1357.408, 1314.531, 1224.973, 1414.961, 1209.178, 1291.963, 1355.997, 1489.962, 1360.799, 1203.088, 1264.755, 1233.414, 1194.178, 1184.797, 1143.059, 1266.452, 1221.531, 1224.318, 1294.97, 1405.752, 1133.838, 1168.927, 1175.011, 1111.818, 1087.909, 1169.401, 1196.301, 1136.116, 1289.421, 1244.346, 1314.658, 1117.651, 1110.062, 1204.354, 1150.308, 1133.154, 1147.451, 1137.913, 1109.198, 1190.805, 1154.671, 1087.922, 1135.138, 1096.768, 1082.311, 1027.734, 1063.657, 1068.232, 1101.884, 1099.913, 1145.222, 1083.683, 1062.594, 1098.939, 995.633, 1099.269, 1090.306, 1013.049, 979.48, 1039.846, 1072.512, 1059.87, 1042.184, 1085.997, 1021.642, 1103.549, 1020.228, 1065.95, 1080.109, 1065.486, 950.147, 1011.135]}	100	100	True
