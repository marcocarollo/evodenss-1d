id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:-1,0,1,2,3 layer:deconv1d out_channels:36 kernel_size:1 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:66 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:conv1d out_channels:9 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:98 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:9 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:10 layer:fc act:selu out_features:200 bias:True input:11 learning:rmsprop lr:0.0004931928948659201 alpha:0.739486597662427 weight_decay:7.08634222081631e-05 batch_size:4 epochs:50	18	200	True	0.12697		407042	13	-1	206.9704658985138	{'train_loss': [1.043, 0.4, 0.203, 0.153, 0.138, 0.131, 0.127, 0.125, 0.123, 0.122, 0.122, 0.12, 0.12, 0.12, 0.12, 0.12, 0.119, 0.12], 'val_loss': [0.604, 0.237, 0.15, 0.116, 0.111, 0.104, 0.104, 0.113, 0.119, 0.101, 0.101, 0.135, 0.123, 0.113, 0.129, 0.108, 0.1, 0.119]}	18	18	False
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:36 kernel_size:1 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:deconv1d out_channels:66 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:9 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:7 layer:deconv1d out_channels:112 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.0004931928948659201 alpha:0.739486597662427 weight_decay:7.08634222081631e-05 batch_size:47 epochs:50	50	200	True	0.2275		559372	12	-1	147.2851448059082	{'train_loss': [7.036, 4.783, 4.258, 3.942, 3.715, 3.544, 3.329, 3.196, 3.034, 2.893, 2.747, 2.592, 2.432, 2.232, 2.017, 1.874, 1.786, 1.679, 1.587, 1.497, 1.382, 1.299, 1.215, 1.118, 1.038, 0.956, 0.883, 0.811, 0.748, 0.701, 0.663, 0.626, 0.595, 0.57, 0.542, 0.526, 0.503, 0.488, 0.469, 0.454, 0.444, 0.427, 0.418, 0.407, 0.397, 0.388, 0.381, 0.371, 0.364, 0.358], 'val_loss': [2.075, 1.666, 1.393, 1.205, 1.177, 1.05, 1.067, 1.029, 0.971, 0.918, 0.919, 0.87, 0.895, 0.828, 0.784, 0.774, 0.724, 0.708, 0.682, 0.634, 0.626, 0.606, 0.567, 0.553, 0.534, 0.508, 0.498, 0.486, 0.478, 0.457, 0.431, 0.407, 0.386, 0.363, 0.347, 0.329, 0.313, 0.304, 0.29, 0.284, 0.267, 0.261, 0.254, 0.243, 0.239, 0.231, 0.227, 0.222, 0.215, 0.216]}	50	50	True
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:12 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:deconv1d out_channels:66 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:conv1d out_channels:9 kernel_size:4 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:deconv1d out_channels:96 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:True input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.0004931928948659201 alpha:0.739486597662427 weight_decay:7.08634222081631e-05 batch_size:49 epochs:50	50	200	True	0.26817		546420	12	-1	144.60508966445923	{'train_loss': [5.962, 4.04, 3.705, 3.457, 3.203, 2.879, 2.517, 2.23, 2.049, 1.917, 1.799, 1.695, 1.598, 1.497, 1.418, 1.328, 1.248, 1.171, 1.103, 1.036, 0.966, 0.907, 0.842, 0.787, 0.727, 0.681, 0.635, 0.594, 0.56, 0.532, 0.51, 0.491, 0.474, 0.458, 0.442, 0.429, 0.417, 0.407, 0.396, 0.388, 0.379, 0.371, 0.363, 0.357, 0.35, 0.343, 0.339, 0.333, 0.328, 0.324], 'val_loss': [3.546, 3.042, 2.947, 2.586, 2.359, 2.027, 1.786, 1.706, 1.579, 1.415, 1.421, 1.29, 1.276, 1.206, 1.066, 1.061, 0.974, 0.919, 0.836, 0.783, 0.772, 0.702, 0.694, 0.617, 0.615, 0.535, 0.508, 0.476, 0.447, 0.435, 0.409, 0.381, 0.382, 0.359, 0.353, 0.335, 0.322, 0.327, 0.315, 0.308, 0.31, 0.302, 0.282, 0.279, 0.276, 0.272, 0.263, 0.265, 0.261, 0.255]}	50	50	True
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:36 kernel_size:1 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:2 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:5 layer:conv1d out_channels:69 kernel_size:2 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:6 layer:deconv1d out_channels:126 kernel_size:3 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:74 kernel_size:3 stride:1 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:rmsprop lr:0.0004931928948659201 alpha:0.739486597662427 weight_decay:7.08634222081631e-05 batch_size:37 epochs:50	50	200	True	0.24284		601078	12	-1	156.50518608093262	{'train_loss': [5.334, 4.052, 3.73, 3.493, 3.29, 3.085, 2.844, 2.58, 2.371, 2.195, 2.013, 1.791, 1.629, 1.518, 1.419, 1.313, 1.218, 1.123, 1.035, 0.947, 0.867, 0.795, 0.735, 0.685, 0.642, 0.604, 0.571, 0.541, 0.515, 0.491, 0.468, 0.45, 0.432, 0.417, 0.402, 0.388, 0.376, 0.364, 0.354, 0.345, 0.336, 0.328, 0.321, 0.315, 0.308, 0.302, 0.296, 0.291, 0.286, 0.281], 'val_loss': [4.31, 3.35, 3.01, 2.782, 2.689, 2.532, 2.193, 2.019, 1.814, 1.8, 1.656, 1.449, 1.314, 1.358, 1.171, 1.108, 0.996, 0.954, 0.854, 0.812, 0.74, 0.709, 0.642, 0.613, 0.564, 0.524, 0.497, 0.468, 0.447, 0.43, 0.413, 0.399, 0.376, 0.367, 0.348, 0.348, 0.332, 0.32, 0.304, 0.303, 0.292, 0.287, 0.281, 0.277, 0.262, 0.251, 0.25, 0.254, 0.247, 0.246]}	50	50	True
