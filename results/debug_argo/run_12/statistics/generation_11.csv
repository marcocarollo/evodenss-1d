id	phenotype	num_epochs	total_training_time_allocated	is_valid_solution	fitness	accuracy	n_trainable_parameters	n_layers	n_layers_projector	training_time_spent	losses	n_epochs	total_epochs_trained	max_epochs_reached
0	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:128 kernel_size:4 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:-1,0,1,2,3 layer:deconv1d out_channels:122 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adam lr:0.026341539887969535 beta1:0.9732440384721085 beta2:0.9470697755428685 weight_decay:0.00016298549688121864 batch_size:4 epochs:50	46	500	True	0.36297		522105	12	-1	507.29249262809753	{'train_loss': [3.448, 0.275, 0.302, 0.302, 0.304, 0.303, 0.304, 0.301, 0.311, 0.308, 0.306, 0.307, 0.307, 0.285, 0.297, 0.315, 0.316, 0.308, 0.307, 0.312, 0.313, 0.312, 0.317, 0.315, 0.317, 0.322, 0.318, 0.319, 0.315, 0.314, 0.309, 0.337, 0.329, 0.334, 0.323, 0.332, 0.347, 0.335, 0.328, 0.342, 0.338, 0.334, 0.331, 0.33, 0.332, 0.351], 'val_loss': [0.264, 0.268, 0.3, 0.327, 0.329, 0.277, 0.383, 0.338, 0.31, 0.306, 0.303, 0.336, 0.327, 0.276, 0.277, 0.31, 0.378, 0.298, 0.317, 0.335, 0.285, 0.372, 0.295, 0.505, 0.326, 0.289, 0.321, 0.303, 0.337, 0.372, 0.303, 0.298, 0.361, 0.545, 0.399, 0.329, 0.383, 0.512, 0.328, 0.354, 0.318, 1.53, 0.411, 0.328, 0.534, 0.353]}	46	46	False
1	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:conv1d out_channels:80 kernel_size:2 stride:1 padding_deconv:0 internal_dropout_p:0.2 act:selu internal_batch_norm:False bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:122 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:5 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:6 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:7 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:8 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:9 layer:fc act:selu out_features:200 bias:True input:10 learning:adam lr:0.026341539887969535 beta1:0.9732440384721085 beta2:0.9470697755428685 weight_decay:0.00016298549688121864 batch_size:4 epochs:50	47	500	True	0.35887		490217	12	-1	511.9024405479431	{'train_loss': [2.973, 0.275, 0.286, 0.286, 0.292, 0.288, 0.29, 0.298, 0.287, 0.285, 0.286, 0.285, 0.283, 0.287, 0.287, 0.289, 0.288, 0.291, 0.29, 0.284, 0.286, 0.322, 0.287, 0.285, 0.299, 0.292, 0.298, 0.298, 0.324, 0.303, 0.315, 0.323, 0.339, 0.335, 0.343, 0.355, 0.323, 0.354, 0.351, 0.345, 0.35, 0.339, 0.336, 0.334, 0.338, 0.339, 0.34], 'val_loss': [0.277, 0.299, 0.296, 0.261, 0.321, 0.269, 0.283, 0.292, 0.258, 0.266, 0.271, 0.289, 0.274, 0.259, 0.284, 0.306, 0.269, 0.275, 0.285, 0.272, 0.385, 0.3, 0.31, 0.309, 0.342, 0.359, 0.329, 0.34, 0.3, 0.29, 0.296, 0.372, 0.376, 0.323, 0.419, 0.322, 0.318, 0.404, 0.375, 0.66, 0.328, 0.36, 0.378, 0.341, 0.364, 0.351, 0.353]}	47	47	False
2	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:122 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:False input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.026341539887969535 beta1:0.5512428369057014 beta2:0.9470697755428685 weight_decay:0.00016298549688121864 batch_size:4 epochs:50	49	500	True	0.23593		445127	11	-1	509.32725644111633	{'train_loss': [2.064, 0.417, 0.453, 0.458, 0.409, 0.427, 0.443, 0.336, 0.4, 0.375, 0.415, 0.472, 0.396, 0.474, 0.448, 0.439, 0.503, 0.421, 0.449, 0.359, 0.425, 0.417, 0.365, 0.353, 0.499, 0.427, 0.355, 0.423, 0.443, 0.412, 0.355, 0.42, 0.447, 0.366, 0.363, 0.472, 0.461, 0.455, 0.439, 0.457, 0.392, 0.388, 0.398, 0.372, 0.417, 0.405, 0.456, 0.464, 0.397], 'val_loss': [0.214, 0.385, 0.407, 0.739, 1.401, 0.172, 0.37, 0.167, 2.93, 0.184, 0.186, 0.747, 0.185, 0.188, 0.569, 0.388, 3.292, 0.572, 0.173, 0.204, 0.176, 0.166, 0.184, 0.304, 0.853, 2.261, 71.428, 0.241, 0.181, 7.713, 0.188, 0.197, 0.57, 0.175, 30.397, 0.301, 0.537, 0.191, 0.185, 0.914, 139.857, 0.176, 0.305, 1.528, 0.305, 0.171, 0.197, 0.176, 0.225]}	49	49	False
3	layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:punctual_mlp input:-1 layer:deconv1d out_channels:122 kernel_size:5 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:-1,0,1,2,3 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:2 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:4 layer:conv1d out_channels:128 kernel_size:3 stride:1 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:5 layer:deconv1d out_channels:128 kernel_size:2 stride:2 padding_deconv:1 internal_dropout_p:0.2 act:selu internal_batch_norm:True bias:True input:6 layer:conv1d out_channels:32 kernel_size:2 stride:2 padding_deconv:1 bias:True act:selu internal_dropout_p:0.2 internal_batch_norm:True input:7 layer:conv1d out_channels:1 kernel_size:3 stride:1 padding_deconv:1 bias:True act:linear internal_batch_norm:False input:8 layer:fc act:selu out_features:200 bias:True input:9 learning:adam lr:0.026341539887969535 beta1:0.9732440384721085 beta2:0.9470697755428685 weight_decay:0.00016298549688121864 batch_size:4 epochs:50	49	500	True	0.3434		445127	11	-1	509.8798575401306	{'train_loss': [5.176, 0.275, 0.287, 0.269, 0.286, 0.283, 0.288, 0.304, 0.283, 0.286, 0.287, 0.282, 0.293, 0.285, 0.287, 0.288, 0.282, 0.291, 0.283, 0.285, 0.292, 0.281, 0.3, 0.3, 0.296, 0.301, 0.305, 0.288, 0.294, 0.3, 0.296, 0.288, 0.311, 0.306, 0.299, 0.297, 0.299, 0.291, 0.295, 0.308, 0.309, 0.298, 0.298, 0.297, 0.302, 0.298, 0.304, 0.303, 0.294], 'val_loss': [0.289, 0.773, 0.255, 0.291, 0.279, 0.267, 0.351, 0.306, 0.305, 0.358, 0.286, 0.272, 0.261, 0.263, 0.314, 0.26, 0.279, 0.265, 0.45, 0.366, 0.291, 0.308, 0.309, 0.276, 0.293, 0.313, 0.293, 0.319, 0.287, 0.285, 0.29, 0.303, 0.371, 0.271, 0.301, 0.3, 0.288, 0.278, 0.338, 0.315, 0.318, 0.3, 0.27, 0.398, 0.345, 0.275, 0.32, 0.272, 0.339]}	49	49	False
