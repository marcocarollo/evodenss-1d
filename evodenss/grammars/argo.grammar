<features> ::= <convolution1d> 
<convolution1d> ::= layer:conv1d [out_channels,int,1,4,200] [kernel_size,int,1,2,5] [stride,int,1,1,2] <padding_deconv> <dropout> <activation_function> <batch_norm> <bias>
<deconvolution1d> ::= layer:deconv1d [out_channels,int,1,200,200] [kernel_size,int,1,2,5] [stride,int,1,1,2] <padding_deconv> <activation_function> <bias>
<batch_norm> ::= layer:batch_norm
<batch_norm1d> ::= batch_norm_layer:batch_norm1d
<pooling> ::= <pool_type> [kernel_size,int,1,2,5] [stride,int,1,1,3] <padding>
<pool_type> ::= layer:pool_avg | layer:pool_max
<padding> ::= padding:same | padding:valid
<padding_deconv> ::= [padding_deconv,int,1,0,2]
<dropout> ::= dropout_layer:dropout [rate,float,1,0.2,0.2]
<classification> ::= <fully_connected> | <dropout>
<fully_connected> ::= layer:fc <activation_function> [out_features,int,1,200,200] <bias>
<activation_function> ::= act:selu
<bias> ::= bias:True
<softmax> ::= layer:fc act:softmax out_features:200 bias:True
<learning> ::= <lars> [batch_size,int,1,32,1024] epochs:100 | <gradient_descent> [batch_size,int,1,32,1024] epochs:100 | <rmsprop> [batch_size,int,1,32,1024] epochs:100 | <adam> [batch_size,int,1,32,1024] epochs:100
<gradient_descent> ::= learning:gradient_descent [lr,float,1,0.0001,0.1] [momentum,float,1,0.68,0.99] [weight_decay,float,1,0.000001,0.001] <nesterov>
<nesterov> ::= nesterov:True | nesterov:False
<adam> ::= learning:adam [lr,float,1,0.0001,0.1] [beta1,float,1,0.5,0.9999] [beta2,float,1,0.5,0.9999] [weight_decay,float,1,0.000001,0.001]
<rmsprop> ::= learning:rmsprop [lr,float,1,0.0001,0.1] [alpha,float,1,0.5,1] [weight_decay,float,1,0.000001,0.001]
<lars> ::= learning:lars [lr_weights,float,1,0.05,0.35] [lr_biases,float,1,0.001,0.01] [momentum,float,1,0.7,0.9] [weight_decay,float,1,0.0000001,0.00001]