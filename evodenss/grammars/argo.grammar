<features> ::= <deconvolution1d> | <convolution1d>
<convolution1d> ::= layer:conv1d <out_channels> [kernel_size,int,1,2,10] [stride,int,1,1,2] <padding_deconv> [internal_dropout_p,float,1,0.2,0.2] <activation_function> <internal_batch_norm> <bias>
<out_channels> ::= [out_channels,int,1,3,128] | [out_channels,int,1,32,128] |[out_channels,int,1,32,96]
<deconvolution1d> ::= layer:deconv1d [out_channels,int,1,3,128] [kernel_size,int,1,2,10] <stride> <padding_deconv> [internal_dropout_p,float,1,0.2,0.2] <activation_function> <internal_batch_norm> <bias>
<stride> ::= [stride,int,1,2,2] | [stride,int,1,1,2]
<batch_norm> ::= layer:batch_norm
<internal_batch_norm> ::= internal_batch_norm:True | internal_batch_norm:False
<pooling> ::= <pool_type> [kernel_size,int,1,2,5] [stride,int,1,1,3] <padding>
<pool_type> ::= layer:pool_avg | layer:pool_max
<padding> ::= padding:same | padding:valid
<padding_deconv> ::= [padding_deconv,int,1,0,3]
<dropout> ::= layer:dropout [rate,float,1,0.2,0.2]
<classification> ::= <fully_connected> 
<fully_connected> ::= layer:fc <activation_function> [out_features,int,1,200,200] <bias>
<activation_function> ::= act:selu
<bias> ::= bias:True
<learning> ::= <gradient_descent> <batch_size> epochs:100 | <rmsprop> <batch_size> epochs:100 | <adam> <batch_size> epochs:100 | <adadelta> <batch_size> epochs:100
<gradient_descent> ::= learning:gradient_descent <lr> [momentum,float,1,0.68,0.99] <weight_decay> <nesterov>
<nesterov> ::= nesterov:True | nesterov:False
<adam> ::= learning:adam <lr> [beta1,float,1,0.8,0.9999] [beta2,float,1,0.8,0.9999] <weight_decay>
<rmsprop> ::= learning:rmsprop <lr> [alpha,float,1,0.8,1] <weight_decay>
<lars> ::= learning:lars [lr_weights,float,1,0.05,0.35] [lr_biases,float,1,0.001,0.01] [momentum,float,1,0.7,0.9] [weight_decay,float,1,0.0000001,0.00001]
<adadelta> ::= learning:adadelta <lr>
<weight_decay> ::= [weight_decay,float,1,0.0001,0.001] | [weight_decay,float,1,0.00001,0.0001] | [weight_decay,float,1,0.000001,0.0001]
<lr> ::= [lr,float,1,0.001,0.01] | [lr,float,1,0.01,0.1] | [lr,float,1,0.001,0.1]
<batch_size> ::= [batch_size,int,1,4,16]
