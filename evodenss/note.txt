fatti uno schemino di tutte le funzioni che sono chiamate, in ordine da main
- batch normalization


bene reverse engineering sul checkpoint.pkl, fallo anche considerando gli init di ogni classe che viene chiamata,
in modo da specificare solo gli attributi che servono. tipo per il fitness dell'individuo può darsi che non serva 
nel caso in cui venga di sicuro sostituito nel prossimo, oppure nel caso che indipendentemente venga evaluated.

guarda quali sono i weights che vengono salvati e come vengono retrieved, perché nell'individual è salvata solo
l'architettura.

aggiungi 1d batch normalization

vedo se quando ho un layer con insieme batch normalization activation e dropout, questo vale come 4 layer o 1

aggiungere sia il dropout e il normalization all'interno del layer, quindi ci satà un non non terminal symbol
in più nelle production rules di un conv1d o trasposedconv1cd
inoltre vanno aggiunti singolarmente come espazioni iniziali dalla prima  regola

fixare la possibilità che ogni volta che viene inizializzato un individuo, viene automaticamente preso come argo
 - mettere una flag 
 - semplicemente crearsi il checkpoint e poi fare tutto a partire da quello

fixare il fatto che non sono sicuro che l'output e la loss funzionino sul serio
fixare il fatto che alla fine c'è un fc layer

da dove arriva la batchsize????? (dalla grammar)

fixare dsge topological (non funziona)
cosa fa dsge non topological? (learning (?))

perché non gliene frega nulla a nessuno di quanto tempo bisogna trainare? perché si utilizzano le max num epochs
dal file config.yaml, inoltre il numero di epoche viene preso dalla grammatica. se hai 10 secondi e il training 
si ferma dopo 70 è perché ha fatto solo 1 epoca, che dura 70 secondi.

problemi nella rimozione, kv sballato => era la argo.grammar che aveva "  "

vedere come aggiungere mlp all'inizio o dopo per vedere cosa cambia. si potrebbe provare a buttare dei layer fc
che però sono connessi solo ad un certo punto, quindi anche se vengono rappresentati linearmente comunque è come se 
arrivassero solo dopo un tot