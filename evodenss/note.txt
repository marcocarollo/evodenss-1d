fatti uno schemino di tutte le funzioni che sono chiamate, in ordine da main
- batch normalization


bene reverse engineering sul checkpoint.pkl, fallo anche considerando gli init di ogni classe che viene chiamata,
in modo da specificare solo gli attributi che servono. tipo per il fitness dell'individuo può darsi che non serva 
nel caso in cui venga di sicuro sostituito nel prossimo, oppure nel caso che indipendentemente venga evaluated.

guarda quali sono i weights che vengono salvati e come vengono retrieved, perché nell'individual è salvata solo
l'architettura.

aggiungi 1d batch normalization

vedo se quando ho un layer con insieme batch normalization activation e dropout, questo vale come 4 layer o 1

aggiungere sia il dropout e il normalization all'interno del layer, quindi ci satà un non non terminal symbol
in più nelle production rules di un conv1d o trasposedconv1cd
inoltre vanno aggiunti singolarmente come espazioni iniziali dalla prima  regola

fixare la possibilità che ogni volta che viene inizializzato un individuo, viene automaticamente preso come argo
 - mettere una flag 
 - semplicemente crearsi il checkpoint e poi fare tutto a partire da quello

fixare il fatto che non sono sicuro che l'output e la loss funzionino sul serio
fixare il fatto che alla fine c'è un fc layer

da dove arriva la batchsize????? (dalla grammar)

fixare dsge topological (non funziona)
cosa fa dsge non topological? (learning (?))

perché non gliene frega nulla a nessuno di quanto tempo bisogna trainare? perché si utilizzano le max num epochs
dal file config.yaml, inoltre il numero di epoche viene preso dalla grammatica. se hai 10 secondi e il training 
si ferma dopo 70 è perché ha fatto solo 1 epoca, che dura 70 secondi.

problemi nella rimozione, kv sballato => era la argo.grammar che aveva "  "

vedere come aggiungere mlp all'inizio o dopo per vedere cosa cambia. si potrebbe provare a buttare dei layer fc
che però sono connessi solo ad un certo punto, quindi anche se vengono rappresentati linearmente comunque è come
se arrivassero solo dopo un tot sì comunque tu hai 4 mlp che buttano fuori 4 vettori da 200 punti. ciascuno di 
questi diventa un canale in pasto al layer successivo. vedere se multiple connections sono trattate ocme diversi
canali o come combinazione lineare di diversi output prima di entrare nel layer successivo.

pulire le grammatiche che servono per inizializzare argo

aggiungere la possibilità di cambiare connection:
 - inserire un nuovo tipo di mutazione che cambia le connections ma solo se il layer è di tipo punctual
   e in quel caso cambia le connections di tutti i layer punctual
 - magari aggiungere un nuovo tipo di id per identificare questi layer
 - servirebbe una cosa che separatamente tratta questi pezzi e li attacca al modellone, ma comunque li riesce
   a integrare
 - in teoria dovrebbe funzionare lo stesso il codice anche aggiungendo a caso questi modelli e connections,
   perché aumenta semplicemente il numero di canali e quello viene dato in pasto al model builder